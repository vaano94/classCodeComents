ITUseMiniCluster (/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseMiniCluster.java)/**
 * Ensure that we can perform operations against the shaded minicluster
 * given the API and runtime jars by performing some simple smoke tests.
 */
COSCredentialProviderList (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/auth/COSCredentialProviderList.java)/**
 * a list of cos credentials provider.
 */
EnvironmentVariableCredentialProvider (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/auth/EnvironmentVariableCredentialProvider.java)/**
 * the provider obtaining the cos credentials from the environment variables.
 */
NoAuthWithCOSException (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/auth/NoAuthWithCOSException.java)/**
 * Exception thrown when no credentials can be obtained.
 */
SimpleCredentialProvider (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/auth/SimpleCredentialProvider.java)/**
 * Get the credentials from the hadoop configuration.
 */
BufferPool (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/BufferPool.java)/**
 * BufferPool class is used to manage the buffers during program execution.
 * It is provided in a thread-safe singleton mode,and
 * keeps the program's memory and disk consumption at a stable value.
 */
ByteBufferInputStream (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/ByteBufferInputStream.java)/**
 * The input stream class is used for buffered files.
 * The purpose of providing this class is to optimize buffer read performance.
 */
ByteBufferOutputStream (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/ByteBufferOutputStream.java)/**
 * The input stream class is used for buffered files.
 * The purpose of providing this class is to optimize buffer write performance.
 */
ByteBufferWrapper (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/ByteBufferWrapper.java)/**
 * The wrapper for memory buffers and disk buffers.
 */
Constants (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/Constants.java)/**
 * constant definition.
 */
CosN (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/CosN.java)/**
 * CosN implementation for the Hadoop's AbstractFileSystem.
 * This implementation delegates to the CosNFileSystem {@link CosNFileSystem}.
 */
CosNativeFileSystemStore (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/CosNativeFileSystemStore.java)/**
 * The class actually performs access operation to the COS blob store.
 * It provides the bridging logic for the Hadoop's abstract filesystem and COS.
 */
CosNConfigKeys (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/CosNConfigKeys.java)/**
 * This class contains constants for configuration keys used in COS.
 */
CosNCopyFileContext (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/CosNCopyFileContext.java)/**
 * The context of the copy task, including concurrency control,
 * asynchronous acquisition of copy results and etc.
 */
CosNCopyFileTask (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/CosNCopyFileTask.java)/**
 * Used by {@link CosNFileSystem} as an task that submitted
 * to the thread pool to accelerate the copy progress.
 * Each task is responsible for copying the source key to the destination.
 */
CosNFileReadTask (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/CosNFileReadTask.java)/**
 * Used by {@link CosNInputStream} as an asynchronous task
 * submitted to the thread pool.
 * Each task is responsible for reading a part of a large file.
 * It is used to pre-read the data from COS to accelerate file reading process.
 */
CosNFileSystem (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/CosNFileSystem.java)/**
 * The core CosN Filesystem implementation.
 */
ReadBuffer (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/CosNInputStream.java)/**
   * This class is used by {@link CosNInputStream}
   * and {@link CosNFileReadTask} to buffer data that read from COS blob store.
   */
CosNInputStream (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/CosNInputStream.java)/**
 * The input stream for the COS blob store.
 * Optimized sequential read flow based on a forward read-ahead queue
 */
CosNOutputStream (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/CosNOutputStream.java)/**
 * The output stream for the COS blob store.
 * Implement streaming upload to COS based on the multipart upload function.
 * ( the maximum size of each part is 5GB)
 * Support up to 40TB single file by multipart upload (each part is 5GB).
 * Improve the upload performance of writing large files by using byte buffers
 * and a fixed thread pool.
 */
CosNUtils (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/CosNUtils.java)/**
 * Utility methods for CosN code.
 */
FileMetadata (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/FileMetadata.java)/**
 * <p>
 * Holds basic metadata for a file stored in a {@link NativeFileSystemStore}.
 * </p>
 */
NativeFileSystemStore (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/NativeFileSystemStore.java)/**
 * <p>
 * An abstraction for a key-based {@link File} store.
 * </p>
 */
PartialListing (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/PartialListing.java)/**
 * <p>
 * Holds information on a directory listing for a
 * {@link NativeFileSystemStore}.
 * This includes the {@link FileMetadata files} and directories
 * (their names) contained in a directory.
 * </p>
 * <p>
 * This listing may be returned in chunks, so a <code>priorLastKey</code>
 * is provided so that the next chunk may be requested.
 * </p>
 *
 * @see NativeFileSystemStore#list(String, int, String, boolean)
 */
Unit (/hadoop-cloud-storage-project/hadoop-cos/src/main/java/org/apache/hadoop/fs/cosn/Unit.java)/**
 * Constant definition of storage unit.
 */
CosNContract (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/contract/CosNContract.java)/**
 * The contract of CosN: only enabled if the test bucket is provided.
 */
TestCosNContractCreate (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/contract/TestCosNContractCreate.java)/**
 * CosN contract tests for creating files.
 */
TestCosNContractDelete (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/contract/TestCosNContractDelete.java)/**
 * CosN contract tests for deleting files.
 */
TestCosNContractDistCp (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/contract/TestCosNContractDistCp.java)/**
 * Contract test suit covering CosN integration with DistCp.
 */
TestCosNContractGetFileStatus (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/contract/TestCosNContractGetFileStatus.java)/**
 * CosN contract tests covering getFileStatus.
 */
TestCosNContractMkdir (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/contract/TestCosNContractMkdir.java)/**
 * CosN contract tests for making directories.
 */
TestCosNContractOpen (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/contract/TestCosNContractOpen.java)/**
 * CosN contract tests for opening files.
 */
TestCosNContractRename (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/contract/TestCosNContractRename.java)/**
 * CosN contract tests for renaming a file.
 */
TestCosNContractRootDir (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/contract/TestCosNContractRootDir.java)/**
 * root dir operations against an COS bucket.
 */
TestCosNContractSeek (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/contract/TestCosNContractSeek.java)/**
 * CosN contract tests for seeking a position in a file.
 */
CosNTestConfigKey (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/CosNTestConfigKey.java)/**
 * Configuration options for the CosN file system for testing.
 */
CosNTestUtils (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/CosNTestUtils.java)/**
 * Utilities for the CosN tests.
 */
TestCosNInputStream (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/TestCosNInputStream.java)/**
 * CosNInputStream Tester.
 */
TestCosNOutputStream (/hadoop-cloud-storage-project/hadoop-cos/src/test/java/org/apache/hadoop/fs/cosn/TestCosNOutputStream.java)/**
 * CosNOutputStream Tester.
 * <p>
 * If the test.fs.cosn.name property is not set, all test case will fail.
 */
InterfaceAudience (/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/InterfaceAudience.java)/**
 * Annotation to inform users of a package, class or method's intended audience.
 * Currently the audience can be {@link Public}, {@link LimitedPrivate} or
 * {@link Private}. <br>
 * All public classes must have InterfaceAudience annotation. <br>
 * <ul>
 * <li>Public classes that are not marked with this annotation must be
 * considered by default as {@link Private}.</li> 
 * 
 * <li>External applications must only use classes that are marked
 * {@link Public}. Avoid using non public classes as these classes
 * could be removed or change in incompatible ways.</li>
 * 
 * <li>Hadoop projects must only use classes that are marked
 * {@link LimitedPrivate} or {@link Public}</li>
 * 
 * <li> Methods may have a different annotation that it is more restrictive
 * compared to the audience classification of the class. Example: A class 
 * might be {@link Public}, but a method may be {@link LimitedPrivate}
 * </li></ul>
 */
InterfaceStability (/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/InterfaceStability.java)/**
 * Annotation to inform users of how much to rely on a particular package,
 * class or method not changing over time. Currently the stability can be
 * {@link Stable}, {@link Evolving} or {@link Unstable}. <br>
 * 
 * <ul><li>All classes that are annotated with {@link Public} or
 * {@link LimitedPrivate} must have InterfaceStability annotation. </li>
 * <li>Classes that are {@link Private} are to be considered unstable unless
 * a different InterfaceStability annotation states otherwise.</li>
 * <li>Incompatible changes must not be made to classes marked as stable.</li>
 * </ul>
 */
ExcludePrivateAnnotationsJDiffDoclet (/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/ExcludePrivateAnnotationsJDiffDoclet.java)/**
 * A <a href="http://java.sun.com/javase/6/docs/jdk/api/javadoc/doclet/">Doclet</a>
 * for excluding elements that are annotated with
 * {@link org.apache.hadoop.classification.InterfaceAudience.Private} or
 * {@link org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate}.
 * It delegates to the JDiff Doclet, and takes the same options.
 */
ExcludePrivateAnnotationsStandardDoclet (/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/ExcludePrivateAnnotationsStandardDoclet.java)/**
 * A <a href="http://java.sun.com/javase/6/docs/jdk/api/javadoc/doclet/">Doclet</a>
 * for excluding elements that are annotated with
 * {@link org.apache.hadoop.classification.InterfaceAudience.Private} or
 * {@link org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate}.
 * It delegates to the Standard Doclet, and takes the same options.
 */
IncludePublicAnnotationsJDiffDoclet (/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/IncludePublicAnnotationsJDiffDoclet.java)/**
 * A <a href="http://java.sun.com/javase/6/docs/jdk/api/javadoc/doclet/">Doclet</a>
 * that only includes class-level elements that are annotated with
 * {@link org.apache.hadoop.classification.InterfaceAudience.Public}.
 * Class-level elements with no annotation are excluded.
 * In addition, all elements that are annotated with
 * {@link org.apache.hadoop.classification.InterfaceAudience.Private} or
 * {@link org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate}
 * are also excluded.
 * It delegates to the JDiff Doclet, and takes the same options.
 */
IncludePublicAnnotationsStandardDoclet (/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/IncludePublicAnnotationsStandardDoclet.java)/**
 * A <a href="http://java.sun.com/javase/6/docs/jdk/api/javadoc/doclet/">Doclet</a>
 * that only includes class-level elements that are annotated with
 * {@link org.apache.hadoop.classification.InterfaceAudience.Public}.
 * Class-level elements with no annotation are excluded.
 * In addition, all elements that are annotated with
 * {@link org.apache.hadoop.classification.InterfaceAudience.Private} or
 * {@link org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate}
 * are also excluded.
 * It delegates to the Standard Doclet, and takes the same options.
 */
RootDocProcessor (/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/RootDocProcessor.java)/**
 * Process the {@link RootDoc} by substituting with (nested) proxy objects that
 * exclude elements with Private or LimitedPrivate annotations.
 * <p>
 * Based on code from http://www.sixlegs.com/blog/java/exclude-javadoc-tag.html.
 */
Token (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/AuthenticatedURL.java)/**
   * Client side authentication token.
   */
AuthenticatedURL (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/AuthenticatedURL.java)/**
 * The {@link AuthenticatedURL} class enables the use of the JDK {@link URL} class
 * against HTTP endpoints protected with the {@link AuthenticationFilter}.
 * <p>
 * The authentication mechanisms supported by default are Hadoop Simple  authentication
 * (also known as pseudo authentication) and Kerberos SPNEGO authentication.
 * <p>
 * Additional authentication mechanisms can be supported via {@link Authenticator} implementations.
 * <p>
 * The default {@link Authenticator} is the {@link KerberosAuthenticator} class which supports
 * automatic fallback from Kerberos SPNEGO to Hadoop Simple authentication.
 * <p>
 * <code>AuthenticatedURL</code> instances are not thread-safe.
 * <p>
 * The usage pattern of the {@link AuthenticatedURL} is:
 * <pre>
 *
 * // establishing an initial connection
 *
 * URL url = new URL("http://foo:8080/bar");
 * AuthenticatedURL.Token token = new AuthenticatedURL.Token();
 * AuthenticatedURL aUrl = new AuthenticatedURL();
 * HttpURLConnection conn = new AuthenticatedURL().openConnection(url, token);
 * ....
 * // use the 'conn' instance
 * ....
 *
 * // establishing a follow up connection using a token from the previous connection
 *
 * HttpURLConnection conn = new AuthenticatedURL().openConnection(url, token);
 * ....
 * // use the 'conn' instance
 * ....
 *
 * </pre>
 */
AuthenticationException (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/AuthenticationException.java)/**
 * Exception thrown when an authentication error occurs.
 */
Authenticator (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/Authenticator.java)/**
 * Interface for client authentication mechanisms.
 * <p>
 * Implementations are use-once instances, they don't need to be thread safe.
 */
ConnectionConfigurator (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/ConnectionConfigurator.java)/**
 * Interface to configure  {@link HttpURLConnection} created by
 * {@link AuthenticatedURL} instances.
 */
KerberosAuthenticator (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/KerberosAuthenticator.java)/**
 * The {@link KerberosAuthenticator} implements the Kerberos SPNEGO authentication sequence.
 * <p>
 * It uses the default principal for the Kerberos cache (normally set via kinit).
 * <p>
 * It falls back to the {@link PseudoAuthenticator} if the HTTP endpoint does not trigger an SPNEGO authentication
 * sequence.
 */
PseudoAuthenticator (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/PseudoAuthenticator.java)/**
 * The {@link PseudoAuthenticator} implementation provides an authentication equivalent to Hadoop's
 * Simple authentication, it trusts the value of the 'user.name' Java System property.
 * <p>
 * The 'user.name' value is propagated using an additional query string parameter {@link #USER_NAME} ('user.name').
 */
AltKerberosAuthenticationHandler (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AltKerberosAuthenticationHandler.java)/**
 * The {@link AltKerberosAuthenticationHandler} behaves exactly the same way as
 * the {@link KerberosAuthenticationHandler}, except that it allows for an
 * alternative form of authentication for browsers while still using Kerberos
 * for Java access.  This is an abstract class that should be subclassed
 * to allow a developer to implement their own custom authentication for browser
 * access.  The alternateAuthenticate method will be called whenever a request
 * comes from a browser.
 */
AuthenticationHandler (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationHandler.java)/**
 * Interface for server authentication mechanisms.
 * The {@link AuthenticationFilter} manages the lifecycle of the authentication handler.
 * Implementations must be thread-safe as one instance is initialized and used for all requests.
 */
AuthenticationHandlerUtil (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationHandlerUtil.java)/**
 * This is a utility class designed to provide functionality related to
 * {@link AuthenticationHandler}.
 */
AuthenticationToken (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationToken.java)/**
 * The {@link AuthenticationToken} contains information about an authenticated
 * HTTP client and doubles as the {@link Principal} to be returned by
 * authenticated {@link HttpServletRequest}s
 * <p>
 * The token can be serialized/deserialized to and from a string as it is sent
 * and received in HTTP client responses and requests as a HTTP cookie (this is
 * done by the {@link AuthenticationFilter}).
 */
CompositeAuthenticationHandler (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/CompositeAuthenticationHandler.java)/**
 * Interface to support multiple authentication mechanisms simultaneously.
 *
 */
HttpConstants (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/HttpConstants.java)/**
 * This class defines constants used for HTTP protocol entities (such as
 * headers, methods and their values).
 */
JWTRedirectAuthenticationHandler (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/JWTRedirectAuthenticationHandler.java)/**
 * The {@link JWTRedirectAuthenticationHandler} extends
 * AltKerberosAuthenticationHandler to add WebSSO behavior for UIs. The expected
 * SSO token is a JsonWebToken (JWT). The supported algorithm is RS256 which
 * uses PKI between the token issuer and consumer. The flow requires a redirect
 * to a configured authentication server URL and a subsequent request with the
 * expected JWT token. This token is cryptographically verified and validated.
 * The user identity is then extracted from the token and used to create an
 * AuthenticationToken - as expected by the AuthenticationFilter.
 *
 * <p>
 * The supported configuration properties are:
 * </p>
 * <ul>
 * <li>authentication.provider.url: the full URL to the authentication server.
 * This is the URL that the handler will redirect the browser to in order to
 * authenticate the user. It does not have a default value.</li>
 * <li>public.key.pem: This is the PEM formatted public key of the issuer of the
 * JWT token. It is required for verifying that the issuer is a trusted party.
 * DO NOT include the PEM header and footer portions of the PEM encoded
 * certificate. It does not have a default value.</li>
 * <li>expected.jwt.audiences: This is a list of strings that identify
 * acceptable audiences for the JWT token. The audience is a way for the issuer
 * to indicate what entity/s that the token is intended for. Default value is
 * null which indicates that all audiences will be accepted.</li>
 * <li>jwt.cookie.name: the name of the cookie that contains the JWT token.
 * Default value is "hadoop-jwt".</li>
 * </ul>
 */
KerberosAuthenticationHandler (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/KerberosAuthenticationHandler.java)/**
 * The {@link KerberosAuthenticationHandler} implements the Kerberos SPNEGO
 * authentication mechanism for HTTP.
 * <p>
 * The supported configuration properties are:
 * <ul>
 * <li>kerberos.principal: the Kerberos principal to used by the server. As
 * stated by the Kerberos SPNEGO specification, it should be
 * <code>HTTP/${HOSTNAME}@{REALM}</code>. The realm can be omitted from the
 * principal as the JDK GSS libraries will use the realm name of the configured
 * default realm.
 * It does not have a default value.</li>
 * <li>kerberos.keytab: the keytab file containing the credentials for the
 * Kerberos principal.
 * It does not have a default value.</li>
 * <li>kerberos.name.rules: kerberos names rules to resolve principal names, see
 * {@link KerberosName#setRules(String)}</li>
 * </ul>
 */
LdapAuthenticationHandler (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/LdapAuthenticationHandler.java)/**
 * The {@link LdapAuthenticationHandler} implements the BASIC authentication
 * mechanism for HTTP using LDAP back-end.
 *
 * The supported configuration properties are:
 * <ul>
 * <li>ldap.providerurl: The url of the LDAP server. It does not have a default
 * value.</li>
 * <li>ldap.basedn: the base distinguished name (DN) to be used with the LDAP
 * server. This value is appended to the provided user id for authentication
 * purpose. It does not have a default value.</li>
 * <li>ldap.binddomain: the LDAP bind domain value to be used with the LDAP
 * server. This property is optional and useful only in case of Active
 * Directory server.
 * <li>ldap.enablestarttls: A boolean value used to define if the LDAP server
 * supports 'StartTLS' extension.</li>
 * </ul>
 */
MultiSchemeAuthenticationHandler (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/MultiSchemeAuthenticationHandler.java)/**
 * The {@link MultiSchemeAuthenticationHandler} supports configuring multiple
 * authentication mechanisms simultaneously. e.g. server can support multiple
 * authentication mechanisms such as Kerberos (SPENGO) and LDAP. During the
 * authentication phase, server will specify all possible authentication schemes
 * and let client choose the appropriate scheme. Please refer to RFC-2616 and
 * HADOOP-12082 for more details.
 * <p>
 * The supported configuration properties are:
 * <ul>
 * <li>multi-scheme-auth-handler.schemes: A comma separated list of HTTP
 * authentication mechanisms supported by this handler. It does not have a
 * default value. e.g. multi-scheme-auth-handler.schemes=basic,negotiate
 * <li>multi-scheme-auth-handler.schemes.${scheme-name}.handler: The
 * authentication handler implementation to be used for the specified
 * authentication scheme. It does not have a default value. e.g.
 * multi-scheme-auth-handler.schemes.negotiate.handler=kerberos
 * </ul>
 *
 * It expected that for every authentication scheme specified in
 * multi-scheme-auth-handler.schemes property, a handler needs to be configured.
 * Note that while scheme values in 'multi-scheme-auth-handler.schemes' property
 * are case-insensitive, the scheme value in the handler configuration property
 * name must be lower case. i.e. property name such as
 * multi-scheme-auth-handler.schemes.Negotiate.handler is invalid.
 */
PseudoAuthenticationHandler (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/PseudoAuthenticationHandler.java)/**
 * The <code>PseudoAuthenticationHandler</code> provides a pseudo authentication mechanism that accepts
 * the user name specified as a query string parameter.
 * <p>
 * This mimics the model of Hadoop Simple authentication which trust the 'user.name' property provided in
 * the configuration object.
 * <p>
 * This handler can be configured to support anonymous users.
 * <p>
 * The only supported configuration property is:
 * <ul>
 * <li>simple.anonymous.allowed: <code>true|false</code>, default value is <code>false</code></li>
 * </ul>
 */
AuthToken (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/AuthToken.java)/**
 */
FileSignerSecretProvider (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/FileSignerSecretProvider.java)/**
 * A SignerSecretProvider that simply loads a secret from a specified file.
 */
Rule (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java)/**
   * An encoding of a rule for translating kerberos names.
   */
KerberosName (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/KerberosName.java)/**
 * This class implements parsing and handling of Kerberos principal names. In
 * particular, it splits them apart and translates them down into local
 * operating system names.
 */
RandomSignerSecretProvider (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/RandomSignerSecretProvider.java)/**
 * A SignerSecretProvider that uses a random number as its secret.  It rolls
 * the secret at a regular interval.
 */
RolloverSignerSecretProvider (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/RolloverSignerSecretProvider.java)/**
 * An abstract SignerSecretProvider that can be use used as the base for a
 * rolling secret.  The secret will roll over at the same interval as the token
 * validity, so there are only ever a maximum of two valid secrets at any
 * given time.  This class handles storing and returning the secrets, as well
 * as the rolling over.  At a minimum, subclasses simply need to implement the
 * generateNewSecret() method.  More advanced implementations can override
 * other methods to provide more advanced behavior, but should be careful when
 * doing so.
 */
Signer (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/Signer.java)/**
 * Signs strings and verifies signed strings using a SHA digest.
 */
SignerException (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SignerException.java)/**
 * Exception thrown by {@link Signer} when a string signature is invalid.
 */
SignerSecretProvider (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/SignerSecretProvider.java)/**
 * The SignerSecretProvider is an abstract way to provide a secret to be used
 * by the Signer so that we can have different implementations that potentially
 * do more complicated things in the backend.
 * See the RolloverSignerSecretProvider class for an implementation that
 * supports rolling over the secret at a regular interval.
 */
SASLOwnerACLProvider (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/ZKSignerSecretProvider.java)/**
   * Simple implementation of an {@link ACLProvider} that simply returns an ACL
   * that gives all permissions only to a single principal.
   */
JaasConfiguration (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/ZKSignerSecretProvider.java)/**
   * Creates a programmatic version of a jaas.conf file. This can be used
   * instead of writing a jaas.conf file and setting the system property,
   * "java.security.auth.login.config", to point to that file. It is meant to be
   * used for connecting to ZooKeeper.
   */
ZKSignerSecretProvider (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/util/ZKSignerSecretProvider.java)/**
 * A SignerSecretProvider that synchronizes a rolling random secret between
 * multiple servers using ZooKeeper.
 * <p>
 * It works by storing the secrets and next rollover time in a ZooKeeper znode.
 * All ZKSignerSecretProviders looking at that znode will use those
 * secrets and next rollover time to ensure they are synchronized.  There is no
 * "leader" -- any of the ZKSignerSecretProviders can choose the next secret;
 * which one is indeterminate.  Kerberos-based ACLs can also be enforced to
 * prevent a malicious third-party from getting or setting the secrets.  It uses
 * its own CuratorFramework client for talking to ZooKeeper.  If you want to use
 * your own Curator client, you can pass it to ZKSignerSecretProvider; see
 * {@link org.apache.hadoop.security.authentication.server.AuthenticationFilter}
 * for more details.
 * <p>
 * Details of the configurations are listed on <a href="../../../../../../../Configuration.html">Configuration Page</a>
 */
PlatformName (/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/util/PlatformName.java)/**
 * A helper class for getting build-info of the java-vm.
 *
 */
TestKerberosAuthenticator (/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/TestKerberosAuthenticator.java)/**
 * Test class for {@link KerberosAuthenticator}.
 */
KerberosTestUtils (/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/KerberosTestUtils.java)/**
 * Test helper class for Java Kerberos setup.
 */
LdapConstants (/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/LdapConstants.java)/**
 * This class defines the constants used by the LDAP integration tests.
 */
TestKerberosAuthenticationHandler (/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java)/**
 * Tests for Kerberos Authentication Handler.
 */
TestLdapAuthenticationHandler (/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestLdapAuthenticationHandler.java)/**
 * This unit test verifies the functionality of LDAP authentication handler.
 */
TestMultiSchemeAuthenticationHandler (/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestMultiSchemeAuthenticationHandler.java)/**
 * This unit test verifies the functionality of "multi-scheme" auth handler.
 */
StringSignerSecretProvider (/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/util/StringSignerSecretProvider.java)/**
 * A SignerSecretProvider that simply creates a secret based on a given String.
 */
StringSignerSecretProviderCreator (/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/util/StringSignerSecretProviderCreator.java)/**
 * Helper class for creating StringSignerSecretProviders in unit tests
 */
MockRandomSignerSecretProvider (/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/util/TestRandomSignerSecretProvider.java)/**
   * A hack to test RandomSignerSecretProvider.
   * We want to test that RandomSignerSecretProvider.rollSecret() is
   * periodically called at the expected frequency, but we want to exclude the
   * race-condition and not take a long time to run the test.
   */
MockZKSignerSecretProvider (/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/util/TestZKSignerSecretProvider.java)/**
   * A hack to test ZKSignerSecretProvider.
   * We want to test that ZKSignerSecretProvider.rollSecret() is periodically
   * called at the expected frequency, but we want to exclude the
   * race-condition and not take a long time to run the test.
   */
OldMockZKSignerSecretProvider (/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/util/TestZKSignerSecretProvider.java)/**
   * A version of {@link MockZKSignerSecretProvider} that uses the old way of
   * generating secrets (160 bit long).
   */
RequestLoggerFilter (/hadoop-common-project/hadoop-auth-examples/src/main/java/org/apache/hadoop/security/authentication/examples/RequestLoggerFilter.java)/**
 * Servlet filter that logs HTTP request/response headers
 */
WhoClient (/hadoop-common-project/hadoop-auth-examples/src/main/java/org/apache/hadoop/security/authentication/examples/WhoClient.java)/**
 * Example that uses <code>AuthenticatedURL</code>.
 */
WhoServlet (/hadoop-common-project/hadoop-auth-examples/src/main/java/org/apache/hadoop/security/authentication/examples/WhoServlet.java)/**
 * Example servlet that returns the user and principal of the request.
 */
Null (/hadoop-common-project/hadoop-common/dev-support/jdiff/Null.java)/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
ConfigRedactor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/ConfigRedactor.java)/**
 * Tool for redacting sensitive information when displaying config parameters.
 *
 * <p>Some config parameters contain sensitive information (for example, cloud
 * storage keys). When these properties are displayed in plaintext, we should
 * redactor their values as appropriate.
 */
Configurable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configurable.java)/** Something that may be configured with a {@link Configuration}. */
DeprecatedKeyInfo (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java)/**
   * Class to keep the information about the keys which replace the deprecated
   * ones.
   * 
   * This class stores the new keys which replace the deprecated keys and also
   * gives a provision to have a custom message for each of the deprecated key
   * that is being replaced. It also provides method to get the appropriate
   * warning message which can be logged whenever the deprecated key is used.
   */
DeprecationDelta (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java)/**
   * A pending addition to the global set of deprecated keys.
   */
DeprecationContext (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java)/**
   * The set of all keys which are deprecated.
   *
   * DeprecationContext objects are immutable.
   */
IntegerRanges (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java)/**
   * A class that represents a set of positive integer ranges. It parses 
   * strings of the form: "2-3,5,7-" where ranges are separated by comma and 
   * the lower/upper bounds are separated by dash. Either the lower or upper 
   * bound may be omitted meaning all values up to or over. So the string 
   * above means 2, 3, 5, and 7, 8, 9, ...
   */
Parser (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java)/**
   * Parser to consume SAX stream of XML elements from a Configuration.
   */
NegativeCacheSentinel (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java)/**
   * A unique class which is used as a sentinel value in the caching
   * for getClassByName. {@link Configuration#getClassByNameOrNull(String)}
   */
Configuration (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java)/**
 * Provides access to configuration parameters.
 *
 * <h3 id="Resources">Resources</h3>
 *
 * <p>Configurations are specified by resources. A resource contains a set of
 * name/value pairs as XML data. Each resource is named by either a 
 * <code>String</code> or by a {@link Path}. If named by a <code>String</code>, 
 * then the classpath is examined for a file with that name.  If named by a 
 * <code>Path</code>, then the local filesystem is examined directly, without 
 * referring to the classpath.
 *
 * <p>Unless explicitly turned off, Hadoop by default specifies two 
 * resources, loaded in-order from the classpath: <ol>
 * <li><tt>
 * <a href="{@docRoot}/../hadoop-project-dist/hadoop-common/core-default.xml">
 * core-default.xml</a></tt>: Read-only defaults for hadoop.</li>
 * <li><tt>core-site.xml</tt>: Site-specific configuration for a given hadoop
 * installation.</li>
 * </ol>
 * Applications may add additional resources, which are loaded
 * subsequent to these resources in the order they are added.
 * 
 * <h4 id="FinalParams">Final Parameters</h4>
 *
 * <p>Configuration parameters may be declared <i>final</i>. 
 * Once a resource declares a value final, no subsequently-loaded 
 * resource can alter that value.  
 * For example, one might define a final parameter with:
 * <pre><code>
 *  &lt;property&gt;
 *    &lt;name&gt;dfs.hosts.include&lt;/name&gt;
 *    &lt;value&gt;/etc/hadoop/conf/hosts.include&lt;/value&gt;
 *    <b>&lt;final&gt;true&lt;/final&gt;</b>
 *  &lt;/property&gt;</code></pre>
 *
 * Administrators typically define parameters as final in 
 * <tt>core-site.xml</tt> for values that user applications may not alter.
 *
 * <h4 id="VariableExpansion">Variable Expansion</h4>
 *
 * <p>Value strings are first processed for <i>variable expansion</i>. The
 * available properties are:<ol>
 * <li>Other properties defined in this Configuration; and, if a name is
 * undefined here,</li>
 * <li>Environment variables in {@link System#getenv()} if a name starts with
 * "env.", or</li>
 * <li>Properties in {@link System#getProperties()}.</li>
 * </ol>
 *
 * <p>For example, if a configuration resource contains the following property
 * definitions: 
 * <pre><code>
 *  &lt;property&gt;
 *    &lt;name&gt;basedir&lt;/name&gt;
 *    &lt;value&gt;/user/${<i>user.name</i>}&lt;/value&gt;
 *  &lt;/property&gt;
 *  
 *  &lt;property&gt;
 *    &lt;name&gt;tempdir&lt;/name&gt;
 *    &lt;value&gt;${<i>basedir</i>}/tmp&lt;/value&gt;
 *  &lt;/property&gt;
 *
 *  &lt;property&gt;
 *    &lt;name&gt;otherdir&lt;/name&gt;
 *    &lt;value&gt;${<i>env.BASE_DIR</i>}/other&lt;/value&gt;
 *  &lt;/property&gt;
 *  </code></pre>
 *
 * <p>When <tt>conf.get("tempdir")</tt> is called, then <tt>${<i>basedir</i>}</tt>
 * will be resolved to another property in this Configuration, while
 * <tt>${<i>user.name</i>}</tt> would then ordinarily be resolved to the value
 * of the System property with that name.
 * <p>When <tt>conf.get("otherdir")</tt> is called, then <tt>${<i>env.BASE_DIR</i>}</tt>
 * will be resolved to the value of the <tt>${<i>BASE_DIR</i>}</tt> environment variable.
 * It supports <tt>${<i>env.NAME:-default</i>}</tt> and <tt>${<i>env.NAME-default</i>}</tt> notations.
 * The former is resolved to "default" if <tt>${<i>NAME</i>}</tt> environment variable is undefined
 * or its value is empty.
 * The latter behaves the same way only if <tt>${<i>NAME</i>}</tt> is undefined.
 * <p>By default, warnings will be given to any deprecated configuration 
 * parameters and these are suppressible by configuring
 * <tt>log4j.logger.org.apache.hadoop.conf.Configuration.deprecation</tt> in
 * log4j.properties file.
 *
 * <h4 id="Tags">Tags</h4>
 *
 * <p>Optionally we can tag related properties together by using tag
 * attributes. System tags are defined by hadoop.tags.system property. Users
 * can define there own custom tags in  hadoop.tags.custom property.
 *
 * <p>For example, we can tag existing property as:
 * <pre><code>
 *  &lt;property&gt;
 *    &lt;name&gt;dfs.replication&lt;/name&gt;
 *    &lt;value&gt;3&lt;/value&gt;
 *    &lt;tag&gt;HDFS,REQUIRED&lt;/tag&gt;
 *  &lt;/property&gt;
 *
 *  &lt;property&gt;
 *    &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;
 *    &lt;value&gt;3&lt;/value&gt;
 *    &lt;tag&gt;HDFS,SECURITY&lt;/tag&gt;
 *  &lt;/property&gt;
 * </code></pre>
 * <p> Properties marked with tags can be retrieved with <tt>conf
 * .getAllPropertiesByTag("HDFS")</tt> or <tt>conf.getAllPropertiesByTags
 * (Arrays.asList("YARN","SECURITY"))</tt>.</p>
 */
ConfigurationWithLogging (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/ConfigurationWithLogging.java)/**
 * Logs access to {@link Configuration}.
 * Sensitive data will be redacted.
 */
Configured (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configured.java)/** Base class for things that may be configured with a {@link Configuration}. */
ConfServlet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/ConfServlet.java)/**
 * A servlet to print out the running configuration data.
 */
Reconfigurable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Reconfigurable.java)/**
 * Something whose {@link Configuration} can be changed at run time.
 */
ReconfigurationThread (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/ReconfigurableBase.java)/**
   * A background thread to apply configuration changes.
   */
ReconfigurableBase (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/ReconfigurableBase.java)/**
 * Utility base class for implementing the Reconfigurable interface.
 *
 * Subclasses should override reconfigurePropertyImpl to change individual
 * properties and getReconfigurableProperties to get all properties that
 * can be changed at run time.
 */
ReconfigurationException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/ReconfigurationException.java)/**
 * Exception indicating that configuration property cannot be changed
 * at run time.
 */
ReconfigurationServlet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/ReconfigurationServlet.java)/**
 * A servlet for changing a node's configuration.
 *
 * Reloads the configuration file, verifies whether changes are
 * possible and asks the admin to approve the change.
 *
 */
StorageSize (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/StorageSize.java)/**
 * A class that contains the numeric value and the unit of measure.
 */
CipherOption (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CipherOption.java)/**
 * Used between client and server to negotiate the 
 * cipher suite, key and iv.
 */
CryptoCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java)/**
 * Crypto codec class, encapsulates encryptor/decryptor pair.
 */
CryptoInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoInputStream.java)/**
 * CryptoInputStream decrypts data. It is not thread-safe. AES CTR mode is
 * required in order to ensure that the plain text and cipher text have a 1:1
 * mapping. The decryption is buffer based. The key points of the decryption
 * are (1) calculating the counter and (2) padding through stream position:
 * <p>
 * counter = base + pos/(algorithm blocksize); 
 * padding = pos%(algorithm blocksize); 
 * <p>
 * The underlying stream offset is maintained as state.
 */
CryptoOutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoOutputStream.java)/**
 * CryptoOutputStream encrypts data. It is not thread-safe. AES CTR mode is
 * required in order to ensure that the plain text and cipher text have a 1:1
 * mapping. The encryption is buffer based. The key points of the encryption are
 * (1) calculating counter and (2) padding through stream position.
 * <p>
 * counter = base + pos/(algorithm blocksize); 
 * padding = pos%(algorithm blocksize); 
 * <p>
 * The underlying stream offset is maintained as state.
 *
 * Note that while some of this class' methods are synchronized, this is just to
 * match the threadsafety behavior of DFSOutputStream. See HADOOP-11710.
 */
JceAesCtrCryptoCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java)/**
 * Implement the AES-CTR crypto codec using JCE provider.
 */
CachingKeyProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java)/**
 * A <code>KeyProviderExtension</code> implementation providing a short lived
 * cache for <code>KeyVersions</code> and <code>Metadata</code>to avoid burst
 * of requests to hit the underlying <code>KeyProvider</code>.
 */
Factory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java)/**
   * The factory to create JksProviders, which is used by the ServiceLoader.
   */
KeyMetadata (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java)/**
   * An adapter between a KeyStore Key and our Metadata. This is used to store
   * the metadata in a KeyStore even though isn't really a key.
   */
JavaKeyStoreProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java)/**
 * KeyProvider based on Java's KeyStore file format. The file may be stored in
 * any Hadoop FileSystem using the following name mangling:
 *  jks://hdfs@nn1.example.com/my/keys.jks {@literal ->}
 *  hdfs://nn1.example.com/my/keys.jks
 *  jks://file/home/owen/keys.jks {@literal ->} file:///home/owen/keys.jks
 * <p>
 * If the <code>HADOOP_KEYSTORE_PASSWORD</code> environment variable is set,
 * its value is used as the password for the keystore.
 * <p>
 * If the <code>HADOOP_KEYSTORE_PASSWORD</code> environment variable is not set,
 * the password for the keystore is read from file specified in the
 * {@link #KEYSTORE_PASSWORD_FILE_KEY} configuration property. The password file
 * is looked up in Hadoop's configuration directory via the classpath.
 * <p>
 * <b>NOTE:</b> Make sure the password in the password file does not have an
 * ENTER at the end, else it won't be valid for the Java KeyStore.
 * <p>
 * If the environment variable, nor the property are not set, the password used
 * is 'none'.
 * <p>
 * It is expected for encrypted InputFormats and OutputFormats to copy the keys
 * from the original provider into the job's Credentials object, which is
 * accessed via the UserProvider. Therefore, this provider won't be used by
 * MapReduce tasks.
 */
KeyVersion (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java)/**
   * The combination of both the key version name and the key material.
   */
Metadata (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java)/**
   * Key metadata that is associated with the key.
   */
Options (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java)/**
   * Options when creating key objects.
   */
KeyProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java)/**
 * A provider of secret key material for Hadoop applications. Provides an
 * abstraction to separate key storage from users of encryption. It
 * is intended to support getting or storing keys in a variety of ways,
 * including third party bindings.
 * <p>
 * <code>KeyProvider</code> implementations must be thread safe.
 */
EncryptedKeyVersion (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java)/**
   * An encrypted encryption key (EEK) and related information. An EEK must be
   * decrypted using the key's encryption key before it can be used.
   */
CryptoExtension (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java)/**
   * CryptoExtension is a type of Extension that exposes methods to generate
   * EncryptedKeys and to decrypt the same.
   */
KeyProviderCryptoExtension (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java)/**
 * A KeyProvider with Cryptographic Extensions specifically for generating
 * and decrypting encrypted encryption keys.
 * 
 */
DelegationTokenExtension (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension.java)/**
   * DelegationTokenExtension is a type of Extension that exposes methods
   * needed to work with Delegation Tokens.
   */
DefaultDelegationTokenExtension (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension.java)/**
   * Default implementation of {@link DelegationTokenExtension} that
   * implements the method as a no-op.
   */
KeyProviderDelegationTokenExtension (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension.java)/**
 * A KeyProvider extension with the ability to add a renewer's Delegation 
 * Tokens to the provided Credentials.
 */
Extension (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java)/**
   * A marker interface for the KeyProviderExtension subclass implement.
   */
KeyProviderExtension (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java)/**
 * This is a utility class used to extend the functionality of KeyProvider, that
 * takes a KeyProvider and an Extension. It implements all the required methods
 * of the KeyProvider by delegating it to the provided KeyProvider.
 */
KeyProviderFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderFactory.java)/**
 * A factory to create a list of KeyProvider based on the path given in a
 * Configuration. It uses a service loader interface to find the available
 * KeyProviders and create them based on the list of URIs.
 */
KeyProviderTokenIssuer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProviderTokenIssuer.java)/**
 * File systems that support Encryption Zones have to implement this interface.
 */
KeyShell (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java)/**
 * This program is the CLI utility for the KeyProvider facilities in Hadoop.
 */
KMSTokenRenewer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java)/**
   * The KMS implementation of {@link TokenRenewer}.
   */
Factory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java)/**
   * The factory to create KMSClientProvider, which is used by the
   * ServiceLoader.
   */
TimeoutConnConfigurator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java)/**
   * This small class exists to set the timeout values for a connection
   */
KMSClientProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java)/**
 * KMS client <code>KeyProvider</code> implementation.
 */
KMSDelegationTokenIdentifier (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSDelegationToken.java)/**
   * DelegationTokenIdentifier used for the KMS.
   */
KMSDelegationToken (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSDelegationToken.java)/**
 * Holder class for KMS delegation tokens.
 */
KMSRESTConstants (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSRESTConstants.java)/**
 * KMS REST and JSON constants and utility methods for the KMSServer.
 */
LoadBalancingKMSClientProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java)/**
 * A simple LoadBalancing KMSClientProvider that round-robins requests
 * across a provided array of KMSClientProviders. It also retries failed
 * requests on the next available provider in the load balancer group. It
 * only retries failed requests that result in an IOException, sending back
 * all other Exceptions to the caller without retry.
 */
QueueRefiller (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java)/**
   * QueueRefiller interface a client must implement to use this class
   */
NamedRunnable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java)/**
   * A <code>Runnable</code> which takes a string name.
   */
UniqueKeyBlockingQueue (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java)/**
   * This backing blocking queue used in conjunction with the
   * <code>ThreadPoolExecutor</code> used by the <code>ValueQueue</code>. This
   * Queue accepts a task only if the task is not currently in the process
   * of being run by a thread which is implied by the presence of the key
   * in the <code>keysInProgress</code> set.
   *
   * NOTE: Only methods that ware explicitly called by the
   * <code>ThreadPoolExecutor</code> need to be over-ridden.
   */
ValueQueue (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java)/**
 * A Utility class that maintains a Queue of entries for a given key. It tries
 * to ensure that there is are always at-least <code>numValues</code> entries
 * available for the client to consume for a particular key.
 * It also uses an underlying Cache to evict queues for keys that have not been
 * accessed for a configurable period of time.
 * Implementing classes are required to implement the
 * <code>QueueRefiller</code> interface that exposes a method to refill the
 * queue, when empty
 */
UserProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/UserProvider.java)/**
 * A KeyProvider factory for UGIs. It uses the credentials object associated
 * with the current user to find keys. This provider is created using a
 * URI of "user:///".
 */
OpensslAesCtrCryptoCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java)/**
 * Implement the AES-CTR crypto codec using JNI into OpenSSL.
 */
Transform (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/OpensslCipher.java)/** Nested class for algorithm, mode and padding. */
OpensslCipher (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/OpensslCipher.java)/**
 * OpenSSL cipher using JNI.
 * Currently only AES-CTR is supported. It's flexible to add 
 * other crypto algorithms/modes.
 */
OpensslSecureRandom (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/random/OpensslSecureRandom.java)/**
 * OpenSSL secure random using JNI.
 * This implementation is thread-safe.
 * <p>
 * 
 * If using an Intel chipset with RDRAND, the high-performance hardware 
 * random number generator will be used and it's much faster than
 * {@link java.security.SecureRandom}. If RDRAND is unavailable, default
 * OpenSSL secure random generator will be used. It's still faster
 * and can generate strong random bytes.
 * <p>
 * See https://wiki.openssl.org/index.php/Random_Numbers
 * See http://en.wikipedia.org/wiki/RdRand
 */
OsSecureRandom (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/random/OsSecureRandom.java)/**
 * A Random implementation that uses random bytes sourced from the
 * operating system.
 */
UnsupportedCodecException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/UnsupportedCodecException.java)/**
 * Thrown to indicate that the specific codec is not supported.
 */
AbstractFileSystem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/AbstractFileSystem.java)/**
 * This class provides an interface for implementors of a Hadoop file system
 * (analogous to the VFS of Unix). Applications do not access this class;
 * instead they access files across all file systems using {@link FileContext}.
 * 
 * Pathnames passed to AbstractFileSystem can be fully qualified URI that
 * matches the "this" file system (ie same scheme and authority) 
 * or a Slash-relative name that is assumed to be relative
 * to the root of the "this" file system .
 */
AvroFSInput (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/AvroFSInput.java)/** Adapts an {@link FSDataInputStream} to Avro's SeekableInput interface. */
BatchedRemoteIterator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/BatchedRemoteIterator.java)/**
 * A RemoteIterator that fetches elements in batches.
 */
BBPartHandle (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/BBPartHandle.java)/**
 * Byte array backed part handle.
 */
BBUploadHandle (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/BBUploadHandle.java)/**
 * Byte array backed upload handle.
 */
BlockLocation (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/BlockLocation.java)/**
 * Represents the network location of a block, information about the hosts
 * that contain block replicas, and other block metadata (E.g. the file
 * offset associated with the block, length, whether it is corrupt, etc).
 *
 * For a single BlockLocation, it will have different meanings for replicated
 * and erasure coded files.
 *
 * If the file is 3-replicated, offset and length of a BlockLocation represent
 * the absolute value in the file and the hosts are the 3 datanodes that
 * holding the replicas. Here is an example:
 * <pre>
 * BlockLocation(offset: 0, length: BLOCK_SIZE,
 *   hosts: {"host1:9866", "host2:9866, host3:9866"})
 * </pre>
 *
 * And if the file is erasure-coded, each BlockLocation represents a logical
 * block groups. Value offset is the offset of a block group in the file and
 * value length is the total length of a block group. Hosts of a BlockLocation
 * are the datanodes that holding all the data blocks and parity blocks of a
 * block group.
 * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).
 * A BlockLocation example will be like:
 * <pre>
 * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {"host1:9866",
 *   "host2:9866","host3:9866","host4:9866","host5:9866"})
 * </pre>
 *
 * Please refer to
 * {@link FileSystem#getFileBlockLocations(FileStatus, long, long)} or
 * {@link FileContext#getFileBlockLocations(Path, long, long)}
 * for more examples.
 */
BlockStoragePolicySpi (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/BlockStoragePolicySpi.java)/**
 * A storage policy specifies the placement of block replicas on specific
 * storage types.
 */
ByteBufferPositionedReadable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ByteBufferPositionedReadable.java)/**
 * Implementers of this interface provide a positioned read API that writes to a
 * {@link ByteBuffer} rather than a {@code byte[]}.
 *
 * @see PositionedReadable
 * @see ByteBufferReadable
 */
ByteBufferReadable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ByteBufferReadable.java)/**
 * Implementers of this interface provide a read API that writes to a
 * ByteBuffer, not a byte[].
 */
CachingGetSpaceUsed (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java)/**
 * Interface for class that can tell estimate much space
 * is used in a directory.
 * <p>
 * The implementor is fee to cache space used. As such there
 * are methods to update the cached value with any known changes.
 */
CanUnbuffer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CanUnbuffer.java)/**
 * FSDataInputStreams implement this interface to indicate that they can clear
 * their buffers on request.
 */
ChecksumException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumException.java)/** Thrown for checksum errors. */
ChecksumFSInputChecker (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java)/*******************************************************
   * For open()'s FSInputStream
   * It verifies that data matches checksums.
   *******************************************************/
ChecksumFSOutputSummer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java)/** This class provides an output stream for a checksummed file.
   * It generates checksums for data. */
ChecksumFileSystem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java)/****************************************************************
 * Abstract Checksumed FileSystem.
 * It provide a basic implementation of a Checksumed FileSystem,
 * which creates a checksum file for each raw file.
 * It generates &amp; verifies checksums at the client side.
 *
 *****************************************************************/
ChecksumFSInputChecker (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java)/*******************************************************
   * For open()'s FSInputStream
   * It verifies that data matches checksums.
   *******************************************************/
ChecksumFSOutputSummer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java)/** This class provides an output stream for a checksummed file.
   * It generates checksums for data. */
ChecksumFs (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java)/**
 * Abstract Checksumed Fs.
 * It provide a basic implementation of a Checksumed Fs,
 * which creates a checksum file for each raw file.
 * It generates &amp; verifies checksums at the client side.
 */
ClusterStorageCapacityExceededException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ClusterStorageCapacityExceededException.java)/**
 * Exception raised by HDFS indicating that storage capacity in the
 * cluster filesystem is exceeded. See also
 * https://issues.apache.org/jira/browse/MAPREDUCE-7148.
 */
CommonPathCapabilities (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonPathCapabilities.java)/**
 * Common path capabilities.
 */
CompositeCrcFileChecksum (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java)/** Composite CRC. */
Builder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ContentSummary.java)/** We don't use generics. Instead override spaceConsumed and other methods
      in order to keep backward compatibility. */
ContentSummary (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ContentSummary.java)/** Store the summary of a content (a directory or a file). */
DelegateToFileSystem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DelegateToFileSystem.java)/**
 * Implementation of AbstractFileSystem based on the existing implementation of 
 * {@link FileSystem}.
 */
Renewable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DelegationTokenRenewer.java)/** The renewable interface used by the renewer. */
RenewAction (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DelegationTokenRenewer.java)/**
   * An action that will renew and replace the file system's delegation 
   * tokens automatically.
   */
DelegationTokenRenewer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DelegationTokenRenewer.java)/**
 * A daemon thread that waits for the next file system to renew.
 */
DF (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DF.java)/** Filesystem disk space usage statistics.
 * Uses the unix 'df' program to get mount points, and java.io.File for
 * space utilization. Tested on Linux, FreeBSD, Windows. */
DFCachingGetSpaceUsed (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DFCachingGetSpaceUsed.java)/**
 * Fast but inaccurate class to tell how much space HDFS is using.
 * This class makes the assumption that the entire mount is used for
 * HDFS and that no two hdfs data dirs are on the same disk.
 *
 * To use set fs.getspaceused.classname
 * to org.apache.hadoop.fs.DFCachingGetSpaceUsed in your core-site.xml
 *
 */
DirectoryListingStartAfterNotFoundException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DirectoryListingStartAfterNotFoundException.java)/** 
 * Thrown when the startAfter can't be found when listing a directory.
 */
DU (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DU.java)/** Filesystem disk space usage statistics.  Uses the unix 'du' program */
EmptyStorageStatistics (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/EmptyStorageStatistics.java)/**
 * EmptyStorageStatistics is a StorageStatistics implementation which has no
 * data.
 */
FileAlreadyExistsException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileAlreadyExistsException.java)/**
 * Used when target file already exists for any operation and 
 * is not configured to be overwritten.  
 */
FileChecksum (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileChecksum.java)/** An abstract class representing file checksums for files. */
FCDataOutputStreamBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileContext.java)/**
   * {@link FSDataOutputStreamBuilder} for {@liink FileContext}.
   */
Util (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileContext.java)/**
   * Utility/library methods built over the basic FileContext methods.
   * Since this are library functions, the oprtation are not atomic
   * and some of them may partially complete if other threads are making
   * changes to the same part of the name space.
   */
FileContextFinalizer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileContext.java)/**
   * Deletes all the paths in deleteOnExit on JVM shutdown.
   */
FSDataInputStreamBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileContext.java)/**
   * Builder returned for {@link #openFile(Path)}.
   */
FileEncryptionInfo (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileEncryptionInfo.java)/**
 * FileEncryptionInfo encapsulates all the encryption-related information for
 * an encrypted file.
 */
FileStatus (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileStatus.java)/** Interface that represents the client side information for a file.
 */
DirectoryEntries (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)/**
   * Represents a batch of directory entries when iteratively listing a
   * directory. This is a private API not meant for use by end users.
   * <p>
   * For internal use by FileSystem subclasses that override
   * {@link FileSystem#listStatusBatch(Path, byte[])} to implement iterative
   * listing.
   */
DirListingIterator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)/**
   * Generic iterator for implementing {@link #listStatusIterator(Path)}.
   */
Key (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)/** FileSystem.Cache.Key */
Cache (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)/** Caching FileSystem objects. */
StatisticsData (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)/**
     * Statistics data.
     *
     * There is only a single writer to thread-local StatisticsData objects.
     * Hence, volatile is adequate here-- we do not need AtomicLong or similar
     * to prevent lost updates.
     * The Java specification guarantees that updates to volatile longs will
     * be perceived as atomic with respect to other threads, which is all we
     * need.
     */
StatisticsDataReference (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)/**
     * A weak reference to a thread that also includes the data associated
     * with that thread. On the thread being garbage collected, it is enqueued
     * to the reference queue for clean-up.
     */
StatisticsDataReferenceCleaner (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)/**
     * Background action to act on references being removed.
     */
Statistics (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)/**
   * Tracks statistics about how many reads, writes, and so forth have been
   * done in a FileSystem.
   *
   * Since there is only one of these objects per FileSystem, there will
   * typically be many threads writing to this object.  Almost every operation
   * on an open file will involve a write to this object.  In contrast, reading
   * statistics is done infrequently by most programs, and not at all by others.
   * Hence, this is optimized for writes.
   *
   * Each thread writes to its own thread-local area of memory.  This removes
   * contention and allows us to scale up to many, many threads.  To read
   * statistics, the reader thread totals up the contents of all of the
   * thread-local data areas.
   */
FileSystemDataOutputStreamBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)/**
   * Standard implementation of the FSDataOutputStreamBuilder; invokes
   * create/createNonRecursive or Append depending upon the options.
   */
FSDataInputStreamBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)/**
   * Builder returned for {@code #openFile(Path)}
   * and {@code #openFile(PathHandle)}.
   */
FileSystem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)/****************************************************************
 * An abstract base class for a fairly generic filesystem.  It
 * may be implemented as a distributed filesystem, or as a "local"
 * one that reflects the locally-connected disk.  The local version
 * exists for small Hadoop instances and for testing.
 *
 * <p>
 *
 * All user code that may potentially use the Hadoop Distributed
 * File System should be written to use a FileSystem object or its
 * successor, {@link FileContext}.
 *
 * <p>
 * The local implementation is {@link LocalFileSystem} and distributed
 * implementation is DistributedFileSystem. There are other implementations
 * for object stores and (outside the Apache Hadoop codebase),
 * third party filesystems.
 * <p>
 * Notes
 * <ol>
 * <li>The behaviour of the filesystem is
 * <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/filesystem/filesystem.html">
 * specified in the Hadoop documentation. </a>
 * However, the normative specification of the behavior of this class is
 * actually HDFS: if HDFS does not behave the way these Javadocs or
 * the specification in the Hadoop documentations define, assume that
 * the documentation is incorrect.
 * </li>
 * <li>The term {@code FileSystem} refers to an instance of this class.</li>
 * <li>The acronym "FS" is used as an abbreviation of FileSystem.</li>
 * <li>The term {@code filesystem} refers to the distributed/local filesystem
 * itself, rather than the class used to interact with it.</li>
 * <li>The term "file" refers to a file in the remote filesystem,
 * rather than instances of {@code java.io.File}.</li>
 * </ol>
 *
 * This is a carefully evolving class.
 * New methods may be marked as Unstable or Evolving for their initial release,
 * as a warning that they are new and may change based on the
 * experience of use in applications.
 *****************************************************************/
FileSystemLinkResolver (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemLinkResolver.java)/**
 * FileSystem-specific class used to operate on and resolve symlinks in a path.
 * Operation can potentially span multiple {@link FileSystem}s.
 * 
 * @see FSLinkResolver
 */
Factory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemMultipartUploader.java)/**
   * Factory for creating MultipartUploaderFactory objects for file://
   * filesystems.
   */
FileSystemMultipartUploader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemMultipartUploader.java)/**
 * A MultipartUploader that uses the basic FileSystem commands.
 * This is done in three stages:
 * <ul>
 *   <li>Init - create a temp {@code _multipart} directory.</li>
 *   <li>PutPart - copying the individual parts of the file to the temp
 *   directory.</li>
 *   <li>Complete - use {@link FileSystem#concat} to merge the files;
 *   and then delete the temp directory.</li>
 * </ul>
 */
FileSystemStorageStatistics (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java)/**
 * A basic StorageStatistics instance which simply returns data from
 * FileSystem#Statistics.
 */
HardLink (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java)/**
   * Class for creating hardlinks.
   * Supports Unix, WindXP.
   * @deprecated Use {@link org.apache.hadoop.fs.HardLink}
   */
FileUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java)/**
 * A collection of file-processing util methods
 */
FilterFileSystem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java)/****************************************************************
 * A <code>FilterFileSystem</code> contains
 * some other file system, which it uses as
 * its  basic file system, possibly transforming
 * the data along the way or providing  additional
 * functionality. The class <code>FilterFileSystem</code>
 * itself simply overrides all  methods of
 * <code>FileSystem</code> with versions that
 * pass all requests to the contained  file
 * system. Subclasses of <code>FilterFileSystem</code>
 * may further override some of  these methods
 * and may also provide additional methods
 * and fields.
 *
 *****************************************************************/
FilterFs (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFs.java)/**
 * A <code>FilterFs</code> contains some other file system, which it uses as its
 * basic file system, possibly transforming the data along the way or providing
 * additional functionality. The class <code>FilterFs</code> itself simply
 * overrides all methods of <code>AbstractFileSystem</code> with versions that
 * pass all requests to the contained file system. Subclasses of
 * <code>FilterFs</code> may further override some of these methods and may also
 * provide additional methods and fields.
 * 
 */
FSBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSBuilder.java)/**
 * The base interface which various FileSystem FileContext Builder
 * interfaces can extend, and which underlying implementations
 * will then implement.
 * @param <S> Return type on the {@link #build()} call.
 * @param <B> type of builder itself.
 */
FsConstants (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsConstants.java)/**
 * FileSystem related constants.
 */
FSDataInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSDataInputStream.java)/** Utility that wraps a {@link FSInputStream} in a {@link DataInputStream}
 * and buffers input through a {@link java.io.BufferedInputStream}. */
FSDataOutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSDataOutputStream.java)/** Utility that wraps a {@link OutputStream} in a {@link DataOutputStream}.
 */
FSDataOutputStreamBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java)/**
 * Builder for {@link FSDataOutputStream} and its subclasses.
 *
 * It is used to create {@link FSDataOutputStream} when creating a new file or
 * appending an existing file on {@link FileSystem}.
 *
 * By default, it does not create parent directory that do not exist.
 * {@link FileSystem#createNonRecursive(Path, boolean, int, short, long,
 * Progressable)}.
 *
 * To create missing parent directory, use {@link #recursive()}.
 *
 * To be more generic, {@link #opt(String, int)} and {@link #must(String, int)}
 * variants provide implementation-agnostic way to customize the builder.
 * Each FS-specific builder implementation can interpret the FS-specific
 * options accordingly, for example:
 *
 * <code>
 *
 * // Don't
 * if (fs instanceof FooFileSystem) {
 *   FooFileSystem fs = (FooFileSystem) fs;
 *   OutputStream out = dfs.createFile(path)
 *     .optionA()
 *     .optionB("value")
 *     .cache()
 *   .build()
 * } else if (fs instanceof BarFileSystem) {
 *   ...
 * }
 *
 * // Do
 * OutputStream out = fs.createFile(path)
 *   .permission(perm)
 *   .bufferSize(bufSize)
 *   .opt("foofs:option.a", true)
 *   .opt("foofs:option.b", "value")
 *   .opt("barfs:cache", true)
 *   .must("foofs:cache", true)
 *   .must("barfs:cache-size", 256 * 1024 * 1024)
 *   .build();
 * </code>
 *
 * If the option is not related to the file system, the option will be ignored.
 * If the option is must, but not supported by the file system, a
 * {@link IllegalArgumentException} will be thrown.
 *
 */
FSError (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSError.java)/** Thrown for unexpected filesystem errors, presumed to reflect disk errors
 * in the native filesystem. */
FSExceptionMessages (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSExceptionMessages.java)/**
 * Standard strings to use in exception messages in filesystems
 * HDFS is used as the reference source of the strings
 */
FSInputChecker (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSInputChecker.java)/**
 * This is a generic input stream for verifying checksums for
 * data before it is read by a user.
 */
FSInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSInputStream.java)/****************************************************************
 * FSInputStream is a generic old InputStream with a little bit
 * of RAF-style seek ability.
 *
 *****************************************************************/
FSLinkResolver (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSLinkResolver.java)/**
 * Used primarily by {@link FileContext} to operate on and resolve
 * symlinks in a path. Operations can potentially span multiple
 * {@link AbstractFileSystem}s.
 * 
 * @see FileSystemLinkResolver
 */
FSOutputSummer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSOutputSummer.java)/**
 * This is a generic output stream for generating checksums for
 * data before it is written to the underlying stream
 */
FsServerDefaults (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsServerDefaults.java)/****************************************************
 * Provides server default configuration values to clients.
 * 
 ****************************************************/
Usage (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsShell.java)/**
   *  Display help for commands with their short usage and long description.
   */
Help (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsShell.java)/**
   * Displays short usage of commands sans the long description
   */
UnknownCommandException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsShell.java)/**
   * The default ctor signals that the command being executed does not exist,
   * while other ctor signals that a specific command does not exist.  The
   * latter is used by commands that process other commands, ex. -usage/-help
   */
FsShell (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsShell.java)/** Provide command line access to a FileSystem. */
Chmod (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsShellPermissions.java)/**
   * The pattern is almost as flexible as mode allowed by chmod shell command.
   * The main restriction is that we recognize only rwxXt. To reduce errors we
   * also enforce octal mode specifications of either 3 digits without a sticky
   * bit setting or four digits with a sticky bit setting.
   */
Chown (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsShellPermissions.java)/**
   * Used to change owner and/or group of files 
   */
Chgrp (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsShellPermissions.java)/**
   * Used to change group of files 
   */
FsShellPermissions (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsShellPermissions.java)/**
 * This class is the home for file permissions related commands.
 * Moved to this separate class since FsShell is getting too large.
 */
FsStatus (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsStatus.java)/** This class is used to represent the capacity, free and used space on a
  * {@link FileSystem}.
  */
FsTracer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsTracer.java)/**
 * Holds the HTrace Tracer used for FileSystem operations.
 *
 * Ideally, this would be owned by the DFSClient, rather than global.  However,
 * the FileContext API may create a new DFSClient for each operation in some
 * cases.  Because of this, we cannot store this Tracer inside DFSClient.  See
 * HADOOP-6356 for details.
 */
FsUrlConnection (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsUrlConnection.java)/**
 * Representation of a URL connection to open InputStreams.
 */
FsUrlStreamHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsUrlStreamHandler.java)/**
 * URLStream handler relying on FileSystem and on a given Configuration to
 * handle URL protocols.
 */
FsUrlStreamHandlerFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java)/**
 * Factory for URL stream handlers.
 * 
 * There is only one handler whose job is to create UrlConnections. A
 * FsUrlConnection relies on FileSystem to choose the appropriate FS
 * implementation.
 * 
 * Before returning our handler, we make sure that FileSystem knows an
 * implementation for the requested scheme/protocol.
 */
FtpConfigKeys (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ftp/FtpConfigKeys.java)/** 
 * This class contains constants for configuration keys used
 * in the ftp file system.
 *
 * Note that the settings for unimplemented features are ignored. 
 * E.g. checksum related settings are just place holders. Even when
 * wrapped with {@link ChecksumFileSystem}, these settings are not
 * used. 
 */
FTPException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ftp/FTPException.java)/**
 * A class to wrap a {@link Throwable} into a Runtime Exception.
 */
FTPFileSystem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java)/**
 * <p>
 * A {@link FileSystem} backed by an FTP client provided by <a
 * href="http://commons.apache.org/net/">Apache Commons Net</a>.
 * </p>
 */
FtpFs (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ftp/FtpFs.java)/**
 * The FtpFs implementation of AbstractFileSystem.
 * This impl delegates to the old FileSystem
 */
FutureDataInputStreamBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FutureDataInputStreamBuilder.java)/**
 * Builder for input streams and subclasses whose return value is
 * actually a completable future: this allows for better asynchronous
 * operation.
 *
 * To be more generic, {@link #opt(String, int)} and {@link #must(String, int)}
 * variants provide implementation-agnostic way to customize the builder.
 * Each FS-specific builder implementation can interpret the FS-specific
 * options accordingly, for example:
 *
 * If the option is not related to the file system, the option will be ignored.
 * If the option is must, but not supported by the file system, a
 * {@link IllegalArgumentException} will be thrown.
 *
 */
Builder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/GetSpaceUsed.java)/**
   * The builder class
   */
StorageStatisticsProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/GlobalStorageStatistics.java)/**
   * A callback API for creating new StorageStatistics instances.
   */
GlobBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Globber.java)/**
   * Builder for Globber instances.
   */
Globber (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Globber.java)/**
 * Implementation of {@link FileSystem#globStatus(Path, PathFilter)}.
 * This has historically been package-private; it has been opened
 * up for object stores within the {@code hadoop-*} codebase ONLY.
 * It could be expanded for external store implementations in future.
 */
GlobFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/GlobFilter.java)/**
 * A filter for POSIX glob pattern with brace expansions.
 */
GlobPattern (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/GlobPattern.java)/**
 * A class for POSIX glob pattern with brace expansions.
 */
HardLinkCommandGetter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HardLink.java)/**
   * This abstract class bridges the OS-dependent implementations of the 
   * needed functionality for querying link counts.
   * The particular implementation class is chosen during 
   * static initialization phase of the HardLink class.
   * The "getter" methods construct shell command strings.
   */
HardLinkCGUnix (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HardLink.java)/**
   * Implementation of HardLinkCommandGetter class for Unix
   */
HardLinkCGWin (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HardLink.java)/**
   * Implementation of HardLinkCommandGetter class for Windows
   */
LinkStats (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HardLink.java)/**
   * HardLink statistics counters and methods.
   * Not multi-thread safe, obviously.
   * Init is called during HardLink instantiation, above.
   * 
   * These are intended for use by knowledgeable clients, not internally, 
   * because many of the internal methods are static and can't update these
   * per-instance counters.
   */
HardLink (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HardLink.java)/**
 * Class for creating hardlinks.
 * Supports Unix/Linux, Windows via winutils , and Mac OS X.
 * 
 * The HardLink class was formerly a static inner class of FSUtil,
 * and the methods provided were blatantly non-thread-safe.
 * To enable volume-parallel Update snapshots, we now provide static 
 * threadsafe methods that allocate new buffer string arrays
 * upon each call.  We also provide an API to hardlink all files in a
 * directory with a single command, which is up to 128 times more 
 * efficient - and minimizes the impact of the extra buffer creations.
 */
HarFsInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java)/**
     * Create an input stream that fakes all the reads/positions/seeking.
     */
HarFSDataInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java)/**
   * Hadoop archives input stream. This input stream fakes EOF 
   * since archive files are part of bigger part files.
   */
HasEnhancedByteBufferAccess (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HasEnhancedByteBufferAccess.java)/**
 * FSDataInputStreams implement this interface to provide enhanced
 * byte buffer access.  Usually this takes the form of mmap support.
 */
HasFileDescriptor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HasFileDescriptor.java)/**
 * Having a FileDescriptor
 */
HttpFileSystem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/http/HttpFileSystem.java)/**
 * A Filesystem that reads from HTTP endpoint.
 */
HttpsFileSystem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/http/HttpsFileSystem.java)/**
 * A Filesystem that reads from HTTPS endpoint.
 */
AbstractFSBuilderImpl (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java)/**
 * Builder for filesystem/filecontext operations of various kinds,
 * with option support.
 *
 * <code>
 *   .opt("foofs:option.a", true)
 *   .opt("foofs:option.b", "value")
 *   .opt("barfs:cache", true)
 *   .must("foofs:cache", true)
 *   .must("barfs:cache-size", 256 * 1024 * 1024)
 *   .build();
 * </code>
 *
 * Configuration keys declared in an {@code opt()} may be ignored by
 * a builder which does not recognise them.
 *
 * Configuration keys declared in a {@code must()} function set must
 * be understood by the implementation or a
 * {@link IllegalArgumentException} will be thrown.
 *
 * @param <S> Return type on the {@link #build()} call.
 * @param <B> type of builder itself.
 */
FsLinkResolutionFunction (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/impl/FsLinkResolution.java)/**
   * The signature of the function to invoke.
   * @param <T> type resolved to
   */
FsLinkResolution (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/impl/FsLinkResolution.java)/**
 * Class to allow Lambda expressions to be used in {@link FileContext}
 * link resolution.
 * @param <T> type of the returned value.
 */
FunctionRaisingIOE (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/impl/FunctionsRaisingIOE.java)/**
   * Function of arity 1 which may raise an IOException.
   * @param <T> type of arg1
   * @param <R> type of return value.
   */
BiFunctionRaisingIOE (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/impl/FunctionsRaisingIOE.java)/**
   * Function of arity 2 which may raise an IOException.
   * @param <T> type of arg1
   * @param <U> type of arg2
   * @param <R> type of return value.
   */
CallableRaisingIOE (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/impl/FunctionsRaisingIOE.java)/**
   * This is a callable which only raises an IOException.
   * @param <R> return type
   */
FunctionsRaisingIOE (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/impl/FunctionsRaisingIOE.java)/**
 * Evolving support for functional programming/lambda-expressions.
 */
FutureDataInputStreamBuilderImpl (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java)/**
 * Builder for input streams and subclasses whose return value is
 * actually a completable future: this allows for better asynchronous
 * operation.
 *
 * To be more generic, {@link #opt(String, int)} and {@link #must(String, int)}
 * variants provide implementation-agnostic way to customize the builder.
 * Each FS-specific builder implementation can interpret the FS-specific
 * options accordingly, for example:
 *
 * If the option is not related to the file system, the option will be ignored.
 * If the option is must, but not supported by the file system, a
 * {@link IllegalArgumentException} will be thrown.
 *
 */
FutureIOSupport (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/impl/FutureIOSupport.java)/**
 * Support for future IO and the FS Builder subclasses.
 */
WrappedIOException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/impl/WrappedIOException.java)/**
 * A wrapper for an IOException which
 * {@link FutureIOSupport#raiseInnerCause(ExecutionException)} knows to
 * always extract the exception.
 *
 * The constructor signature guarantees the cause will be an IOException,
 * and as it checks for a null-argument, non-null.
 */
InvalidPathException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/InvalidPathException.java)/**
 * Path string is invalid either because it has invalid characters or due to
 * other file system specific reasons.
 */
InvalidPathHandleException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/InvalidPathHandleException.java)/**
 * Thrown when the constraints enoded in a {@link PathHandle} do not hold.
 * For example, if a handle were created with the default
 * {@link Options.HandleOpt#path()} constraints, a call to
 * {@link FileSystem#open(PathHandle)} would succeed if the file were
 * modified, but if a different file was at that location then it would throw
 * this exception.
 */
InvalidRequestException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/InvalidRequestException.java)/**
 * Thrown when the user makes a malformed request, for example missing required
 * parameters or parameters that are not valid.
 */
LocalFs (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/local/LocalFs.java)/**
 * The LocalFs implementation of ChecksumFs.
 */
RawLocalFs (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/local/RawLocalFs.java)/**
 * The RawLocalFs implementation of AbstractFileSystem.
 *  This impl delegates to the old FileSystem
 */
LocalDirAllocator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java)/** An implementation of a round-robin scheme for disk allocation for creating
 * files. The way it works is that it is kept track what disk was last
 * allocated for a file write. For the current request, the next disk from
 * the set of disks would be allocated if the free space on the disk is 
 * sufficient enough to accommodate the file that is being considered for
 * creation. If the space requirements cannot be met, the next disk in order
 * would be tried and so on till a disk is found with sufficient capacity.
 * Once a disk with sufficient space is identified, a check is done to make
 * sure that the disk is writable. Also, there is an API provided that doesn't
 * take the space requirements into consideration but just checks whether the
 * disk under consideration is writable (this should be used for cases where
 * the file size is not known apriori). An API is provided to read a path that
 * was created earlier. That API works by doing a scan of all the disks for the
 * input pathname.
 * This implementation also provides the functionality of having multiple 
 * allocators per JVM (one for each unique functionality or context, like 
 * mapred, dfs-client, etc.). It ensures that there is only one instance of
 * an allocator per context per JVM.
 * Note:
 * 1. The contexts referred above are actually the configuration items defined
 * in the Configuration class like "mapred.local.dir" (for which we want to 
 * control the dir allocations). The context-strings are exactly those 
 * configuration items.
 * 2. This implementation does not take into consideration cases where
 * a disk becomes read-only or goes out of space while a file is being written
 * to (disks are shared between multiple processes, and so the latter situation
 * is probable).
 * 3. In the class implementation, "Disk" is referred to as "Dir", which
 * actually points to the configured directory on the Disk which will be the
 * parent for all file write/read allocations.
 */
LocalFileSystem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java)/****************************************************************
 * Implement the FileSystem API for the checksumed local filesystem.
 *
 *****************************************************************/
LocalFileSystemConfigKeys (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystemConfigKeys.java)/** 
 * This class contains constants for configuration keys used
 * in the local file system, raw local fs and checksum fs.
 *
 */
LocalFileSystemPathHandle (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystemPathHandle.java)/**
 * Opaque handle to an entity in a FileSystem.
 */
LocatedFileStatus (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocatedFileStatus.java)/**
 * This class defines a FileStatus that includes a file's block locations.
 */
MD5MD5CRC32CastagnoliFileChecksum (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java)/** For CRC32 with the Castagnoli polynomial */
MD5MD5CRC32FileChecksum (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java)/** MD5 of MD5 of CRC32. */
MD5MD5CRC32GzipFileChecksum (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java)/** For CRC32 with the Gzip polynomial */
MultipartUploader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MultipartUploader.java)/**
 * MultipartUploader is an interface for copying files multipart and across
 * multiple nodes. Users should:
 * <ol>
 *   <li>Initialize an upload.</li>
 *   <li>Upload parts in any order.</li>
 *   <li>Complete the upload in order to have it materialize in the destination
 *   FS.</li>
 * </ol>
 */
MultipartUploaderFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MultipartUploaderFactory.java)/**
 * {@link ServiceLoader}-driven uploader API for storage services supporting
 * multipart uploads.
 */
BytesPerChecksum (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Options.java)/** This is not needed if ChecksumParam is specified. **/
CreateOpts (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Options.java)/**
   * Class to support the varargs for create() options.
   *
   */
ChecksumOpt (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Options.java)/**
   * This is used in FileSystem and FileContext to specify checksum options.
   */
Data (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Options.java)/**
     * Option storing standard constraints on data.
     */
Location (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Options.java)/**
     * Option storing standard constraints on location.
     */
HandleOpt (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Options.java)/**
   * Options for creating {@link PathHandle} references.
   */
Options (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Options.java)/**
 * This class contains options related to file system operations.
 */
ParentNotDirectoryException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ParentNotDirectoryException.java)/**
 * Indicates that the parent of specified Path is not a directory
 * as expected.
 */
PartHandle (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/PartHandle.java)/**
 * Opaque, serializable reference to a part id for multipart uploads.
 */
Path (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Path.java)/**
 * Names a file or directory in a {@link FileSystem}.
 * Path strings use slash as the directory separator.
 */
PathAccessDeniedException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/PathAccessDeniedException.java)/** EACCES */
PathCapabilities (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/PathCapabilities.java)/**
 * The Path counterpoint to {@link StreamCapabilities}; a query to see if,
 * a FileSystem/FileContext instance has a specific capability under the given
 * path.
 * Other classes may also implement the interface, as desired.
 *
 * See {@link CommonPathCapabilities} for the well-known capabilities.
 */
PathExistsException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/PathExistsException.java)/**
 * Exception corresponding to File Exists - EEXISTS
 */
PathHandle (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/PathHandle.java)/**
 * Opaque, serializable reference to an entity in the FileSystem. May contain
 * metadata sufficient to resolve or verify subsequent accesses independent of
 * other modifications to the FileSystem.
 */
PathIOException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/PathIOException.java)/**
 * Exceptions based on standard posix/linux style exceptions for path related
 * errors. Returns an exception with the format "path: standard error string".
 * 
 * This exception corresponds to Error Input/ouput(EIO)
 */
PathIsDirectoryException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/PathIsDirectoryException.java)/** EISDIR */
PathIsNotDirectoryException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/PathIsNotDirectoryException.java)/** ENOTDIR */
PathIsNotEmptyDirectoryException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/PathIsNotEmptyDirectoryException.java)/** Generated by rm commands */
PathNotFoundException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/PathNotFoundException.java)/**
 * Exception corresponding to path not found: ENOENT/ENOFILE
 */
PathOperationException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/PathOperationException.java)/** ENOTSUP */
PathPermissionException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/PathPermissionException.java)/**
 * Exception corresponding to Operation Not Permitted - EPERM
 */
Builder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/AclEntry.java)/**
   * Builder for creating new AclEntry instances.
   */
AclEntry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/AclEntry.java)/**
 * Defines a single entry in an ACL.  An ACL entry has a type (user, group,
 * mask, or other), an optional name (referring to a specific user or group), a
 * set of permissions (any combination of read, write and execute), and a scope
 * (access or default).  AclEntry instances are immutable.  Use a {@link Builder}
 * to create a new instance.
 */
Builder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/AclStatus.java)/**
   * Builder for creating new Acl instances.
   */
AclStatus (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/AclStatus.java)/**
 * An AclStatus contains the ACL information of a specific file. AclStatus
 * instances are immutable. Use a {@link Builder} to create a new instance.
 */
AclUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/AclUtil.java)/**
 * AclUtil contains utility methods for manipulating ACLs.
 */
ChmodParser (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/ChmodParser.java)/**
 * Parse a permission mode passed in from a chmod command and apply that
 * mode against an existing file.
 */
FsCreateModes (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/FsCreateModes.java)/**
 * A class that stores both masked and unmasked create modes
 * and is a drop-in replacement for masked permission.
 */
FsPermission (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/FsPermission.java)/**
 * A class for file/directory permissions.
 */
PermissionParser (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/PermissionParser.java)/**
 * Base class for parsing either chmod permissions or umask permissions.
 * Includes common code needed by either operation as implemented in
 * UmaskParser and ChmodParser classes.
 */
PermissionStatus (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/PermissionStatus.java)/**
 * Store permission related information.
 */
ScopedAclEntries (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/ScopedAclEntries.java)/**
 * Groups a list of ACL entries into separate lists for access entries vs.
 * default entries.
 */
UmaskParser (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/UmaskParser.java)/**
 * Parse umask value provided as a string, either in octal or symbolic
 * format and return it as a short value. Umask values are slightly
 * different from standard modes as they cannot specify sticky bit
 * or X.
 *
 */
PositionedReadable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/PositionedReadable.java)/**
 * Stream that permits positional reading.
 *
 * Implementations are required to implement thread-safe operations; this may
 * be supported by concurrent access to the data, or by using a synchronization
 * mechanism to serialize access.
 *
 * Not all implementations meet this requirement. Those that do not cannot
 * be used as a backing store for some applications, such as Apache HBase.
 *
 * Independent of whether or not they are thread safe, some implementations
 * may make the intermediate state of the system, specifically the position
 * obtained in {@code Seekable.getPos()} visible.
 */
PBHelper (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/protocolPB/PBHelper.java)/**
 * Utility methods aiding conversion of fs data structures.
 */
Builder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/QuotaUsage.java)/** Builder class for QuotaUsage. */
QuotaUsage (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/QuotaUsage.java)/** Store the quota usage of a directory. */
LocalFSFileInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java)/*******************************************************
   * For open()'s FSInputStream.
   *******************************************************/
LocalFSFileOutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java)/*********************************************************
   * For create()'s FSOutputStream.
   *********************************************************/
RawLocalFileSystem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java)/****************************************************************
 * Implement the FileSystem API for the raw local filesystem.
 *
 *****************************************************************/
RawPathHandle (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawPathHandle.java)/**
 * Generic format of FileStatus objects. When the origin is unknown, the
 * attributes of the handle are undefined.
 */
RemoteIterator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RemoteIterator.java)/**
 * An iterator over a collection whose elements need to be fetched remotely
 */
Seekable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Seekable.java)/**
 *  Stream that permits seeking.
 */
ConnectionInfo (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java)/**
   * Class to capture the minimal set of information that distinguish
   * between different connections.
   */
SFTPConnectionPool (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java)/** Concurrent/Multiple Connections. */
SFTPFileSystem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java)/** SFTP FileSystem. */
SFTPInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java)/** SFTP FileSystem input stream. */
GetfaclCommand (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/AclCommands.java)/**
   * Implementing the '-getfacl' command for the the FsShell.
   */
SetfaclCommand (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/AclCommands.java)/**
   * Implementing the '-setfacl' command for the the FsShell.
   */
AclCommands (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/AclCommands.java)/**
 * Acl related operations
 */
Command (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java)/**
 * An abstract class for the execution of a file system command
 */
IllegalNumberOfArgumentsException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandFormat.java)/** Used when the arguments exceed their bounds 
   */
TooManyArgumentsException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandFormat.java)/** Used when too many arguments are supplied to a command
   */
NotEnoughArgumentsException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandFormat.java)/** Used when too few arguments are supplied to a command
   */
UnknownOptionException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandFormat.java)/** Used when an unsupported option is supplied to a command
   */
DuplicatedOptionException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandFormat.java)/**
   * Used when a duplicated option is supplied to a command.
   */
CommandFormat (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandFormat.java)/**
 * Parse the args of a command and check the format of args.
 */
CommandWithDestination (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java)/**
 * Provides: argument processing to ensure the destination is valid
 * for the number of source arguments.  A processPaths that accepts both
 * a source and resolved target.  Sources are resolved as children of
 * a destination directory.
 */
Merge (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java)/** merge multiple files together */
Get (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java)/** 
   * Copy local files to a remote filesystem
   */
Put (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java)/**
   *  Copy local files to a remote filesystem
   */
AppendToFile (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java)/**
   *  Append the contents of one or more local files to a remote
   *  file.
   */
CopyCommands (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java)/** Various commands for copy files */
Count (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Count.java)/**
 * Count the number of directories, files, bytes, quota, and remaining quota.
 */
Rm (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Delete.java)/** remove non-directory paths */
Rmr (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Delete.java)/** remove any path */
Rmdir (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Delete.java)/** remove only empty directories */
Delete (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Delete.java)/**
 * Classes that delete paths.
 */
Cat (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java)/**
   * Displays file content to stdout
   */
Text (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java)/**
   * Same behavior as "-cat", but handles zip and TextRecordInputStream
   * and Avro encodings. 
   */
AvroFileInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java)/**
   * This class transforms a binary Avro data file into an InputStream
   * with data that is in a human readable JSON format.
   */
Display (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java)/**
 * Display contents or checksums of files 
 */
And (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/And.java)/**
 * Implements the -a (and) operator for the
 * {@link org.apache.hadoop.fs.shell.find.Find} command.
 */
BaseExpression (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/BaseExpression.java)/**
 * Abstract expression for use in the
 * {@link org.apache.hadoop.fs.shell.find.Find} command. Provides default
 * behavior for a no-argument primary expression.
 */
Expression (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/Expression.java)/**
 * Interface describing an expression to be used in the
 * {@link org.apache.hadoop.fs.shell.find.Find} command.
 */
ExpressionFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java)/**
 * Factory class for registering and searching for expressions for use in the
 * {@link org.apache.hadoop.fs.shell.find.Find} command.
 */
FilterExpression (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/FilterExpression.java)/**
 * Provides an abstract composition filter for the {@link Expression} interface.
 * Allows other {@link Expression} implementations to be reused without
 * inheritance.
 */
FindOptions (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/FindOptions.java)/**
 * Options to be used by the {@link Find} command and its {@link Expression}s.
 */
Iname (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/Name.java)/** Case insensitive version of the -name expression. */
Name (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/Name.java)/**
 * Implements the -name expression for the
 * {@link org.apache.hadoop.fs.shell.find.Find} command.
 */
Print0 (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/Print.java)/** Implements the -print0 expression. */
Print (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/Print.java)/**
 * Implements the -print expression for the
 * {@link org.apache.hadoop.fs.shell.find.Find} command.
 */
Df (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/FsUsage.java)/** Show the size of a partition in the filesystem */
Du (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/FsUsage.java)/** show disk usage */
Dus (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/FsUsage.java)/** show disk usage summary */
TableBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/FsUsage.java)/**
   * Creates a table of aligned values based on the maximum width of each
   * column as a string
   */
FsUsage (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/FsUsage.java)/**
 * Base class for commands related to viewing filesystem usage,
 * such as du and df.
 */
Head (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Head.java)/**
 * Show the first 1KB of the file.
 */
Lsr (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Ls.java)/**
   * Get a recursive listing of all files in that match the file patterns.
   * Same as "-ls -R"
   */
Ls (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Ls.java)/**
 * Get a listing of all files in that match the file patterns.
 */
Mkdir (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Mkdir.java)/**
 * Create the given dir
 */
MoveFromLocal (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/MoveCommands.java)/**
   *  Move local files to a remote filesystem
   */
MoveToLocal (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/MoveCommands.java)/**
   *  Move remote files to a local filesystem
   */
Rename (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/MoveCommands.java)/** move/rename paths on the same filesystem */
MoveCommands (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/MoveCommands.java)/** Various commands for moving files */
PathData (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java)/**
 * Encapsulates a Path (path), its FileStatus (stat), and its FileSystem (fs).
 * PathData ensures that the returned path string will be the same as the
 * one passed in during initialization (unlike Path objects which can
 * modify the path string).
 * The stat field will be null if the path does not exist.
 */
PrintableString (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PrintableString.java)/**
 * The {code PrintableString} class converts any string to a printable string
 * by replacing non-printable characters with ?.
 *
 * Categories of Unicode non-printable characters:
 * <ul>
 * <li> Control characters   (Cc)
 * <li> Formatting Unicode   (Cf)
 * <li> Private use Unicode  (Co)
 * <li> Unassigned Unicode   (Cn)
 * <li> Standalone surrogate (Unfortunately no matching Unicode category)
 * </ul>
 *
 * @see Character
 * @see <a href="http://www.unicode.org/">The Unicode Consortium</a>
 */
SetReplication (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/SetReplication.java)/**
 * Modifies the replication factor
 */
CreateSnapshot (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/SnapshotCommands.java)/**
   *  Create a snapshot
   */
DeleteSnapshot (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/SnapshotCommands.java)/**
   * Delete a snapshot
   */
RenameSnapshot (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/SnapshotCommands.java)/**
   * Rename a snapshot
   */
SnapshotCommands (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/SnapshotCommands.java)/**
 * Snapshot related operations
 */
Stat (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Stat.java)/**
 * Print statistics about path in specified format.
 * Format sequences:<br>
 *   %a: Permissions in octal<br>
 *   %A: Permissions in symbolic style<br>
 *   %b: Size of file in bytes<br>
 *   %F: Type<br>
 *   %g: Group name of owner<br>
 *   %n: Filename<br>
 *   %o: Block size<br>
 *   %r: replication<br>
 *   %u: User name of owner<br>
 *   %x: atime UTC date as &quot;yyyy-MM-dd HH:mm:ss&quot;<br>
 *   %X: atime Milliseconds since January 1, 1970 UTC<br>
 *   %y: mtime UTC date as &quot;yyyy-MM-dd HH:mm:ss&quot;<br>
 *   %Y: mtime Milliseconds since January 1, 1970 UTC<br>
 * If the format is not specified, %y is used by default.
 */
Tail (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Tail.java)/**
 * Get a listing of all files in that match the file patterns.
 */
Test (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Test.java)/**
 * Perform shell-like file tests 
 */
Touchz (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/TouchCommands.java)/**
   * (Re)create zero-length file at the specified path.
   * This will be replaced by a more UNIX-like touch when files may be
   * modified.
   */
Touch (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/TouchCommands.java)/**
   * A UNIX like touch command.
   */
TouchCommands (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/TouchCommands.java)/**
 * Unix touch like commands
 */
Truncate (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Truncate.java)/**
 * Truncates a file to a new size
 */
GetfattrCommand (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/XAttrCommands.java)/**
   * Implements the '-getfattr' command for the FsShell.
   */
SetfattrCommand (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/XAttrCommands.java)/**
   * Implements the '-setfattr' command for the FsShell.
   */
XAttrCommands (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/XAttrCommands.java)/**
 * XAttr related operations
 */
Stat (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Stat.java)/**
 * Wrapper for the Unix stat(1) command. Used to workaround the lack of 
 * lstat(2) in Java 6.
 */
CommonStatisticNames (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageStatistics.java)/**
   * These are common statistic names.
   *
   * The following names are considered general and preserved across different
   * StorageStatistics classes. When implementing a new StorageStatistics, it is
   * highly recommended to use the common statistic names.
   *
   * When adding new common statistic name constants, please make them unique.
   * By convention, they are implicitly unique:
   * <ul>
   *   <li>the name of the constants are uppercase, words separated by
   *   underscores.</li>
   *   <li>the value of the constants are lowercase of the constant names.</li>
   * </ul>
   */
LongStatistic (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageStatistics.java)/**
   * A 64-bit storage statistic.
   */
StorageStatistics (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageStatistics.java)/**
 * StorageStatistics contains statistics data for a FileSystem or FileContext
 * instance.
 */
EtagChecksum (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/store/EtagChecksum.java)/**
 * An etag as a checksum.
 * Consider these suitable for checking if an object has changed, but
 * not suitable for comparing two different objects for equivalence,
 * especially between object stores.
 */
StreamCapabilities (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StreamCapabilities.java)/**
 * Interface to query streams for supported capabilities.
 *
 * Capability strings must be in lower case.
 *
 * Constant strings are chosen over enums in order to allow other file systems
 * to define their own capabilities.
 */
StreamCapabilitiesPolicy (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StreamCapabilitiesPolicy.java)/**
 * Static methods to implement policies for {@link StreamCapabilities}.
 */
Syncable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Syncable.java)/** This interface for flush/sync operation. */
Trash (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/Trash.java)/** 
 * Provides a trash facility which supports pluggable Trash policies. 
 *
 * See the implementation of the configured TrashPolicy for more
 * details.
 */
TrashPolicy (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/TrashPolicy.java)/** 
 * This interface is used for implementing different Trash policies.
 * Provides factory method to create instances of the configured Trash policy.
 */
TrashPolicyDefault (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/TrashPolicyDefault.java)/** Provides a <i>trash</i> feature.  Files are moved to a user's trash
 * directory, a subdirectory of their home directory named ".Trash".  Files are
 * initially moved to a <i>current</i> sub-directory of the trash directory.
 * Within that sub-directory their original path is preserved.  Periodically
 * one may checkpoint the current trash and remove older checkpoints.  (This
 * design permits trash management without enumeration of the full trash
 * content, without date support in the filesystem, and without clock
 * synchronization.)
 */
UnionStorageStatistics (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/UnionStorageStatistics.java)/**
 * A StorageStatistics instance which combines the outputs of several other
 * StorageStatistics instances.
 */
UnresolvedLinkException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/UnresolvedLinkException.java)/** 
 * Thrown when a symbolic link is encountered in a path.
 */
UnsupportedFileSystemException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/UnsupportedFileSystemException.java)/**
 * File system for a given file system name/scheme is not supported
 */
UnsupportedMultipartUploaderException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/UnsupportedMultipartUploaderException.java)/**
 * MultipartUploader for a given file system name/scheme is not supported.
 */
UploadHandle (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/UploadHandle.java)/**
 * Opaque, serializable reference to an uploadId for multipart uploads.
 */
ChRootedFs (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java)/**
 * <code>ChrootedFs</code> is a file system with its root some path
 * below the root of its base file system.
 * Example: For a base file system hdfs://nn1/ with chRoot at /usr/foo, the
 * members will be setup as shown below.
 * <ul>
 * <li>myFs is the base file system and points to hdfs at nn1</li>
 * <li>myURI is hdfs://nn1/user/foo</li>
 * <li>chRootPathPart is /user/foo</li>
 * <li>workingDir is a directory related to chRoot</li>
 * </ul>
 * 
 * The paths are resolved as follows by ChRootedFileSystem:
 * <ul>
 * <li> Absolute path /a/b/c is resolved to /user/foo/a/b/c at myFs</li>
 * <li> Relative path x/y is resolved to /user/foo/<workingDir>/x/y</li>
 * </ul>

 * 
 */
ConfigUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java)/**
 * Utilities for config variables of the viewFs See {@link ViewFs}
 */
Constants (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/Constants.java)/**
 * Config variable prefixes for ViewFs -
 *     see {@link org.apache.hadoop.fs.viewfs.ViewFs} for examples.
 * The mount table is specified in the config using these prefixes.
 * See {@link org.apache.hadoop.fs.viewfs.ConfigUtil} for convenience lib.
 */
INode (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java)/**
   * Internal class for INode tree.
   * @param <T>
   */
INodeDir (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java)/**
   * Internal class to represent an internal dir of the mount table.
   * @param <T>
   */
INodeLink (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java)/**
   * An internal class to represent a mount link.
   * A mount link can be single dir link or a merge dir link.

   * A merge dir link is  a merge (junction) of links to dirs:
   * example : merge of 2 dirs
   *     /users -> hdfs:nn1//users
   *     /users -> hdfs:nn2//users
   *
   * For a merge, each target is checked to be dir when created but if target
   * is changed later it is then ignored (a dir with null entries)
   */
LinkEntry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java)/**
   * An internal class representing the ViewFileSystem mount table
   * link entries and their attributes.
   * @see LinkType
   */
ResolveResult (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/InodeTree.java)/**
   * Resolve returns ResolveResult.
   * The caller can continue the resolution of the remainingPath
   * in the targetFileSystem.
   *
   * If the input pathname leads to link to another file system then
   * the targetFileSystem is the one denoted by the link (except it is
   * file system chrooted to link target.
   * If the input pathname leads to an internal mount-table entry then
   * the target file system is one that represents the internal inode.
   */
NflyNode (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java)/**
   * URI's authority is used as an approximation of the distance from the
   * client. It's sufficient for DC but not accurate because worker nodes can be
   * closer.
   */
NflyOutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java)/**
   * Transactional output stream. When creating path /dir/file
   * 1) create invisible /real/dir_i/_nfly_tmp_file
   * 2) when more than min replication was written, write is committed by
   *   renaming all successfully written files to /real/dir_i/file
   */
NflyStatus (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java)/**
   * // TODO
   * Some file status implementations have expensive deserialization or metadata
   * retrieval. This probably does not go beyond RawLocalFileSystem. Wrapping
   * the the real file status to preserve this behavior. Otherwise, calling
   * realStatus getters in constructor defeats this design.
   */
NflyFSystem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java)/**
 * Nfly is a multi filesystem mount point.
 */
NotInMountpointException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/NotInMountpointException.java)/**
 * NotInMountpointException extends the UnsupportedOperationException.
 * Exception class used in cases where the given path is not mounted 
 * through viewfs.
 */
Key (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java)/**
     * All the cached instances share the same UGI so there is no need to have a
     * URI in the Key. Make the Key simple with just the scheme and authority.
     */
InnerCache (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java)/**
   * Caching children filesystems. HADOOP-15565.
   */
MountPoint (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java)/**
   * MountPoint representation built from the configuration.
   */
InternalDirOfViewFs (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java)/**
   * An instance of this class represents an internal dir of the viewFs
   * that is internal dir of the mount table.
   * It is a read only mount tables and create, mkdir or delete operations
   * are not allowed.
   * If called on create or mkdir then this target is the parent of the
   * directory in which one is trying to create or mkdir; hence
   * in this case the path name passed in is the last component. 
   * Otherwise this target is the end point of the path and hence
   * the path name passed in is null. 
   */
ViewFileSystemUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java)/**
 * Utility APIs for ViewFileSystem.
 */
WrappingRemoteIterator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java)/**
   * Helper class to perform some transformation on results returned
   * from a RemoteIterator.
   */
ViewFsFileStatus (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java)/**
 * This class is needed to address the  problem described in
 * {@link ViewFileSystem#getFileStatus(org.apache.hadoop.fs.Path)} and
 * {@link ViewFs#getFileStatus(org.apache.hadoop.fs.Path)}
 */
WindowsGetSpaceUsed (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/WindowsGetSpaceUsed.java)/**
 * Class to tell the size of a path on windows.
 * Rather than shelling out, on windows this uses DUHelper.getFolderUsage
 */
ActiveStandbyElectorCallback (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java)/**
   * Callback interface to interact with the ActiveStandbyElector object. <br>
   * The application will be notified with a callback only on state changes
   * (i.e. there will never be successive calls to becomeActive without an
   * intermediate call to enterNeutralMode). <br>
   * The callbacks will be running on Zookeeper client library threads. The
   * application should return from these callbacks quickly so as not to impede
   * Zookeeper client library performance and notifications. The app will
   * typically remember the state change and return from the callback. It will
   * then proceed with implementing actions around that state change. It is
   * possible to be called back again while these actions are in flight and the
   * app should handle this scenario.
   */
ActiveNotFoundException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java)/**
   * Exception thrown when there is no active leader
   */
WatcherWithClientRef (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java)/**
   * Watcher implementation which keeps a reference around to the
   * original ZK connection, and passes it back along with any
   * events.
   */
ActiveStandbyElector (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java)/**
 * 
 * This class implements a simple library to perform leader election on top of
 * Apache Zookeeper. Using Zookeeper as a coordination service, leader election
 * can be performed by atomically creating an ephemeral lock file (znode) on
 * Zookeeper. The service instance that successfully creates the znode becomes
 * active and the rest become standbys. <br>
 * This election mechanism is only efficient for small number of election
 * candidates (order of 10's) because contention on single znode by a large
 * number of candidates can result in Zookeeper overload. <br>
 * The elector does not guarantee fencing (protection of shared resources) among
 * service instances. After it has notified an instance about becoming a leader,
 * then that instance must ensure that it meets the service consistency
 * requirements. If it cannot do so, then it is recommended to quit the
 * election. The application implements the {@link ActiveStandbyElectorCallback}
 * to interact with the elector
 */
BadFencingConfigurationException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java)/**
 * Indicates that the operator has specified an invalid configuration
 * for fencing methods.
 */
FailoverController (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java)/**
 * The FailOverController is responsible for electing an active service
 * on startup or when the current active is changing (eg due to failure),
 * monitoring the health of a service, and performing a fail-over when a
 * new active service is either manually selected by a user or elected.
 */
FailoverFailedException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverFailedException.java)/**
 * Exception thrown to indicate service failover has failed.
 */
FenceMethod (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java)/**
 * A fencing method is a method by which one node can forcibly prevent
 * another node from making continued progress. This might be implemented
 * by killing a process on the other node, by denying the other node's
 * access to shared storage, or by accessing a PDU to cut the other node's
 * power.
 * <p>
 * Since these methods are often vendor- or device-specific, operators
 * may implement this interface in order to achieve fencing.
 * <p>
 * Fencing is configured by the operator as an ordered list of methods to
 * attempt. Each method will be tried in turn, and the next in the list
 * will only be attempted if the previous one fails. See {@link NodeFencer}
 * for more information.
 * <p>
 * If an implementation also implements {@link Configurable} then its
 * <code>setConf</code> method will be called upon instantiation.
 */
UsageInfo (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java)/**
   * UsageInfo class holds args and help details.
   */
HAAdmin (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java)/**
 * A command-line tool for making calls in the HAServiceProtocol.
 * For example,. this can be used to force a service to standby or active
 * mode, or to trigger a health-check.
 */
StateChangeRequestInfo (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceProtocol.java)/**
   * Information describing the source for a request to change state.
   * This is used to differentiate requests from automatic vs CLI
   * failover controllers, and in the future may include epoch
   * information.
   */
HAServiceProtocol (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceProtocol.java)/**
 * Protocol interface that provides High Availability related primitives to
 * monitor and fail-over the service.
 * 
 * This interface could be used by HA frameworks to manage the service.
 */
HAServiceProtocolHelper (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java)/**
 * Helper for making {@link HAServiceProtocol} RPC calls. This helper
 * unwraps the {@link RemoteException} to specific exceptions.
 */
HAServiceTarget (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java)/**
 * Represents a target of the client side HA administration commands.
 */
HealthCheckFailedException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HealthCheckFailedException.java)/**
 * Exception thrown to indicate that health check of a service failed.
 */
Callback (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HealthMonitor.java)/**
   * Callback interface for state change events.
   * 
   * This interface is called from a single thread which also performs
   * the health monitoring. If the callback processing takes a long time,
   * no further health checks will be made during this period, nor will
   * other registered callbacks be called.
   * 
   * If the callback itself throws an unchecked exception, no other
   * callbacks following it will be called, and the health monitor
   * will terminate, entering HEALTH_MONITOR_FAILED state.
   */
ServiceStateCallback (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HealthMonitor.java)/**
   * Callback interface for service states.
   */
HealthMonitor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HealthMonitor.java)/**
 * This class is a daemon which runs in a loop, periodically heartbeating
 * with an HA service. It is responsible for keeping track of that service's
 * health and exposing callbacks to the failover controller when the health
 * status changes.
 * 
 * Classes which need callbacks should implement the {@link Callback}
 * interface.
 */
NodeFencer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java)/**
 * This class parses the configured list of fencing methods, and
 * is responsible for trying each one in turn while logging informative
 * output.<p>
 * 
 * The fencing methods are configured as a carriage-return separated list.
 * Each line in the list is of the form:<p>
 * <code>com.example.foo.MyMethod(arg string)</code>
 * or
 * <code>com.example.foo.MyMethod</code>
 * The class provided must implement the {@link FenceMethod} interface.
 * The fencing methods that ship with Hadoop may also be referred to
 * by shortened names:<br>
 * <ul>
 * <li><code>shell(/path/to/some/script.sh args...)</code></li>
 * <li><code>sshfence(...)</code> (see {@link SshFenceByTcpPort})
 * </ul>
 */
PowerShellFencer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/PowerShellFencer.java)/**
 * Fencer method that uses PowerShell to remotely connect to a machine and kill
 * the required process. This only works in Windows.
 *
 * The argument passed to this fencer should be a unique string in the
 * "CommandLine" attribute for the "java.exe" process. For example, the full
 * path for the Namenode: "org.apache.hadoop.hdfs.server.namenode.NameNode".
 * The administrator can also shorten the name to "Namenode" if it's unique.
 */
HAServiceProtocolClientSideTranslatorPB (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java)/**
 * This class is the client side translator to translate the requests made on
 * {@link HAServiceProtocol} interfaces to the RPC server implementing
 * {@link HAServiceProtocolPB}.
 */
HAServiceProtocolServerSideTranslatorPB (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java)/**
 * This class is used on the server side. Calls come across the wire for the
 * for protocol {@link HAServiceProtocolPB}.
 * This class translates the PB data types
 * to the native data types used inside the NN as specified in the generic
 * ClientProtocol.
 */
ServiceFailedException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ServiceFailedException.java)/**
 * Exception thrown to indicate that an operation performed
 * to modify the state of a service or application failed.
 */
ShellCommandFencer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java)/**
 * Fencing method that runs a shell command. It should be specified
 * in the fencing configuration like:<br>
 * <code>
 *   shell(/path/to/my/script.sh arg1 arg2 ...)
 * </code><br>
 * The string between '(' and ')' is passed directly to a bash shell
 * (cmd.exe on Windows) and may not include any closing parentheses.<p>
 * 
 * The shell command will be run with an environment set up to contain
 * all of the current Hadoop configuration variables, with the '_' character 
 * replacing any '.' characters in the configuration keys.<p>
 * 
 * If the shell command returns an exit code of 0, the fencing is
 * determined to be successful. If it returns any other exit code, the
 * fencing was not successful and the next fencing method in the list
 * will be attempted.<p>
 * 
 * <em>Note:</em> this fencing method does not implement any timeout.
 * If timeouts are necessary, they should be implemented in the shell
 * script itself (eg by forking a subshell to kill its parent in
 * some number of seconds).
 */
Args (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java)/**
   * Container for the parsed arg line for this fencing method.
   */
LogAdapter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java)/**
   * Adapter from JSch's logger interface to our log4j
   */
SshFenceByTcpPort (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java)/**
 * This fencing implementation sshes to the target node and uses 
 * <code>fuser</code> to kill the process listening on the service's
 * TCP port. This is more accurate than using "jps" since it doesn't 
 * require parsing, and will work even if there are multiple service
 * processes running on the same machine.<p>
 * It returns a successful status code if:
 * <ul>
 * <li><code>fuser</code> indicates it successfully killed a process, <em>or</em>
 * <li><code>nc -z</code> indicates that nothing is listening on the target port
 * </ul>
 * <p>
 * This fencing mechanism is configured as following in the fencing method
 * list:
 * <code>sshfence([[username][:ssh-port]])</code>
 * where the optional argument specifies the username and port to use
 * with ssh.
 * <p>
 * In order to achieve passwordless SSH, the operator must also configure
 * <code>dfs.ha.fencing.ssh.private-key-files</code> to point to an
 * SSH key that has passphrase-less access to the given username and host.
 */
StreamPumper (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/StreamPumper.java)/**
 * Class responsible for pumping the streams of the subprocess
 * out to log4j. stderr is pumped to WARN level and stdout is
 * pumped to INFO level
 */
ElectorCallbacks (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java)/**
   * Callbacks from elector
   */
HealthCallbacks (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java)/**
   * Callbacks from HealthMonitor
   */
ServiceStateCallBacks (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java)/**
   * Callbacks for HAServiceStatus
   */
ZKFCProtocol (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFCProtocol.java)/**
 * Protocol exposed by the ZKFailoverController, allowing for graceful
 * failover.
 */
HadoopIllegalArgumentException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/HadoopIllegalArgumentException.java)/**
 * Indicates that a method has been passed illegal or invalid argument. This
 * exception is thrown instead of IllegalArgumentException to differentiate the
 * exception thrown in Hadoop implementation from the one thrown in JDK.
 */
AdminAuthorizedServlet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/AdminAuthorizedServlet.java)/**
 * General servlet which is admin-authorized.
 *
 */
FilterContainer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/FilterContainer.java)/**
 * A container class for javax.servlet.Filter. 
 */
FilterInitializer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/FilterInitializer.java)/**
 * Initialize a javax.servlet.Filter. 
 */
HtmlQuoting (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HtmlQuoting.java)/**
 * This class is responsible for quoting HTML characters.
 */
HttpConfig (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpConfig.java)/**
 * Singleton to get access to Http related configuration.
 */
HttpRequestLog (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpRequestLog.java)/**
 * RequestLog object for use with Http
 */
HttpRequestLogAppender (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpRequestLogAppender.java)/**
 * Log4j Appender adapter for HttpRequestLog
 */
Builder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java)/**
   * Class to construct instances of HTTP server with specific options.
   */
StackServlet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java)/**
   * A very simple servlet to serve up a text representation of the current
   * stack traces. It both returns the stacks to the caller and logs them.
   * Currently the stack traces are done sequentially rather than exactly the
   * same data.
   */
QuotingInputFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java)/**
   * A Servlet input filter that quotes all HTML active characters in the
   * parameter names and values. The goal is to quote the characters to make
   * all of the servlets resistant to cross-site scripting attacks. It also
   * sets X-FRAME-OPTIONS in the header to mitigate clickjacking attacks.
   */
HttpServer2 (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java)/**
 * Create a Jetty embedded server to answer http requests. The primary goal is
 * to serve up status information for the server. There are three contexts:
 * "/logs/" {@literal ->} points to the log directory "/static/" {@literal ->}
 * points to common static files (src/webapps/static) "/" {@literal ->} the
 * jsp server code from (src/webapps/{@literal <}name{@literal >})
 *
 * This class is a fork of the old HttpServer. HttpServer exists for
 * compatibility reasons. See HBASE-10336 for more details.
 */
IsActiveServlet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/IsActiveServlet.java)/**
 * Used by Load Balancers to detect the active NameNode/ResourceManager/Router.
 */
JettyUtils (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/JettyUtils.java)/**
 * Contains utility methods and constants relating to Jetty.
 */
StaticUserWebFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java)/**
 * Provides a servlet filter that pretends to authenticate a fake user (Dr.Who)
 * so that the web UI is usable for a secure cluster without authentication.
 */
PrometheusServlet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/PrometheusServlet.java)/**
 * Servlet to publish hadoop metrics in prometheus format.
 */
WebServlet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/WebServlet.java)/**
 * Hadoop DefaultServlet for serving static web content.
 */
AbstractMapWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/AbstractMapWritable.java)/**
 * Abstract base class for MapWritable and SortedMapWritable
 * 
 * Unlike org.apache.nutch.crawl.MapWritable, this class allows creation of
 * MapWritable&lt;Writable, MapWritable&gt; so the CLASS_TO_ID and ID_TO_CLASS
 * maps travel with the class instead of being static.
 * 
 * Class ids range from 1 to 127 so there can be at most 127 distinct classes
 * in any specific map instance.
 */
Writer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ArrayFile.java)/** Write a new array file. */
Reader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ArrayFile.java)/** Provide access to an existing array file. */
ArrayFile (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ArrayFile.java)/** A dense file-based mapping from integers to values. */
Internal (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java)/**
   * Do not use this class.
   * This is an internal class, purely for ObjectWritable to use as
   * a label class for transparent conversions of arrays of primitives
   * during wire protocol reads and writes.
   */
ArrayPrimitiveWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java)/**
 * This is a wrapper class.  It wraps a Writable implementation around
 * an array of primitives (e.g., int[], long[], etc.), with optimized 
 * wire format, and without creating new objects per element.
 * 
 * This is a wrapper class only; it does not make a copy of the 
 * underlying array.
 */
ArrayWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ArrayWritable.java)/** 
 * A Writable for arrays containing instances of a class. The elements of this
 * writable must all be instances of the same class. If this writable will be
 * the input for a Reducer, you will need to create a subclass that sets the
 * value to be of the proper type.
 *
 * For example:
 * <code>
 * public class IntArrayWritable extends ArrayWritable {
 *   public IntArrayWritable() { 
 *     super(IntWritable.class); 
 *   }	
 * }
 * </code>
 */
BinaryComparable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/BinaryComparable.java)/**
 * Interface supported by {@link org.apache.hadoop.io.WritableComparable}
 * types supporting ordering/permutation by a representative set of bytes.
 */
BloomMapFile (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/BloomMapFile.java)/**
 * This class extends {@link MapFile} and provides very much the same
 * functionality. However, it uses dynamic Bloom filters to provide
 * quick membership test for keys, and it offers a fast version of 
 * {@link Reader#get(WritableComparable, Writable)} operation, especially in
 * case of sparsely populated MapFile-s.
 */
Comparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/BooleanWritable.java)/** 
   * A Comparator optimized for BooleanWritable. 
   */
BooleanWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/BooleanWritable.java)/** 
 * A WritableComparable for booleans. 
 */
BoundedByteArrayOutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java)/**
 * A byte array backed output stream with a limit. The limit should be smaller
 * than the buffer capacity. The object can be reused through <code>reset</code>
 * API and choose different limits in each round.
 */
Comparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/BytesWritable.java)/** A Comparator optimized for BytesWritable. */
BytesWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/BytesWritable.java)/** 
 * A byte sequence that is usable as a key or value.
 * It is resizable and distinguishes between the size of the sequence and
 * the current capacity. The hash function is the front of the md5 of the 
 * buffer. The sort order is the same as memcmp.
 */
Comparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ByteWritable.java)/** A Comparator optimized for ByteWritable. */
ByteWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ByteWritable.java)/** A WritableComparable for a single byte. */
Closeable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Closeable.java)/** @deprecated use java.io.Closeable */
BlockCompressorStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockCompressorStream.java)/**
 * A {@link org.apache.hadoop.io.compress.CompressorStream} which works
 * with 'block-based' based compression algorithms, as opposed to 
 * 'stream-based' compression algorithms.
 *
 * It should be noted that this wrapper does not guarantee that blocks will
 * be sized for the compressor. If the
 * {@link org.apache.hadoop.io.compress.Compressor} requires buffering to
 * effect meaningful compression, it is responsible for it.
 */
BlockDecompressorStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java)/**
 * A {@link org.apache.hadoop.io.compress.DecompressorStream} which works
 * with 'block-based' based compression algorithms, as opposed to 
 * 'stream-based' compression algorithms.
 *  
 */
Bzip2Compressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java)/**
 * A {@link Compressor} based on the popular 
 * bzip2 compression algorithm.
 * http://www.bzip2.org/
 * 
 */
BZip2Constants (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/bzip2/BZip2Constants.java)/**
 * Base class for both the compress and decompress classes. Holds common arrays,
 * and static data.
 * <p>
 * This interface is public for historical purposes. You should have no need to
 * use it.
 * </p>
 */
Bzip2Decompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java)/**
 * A {@link Decompressor} based on the popular 
 * bzip2 compression algorithm.
 * http://www.bzip2.org/
 * 
 */
BZip2DummyCompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/bzip2/BZip2DummyCompressor.java)/**
 * This is a dummy compressor for BZip2.
 */
BZip2DummyDecompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/bzip2/BZip2DummyDecompressor.java)/**
 * This is a dummy decompressor for BZip2.
 */
Bzip2Factory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java)/**
 * A collection of factories to create the right 
 * bzip2 compressor/decompressor instances.
 * 
 */
CBZip2InputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java)/**
 * An input stream that decompresses from the BZip2 format (without the file
 * header chars) to be read as any other stream.
 *
 * <p>
 * The decompression requires large amounts of memory. Thus you should call the
 * {@link #close() close()} method as soon as possible, to force
 * <tt>CBZip2InputStream</tt> to release the allocated memory. See
 * {@link CBZip2OutputStream CBZip2OutputStream} for information about memory
 * usage.
 * </p>
 *
 * <p>
 * <tt>CBZip2InputStream</tt> reads bytes from the compressed source stream via
 * the single byte {@link java.io.InputStream#read() read()} method exclusively.
 * Thus you should consider to use a buffered source stream.
 * </p>
 *
 * <p>
 * This Ant code was enhanced so that it can de-compress blocks of bzip2 data.
 * Current position in the stream is an important statistic for Hadoop. For
 * example in LineRecordReader, we solely depend on the current position in the
 * stream to know about the progress. The notion of position becomes complicated
 * for compressed files. The Hadoop splitting is done in terms of compressed
 * file. But a compressed file deflates to a large amount of data. So we have
 * handled this problem in the following way.
 *
 * On object creation time, we find the next block start delimiter. Once such a
 * marker is found, the stream stops there (we discard any read compressed data
 * in this process) and the position is reported as the beginning of the block
 * start delimiter. At this point we are ready for actual reading
 * (i.e. decompression) of data.
 *
 * The subsequent read calls give out data. The position is updated when the
 * caller of this class has read off the current block + 1 bytes. In between the
 * block reading, position is not updated. (We can only update the position on
 * block boundaries).
 * </p>
 *
 * <p>
 * Instances of this class are not threadsafe.
 * </p>
 */
CBZip2OutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java)/**
 * An output stream that compresses into the BZip2 format (without the file
 * header chars) into another stream.
 *
 * <p>
 * The compression requires large amounts of memory. Thus you should call the
 * {@link #close() close()} method as soon as possible, to force
 * <tt>CBZip2OutputStream</tt> to release the allocated memory.
 * </p>
 *
 * <p>
 * You can shrink the amount of allocated memory and maybe raise the compression
 * speed by choosing a lower blocksize, which in turn may cause a lower
 * compression ratio. You can avoid unnecessary memory allocation by avoiding
 * using a blocksize which is bigger than the size of the input.
 * </p>
 *
 * <p>
 * You can compute the memory usage for compressing by the following formula:
 * </p>
 *
 * <pre>
 * &lt;code&gt;400k + (9 * blocksize)&lt;/code&gt;.
 * </pre>
 *
 * <p>
 * To get the memory required for decompression by {@link CBZip2InputStream
 * CBZip2InputStream} use
 * </p>
 *
 * <pre>
 * &lt;code&gt;65k + (5 * blocksize)&lt;/code&gt;.
 * </pre>
 *
 * <table width="100%" border="1">
 *   <caption></caption>
 * <colgroup> <col width="33%" > <col width="33%" > <col width="33%" >
 * </colgroup>
 * <tr>
 * <th colspan="3">Memory usage by blocksize</th>
 * </tr>
 * <tr>
 * <th align="right">Blocksize</th> <th align="right">Compression<br>
 * memory usage</th> <th align="right">Decompression<br>
 * memory usage</th>
 * </tr>
 * <tr>
 * <td align="right">100k</td>
 * <td align="right">1300k</td>
 * <td align="right">565k</td>
 * </tr>
 * <tr>
 * <td align="right">200k</td>
 * <td align="right">2200k</td>
 * <td align="right">1065k</td>
 * </tr>
 * <tr>
 * <td align="right">300k</td>
 * <td align="right">3100k</td>
 * <td align="right">1565k</td>
 * </tr>
 * <tr>
 * <td align="right">400k</td>
 * <td align="right">4000k</td>
 * <td align="right">2065k</td>
 * </tr>
 * <tr>
 * <td align="right">500k</td>
 * <td align="right">4900k</td>
 * <td align="right">2565k</td>
 * </tr>
 * <tr>
 * <td align="right">600k</td>
 * <td align="right">5800k</td>
 * <td align="right">3065k</td>
 * </tr>
 * <tr>
 * <td align="right">700k</td>
 * <td align="right">6700k</td>
 * <td align="right">3565k</td>
 * </tr>
 * <tr>
 * <td align="right">800k</td>
 * <td align="right">7600k</td>
 * <td align="right">4065k</td>
 * </tr>
 * <tr>
 * <td align="right">900k</td>
 * <td align="right">8500k</td>
 * <td align="right">4565k</td>
 * </tr>
 * </table>
 *
 * <p>
 * For decompression <tt>CBZip2InputStream</tt> allocates less memory if the
 * bzipped input is smaller than one block.
 * </p>
 *
 * <p>
 * Instances of this class are not threadsafe.
 * </p>
 *
 * <p>
 * TODO: Update to BZip2 1.0.1
 * </p>
 *
 */
CRC (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/bzip2/CRC.java)/**
 * A simple class the hold and calculate the CRC for sanity checking of the
 * data.
 *
 */
BZip2Codec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BZip2Codec.java)/**
 * This class provides output and input streams for bzip2 compression
 * and decompression.  It uses the native bzip2 library on the system
 * if possible, else it uses a pure-Java implementation of the bzip2
 * algorithm.  The configuration parameter
 * io.compression.codec.bzip2.library can be used to control this
 * behavior.
 *
 * In the pure-Java mode, the Compressor and Decompressor interfaces
 * are not implemented.  Therefore, in that mode, those methods of
 * CompressionCodec which have a Compressor or Decompressor type
 * argument, throw UnsupportedOperationException.
 *
 * Currently, support for splittability is available only in the
 * pure-Java mode; therefore, if a SplitCompressionInputStream is
 * requested, the pure-Java implementation is used, regardless of the
 * setting of the configuration parameter mentioned above.
 */
CodecPool (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CodecPool.java)/**
 * A global compressor/decompressor pool used to save and reuse 
 * (possibly native) compression/decompression codecs.
 */
CompressionCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionCodec.java)/**
 * This class encapsulates a streaming compression/decompression pair.
 */
CompressionCodecFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java)/**
 * A factory that will find the correct codec for a given filename.
 */
CompressionInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionInputStream.java)/**
 * A compression input stream.
 *
 * <p>Implementations are assumed to be buffered.  This permits clients to
 * reposition the underlying input stream then call {@link #resetState()},
 * without having to also synchronize client buffers.
 */
CompressionOutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionOutputStream.java)/**
 * A compression output stream.
 */
Compressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/Compressor.java)/**
 * Specification of a stream-based 'compressor' which can be  
 * plugged into a {@link CompressionOutputStream} to compress data.
 * This is modelled after {@link java.util.zip.Deflater}
 * 
 */
Decompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/Decompressor.java)/**
 * Specification of a stream-based 'de-compressor' which can be  
 * plugged into a {@link CompressionInputStream} to compress data.
 * This is modelled after {@link java.util.zip.Inflater}
 * 
 */
DeflateCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/DeflateCodec.java)/**
 * Alias class for DefaultCodec to enable codec discovery by 'deflate' name.
 */
DirectDecompressionCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/DirectDecompressionCodec.java)/**
 * This class encapsulates a codec which can decompress direct bytebuffers.
 */
DirectDecompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/DirectDecompressor.java)/**
 * Specification of a direct ByteBuffer 'de-compressor'. 
 */
GzipOutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java)/**
   * A bridge that wraps around a DeflaterOutputStream to make it
   * a CompressionOutputStream.
   */
GzipCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java)/**
 * This class creates gzip compressors/decompressors. 
 */
Lz4Compressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java)/**
 * A {@link Compressor} based on the lz4 compression algorithm.
 * http://code.google.com/p/lz4/
 */
Lz4Decompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java)/**
 * A {@link Decompressor} based on the lz4 compression algorithm.
 * http://code.google.com/p/lz4/
 */
Lz4Codec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/Lz4Codec.java)/**
 * This class creates lz4 compressors/decompressors.
 */
PassthroughDecompressorStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/PassthroughCodec.java)/**
   * The decompressor.
   */
StubDecompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/PassthroughCodec.java)/**
   * The decompressor is a no-op. It is not needed other than
   * to complete the methods offered by the interface.
   */
PassthroughCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/PassthroughCodec.java)/**
 * This is a special codec which does not transform the output.
 * It can be declared as a codec in the option "io.compression.codecs",
 * and then it will declare that it supports the file extension
 * set in {@link #OPT_EXTENSION}.
 *
 * This allows decompression to be disabled on a job, even when there is
 * a registered/discoverable decompression codec for a file extension
 * -without having to change the standard codec binding mechanism.
 *
 * For example, to disable decompression for a gzipped files, set the
 * options
 * <pre>
 *   io.compression.codecs = org.apache.hadoop.io.compress.PassthroughCodec
 *   io.compress.passthrough.extension = .gz
 * </pre>
 *
 * <i>Note:</i> this is not a Splittable codec: it doesn't know the
 * capabilities of the passed in stream. It should be possible to
 * extend this in a subclass: the inner classes are marked as protected
 * to enable this. <i>Do not retrofit splitting to this class.</i>.
 *
 */
SnappyCompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java)/**
 * A {@link Compressor} based on the snappy compression algorithm.
 * http://code.google.com/p/snappy/
 */
SnappyDecompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java)/**
 * A {@link Decompressor} based on the snappy compression algorithm.
 * http://code.google.com/p/snappy/
 */
SnappyCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/SnappyCodec.java)/**
 * This class creates snappy compressors/decompressors.
 */
SplitCompressionInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/SplitCompressionInputStream.java)/**
 * An InputStream covering a range of compressed data. The start and end
 * offsets requested by a client may be modified by the codec to fit block
 * boundaries or other algorithm-dependent requirements.
 */
SplittableCompressionCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/SplittableCompressionCodec.java)/**
 * This interface is meant to be implemented by those compression codecs
 * which are capable to compress / de-compress a stream starting at any
 * arbitrary position.
 *
 * Especially the process of de-compressing a stream starting at some arbitrary
 * position is challenging.  Most of the codecs are only able to successfully
 * de-compress a stream, if they start from the very beginning till the end.
 * One of the reasons is the stored state at the beginning of the stream which
 * is crucial for de-compression.
 *
 * Yet there are few codecs which do not save the whole state at the beginning
 * of the stream and hence can be used to de-compress stream starting at any
 * arbitrary points.  This interface is meant to be used by such codecs.  Such
 * codecs are highly valuable, especially in the context of Hadoop, because
 * an input compressed file can be split and hence can be worked on by multiple
 * machines in parallel.
 */
BuiltInGzipDecompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java)/**
 * A {@link Decompressor} based on the popular gzip compressed file format.
 * http://www.gzip.org/
 *
 */
BuiltInZlibDeflater (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/BuiltInZlibDeflater.java)/**
 * A wrapper around java.util.zip.Deflater to make it conform 
 * to org.apache.hadoop.io.compress.Compressor interface.
 * 
 */
BuiltInZlibInflater (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/BuiltInZlibInflater.java)/**
 * A wrapper around java.util.zip.Inflater to make it conform 
 * to org.apache.hadoop.io.compress.Decompressor interface.
 * 
 */
ZlibCompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java)/**
 * A {@link Compressor} based on the popular 
 * zlib compression algorithm.
 * http://www.zlib.net/
 * 
 */
ZlibDecompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java)/**
 * A {@link Decompressor} based on the popular 
 * zlib compression algorithm.
 * http://www.zlib.net/
 * 
 */
ZlibFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java)/**
 * A collection of factories to create the right 
 * zlib/gzip compressor/decompressor instances.
 * 
 */
ZStandardCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/ZStandardCodec.java)/**
 * This class creates zstd compressors/decompressors.
 */
ZStandardCompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java)/**
 * A {@link Compressor} based on the zStandard compression algorithm.
 * https://github.com/facebook/zstd
 */
ZStandardDirectDecompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java)/**
   * A {@link DirectDecompressor} for ZStandard
   * https://github.com/facebook/zstd.
   */
ZStandardDecompressor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java)/**
 * A {@link Decompressor} based on the zStandard compression algorithm.
 * https://github.com/facebook/zstd
 */
CompressedWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/CompressedWritable.java)/** A base-class for Writables which store themselves compressed and lazily
 * inflate on field access.  This is useful for large objects whose fields are
 * not be altered during a map or reduce operation: leaving the field data
 * compressed makes copying the instance from one file to another much
 * faster. */
DataInputBuffer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DataInputBuffer.java)/** A reusable {@link java.io.DataInput} implementation
 * that reads from an in-memory buffer.
 *
 * <p>This saves memory over creating a new DataInputStream and
 * ByteArrayInputStream each time data is read.
 *
 * <p>Typical usage is something like the following:<pre>
 *
 * DataInputBuffer buffer = new DataInputBuffer();
 * while (... loop condition ...) {
 *   byte[] data = ... get data ...;
 *   int dataLength = ... get data length ...;
 *   buffer.reset(data, dataLength);
 *   ... read buffer using DataInput methods ...
 * }
 * </pre>
 *  
 */
DataOutputBuffer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DataOutputBuffer.java)/** A reusable {@link DataOutput} implementation that writes to an in-memory
 * buffer.
 *
 * <p>This saves memory over creating a new DataOutputStream and
 * ByteArrayOutputStream each time data is written.
 *
 * <p>Typical usage is something like the following:<pre>
 *
 * DataOutputBuffer buffer = new DataOutputBuffer();
 * while (... loop condition ...) {
 *   buffer.reset();
 *   ... write buffer using DataOutput methods ...
 *   byte[] data = buffer.getData();
 *   int dataLength = buffer.getLength();
 *   ... write data to its ultimate destination ...
 * }
 * </pre>
 *  
 */
DataOutputOutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DataOutputOutputStream.java)/**
 * OutputStream implementation that wraps a DataOutput.
 */
DefaultStringifier (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DefaultStringifier.java)/**
 * DefaultStringifier is the default implementation of the {@link Stringifier}
 * interface which stringifies the objects using base64 encoding of the
 * serialized version of the objects. The {@link Serializer} and
 * {@link Deserializer} are obtained from the {@link SerializationFactory}.
 * <br>
 * DefaultStringifier offers convenience methods to store/load objects to/from
 * the configuration.
 * 
 * @param <T> the class of the objects to stringify
 */
Comparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DoubleWritable.java)/** A Comparator optimized for DoubleWritable. */
DoubleWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DoubleWritable.java)/**
 * Writable for Double values.
 */
ElasticByteBufferPool (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ElasticByteBufferPool.java)/**
 * This is a simple ByteBufferPool which just creates ByteBuffers as needed.
 * It also caches ByteBuffers after they're released.  It will always return
 * the smallest cached buffer with at least the capacity you request.
 * We don't try to do anything clever here like try to limit the maximum cache
 * size.
 */
EnumSetWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/EnumSetWritable.java)/** A Writable wrapper for EnumSet. */
DummyErasureCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java)/**
 * Dummy erasure coder does not real coding computing. This is used for only
 * test or performance comparison with other erasure coders.
 */
ErasureCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java)/**
 * Abstract Erasure Codec is defines the interface of each actual erasure
 * codec classes.
 */
HHXORErasureCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java)/**
 * A Hitchhiker-XOR erasure codec.
 */
RSErasureCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java)/**
 * A Reed-Solomon erasure codec.
 */
XORErasureCodec (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java)/**
 * A XOR erasure codec.
 */
CodecRegistry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/CodecRegistry.java)/**
 * This class registers all coder implementations.
 *
 * {@link CodecRegistry} maps codec names to coder factories. All coder
 * factories are dynamically identified and loaded using ServiceLoader.
 */
CodecUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/CodecUtil.java)/**
 * A codec &amp; coder utility to help create coders conveniently.
 *
 * {@link CodecUtil} includes erasure coder configurations key and default
 * values such as coder class name and erasure codec option values included
 * by {@link ErasureCodecOptions}. {@link ErasureEncoder} and
 * {@link ErasureDecoder} are created by createEncoder and createDecoder
 * respectively.{@link RawErasureEncoder} and {@link RawErasureDecoder} are
 * are created by createRawEncoder and createRawDecoder.
 */
DummyErasureDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureDecoder.java)/**
 * Dummy erasure decoder does no real computation. Instead, it just returns
 * zero bytes. This decoder can be used to isolate the performance issue to
 * HDFS side logic instead of codec, and is intended for test only.
 */
DummyErasureEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureEncoder.java)/**
 * Dummy erasure encoder does no real computation. Instead, it just returns
 * zero bytes. This decoder can be used to isolate the performance issue to
 * HDFS side logic instead of codec, and is intended for test only.
 */
ErasureCoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/ErasureCoder.java)/**
 * An erasure coder to perform encoding or decoding given a group. Generally it
 * involves calculating necessary internal steps according to codec logic. For
 * each step,it calculates necessary input blocks to read chunks from and output
 * parity blocks to write parity chunks into from the group. It also takes care
 * of appropriate raw coder to use for the step. And encapsulates all the
 * necessary info (input blocks, output blocks and raw coder) into a step
 * represented by {@link ErasureCodingStep}. ErasureCoder callers can use the
 * step to do the real work with retrieved input and output chunks.
 *
 * Note, currently only one coding step is supported. Will support complex cases
 * of multiple coding steps.
 *
 */
ErasureCodingStep (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/ErasureCodingStep.java)/**
 * Erasure coding step that's involved in encoding/decoding of a block group.
 */
ErasureDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java)/**
 * An abstract erasure decoder that's to be inherited by new decoders.
 *
 * It implements the {@link ErasureCoder} interface.
 */
ErasureDecodingStep (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecodingStep.java)/**
 * Erasure decoding step, a wrapper of all the necessary information to perform
 * a decoding step involved in the whole process of decoding a block group.
 */
ErasureEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java)/**
 * An abstract erasure encoder that's to be inherited by new encoders.
 *
 * It implements the {@link ErasureCoder} interface.
 */
ErasureEncodingStep (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncodingStep.java)/**
 * Erasure encoding step, a wrapper of all the necessary information to perform
 * an encoding step involved in the whole process of encoding a block group.
 */
HHErasureCodingStep (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHErasureCodingStep.java)/**
 * Abstract class for Hitchhiker common facilities shared by
 * {@link HHXORErasureEncodingStep}and {@link HHXORErasureDecodingStep}.
 *
 * It implements {@link ErasureCodingStep}.
 */
HHXORErasureDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java)/**
 * Hitchhiker is a new erasure coding algorithm developed as a research project
 * at UC Berkeley by Rashmi Vinayak.
 * It has been shown to reduce network traffic and disk I/O by 25%-45% during
 * data reconstruction while retaining the same storage capacity and failure
 * tolerance capability of RS codes.
 * The Hitchhiker algorithm is described in K.V.Rashmi, et al.,
 * "A "Hitchhiker's" Guide to Fast and Efficient Data Reconstruction in
 * Erasure-coded Data Centers", in ACM SIGCOMM 2014.
 * This is Hitchhiker-XOR erasure decoder that decodes a block group.
 */
HHXORErasureDecodingStep (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java)/**
 * Hitchhiker-XOR Erasure decoding step, a wrapper of all the necessary
 * information to perform a decoding step involved in the whole process of
 * decoding a block group.
 */
HHXORErasureEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java)/**
 * Hitchhiker is a new erasure coding algorithm developed as a research project
 * at UC Berkeley by Rashmi Vinayak.
 * It has been shown to reduce network traffic and disk I/O by 25%-45% during
 * data reconstruction while retaining the same storage capacity and failure
 * tolerance capability of RS codes.
 * The Hitchhiker algorithm is described in K.V.Rashmi, et al.,
 * "A "Hitchhiker's" Guide to Fast and Efficient Data Reconstruction in
 * Erasure-coded Data Centers", in ACM SIGCOMM 2014.
 * This is Hitchhiker-XOR erasure encoder that encodes a block group.
 */
HHXORErasureEncodingStep (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java)/**
 * Hitchhiker-XOR Erasure encoding step, a wrapper of all the necessary
 * information to perform an encoding step involved in the whole process of
 * encoding a block group.
 */
RSErasureDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java)/**
 * Reed-Solomon erasure decoder that decodes a block group.
 *
 * It implements {@link ErasureCoder}.
 */
RSErasureEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java)/**
 * Reed-Solomon erasure encoder that encodes a block group.
 *
 * It implements {@link ErasureCoder}.
 */
HHUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java)/**
 * Some utilities for Hitchhiker coding.
 */
XORErasureDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java)/**
 * Xor erasure decoder that decodes a block group.
 *
 * It implements {@link ErasureCoder}.
 */
XORErasureEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/coder/XORErasureEncoder.java)/**
 * Xor erasure encoder that encodes a block group.
 *
 * It implements {@link ErasureCoder}.
 */
ECBlock (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/ECBlock.java)/**
 * A wrapper of block level data source/output that {@link ECChunk}s can be
 * extracted from. For HDFS, it can be an HDFS block (250MB). Note it only cares
 * about erasure coding specific logic thus avoids coupling with any HDFS block
 * details. We can have something like HdfsBlock extend it.
 */
ECBlockGroup (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/ECBlockGroup.java)/**
 * A group of blocks or {@link ECBlock} incurred in an erasure coding task.
 */
ECChunk (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/ECChunk.java)/**
 * A wrapper for ByteBuffer or bytes array for an erasure code chunk.
 */
ECSchema (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/ECSchema.java)/**
 * Erasure coding schema to housekeeper relevant information.
 */
ErasureCodeConstants (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/ErasureCodeConstants.java)/**
 * Constants related to the erasure code feature.
 */
ErasureCodecOptions (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/ErasureCodecOptions.java)/**
 * Erasure codec options.
 */
ErasureCodeNative (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/ErasureCodeNative.java)/**
 * Erasure code native libraries (for now, Intel ISA-L) related utilities.
 */
ErasureCoderOptions (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/ErasureCoderOptions.java)/**
 * Erasure coder configuration that maintains schema info and coder options.
 */
BlockGrouper (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java)/**
 * As part of a codec, to handle how to form a block group for encoding
 * and provide instructions on how to recover erased blocks from a block group
 */
AbstractNativeRawDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java)/**
 * Abstract native raw decoder for all native coders to extend with.
 */
AbstractNativeRawEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java)/**
 * Abstract native raw encoder for all native coders to extend with.
 */
ByteArrayDecodingState (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java)/**
 * A utility class that maintains decoding state during a decode call using
 * byte array inputs.
 */
ByteArrayEncodingState (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java)/**
 * A utility class that maintains encoding state during an encode call using
 * byte array inputs.
 */
ByteBufferDecodingState (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java)/**
 * A utility class that maintains decoding state during a decode call using
 * ByteBuffer inputs.
 */
ByteBufferEncodingState (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java)/**
 * A utility class that maintains encoding state during an encode call using
 * ByteBuffer inputs.
 */
CoderUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java)/**
 * Helpful utilities for implementing some raw erasure coders.
 */
DecodingState (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingState.java)/**
 * A utility class that maintains decoding state during a decode call.
 */
DummyRawDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawDecoder.java)/**
 * A dummy raw decoder that does no real computation.
 * Instead, it just returns zero bytes.
 * This decoder can be used to isolate the performance issue to HDFS side logic
 * instead of codec, and is intended for test only.
 */
DummyRawEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawEncoder.java)/**
 * A dummy raw encoder that does no real computation.
 * Instead, it just returns zero bytes.
 * This encoder can be used to isolate the performance issue to HDFS side logic
 * instead of codec, and is intended for test only.
 */
DummyRawErasureCoderFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawErasureCoderFactory.java)/**
 * A raw erasure coder factory for dummy raw coders.
 */
EncodingState (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/EncodingState.java)/**
 * A utility class that maintains encoding state during an encode call.
 */
NativeRSRawDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawDecoder.java)/**
 * A Reed-Solomon raw decoder using Intel ISA-L library.
 */
NativeRSRawEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawEncoder.java)/**
 * A Reed-Solomon raw encoder using Intel ISA-L library.
 */
NativeXORRawDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawDecoder.java)/**
 * A XOR raw decoder using Intel ISA-L library.
 */
NativeXORRawEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawEncoder.java)/**
 * A XOR raw encoder using Intel ISA-L library.
 */
RawErasureCoderFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureCoderFactory.java)/**
 * Raw erasure coder factory that can be used to create raw encoder and decoder.
 * It helps in configuration since only one factory class is needed to be
 * configured.
 */
RawErasureDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java)/**
 * An abstract raw erasure decoder that's to be inherited by new decoders.
 *
 * Raw erasure coder is part of erasure codec framework, where erasure coder is
 * used to encode/decode a group of blocks (BlockGroup) according to the codec
 * specific BlockGroup layout and logic. An erasure coder extracts chunks of
 * data from the blocks and can employ various low level raw erasure coders to
 * perform encoding/decoding against the chunks.
 *
 * To distinguish from erasure coder, here raw erasure coder is used to mean the
 * low level constructs, since it only takes care of the math calculation with
 * a group of byte buffers.
 *
 * Note it mainly provides decode() calls, which should be stateless and may be
 * made thread-safe in future.
 */
RawErasureEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java)/**
 * An abstract raw erasure encoder that's to be inherited by new encoders.
 *
 * Raw erasure coder is part of erasure codec framework, where erasure coder is
 * used to encode/decode a group of blocks (BlockGroup) according to the codec
 * specific BlockGroup layout and logic. An erasure coder extracts chunks of
 * data from the blocks and can employ various low level raw erasure coders to
 * perform encoding/decoding against the chunks.
 *
 * To distinguish from erasure coder, here raw erasure coder is used to mean the
 * low level constructs, since it only takes care of the math calculation with
 * a group of byte buffers.
 *
 * Note it mainly provides encode() calls, which should be stateless and may be
 * made thread-safe in future.
 */
RSLegacyRawDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java)/**
 * A raw erasure decoder in RS code scheme in pure Java in case native one
 * isn't available in some environment. Please always use native implementations
 * when possible.
 *
 * Currently this implementation will compute and decode not to read units
 * unnecessarily due to the underlying implementation limit in GF. This will be
 * addressed in HADOOP-11871.
 */
RSLegacyRawEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java)/**
 * A raw erasure encoder in RS code scheme in pure Java in case native one
 * isn't available in some environment. Please always use native implementations
 * when possible.
 */
RSLegacyRawErasureCoderFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawErasureCoderFactory.java)/**
 * A raw coder factory for the legacy raw Reed-Solomon coder in Java.
 */
RSRawDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java)/**
 * A raw erasure decoder in RS code scheme in pure Java in case native one
 * isn't available in some environment. Please always use native implementations
 * when possible. This new Java coder is about 5X faster than the one originated
 * from HDFS-RAID, and also compatible with the native/ISA-L coder.
 */
RSRawEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java)/**
 * A raw erasure encoder in RS code scheme in pure Java in case native one
 * isn't available in some environment. Please always use native implementations
 * when possible. This new Java coder is about 5X faster than the one originated
 * from HDFS-RAID, and also compatible with the native/ISA-L coder.
 */
RSRawErasureCoderFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawErasureCoderFactory.java)/**
 * A raw coder factory for the new raw Reed-Solomon coder in Java.
 */
DumpUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/util/DumpUtil.java)/**
 * A dump utility class for debugging data erasure coding/decoding issues.
 * Don't suggest they are used in runtime production codes.
 */
GaloisField (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java)/**
 * Implementation of Galois field arithmetic with 2^p elements. The input must
 * be unsigned integers. It's ported from HDFS-RAID, slightly adapted.
 */
GF256 (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GF256.java)/**
 * A GaloisField utility class only caring of 256 fields for efficiency. Some
 * of the codes are borrowed from ISA-L implementation (C or ASM codes).
 */
RSUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java)/**
 * Utilities for implementing Reed-Solomon code, used by RS coder. Some of the
 * codes are borrowed from ISA-L implementation (C or ASM codes).
 */
XORRawDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java)/**
 * A raw decoder in XOR code scheme in pure Java, adapted from HDFS-RAID.
 *
 * XOR code is an important primitive code scheme in erasure coding and often
 * used in advanced codes, like HitchHiker and LRC, though itself is rarely
 * deployed independently.
 */
XORRawEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java)/**
 * A raw encoder in XOR code scheme in pure Java, adapted from HDFS-RAID.
 *
 * XOR code is an important primitive code scheme in erasure coding and often
 * used in advanced codes, like HitchHiker and LRC, though itself is rarely
 * deployed independently.
 */
XORRawErasureCoderFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawErasureCoderFactory.java)/**
 * A raw coder factory for raw XOR coder.
 */
LexicographicalComparerHolder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/FastByteComparisons.java)/**
   * Provides a lexicographical comparer implementation; either a Java
   * implementation or a faster implementation based on {@link Unsafe}.
   *
   * <p>Uses reflection to gracefully fall back to the Java implementation if
   * {@code Unsafe} isn't available.
   */
FastByteComparisons (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/FastByteComparisons.java)/**
 * Utility code to do optimized byte-array comparison.
 * This is borrowed and slightly modified from Guava's {@link UnsignedBytes}
 * class to be able to compare arrays that start at non-zero offsets.
 */
BlockRegister (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
     * Call-back interface to register a block after a block is closed.
     */
WBlockState (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
     * Intermediate class that maintain the state of a Writable Compression
     * Block.
     */
BlockAppender (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
     * Access point to stuff data into a block.
     * 
     * TODO: Change DataOutputStream to something else that tracks the size as
     * long instead of int. Currently, we will wrap around if the row block size
     * is greater than 4GB.
     */
MetaBlockRegister (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
     * Callback to make sure a meta block is added to the internal list when its
     * stream is closed.
     */
DataBlockRegister (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
     * Callback to make sure a data block is added to the internal list when
     * it's being closed.
     * 
     */
Writer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
   * BCFile writer, the entry point for creating a new BCFile.
   */
RBlockState (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
     * Intermediate class that maintain the state of a Readable Compression
     * Block.
     */
BlockReader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
     * Access point to read a block.
     */
Reader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
   * BCFile Reader, interface to read the file's data and meta blocks.
   */
MetaIndex (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
   * Index for all Meta blocks.
   */
MetaIndexEntry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
   * An entry describes a meta block in the MetaIndex.
   */
DataIndex (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
   * Index of all compressed data blocks.
   */
Magic (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
   * Magic number uniquely identifying a BCFile in the header/footer.
   */
BlockRegion (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
   * Block region.
   */
BCFile (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BCFile.java)/**
 * Block Compressed file, the underlying physical storage layer for TFile.
 * BCFile provides the basic block level compression for the data block and meta
 * blocks. It is separated from TFile as it may be used for other
 * block-compressed file implementation.
 */
BoundedRangeFileInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java)/**
 * BoundedRangeFIleInputStream abstracts a contiguous region of a Hadoop
 * FSDataInputStream as a regular input stream. One can create multiple
 * BoundedRangeFileInputStream on top of the same FSDataInputStream and they
 * would not interfere with each other.
 */
ByteArray (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/ByteArray.java)/**
 * Adaptor class to wrap byte-array backed objects (including java byte array)
 * as RawComparable objects.
 */
ChunkDecoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/Chunk.java)/**
   * Decoding a chain of chunks encoded through ChunkEncoder or
   * SingleChunkEncoder.
   */
ChunkEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/Chunk.java)/**
   * Chunk Encoder. Encoding the output data into a chain of chunks in the
   * following sequences: -len1, byte[len1], -len2, byte[len2], ... len_n,
   * byte[len_n]. Where len1, len2, ..., len_n are the lengths of the data
   * chunks. Non-terminal chunks have their lengths negated. Non-terminal chunks
   * cannot have length 0. All lengths are in the range of 0 to
   * Integer.MAX_VALUE and are encoded in Utils.VInt format.
   */
SingleChunkEncoder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/Chunk.java)/**
   * Encode the whole stream as a single chunk. Expecting to know the size of
   * the chunk up-front.
   */
Chunk (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/Chunk.java)/**
 * Several related classes to support chunk-encoded sub-streams on top of a
 * regular stream.
 */
BytesComparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/CompareUtils.java)/**
   * A comparator to compare anything that implements {@link RawComparable}
   * using a customized comparator.
   */
Scalar (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/CompareUtils.java)/**
   * Interface for all objects that has a single integer magnitude.
   */
Compression (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/Compression.java)/**
 * Compression related stuff.
 */
MetaBlockAlreadyExists (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/MetaBlockAlreadyExists.java)/**
 * Exception - Meta Block with the same name already exists.
 */
MetaBlockDoesNotExist (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/MetaBlockDoesNotExist.java)/**
 * Exception - No such Meta Block with the given name.
 */
RawComparable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/RawComparable.java)/**
 * Interface for objects that can be compared through {@link RawComparator}.
 * This is useful in places where we need a single object reference to specify a
 * range of bytes in a byte array, such as {@link Comparable} or
 * {@link Collections#binarySearch(java.util.List, Object, Comparator)}
 * 
 * The actual comparison among RawComparable's requires an external
 * RawComparator and it is applications' responsibility to ensure two
 * RawComparable are supposed to be semantically comparable with the same
 * RawComparator.
 */
SimpleBufferedOutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java)/**
 * A simplified BufferedOutputStream with borrowed buffer, and allow users to
 * see how much data have been buffered.
 */
KeyRegister (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/TFile.java)/**
     * Helper class to register key after close call on key append stream.
     */
ValueRegister (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/TFile.java)/**
     * Helper class to register value after close call on value append stream.
     */
Writer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/TFile.java)/**
   * TFile Writer.
   */
Location (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/TFile.java)/**
     * Location representing a virtual position in the TFile.
     */
Entry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/TFile.java)/**
       * Entry to a &lt;Key, Value&gt; pair.
       */
Scanner (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/TFile.java)/**
     * The TFile Scanner. The Scanner has an implicit cursor, which, upon
     * creation, points to the first key-value pair in the scan range. If the
     * scan range is empty, the cursor will point to the end of the scan range.
     * <p>
     * Use {@link Scanner#atEnd()} to test whether the cursor is at the end
     * location of the scanner.
     * <p>
     * Use {@link Scanner#advance()} to move the cursor to the next key-value
     * pair (or end if none exists). Use seekTo methods (
     * {@link Scanner#seekTo(byte[])} or
     * {@link Scanner#seekTo(byte[], int, int)}) to seek to any arbitrary
     * location in the covered range (including backward seeking). Use
     * {@link Scanner#rewind()} to seek back to the beginning of the scanner.
     * Use {@link Scanner#seekToEnd()} to seek to the end of the scanner.
     * <p>
     * Actual keys and values may be obtained through {@link Scanner.Entry}
     * object, which is obtained through {@link Scanner#entry()}.
     */
Reader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/TFile.java)/**
   * TFile Reader. Users may only read TFiles by creating TFile.Reader.Scanner.
   * objects. A scanner may scan the whole TFile ({@link Reader#createScanner()}
   * ) , a portion of TFile based on byte offsets (
   * {@link Reader#createScannerByByteRange(long, long)}), or a portion of TFile with keys
   * fall in a certain key range (for sorted TFile only,
   * {@link Reader#createScannerByKey(byte[], byte[])} or
   * {@link Reader#createScannerByKey(RawComparable, RawComparable)}).
   */
TFileMeta (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/TFile.java)/**
   * Data structure representing "TFile.meta" meta block.
   */
TFileIndex (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/TFile.java)/**
   * Data structure representing "TFile.index" meta block.
   */
TFileIndexEntry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/TFile.java)/**
   * TFile Data Index entry. We should try to make the memory footprint of each
   * index entry as small as possible.
   */
TFile (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/TFile.java)/**
 * A TFile is a container of key-value pairs. Both keys and values are type-less
 * bytes. Keys are restricted to 64KB, value length is not restricted
 * (practically limited to the available disk storage). TFile further provides
 * the following features:
 * <ul>
 * <li>Block Compression.
 * <li>Named meta data blocks.
 * <li>Sorted or unsorted keys.
 * <li>Seek by key or by file offset.
 * </ul>
 * The memory footprint of a TFile includes the following:
 * <ul>
 * <li>Some constant overhead of reading or writing a compressed block.
 * <ul>
 * <li>Each compressed block requires one compression/decompression codec for
 * I/O.
 * <li>Temporary space to buffer the key.
 * <li>Temporary space to buffer the value (for TFile.Writer only). Values are
 * chunk encoded, so that we buffer at most one chunk of user data. By default,
 * the chunk buffer is 1MB. Reading chunked value does not require additional
 * memory.
 * </ul>
 * <li>TFile index, which is proportional to the total number of Data Blocks.
 * The total amount of memory needed to hold the index can be estimated as
 * (56+AvgKeySize)*NumBlocks.
 * <li>MetaBlock index, which is proportional to the total number of Meta
 * Blocks.The total amount of memory needed to hold the index for Meta Blocks
 * can be estimated as (40+AvgMetaBlockName)*NumMetaBlock.
 * </ul>
 * <p>
 * The behavior of TFile can be customized by the following variables through
 * Configuration:
 * <ul>
 * <li><b>tfile.io.chunk.size</b>: Value chunk size. Integer (in bytes). Default
 * to 1MB. Values of the length less than the chunk size is guaranteed to have
 * known value length in read time (See
 * {@link TFile.Reader.Scanner.Entry#isValueLengthKnown()}).
 * <li><b>tfile.fs.output.buffer.size</b>: Buffer size used for
 * FSDataOutputStream. Integer (in bytes). Default to 256KB.
 * <li><b>tfile.fs.input.buffer.size</b>: Buffer size used for
 * FSDataInputStream. Integer (in bytes). Default to 256KB.
 * </ul>
 * <p>
 * Suggestions on performance optimization.
 * <ul>
 * <li>Minimum block size. We recommend a setting of minimum block size between
 * 256KB to 1MB for general usage. Larger block size is preferred if files are
 * primarily for sequential access. However, it would lead to inefficient random
 * access (because there are more data to decompress). Smaller blocks are good
 * for random access, but require more memory to hold the block index, and may
 * be slower to create (because we must flush the compressor stream at the
 * conclusion of each data block, which leads to an FS I/O flush). Further, due
 * to the internal caching in Compression codec, the smallest possible block
 * size would be around 20KB-30KB.
 * <li>The current implementation does not offer true multi-threading for
 * reading. The implementation uses FSDataInputStream seek()+read(), which is
 * shown to be much faster than positioned-read call in single thread mode.
 * However, it also means that if multiple threads attempt to access the same
 * TFile (using multiple scanners) simultaneously, the actual I/O is carried out
 * sequentially even if they access different DFS blocks.
 * <li>Compression codec. Use "none" if the data is not very compressable (by
 * compressable, I mean a compression ratio at least 2:1). Generally, use "lzo"
 * as the starting point for experimenting. "gz" overs slightly better
 * compression ratio over "lzo" but requires 4x CPU to compress and 2x CPU to
 * decompress, comparing to "lzo".
 * <li>File system buffering, if the underlying FSDataInputStream and
 * FSDataOutputStream is already adequately buffered; or if applications
 * reads/writes keys and values in large buffers, we can reduce the sizes of
 * input/output buffering in TFile layer by setting the configuration parameters
 * "tfile.fs.input.buffer.size" and "tfile.fs.output.buffer.size".
 * </ul>
 * 
 * Some design rationale behind TFile can be found at <a
 * href=https://issues.apache.org/jira/browse/HADOOP-3315>Hadoop-3315</a>.
 */
TFileDumper (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/TFileDumper.java)/**
 * Dumping the information of a TFile.
 */
Version (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/Utils.java)/**
   * A generic Version class. We suggest applications built on top of TFile use
   * this class to maintain version information in their meta blocks.
   * 
   * A version number consists of a major version and a minor version. The
   * suggested usage of major and minor version number is to increment major
   * version number when the new storage format is not backward compatible, and
   * increment the minor version otherwise.
   */
Utils (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/file/tfile/Utils.java)/**
 * Supporting Utility classes used by TFile, and shared by users of TFile.
 */
Comparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/FloatWritable.java)/** A Comparator optimized for FloatWritable. */
FloatWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/FloatWritable.java)/** A WritableComparable for floats. */
GenericWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/GenericWritable.java)/**
 * A wrapper for Writable instances.
 * <p>
 * When two sequence files, which have same Key type but different Value
 * types, are mapped out to reduce, multiple Value types is not allowed.
 * In this case, this class can help you wrap instances with different types.
 * </p>
 * 
 * <p>
 * Compared with <code>ObjectWritable</code>, this class is much more effective,
 * because <code>ObjectWritable</code> will append the class declaration as a String 
 * into the output file in every Key-Value pair.
 * </p>
 * 
 * <p>
 * Generic Writable implements {@link Configurable} interface, so that it will be 
 * configured by the framework. The configuration is passed to the wrapped objects
 * implementing {@link Configurable} interface <i>before deserialization</i>. 
 * </p>
 * 
 * how to use it: <br>
 * 1. Write your own class, such as GenericObject, which extends GenericWritable.<br> 
 * 2. Implements the abstract method <code>getTypes()</code>, defines 
 *    the classes which will be wrapped in GenericObject in application.
 *    Attention: this classes defined in <code>getTypes()</code> method, must
 *    implement <code>Writable</code> interface.
 * <br><br>
 * 
 * The code looks like this:
 * <blockquote><pre>
 * public class GenericObject extends GenericWritable {
 * 
 *   private static Class[] CLASSES = {
 *               ClassType1.class, 
 *               ClassType2.class,
 *               ClassType3.class,
 *               };
 *
 *   protected Class[] getTypes() {
 *       return CLASSES;
 *   }
 *
 * }
 * </pre></blockquote>
 * 
 * @since Nov 8, 2006
 */
InputBuffer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/InputBuffer.java)/** A reusable {@link InputStream} implementation that reads from an in-memory
 * buffer.
 *
 * <p>This saves memory over creating a new InputStream and
 * ByteArrayInputStream each time data is read.
 *
 * <p>Typical usage is something like the following:<pre>
 *
 * InputBuffer buffer = new InputBuffer();
 * while (... loop condition ...) {
 *   byte[] data = ... get data ...;
 *   int dataLength = ... get data length ...;
 *   buffer.reset(data, dataLength);
 *   ... read buffer using InputStream methods ...
 * }
 * </pre>
 * @see DataInputBuffer
 * @see DataOutput
 */
Comparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/IntWritable.java)/** A Comparator optimized for IntWritable. */
IntWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/IntWritable.java)/** A WritableComparable for ints. */
NullOutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/IOUtils.java)/**
   * The /dev/null of OutputStreams.
   */
IOUtils (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/IOUtils.java)/**
 * An utility class for I/O related functionality. 
 */
Comparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/LongWritable.java)/** A Comparator optimized for LongWritable. */
DecreasingComparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/LongWritable.java)/** A decreasing Comparator optimized for LongWritable. */
LongWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/LongWritable.java)/** A WritableComparable for longs. */
Writer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/MapFile.java)/** Writes a new map. */
Reader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/MapFile.java)/** Provide access to an existing map. */
Merger (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/MapFile.java)/**
   * Class to merge multiple MapFiles of same Key and Value types to one MapFile
   */
MapFile (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/MapFile.java)/** A file-based map from keys to values.
 * 
 * <p>A map is a directory containing two files, the <code>data</code> file,
 * containing all keys and values in the map, and a smaller <code>index</code>
 * file, containing a fraction of the keys.  The fraction is determined by
 * {@link Writer#getIndexInterval()}.
 *
 * <p>The index file is read entirely into memory.  Thus key implementations
 * should try to keep themselves small.
 *
 * <p>Map files are created by adding entries in-order.  To maintain a large
 * database, perform updates by copying the previous version of a database and
 * merging in a sorted change list, to create a new version of the database in
 * a new file.  Sorting large change lists can be done with {@link
 * SequenceFile.Sorter}.
 */
MapWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/MapWritable.java)/**
 * A Writable Map.
 */
Comparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/MD5Hash.java)/** A WritableComparator optimized for MD5Hash keys. */
MD5Hash (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/MD5Hash.java)/** A Writable for MD5 hash values.
 */
Builder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/MultipleIOException.java)/**
   * Build an {@link IOException} using {@link MultipleIOException}
   * if there are more than one.
   */
MultipleIOException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/MultipleIOException.java)/** Encapsulate a list of {@link IOException} into an {@link IOException} */
PmemMappedRegion (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java)/**
     * Denote memory region for a file mapped.
     */
Pmem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java)/**
     * JNI wrapper of persist memory operations.
     */
CacheManipulator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java)/**
     * Used to manipulate the operating system cache.
     */
NoMlockCacheManipulator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java)/**
     * A CacheManipulator used for testing which does not actually call mlock.
     * This allows many tests to be run even when the operating system does not
     * allow mlock, or only allows limited mlocking.
     */
Stat (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java)/**
     * Result type of the fstat call
     */
NativeIO (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java)/**
 * JNI wrappers for various native IO-related calls not available in Java.
 * These functions should generally be used alongside a fallback to another
 * more portable mechanism.
 */
NativeIOException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIOException.java)/**
 * An exception generated by a call to the native IO code.
 *
 * These exceptions simply wrap <i>errno</i> result codes on Linux,
 * or the System Error Code on Windows.
 */
SharedFileDescriptorFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.java)/**
 * A factory for creating shared file descriptors inside a given directory.
 * Typically, the directory will be /dev/shm or /tmp.
 *
 * We will hand out file descriptors that correspond to unlinked files residing
 * in that directory.  These file descriptors are suitable for sharing across
 * multiple processes and are both readable and writable.
 *
 * Because we unlink the temporary files right after creating them, a JVM crash
 * usually does not leave behind any temporary files in the directory.  However,
 * it may happen that we crash right after creating the file and before
 * unlinking it.  In the constructor, we attempt to clean up after any such
 * remnants by trying to unlink any temporary files created by previous
 * SharedFileDescriptorFactory instances that also used our prefix.
 */
Comparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/NullWritable.java)/** A Comparator &quot;optimized&quot; for NullWritable. */
NullWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/NullWritable.java)/** Singleton Writable with no data. */
ObjectWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ObjectWritable.java)/** A polymorphic Writable that writes an instance with it's class name.
 * Handles arrays, strings and primitive types without a Writable wrapper.
 */
OutputBuffer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/OutputBuffer.java)/** A reusable {@link OutputStream} implementation that writes to an in-memory
 * buffer.
 *
 * <p>This saves memory over creating a new OutputStream and
 * ByteArrayOutputStream each time data is written.
 *
 * <p>Typical usage is something like the following:<pre>
 *
 * OutputBuffer buffer = new OutputBuffer();
 * while (... loop condition ...) {
 *   buffer.reset();
 *   ... write buffer using OutputStream methods ...
 *   byte[] data = buffer.getData();
 *   int dataLength = buffer.getLength();
 *   ... write data to its ultimate destination ...
 * }
 * </pre>
 * @see DataOutputBuffer
 * @see InputBuffer
 */
RawComparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/RawComparator.java)/**
 * <p>
 * A {@link Comparator} that operates directly on byte representations of
 * objects.
 * </p>
 * @param <T>
 * @see DeserializerComparator
 */
ReadaheadRequest (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ReadaheadPool.java)/**
   * An outstanding readahead request that has been submitted to
   * the pool. This request may be pending or may have been
   * completed.
   */
ReadaheadPool (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ReadaheadPool.java)/**
 * Manages a pool of threads which can issue readahead requests on file descriptors.
 */
ConcurrentQueue (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/AsyncCallHandler.java)/** A simple concurrent queue which keeping track the empty start time. */
Processor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/AsyncCallHandler.java)/** Process the async calls in the queue. */
AsyncCallQueue (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/AsyncCallHandler.java)/** A queue for handling async calls. */
AsyncCallHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/AsyncCallHandler.java)/** Handle async calls. */
CallReturn (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/CallReturn.java)/** The call return from a method invocation. */
DefaultFailoverProxyProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/DefaultFailoverProxyProvider.java)/**
 * An implementation of {@link FailoverProxyProvider} which does nothing in the
 * event of failover, and always returns the same proxy object. 
 */
FailoverProxyProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/FailoverProxyProvider.java)/**
 * An implementer of this interface is capable of providing proxy objects for
 * use in IPC communication, and potentially modifying these objects or creating
 * entirely new ones in the event of certain types of failures. The
 * determination of whether or not to fail over is handled by
 * {@link RetryPolicy}.
 */
LossyRetryInvocationHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java)/**
 * A dummy invocation handler extending RetryInvocationHandler. It drops the
 * first N number of responses. This invocation handler is only used for testing.
 */
MultiException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/MultiException.java)/**
 * Holder class that clients can use to return multiple exceptions.
 */
RetryInvocationHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java)/**
 * A {@link RpcInvocationHandler} which supports client side retry .
 */
RetryLimited (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/RetryPolicies.java)/**
   * Retry up to maxRetries.
   * The actual sleep time of the n-th retry is f(n, sleepTime),
   * where f is a function provided by the subclass implementation.
   *
   * The object of the subclasses should be immutable;
   * otherwise, the subclass must override hashCode(), equals(..) and toString().
   */
Pair (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/RetryPolicies.java)/** Pairs of numRetries and sleepSeconds */
MultipleLinearRandomRetry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/RetryPolicies.java)/**
   * Given pairs of number of retries and sleep time (n0, t0), (n1, t1), ...,
   * the first n0 retries sleep t0 milliseconds on average,
   * the following n1 retries sleep t1 milliseconds on average, and so on.
   * 
   * For all the sleep, the actual sleep time is randomly uniform distributed
   * in the close interval [0.5t, 1.5t], where t is the sleep time specified.
   *
   * The objects of this class are immutable.
   */
FailoverOnNetworkExceptionRetry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/RetryPolicies.java)/**
   * Fail over and retry in the case of:
   *   Remote StandbyException (server is up, but is not the active server)
   *   Immediate socket exceptions (e.g. no route to host, econnrefused)
   *   Socket exceptions after initial connection when operation is idempotent
   * 
   * The first failover is immediate, while all subsequent failovers wait an
   * exponentially-increasing random amount of time.
   * 
   * Fail immediately in the case of:
   *   Socket exceptions after initial connection when operation is not idempotent
   * 
   * Fall back on underlying retry policy otherwise.
   */
RetryPolicies (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/RetryPolicies.java)/**
 * <p>
 * A collection of useful implementations of {@link RetryPolicy}.
 * </p>
 */
RetryAction (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/RetryPolicy.java)/**
   * Returned by {@link RetryPolicy#shouldRetry(Exception, int, int, boolean)}.
   */
RetryPolicy (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/RetryPolicy.java)/**
 * <p>
 * Specifies a policy for retrying method failures.
 * Implementations of this interface should be immutable.
 * </p>
 */
RetryProxy (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/RetryProxy.java)/**
 * <p>
 * A factory for creating retry proxies.
 * </p>
 */
AlreadyExistsException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SecureIOUtils.java)/**
   * Signals that an attempt to create a file at a given pathname has failed
   * because another file already existed at that path.
   */
SecureIOUtils (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SecureIOUtils.java)/**
 * This class provides secure APIs for opening and creating files on the local
 * disk. The main issue this class tries to handle is that of symlink traversal.
 * <br>
 * An example of such an attack is:
 * <ol>
 * <li> Malicious user removes his task's syslog file, and puts a link to the
 * jobToken file of a target user.</li>
 * <li> Malicious user tries to open the syslog file via the servlet on the
 * tasktracker.</li>
 * <li> The tasktracker is unaware of the symlink, and simply streams the contents
 * of the jobToken file. The malicious user can now access potentially sensitive
 * map outputs, etc. of the target user's job.</li>
 * </ol>
 * A similar attack is possible involving task log truncation, but in that case
 * due to an insecure write to a file.
 * <br>
 */
ValueBytes (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/** The interface to 'raw' values of SequenceFiles. */
Metadata (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/**
   * The class encapsulating with the metadata of a file.
   * The metadata of a file is a list of attribute name/value
   * pairs of Text type.
   *
   */
FileSystemOption (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/**
     * @deprecated only used for backwards-compatibility in the createWriter methods
     * that take FileSystem.
     */
Writer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/** Write key/value pairs to a sequence-format file. */
RecordCompressWriter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/** Write key/compressed-value pairs to a sequence-format file. */
BlockCompressWriter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/** Write compressed key/value blocks to a sequence-format file. */
Option (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/**
     * A tag interface for all of the Reader options
     */
Reader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/** Reads key/value pairs from a sequence-format file. */
RawKeyValueIterator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/** The interface to iterate over raw keys/values of SequenceFiles. */
MergeQueue (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/** This class implements the core of the merge logic */
SegmentDescriptor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/** This class defines a merge segment. This class can be subclassed to 
     * provide a customized cleanup method implementation. In this 
     * implementation, cleanup closes the file handle and deletes the file 
     */
LinkedSegmentsDescriptor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/** This class provisions multiple segments contained within a single
     *  file
     */
SegmentContainer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/** The class that defines a container for segments to be merged. Primarily
     * required to delete temp files as soon as all the contained segments
     * have been looked at */
Sorter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/** Sorts key/value pairs in a sequence-format file.
   *
   * <p>For best performance, applications should make sure that the {@link
   * Writable#readFields(DataInput)} implementation of their keys is
   * very efficient.  In particular, it should avoid allocating memory.
   */
SequenceFile (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java)/** 
 * <code>SequenceFile</code>s are flat files consisting of binary key/value 
 * pairs.
 * 
 * <p><code>SequenceFile</code> provides {@link SequenceFile.Writer},
 * {@link SequenceFile.Reader} and {@link Sorter} classes for writing,
 * reading and sorting respectively.</p>
 * 
 * There are three <code>SequenceFile</code> <code>Writer</code>s based on the 
 * {@link CompressionType} used to compress key/value pairs:
 * <ol>
 *   <li>
 *   <code>Writer</code> : Uncompressed records.
 *   </li>
 *   <li>
 *   <code>RecordCompressWriter</code> : Record-compressed files, only compress 
 *                                       values.
 *   </li>
 *   <li>
 *   <code>BlockCompressWriter</code> : Block-compressed files, both keys &amp;
 *                                      values are collected in 'blocks' 
 *                                      separately and compressed. The size of 
 *                                      the 'block' is configurable.
 * </ol>
 * 
 * <p>The actual compression algorithm used to compress key and/or values can be
 * specified by using the appropriate {@link CompressionCodec}.</p>
 * 
 * <p>The recommended way is to use the static <tt>createWriter</tt> methods
 * provided by the <code>SequenceFile</code> to chose the preferred format.</p>
 *
 * <p>The {@link SequenceFile.Reader} acts as the bridge and can read any of the
 * above <code>SequenceFile</code> formats.</p>
 *
 * <h3 id="Formats">SequenceFile Formats</h3>
 * 
 * <p>Essentially there are 3 different formats for <code>SequenceFile</code>s
 * depending on the <code>CompressionType</code> specified. All of them share a
 * <a href="#Header">common header</a> described below.
 * 
 * <h4 id="Header">SequenceFile Header</h4>
 * <ul>
 *   <li>
 *   version - 3 bytes of magic header <b>SEQ</b>, followed by 1 byte of actual 
 *             version number (e.g. SEQ4 or SEQ6)
 *   </li>
 *   <li>
 *   keyClassName -key class
 *   </li>
 *   <li>
 *   valueClassName - value class
 *   </li>
 *   <li>
 *   compression - A boolean which specifies if compression is turned on for 
 *                 keys/values in this file.
 *   </li>
 *   <li>
 *   blockCompression - A boolean which specifies if block-compression is 
 *                      turned on for keys/values in this file.
 *   </li>
 *   <li>
 *   compression codec - <code>CompressionCodec</code> class which is used for  
 *                       compression of keys and/or values (if compression is 
 *                       enabled).
 *   </li>
 *   <li>
 *   metadata - {@link Metadata} for this file.
 *   </li>
 *   <li>
 *   sync - A sync marker to denote end of the header.
 *   </li>
 * </ul>
 * 
 * <h5>Uncompressed SequenceFile Format</h5>
 * <ul>
 * <li>
 * <a href="#Header">Header</a>
 * </li>
 * <li>
 * Record
 *   <ul>
 *     <li>Record length</li>
 *     <li>Key length</li>
 *     <li>Key</li>
 *     <li>Value</li>
 *   </ul>
 * </li>
 * <li>
 * A sync-marker every few <code>100</code> kilobytes or so.
 * </li>
 * </ul>
 *
 * <h5>Record-Compressed SequenceFile Format</h5>
 * <ul>
 * <li>
 * <a href="#Header">Header</a>
 * </li>
 * <li>
 * Record
 *   <ul>
 *     <li>Record length</li>
 *     <li>Key length</li>
 *     <li>Key</li>
 *     <li><i>Compressed</i> Value</li>
 *   </ul>
 * </li>
 * <li>
 * A sync-marker every few <code>100</code> kilobytes or so.
 * </li>
 * </ul>
 * 
 * <h5>Block-Compressed SequenceFile Format</h5>
 * <ul>
 * <li>
 * <a href="#Header">Header</a>
 * </li>
 * <li>
 * Record <i>Block</i>
 *   <ul>
 *     <li>Uncompressed number of records in the block</li>
 *     <li>Compressed key-lengths block-size</li>
 *     <li>Compressed key-lengths block</li>
 *     <li>Compressed keys block-size</li>
 *     <li>Compressed keys block</li>
 *     <li>Compressed value-lengths block-size</li>
 *     <li>Compressed value-lengths block</li>
 *     <li>Compressed values block-size</li>
 *     <li>Compressed values block</li>
 *   </ul>
 * </li>
 * <li>
 * A sync-marker every block.
 * </li>
 * </ul>
 * 
 * <p>The compressed blocks of key lengths and value lengths consist of the 
 * actual lengths of individual keys/values encoded in ZeroCompressedInteger 
 * format.</p>
 * 
 * @see CompressionCodec
 */
AvroReflectSerializable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerializable.java)/**
 * Tag interface for Avro 'reflect' serializable classes. Classes implementing 
 * this interface can be serialized/deserialized using 
 * {@link AvroReflectSerialization}.
 */
AvroReflectSerialization (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java)/**
 * Serialization for Avro Reflect classes. For a class to be accepted by this 
 * serialization, it must either be in the package list configured via 
 * <code>avro.reflect.pkgs</code> or implement 
 * {@link AvroReflectSerializable} interface.
 *
 */
AvroSerialization (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroSerialization.java)/**
 * Base class for providing serialization to Avro types.
 */
AvroSpecificSerialization (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroSpecificSerialization.java)/**
 * Serialization for Avro Specific classes. This serialization is to be used 
 * for classes generated by Avro's 'specific' compiler.
 */
Deserializer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/Deserializer.java)/**
 * <p>
 * Provides a facility for deserializing objects of type {@literal <T>} from an
 * {@link InputStream}.
 * </p>
 * 
 * <p>
 * Deserializers are stateful, but must not buffer the input since
 * other producers may read from the input between calls to
 * {@link #deserialize(Object)}.
 * </p>
 * @param <T>
 */
DeserializerComparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/DeserializerComparator.java)/**
 * <p>
 * A {@link RawComparator} that uses a {@link Deserializer} to deserialize
 * the objects to be compared so that the standard {@link Comparator} can
 * be used to compare them.
 * </p>
 * <p>
 * One may optimize compare-intensive operations by using a custom
 * implementation of {@link RawComparator} that operates directly
 * on byte representations.
 * </p>
 * @param <T>
 */
JavaSerialization (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/JavaSerialization.java)/**
 * <p>
 * An experimental {@link Serialization} for Java {@link Serializable} classes.
 * </p>
 * @see JavaSerializationComparator
 */
JavaSerializationComparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/JavaSerializationComparator.java)/**
 * <p>
 * A {@link RawComparator} that uses a {@link JavaSerialization}
 * {@link Deserializer} to deserialize objects that are then compared via
 * their {@link Comparable} interfaces.
 * </p>
 * @param <T>
 * @see JavaSerialization
 */
Serialization (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/Serialization.java)/**
 * <p>
 * Encapsulates a {@link Serializer}/{@link Deserializer} pair.
 * </p>
 * @param <T>
 */
SerializationFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java)/**
 * <p>
 * A factory for {@link Serialization}s.
 * </p>
 */
Serializer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/Serializer.java)/**
 * <p>
 * Provides a facility for serializing objects of type &lt;T&gt; to an
 * {@link OutputStream}.
 * </p>
 * 
 * <p>
 * Serializers are stateful, but must not buffer the output since
 * other producers may write to the output between calls to
 * {@link #serialize(Object)}.
 * </p>
 * @param <T>
 */
WritableSerialization (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/WritableSerialization.java)/**
 * A {@link Serialization} for {@link Writable}s that delegates to
 * {@link Writable#write(java.io.DataOutput)} and
 * {@link Writable#readFields(java.io.DataInput)}.
 */
Writer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SetFile.java)/** 
   * Write a new set file.
   */
Reader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SetFile.java)/** Provide access to an existing set file. */
SetFile (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SetFile.java)/** A file-based set of keys. */
Comparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ShortWritable.java)/** A Comparator optimized for ShortWritable. */
ShortWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/ShortWritable.java)/** A WritableComparable for shorts. */
SortedMapWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SortedMapWritable.java)/**
 * A Writable SortedMap.
 */
Stringifier (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Stringifier.java)/**
 * Stringifier interface offers two methods to convert an object 
 * to a string representation and restore the object given its 
 * string representation.
 * @param <T> the class of the objects to stringify
 */
Comparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Text.java)/** A WritableComparator optimized for Text keys. */
Text (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Text.java)/** This class stores text using standard UTF8 encoding.  It provides methods
 * to serialize, deserialize, and compare texts at byte level.  The type of
 * length is integer and is serialized using zero-compressed format.  <p>In
 * addition, it provides methods for string traversal without converting the
 * byte array to a string.  <p>Also includes utilities for
 * serializing/deserialing a string, coding/decoding a string, checking if a
 * byte array contains valid UTF8 code, calculating the length of an encoded
 * string.
 */
TwoDArrayWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/TwoDArrayWritable.java)/** A Writable for 2D arrays containing a matrix of instances of a class. */
Comparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/UTF8.java)/** A WritableComparator optimized for UTF8 keys. */
UTF8 (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/UTF8.java)/** A WritableComparable for strings that uses the UTF8 encoding.
 * 
 * <p>Also includes utilities for efficiently reading and writing UTF-8.
 *
 * Note that this decodes UTF-8 but actually encodes CESU-8, a variant of
 * UTF-8: see http://en.wikipedia.org/wiki/CESU-8
 *
 * @deprecated replaced by Text
 */
VersionedWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/VersionedWritable.java)/** A base class for Writables that provides version checking.
 *
 * <p>This is useful when a class may evolve, so that instances written by the
 * old version of the class may still be processed by the new version.  To
 * handle this situation, {@link #readFields(DataInput)}
 * implementations should catch {@link VersionMismatchException}.
 */
VersionMismatchException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/VersionMismatchException.java)/** Thrown by {@link VersionedWritable#readFields(DataInput)} when the
 * version of an object being read does not match the current implementation
 * version as returned by {@link VersionedWritable#getVersion()}. */
VIntWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/VIntWritable.java)/** A WritableComparable for integer values stored in variable-length format.
 * Such values take between one and five bytes.  Smaller values take fewer bytes.
 * 
 * @see org.apache.hadoop.io.WritableUtils#readVInt(DataInput)
 */
VLongWritable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/VLongWritable.java)/** A WritableComparable for longs in a variable-length format. Such values take
 *  between one and five bytes.  Smaller values take fewer bytes.
 *  
 *  @see org.apache.hadoop.io.WritableUtils#readVLong(DataInput)
 */
Writable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Writable.java)/**
 * A serializable object which implements a simple, efficient, serialization 
 * protocol, based on {@link DataInput} and {@link DataOutput}.
 *
 * <p>Any <code>key</code> or <code>value</code> type in the Hadoop Map-Reduce
 * framework implements this interface.</p>
 * 
 * <p>Implementations typically implement a static <code>read(DataInput)</code>
 * method which constructs a new instance, calls {@link #readFields(DataInput)} 
 * and returns the instance.</p>
 * 
 * <p>Example:</p>
 * <blockquote><pre>
 *     public class MyWritable implements Writable {
 *       // Some data
 *       private int counter;
 *       private long timestamp;
 *
 *       // Default constructor to allow (de)serialization
 *       MyWritable() { }
 *
 *       public void write(DataOutput out) throws IOException {
 *         out.writeInt(counter);
 *         out.writeLong(timestamp);
 *       }
 *
 *       public void readFields(DataInput in) throws IOException {
 *         counter = in.readInt();
 *         timestamp = in.readLong();
 *       }
 *
 *       public static MyWritable read(DataInput in) throws IOException {
 *         MyWritable w = new MyWritable();
 *         w.readFields(in);
 *         return w;
 *       }
 *     }
 * </pre></blockquote>
 */
WritableComparable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/WritableComparable.java)/**
 * A {@link Writable} which is also {@link Comparable}. 
 *
 * <p><code>WritableComparable</code>s can be compared to each other, typically 
 * via <code>Comparator</code>s. Any type which is to be used as a 
 * <code>key</code> in the Hadoop Map-Reduce framework should implement this
 * interface.</p>
 *
 * <p>Note that <code>hashCode()</code> is frequently used in Hadoop to partition
 * keys. It's important that your implementation of hashCode() returns the same 
 * result across different instances of the JVM. Note also that the default 
 * <code>hashCode()</code> implementation in <code>Object</code> does <b>not</b>
 * satisfy this property.</p>
 *  
 * <p>Example:</p>
 * <blockquote><pre>
 *     public class MyWritableComparable implements
 *      WritableComparable{@literal <MyWritableComparable>} {
 *       // Some data
 *       private int counter;
 *       private long timestamp;
 *       
 *       public void write(DataOutput out) throws IOException {
 *         out.writeInt(counter);
 *         out.writeLong(timestamp);
 *       }
 *       
 *       public void readFields(DataInput in) throws IOException {
 *         counter = in.readInt();
 *         timestamp = in.readLong();
 *       }
 *       
 *       public int compareTo(MyWritableComparable o) {
 *         int thisValue = this.value;
 *         int thatValue = o.value;
 *         return (thisValue &lt; thatValue ? -1 : (thisValue==thatValue ? 0 : 1));
 *       }
 *
 *       public int hashCode() {
 *         final int prime = 31;
 *         int result = 1;
 *         result = prime * result + counter;
 *         result = prime * result + (int) (timestamp ^ (timestamp &gt;&gt;&gt; 32));
 *         return result
 *       }
 *     }
 * </pre></blockquote>
 */
WritableComparator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/WritableComparator.java)/** A Comparator for {@link WritableComparable}s.
 *
 * <p>This base implementation uses the natural ordering.  To define alternate
 * orderings, override {@link #compare(WritableComparable,WritableComparable)}.
 *
 * <p>One may optimize compare-intensive operations by overriding
 * {@link #compare(byte[],int,int,byte[],int,int)}.  Static utility methods are
 * provided to assist in optimized implementations of this method.
 */
WritableFactories (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/WritableFactories.java)/** Factories for non-public writables.  Defining a factory permits {@link
 * ObjectWritable} to be able to construct instances of non-public classes. */
WritableFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/WritableFactory.java)/** A factory for a class of Writable.
 * @see WritableFactories
 */
WritableName (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/WritableName.java)/** Utility to permit renaming of Writable implementation classes without
 * invalidiating files that contain their class name.
 */
AlignmentContext (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/AlignmentContext.java)/**
 * This interface intends to align the state between client and server
 * via RPC communication.
 *
 * This should be implemented separately on the client side and server side
 * and can be used to pass state information on RPC responses from server
 * to client.
 */
AsyncCallLimitExceededException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/AsyncCallLimitExceededException.java)/**
 * Signals that an AsyncCallLimitExceededException has occurred. This class is
 * used to make application code using async RPC aware that limit of max async
 * calls is reached, application code need to retrieve results from response of
 * established async calls to avoid buffer overflow in order for follow-on async
 * calls going correctly.
 */
Builder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/CallerContext.java)/** The caller context builder. */
CurrentCallerContextHolder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/CallerContext.java)/**
   * The thread local current caller context.
   * <p>
   * Internal class for defered singleton idiom.
   * https://en.wikipedia.org/wiki/Initialization_on_demand_holder_idiom
   */
CallerContext (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/CallerContext.java)/**
 * A class defining the caller context for auditing coarse granularity
 * operations.
 *
 * This class is immutable.
 */
CallQueueManager (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/CallQueueManager.java)/**
 * Abstracts queue operations for different blocking queues.
 */
Call (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java)/** 
   * Class that represents an RPC call
   */
PingInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java)/** This class sends a ping to the remote side when timeout on
     * reading. If no failure is detected, it retries until at least
     * a byte is read.
     */
Connection (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java)/** Thread that reads responses and notifies callers.  Each connection owns a
   * socket connected to a remote address.  Calls are multiplexed through this
   * socket: responses may be delivered out of order. */
ConnectionId (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java)/**
   * This class holds the address and the user ticket. The client connections
   * to servers are uniquely identified by {@literal <}remoteAddress, protocol,
   * ticket{@literal >}
   */
IpcStreams (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java)/** Manages the input and output streams for an IPC connection.
   *  Only exposed for use by SaslRpcClient.
   */
Client (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java)/** A client for an IPC service.  IPC calls take a single {@link Writable} as a
 * parameter, and return a {@link Writable} as their value.  A service runs on
 * a port and is defined by a parameter class and a value class.
 * 
 * @see Server
 */
ClientId (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ClientId.java)/**
 * A class defining a set of static helper methods to provide conversion between
 * bytes and string for UUID-based client Id.
 */
CostProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/CostProvider.java)/**
 * Used by {@link DecayRpcScheduler} to get the cost of users' operations. This
 * is configurable using
 * {@link org.apache.hadoop.fs.CommonConfigurationKeys#IPC_COST_PROVIDER_KEY}.
 */
DecayTask (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java)/**
   * This TimerTask will call decayCurrentCosts until
   * the scheduler has been garbage collected.
   */
MetricsProxy (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java)/**
   * MetricsProxy is a singleton because we may init multiple schedulers and we
   * want to clean up resources when a new scheduler replaces the old one.
   */
DecayRpcScheduler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java)/**
 * The decay RPC scheduler tracks the cost of incoming requests in a map, then
 * decays the costs at a fixed time interval. The scheduler is optimized
 * for large periods (on the order of seconds), as it offloads work to the
 * decay sweep.
 */
DecayRpcSchedulerMXBean (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcSchedulerMXBean.java)/**
 * Provides metrics for Decay scheduler.
 */
DefaultCostProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DefaultCostProvider.java)/**
 * Ignores process details and returns a constant value for each call.
 */
DefaultRpcScheduler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DefaultRpcScheduler.java)/**
 * No op default RPC scheduler.
 */
MetricsProxy (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java)/**
   * MetricsProxy is a singleton because we may init multiple
   * FairCallQueues, but the metrics system cannot unregister beans cleanly.
   */
FairCallQueue (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java)/**
 * A queue with multiple levels for each priority.
 */
GenericRefreshProtocol (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/GenericRefreshProtocol.java)/**
 * Protocol which is used to refresh arbitrary things at runtime.
 */
IdentityProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IdentityProvider.java)/**
 * The IdentityProvider creates identities for each schedulable
 * by extracting fields and returning an identity string.
 *
 * Implementers will be able to change how schedulers treat
 * Schedulables.
 */
IpcException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IpcException.java)/**
 * IPC exception is thrown by IPC layer when the IPC
 * connection cannot be established.
 */
RetryCacheMetrics (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java)/**
 * This class is for maintaining the various RetryCache-related statistics
 * and publishing them through the metrics interfaces.
 */
RpcDetailedMetrics (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java)/**
 * This class is for maintaining RPC method related statistics
 * and publishing them through the metrics interfaces.
 */
RpcMetrics (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java)/**
 * This class is for maintaining  the various RPC statistics
 * and publishing them through the metrics interfaces.
 */
ObserverRetryOnActiveException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ObserverRetryOnActiveException.java)/**
 * Thrown by a remote ObserverNode indicating the operation has failed and the
 * client should retry active namenode directly (instead of retry other
 * ObserverNodes).
 */
ProcessingDetails (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ProcessingDetails.java)/**
 * Stores the times that a call takes to be processed through each step.
 */
ProtobufHelper (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ProtobufHelper.java)/**
 * Helper methods for protobuf related RPC implementation
 */
ProtoBufRpcInvoker (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java)/**
     * Protobuf invoker for {@link RpcInvoker}
     */
ProtobufRpcEngine (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java)/**
 * RPC Engine for for protobuf based RPCs.
 */
ProtocolMetaInfoPB (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ProtocolMetaInfoPB.java)/**
 * Protocol to get versions and signatures for supported protocols from the
 * server.
 * 
 * Note: This extends the protocolbuffer service based interface to
 * add annotations.
 */
ProtocolMetaInfoServerSideTranslatorPB (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java)/**
 * This class serves the requests for protocol versions and signatures by
 * looking them up in the server registry.
 */
ProtocolMetaInterface (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ProtocolMetaInterface.java)/**
 * This interface is implemented by the client side translators and can be used
 * to obtain information about underlying protocol e.g. to check if a method is
 * supported on the server side.
 */
ProtocolProxy (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ProtocolProxy.java)/**
 * a class wraps around a server's proxy, 
 * containing a list of its supported methods.
 * 
 * A list of methods with a value of null indicates that the client and server
 * have the same protocol.
 */
ProtocolTranslator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ProtocolTranslator.java)/**
 * An interface implemented by client-side protocol translators to get the
 * underlying proxy object the translator is operating on.
 */
ProxyCombiner (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/ProxyCombiner.java)/**
 * A utility class used to combine two protocol proxies.
 * See {@link #combine(Class, Object...)}.
 */
RefreshCallQueueProtocol (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RefreshCallQueueProtocol.java)/**
 * Protocol which is used to refresh the call queue in use currently.
 */
RefreshHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RefreshHandler.java)/**
 * Used to registry custom methods to refresh at runtime.
 */
RefreshRegistry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RefreshRegistry.java)/**
 * Used to registry custom methods to refresh at runtime.
 * Each identifier maps to one or more RefreshHandlers.
 */
RefreshResponse (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RefreshResponse.java)/**
 * Return a response in the handler method for the user to see.
 * Useful since you may want to display status to a user even though an
 * error has not occurred.
 */
RetriableException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RetriableException.java)/**
 * Exception thrown by a server typically to indicate that server is in a state
 * where request cannot be processed temporarily (such as still starting up).
 * Client may retry the request. If the service is up, the server may be able to
 * process a retried request.
 */
CacheEntry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RetryCache.java)/**
   * CacheEntry is tracked using unique client ID and callId of the RPC request
   */
CacheEntryWithPayload (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RetryCache.java)/**
   * CacheEntry with payload that tracks the previous response or parts of
   * previous response to be used for generating response for retried requests.
   */
RetryCache (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RetryCache.java)/**
 * Maintains a cache of non-idempotent requests that have been successfully
 * processed by the RPC server implementation, to handle the retries. A request
 * is uniquely identified by the unique client ID + call ID of the RPC request.
 * On receiving retried request, an entry will be found in the
 * {@link RetryCache} and the previous response is sent back to the request.
 * <p>
 * To look an implementation using this cache, see HDFS FSNamesystem class.
 */
VersionMismatch (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java)/**
   * A version mismatch for the RPC protocol.
   */
Builder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java)/**
   * Class to construct instances of RPC server with specific options.
   */
ProtoNameVer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java)/**
    *  The key in Map
    */
ProtoClassProtoImpl (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java)/**
    * The value in map
    */
Server (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java)/** An RPC Server. */
RPC (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java)/** A simple RPC mechanism.
 *
 * A <i>protocol</i> is a Java interface.  All parameters and return types must
 * be one of:
 *
 * <ul> <li>a primitive type, <code>boolean</code>, <code>byte</code>,
 * <code>char</code>, <code>short</code>, <code>int</code>, <code>long</code>,
 * <code>float</code>, <code>double</code>, or <code>void</code>; or</li>
 *
 * <li>a {@link String}; or</li>
 *
 * <li>a {@link Writable}; or</li>
 *
 * <li>an array of the above types</li> </ul>
 *
 * All methods in the protocol should throw only IOException.  No field data of
 * the protocol instance is transmitted.
 */
RpcClientException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RpcClientException.java)/**
 * Indicates an exception in the RPC client 
 */
RpcClientUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RpcClientUtil.java)/**
 * This class maintains a cache of protocol versions and corresponding protocol
 * signatures, keyed by server address, protocol and rpc kind.
 * The cache is lazily populated. 
 */
RpcEngine (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RpcEngine.java)/** An RPC implementation. */
RpcException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RpcException.java)/**
 * Indicates an exception during the execution of remote procedure call.
 */
RpcInvocationHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RpcInvocationHandler.java)/**
 * This interface must be implemented by all InvocationHandler
 * implementations.
 */
RpcMultiplexer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RpcMultiplexer.java)/**
 * Implement this interface to make a pluggable multiplexer in the
 * FairCallQueue.
 */
RpcNoSuchMethodException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RpcNoSuchMethodException.java)/**
 * No such Method for an Rpc Call
 *
 */
RpcNoSuchProtocolException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RpcNoSuchProtocolException.java)/**
 * No such protocol (i.e. interface) for and Rpc Call
 *
 */
RpcScheduler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RpcScheduler.java)/**
 * Implement this interface to be used for RPC scheduling and backoff.
 *
 */
RpcServerException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RpcServerException.java)/**
 * Indicates an exception on the RPC server 
 */
Buffer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RpcWritable.java)/**
   * adapter to allow decoding of writables and protobufs from a byte buffer.
   */
Schedulable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Schedulable.java)/**
 * Interface which allows extracting information necessary to
 * create schedulable identity strings.
 */
ExceptionsHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java)/**
   * ExceptionsHandler manages Exception groups for special handling
   * e.g., terse exception group for concise logging messages
   */
Call (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java)/** A generic call queued for handling. */
ResponseParams (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java)/**
     * Holds response parameters. Defaults set to work for successful
     * invocations
     */
RpcCall (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java)/** A RPC extended call queued for handling. */
Listener (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java)/** Listens on the socket. Creates jobs for the handler threads*/
FatalRpcServerException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java)/**
   * Wrapper for RPC IOExceptions to be returned to the client.  Used to
   * let exceptions bubble up to top of processOneRpc where the correct
   * callId can be associated with the response.  Also used to prevent
   * unnecessary stack trace logging if it's not an internal server error. 
   */
Connection (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java)/** Reads calls from a connection and queues them for handling. */
Handler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java)/** Handles queued calls . */
Server (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java)/** An abstract IPC service.  IPC calls take a single {@link Writable} as a
 * parameter, and return a {@link Writable} as their value.  A service runs on
 * a port and is defined by a parameter class and a value class.
 * 
 * @see Client
 */
StandbyException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/StandbyException.java)/**
 * Thrown by a remote server when it is up, but is not the active server in a
 * set of servers in which only a subset may be active.
 */
UnexpectedServerException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/UnexpectedServerException.java)/**
 * Indicates that the RPC server encountered an undeclared exception from the
 * service 
 */
UserIdentityProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/UserIdentityProvider.java)/**
 * The UserIdentityProvider creates uses the username as the
 * identity. All jobs launched by a user will be grouped together.
 */
VersionedProtocol (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/VersionedProtocol.java)/**
 * Superclass of all protocols that use Hadoop RPC.
 * Subclasses of this interface are also supposed to have
 * a static final long versionID field.
 */
WeightedRoundRobinMultiplexer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java)/**
 * Determines which queue to start reading from, occasionally drawing from
 * low-priority queues in order to prevent starvation. Given the pull pattern
 * [9, 4, 1] for 3 queues:
 *
 * The cycle is (a minimum of) 9+4+1=14 reads.
 * Queue 0 is read (at least) 9 times
 * Queue 1 is read (at least) 4 times
 * Queue 2 is read (at least) 1 time
 * Repeat
 *
 * There may be more reads than the minimum due to race conditions. This is
 * allowed by design for performance reasons.
 */
WeightedTimeCostProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WeightedTimeCostProvider.java)/**
 * A {@link CostProvider} that calculates the cost for an operation
 * as a weighted sum of its processing time values (see
 * {@link ProcessingDetails}). This can be used by specifying the
 * {@link org.apache.hadoop.fs.CommonConfigurationKeys#IPC_COST_PROVIDER_KEY}
 * configuration key.
 *
 * <p/>This allows for configuration of how heavily each of the operations
 * within {@link ProcessingDetails} is weighted. By default,
 * {@link ProcessingDetails.Timing#LOCKFREE},
 * {@link ProcessingDetails.Timing#RESPONSE}, and
 * {@link ProcessingDetails.Timing#HANDLER} times have a weight of
 * {@value #DEFAULT_LOCKFREE_WEIGHT},
 * {@link ProcessingDetails.Timing#LOCKSHARED} has a weight of
 * {@value #DEFAULT_LOCKSHARED_WEIGHT},
 * {@link ProcessingDetails.Timing#LOCKEXCLUSIVE} has a weight of
 * {@value #DEFAULT_LOCKEXCLUSIVE_WEIGHT}, and others are ignored.
 * These values can all be configured using the {@link #WEIGHT_CONFIG_PREFIX}
 * key, prefixed with the IPC namespace, and suffixed with the name of the
 * timing measurement from {@link ProcessingDetails} (all lowercase).
 * For example, to set the lock exclusive weight to be 1000, set:
 * <pre>
 *   ipc.8020.cost-provider.impl=org.apache.hadoop.ipc.WeightedTimeCostProvider
 *   ipc.8020.weighted-cost.lockexclusive=1000
 * </pre>
 */
Invocation (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WritableRpcEngine.java)/** A method invocation, including the method name and its parameters.*/
Server (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WritableRpcEngine.java)/** An RPC Server. */
WritableRpcEngine (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WritableRpcEngine.java)/** An RpcEngine implementation for Writable data. */
JMXJsonServlet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/jmx/JMXJsonServlet.java)/**
 * Provides Read only web access to JMX.
 * <p>
 * This servlet generally will be placed under the /jmx URL for each
 * HttpServer.  It provides read only
 * access to JMX metrics.  The optional <code>qry</code> parameter
 * may be used to query only a subset of the JMX Beans.  This query
 * functionality is provided through the
 * {@link MBeanServer#queryNames(ObjectName, javax.management.QueryExp)}
 * method.
 * <p>
 * For example <code>http://.../jmx?qry=Hadoop:*</code> will return
 * all hadoop metrics exposed through JMX.
 * <p>
 * The optional <code>get</code> parameter is used to query an specific 
 * attribute of a JMX bean.  The format of the URL is
 * <code>http://.../jmx?get=MXBeanName::AttributeName</code>
 * <p>
 * For example 
 * <code>
 * http://../jmx?get=Hadoop:service=NameNode,name=NameNodeInfo::ClusterId
 * </code> will return the cluster id of the namenode mxbean.
 * <p>
 * If the <code>qry</code> or the <code>get</code> parameter is not formatted 
 * correctly then a 400 BAD REQUEST http response code will be returned. 
 * <p>
 * If a resouce such as a mbean or attribute can not be found, 
 * a 404 SC_NOT_FOUND http response code will be returned. 
 * <p>
 * The return format is JSON and in the form
 * <p>
 *  <pre><code>
 *  {
 *    "beans" : [
 *      {
 *        "name":"bean-name"
 *        ...
 *      }
 *    ]
 *  }
 *  </code></pre>
 *  <p>
 *  The servlet attempts to convert the the JMXBeans into JSON. Each
 *  bean's attributes will be converted to a JSON object member.
 *  
 *  If the attribute is a boolean, a number, a string, or an array
 *  it will be converted to the JSON equivalent. 
 *  
 *  If the value is a {@link CompositeData} then it will be converted
 *  to a JSON object with the keys as the name of the JSON member and
 *  the value is converted following these same rules.
 *  
 *  If the value is a {@link TabularData} then it will be converted
 *  to an array of the {@link CompositeData} elements that it contains.
 *  
 *  All other objects will be converted to a string and output as such.
 *  
 *  The bean's name and modelerType will be returned for all beans.
 *
 */
EventCounter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/log/EventCounter.java)/**
 * A log4J Appender that simply counts logging events in three levels:
 * fatal, error and warn. The class name is used in log4j.properties
 * @deprecated use {@link org.apache.hadoop.log.metrics.EventCounter} instead
 */
Log4Json (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/log/Log4Json.java)/**
 * This offers a log layout for JSON, with some test entry points. It's purpose is
 * to allow Log4J to generate events that are easy for other programs to parse, but which are somewhat
 * human-readable.
 *
 * Some features.
 *
 * <ol>
 *     <li>Every event is a standalone JSON clause</li>
 *     <li>Time is published as a time_t event since 1/1/1970
 *      -this is the fastest to generate.</li>
 *     <li>An ISO date is generated, but this is cached and will only be accurate to within a second</li>
 *     <li>the stack trace is included as an array</li>
 * </ol>
 *
 * A simple log event will resemble the following
 * <pre>
 *     {"name":"test","time":1318429136789,"date":"2011-10-12 15:18:56,789","level":"INFO","thread":"main","message":"test message"}
 * </pre>
 *
 * An event with an error will contain data similar to that below (which has been reformatted to be multi-line).
 *
 * <pre>
 *     {
 *     "name":"testException",
 *     "time":1318429136789,
 *     "date":"2011-10-12 15:18:56,789",
 *     "level":"INFO",
 *     "thread":"quoted\"",
 *     "message":"new line\n and {}",
 *     "exceptionclass":"java.net.NoRouteToHostException",
 *     "stack":[
 *         "java.net.NoRouteToHostException: that box caught fire 3 years ago",
 *         "\tat org.apache.hadoop.log.TestLog4Json.testException(TestLog4Json.java:49)",
 *         "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
 *         "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
 *         "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
 *         "\tat java.lang.reflect.Method.invoke(Method.java:597)",
 *         "\tat junit.framework.TestCase.runTest(TestCase.java:168)",
 *         "\tat junit.framework.TestCase.runBare(TestCase.java:134)",
 *         "\tat junit.framework.TestResult$1.protect(TestResult.java:110)",
 *         "\tat junit.framework.TestResult.runProtected(TestResult.java:128)",
 *         "\tat junit.framework.TestResult.run(TestResult.java:113)",
 *         "\tat junit.framework.TestCase.run(TestCase.java:124)",
 *         "\tat junit.framework.TestSuite.runTest(TestSuite.java:232)",
 *         "\tat junit.framework.TestSuite.run(TestSuite.java:227)",
 *         "\tat org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)",
 *         "\tat org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)",
 *         "\tat org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:120)",
 *         "\tat org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:145)",
 *         "\tat org.apache.maven.surefire.Surefire.run(Surefire.java:104)",
 *         "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
 *         "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
 *         "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
 *         "\tat java.lang.reflect.Method.invoke(Method.java:597)",
 *         "\tat org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:290)",
 *         "\tat org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1017)"
 *         ]
 *     }
 * </pre>
 */
Servlet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/log/LogLevel.java)/**
   * A servlet implementation
   */
LogLevel (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/log/LogLevel.java)/**
 * Change log level in runtime.
 */
LogAction (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/log/LogThrottlingHelper.java)/**
   * An indication of what action the caller should take. If
   * {@link #shouldLog()} is false, no other action should be taken, and it is
   * an error to try to access any of the summary information. If
   * {@link #shouldLog()} is true, then the caller should write to its log, and
   * can use the {@link #getCount()} and {@link #getStats(int)} methods to
   * determine summary information about what has been recorded into this
   * helper.
   *
   * All summary information in this action only represents
   * {@link #record(double...)} statements which were called <i>after</i> the
   * last time the caller logged something; that is, since the last time a log
   * action was returned with a true value for {@link #shouldLog()}. Information
   * about the {@link #record(double...)} statement which created this log
   * action is included.
   */
LoggingAction (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/log/LogThrottlingHelper.java)/**
   * A standard log action which keeps track of all of the values which have
   * been logged. This is also used for internal bookkeeping via its private
   * fields and methods; it will maintain whether or not it is ready to be
   * logged ({@link #shouldLog()}) as well as whether or not it has been
   * returned for logging yet ({@link #hasLogged()}).
   */
NoLogAction (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/log/LogThrottlingHelper.java)/**
   * A non-logging action.
   *
   * @see #DO_NOT_LOG
   */
LogThrottlingHelper (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/log/LogThrottlingHelper.java)/**
 * This is a class to help easily throttle log statements, so that they will
 * not be emitted more frequently than a certain rate. It is useful to help
 * prevent flooding the application logs with redundant messages.
 *
 * The instantiator specifies a minimum period at which statements should be
 * logged. When {@link #record(double...)} is called, if enough time has elapsed
 * since the last time it was called, the return value will indicate to the
 * caller that it should write to its actual log. Note that this class does not
 * write to any actual log; it only records information about how many times
 * {@code record} has been called and with what arguments, and indicates to the
 * caller whether or not it should write to its log. If not enough time has yet
 * elapsed, this class records the arguments and updates its summary
 * information, and indicates to the caller that it should not log.
 *
 * For example, say that you want to know whenever too large of a request is
 * received, but want to avoid flooding the logs if many such requests are
 * received.
 * <pre>{@code
 *   // Helper with a minimum period of 5 seconds
 *   private LogThrottlingHelper helper = new LogThrottlingHelper(5000);
 *
 *   public void receiveRequest(int requestedObjects) {
 *     if (requestedObjects > MAXIMUM_REQUEST_SIZE) {
 *       LogAction logAction = helper.record(requestedObjects);
 *       if (logAction.shouldLog()) {
 *         LOG.warn("Received {} large request(s) with a total of {} objects " +
 *             "requested; maximum objects requested was {}",
 *             logAction.getCount(), logAction.getStats(0).getSum(),
 *             logAction.getStats(0).getMax());
 *       }
 *     }
 *   }
 * }</pre>
 * The above snippet allows you to record extraneous events, but if they become
 * frequent, to limit their presence in the log to only every 5 seconds while
 * still maintaining overall information about how many large requests were
 * received.
 *
 * <p>This class can also be used to coordinate multiple logging points; see
 * {@link #record(String, long, double...)} for more details.
 *
 * <p>This class is not thread-safe.
 */
EventCounter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/log/metrics/EventCounter.java)/**
 * A log4J Appender that simply counts logging events in three levels:
 * fatal, error and warn. The class name is used in log4j.properties
 */
AbstractMetric (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/AbstractMetric.java)/**
 * The immutable metric
 */
AbstractPatternFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java)/**
 * Base class for pattern based filters
 */
GlobFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/filter/GlobFilter.java)/**
 * A glob pattern filter for metrics.
 *
 * The class name is used in metrics config files
 */
RegexFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/filter/RegexFilter.java)/**
 * A regex pattern filter for metrics
 */
MBeanInfoBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java)/**
 * Helper class to build MBeanInfo from metrics records
 */
MetricsBuffer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsBuffer.java)/**
 * An immutable element for the sink queues.
 */
MetricsBufferBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsBufferBuilder.java)/**
 * Builder for the immutable metrics buffers
 */
MetricsConfig (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java)/**
 * Metrics configuration for MetricsSystemImpl
 */
MetricsConfigException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java)/**
 *  The metrics configuration runtime exception
 */
MetricsRecordBuilderImpl (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java)/**
 * {@link MetricsRecordBuilder} implementation used for building metrics records
 * by the {@link MetricsCollector}. It provides the following functionality:
 * <ul>
 * <li>Allows configuring filters for metrics.
 * </ul>
 *
 */
MetricsSinkAdapter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java)/**
 * An adapter class for metrics sink and associated filters
 */
MetricsSourceAdapter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java)/**
 * An adapter class for metrics source and associated filter and jmx impl
 */
MetricsSystemImpl (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java)/**
 * A base class for metrics system singletons
 */
SinkQueue (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/SinkQueue.java)/**
 * A half-blocking (nonblocking for producers, blocking for consumers) queue
 * for metrics sinks.
 *
 * New elements are dropped when the queue is full to preserve "interesting"
 * elements at the onset of queue filling events
 */
Interns (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/Interns.java)/**
 * Helpers to create interned metrics info.
 */
MethodMetric (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MethodMetric.java)/**
 * Metric generated from a method, mostly used by annotation
 */
MetricsAnnotations (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MetricsAnnotations.java)/**
 * Metrics annotation helpers.
 */
MetricsInfoImpl (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MetricsInfoImpl.java)/**
 * Making implementing metric info a little easier
 */
MetricsRegistry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java)/**
 * An optional metrics registry class for creating and maintaining a
 * collection of MetricsMutables, making writing metrics source easier.
 */
MetricsSourceBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java)/**
 * Helper class to build {@link MetricsSource} object from annotations.
 * <p>
 * For a given source object:
 * <ul>
 * <li>Sets the {@link Field}s annotated with {@link Metric} to
 * {@link MutableMetric} and adds it to the {@link MetricsRegistry}.</li>
 * <li>
 * For {@link Method}s annotated with {@link Metric} creates
 * {@link MutableMetric} and adds it to the {@link MetricsRegistry}.</li>
 * </ul>
 */
MutableCounter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableCounter.java)/**
 * The mutable counter (monotonically increasing) metric interface
 */
MutableCounterInt (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java)/**
 * A mutable int counter for implementing metrics sources
 */
MutableCounterLong (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java)/**
 * A mutable long counter
 */
MutableGauge (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableGauge.java)/**
 * The mutable gauge metric interface
 */
MutableGaugeFloat (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java)/**
 * A mutable float gauge.
 */
MutableGaugeInt (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java)/**
 * A mutable int gauge
 */
MutableGaugeLong (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java)/**
 * A mutable long gauge
 */
MutableMetric (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableMetric.java)/**
 * The mutable metric interface
 */
RolloverSample (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java)/**
   * Runnable used to periodically roll over the internal
   * {@link SampleQuantiles} every interval.
   */
MutableQuantiles (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java)/**
 * Watches a stream of long values, maintaining online estimates of specific
 * quantiles with provably low error bounds. This is particularly useful for
 * accurate high-percentile (e.g. 95th, 99th) latency metrics.
 */
MutableRate (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableRate.java)/**
 * A convenient mutable metric for throughput measurement
 */
MutableRates (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableRates.java)/**
 * Helper class to manage a group of mutable rate metrics
 *
 * This class synchronizes all accesses to the metrics it
 * contains, so it should not be used in situations where
 * there is high contention on the metrics.
 * {@link MutableRatesWithAggregation} is preferable in that
 * situation.
 */
MutableRatesWithAggregation (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java)/**
 * Helper class to manage a group of mutable rate metrics.
 *
 * Each thread will maintain a local rate count, and upon snapshot,
 * these values will be aggregated into a global rate. This class
 * should only be used for long running threads, as any metrics
 * produced between the last snapshot and the death of a thread
 * will be lost. This allows for significantly higher concurrency
 * than {@link MutableRates}. See HADOOP-24420.
 */
MutableRollingAverages (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java)/**
 * <p>
 * This class maintains a group of rolling average metrics. It implements the
 * algorithm of rolling average, i.e. a number of sliding windows are kept to
 * roll over and evict old subsets of samples. Each window has a subset of
 * samples in a stream, where sub-sum and sub-total are collected. All sub-sums
 * and sub-totals in all windows will be aggregated to final-sum and final-total
 * used to compute final average, which is called rolling average.
 * </p>
 */
MutableStat (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableStat.java)/**
 * A mutable metric with stats.
 *
 * Useful for keeping throughput/latency stats.
 */
UniqueNames (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/UniqueNames.java)/**
 * Generates predictable and user-friendly unique names
 */
MetricsCollector (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsCollector.java)/**
 * The metrics collector interface
 */
MetricsException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsException.java)/**
 * A general metrics exception wrapper
 */
MetricsFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsFilter.java)/**
 * The metrics filter interface. The MetricsFilter objects can be used either to
 * filter the metrics from {@link MetricsSource}s or to filter metrics per
 * {@link MetricsSink}.
 */
MetricsInfo (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsInfo.java)/**
 * Interface to provide immutable metainfo for metrics.
 */
MetricsJsonBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java)/**
 * Build a JSON dump of the metrics.
 *
 * The {@link #toString()} operator dumps out all values collected.
 *
 */
MetricsPlugin (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsPlugin.java)/**
 * The plugin interface for the metrics framework
 */
MetricsRecord (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsRecord.java)/**
 * An immutable snapshot of metrics with a timestamp
 */
MetricsRecordBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsRecordBuilder.java)/**
 * The metrics record builder interface
 */
MetricsSink (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsSink.java)/**
 * The metrics sink interface. <p>
 * Implementations of this interface consume the {@link MetricsRecord} generated
 * from {@link MetricsSource}. It registers with {@link MetricsSystem} which
 * periodically pushes the {@link MetricsRecord} to the sink using
 * {@link #putMetrics(MetricsRecord)} method.  If the implementing class also
 * implements {@link Closeable}, then the MetricsSystem will close the sink when
 * it is stopped.
 */
MetricsSource (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsSource.java)/**
 * The source of metrics information. It generates and updates metrics. It
 * registers with {@link MetricsSystem}, which periodically polls it to collect
 * {@link MetricsRecord} and passes it to {@link MetricsSink}.
 */
Callback (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsSystem.java)/**
   * The metrics system callback interface (needed for proxies.)
   */
AbstractCallback (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsSystem.java)/**
   * Convenient abstract class for implementing callback interface
   */
MetricsSystem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsSystem.java)/**
 * The metrics system interface.
 * 
 * The following components are used for metrics.
 * <ul>
 * <li>{@link MetricsSource} generate and update metrics information.</li>
 * <li>{@link MetricsSink} consume the metrics information</li>
 * </ul>
 * 
 * {@link MetricsSource} and {@link MetricsSink} register with the metrics
 * system. Implementations of {@link MetricsSystem} polls the
 * {@link MetricsSource}s periodically and pass the {@link MetricsRecord}s to
 * {@link MetricsSink}.
 */
MetricsSystemMXBean (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsSystemMXBean.java)/**
 * The JMX interface to the metrics system
 */
MetricsTag (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsTag.java)/**
 * Immutable tag for metrics (for grouping on host/queue/username etc.)
 */
MetricStringBuilder (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricStringBuilder.java)/**
 * Build a string dump of the metrics.
 *
 * The {@link #toString()} operator dumps out all values collected.
 *
 * Every entry is formatted as
 * {@code prefix + name + separator + value + suffix}
 */
MetricsVisitor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/MetricsVisitor.java)/**
 * A visitor interface for metrics
 */
FileSink (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/FileSink.java)/**
 * A metrics sink that writes to a file
 */
AbstractGangliaSink (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java)/**
 * This the base class for Ganglia sink classes using metrics2. Lot of the code
 * has been derived from org.apache.hadoop.metrics.ganglia.GangliaContext.
 * As per the documentation, sink implementations doesn't have to worry about
 * thread safety. Hence the code wasn't written for thread safety and should
 * be modified in case the above assumption changes in the future.
 */
GangliaConf (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaConf.java)/**
 * class which is used to store ganglia properties
 */
GangliaMetricVisitor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaMetricVisitor.java)/**
 * Since implementations of Metric are not public, hence use a visitor to figure
 * out the type and slope of the metric. Counters have "positive" slope.
 */
GangliaSink30 (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java)/**
 * This code supports Ganglia 3.0
 * 
 */
GangliaSink31 (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.java)/**
 * This code supports Ganglia 3.1
 *
 */
GraphiteSink (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java)/**
 * A metrics sink that writes to a Graphite server
 */
PrometheusMetricsSink (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java)/**
 * Metrics sink for prometheus exporter.
 * <p>
 * Stores the metric data in-memory and return with it on request.
 */
RollingFileSystemSink (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java)/**
 * <p>This class is a metrics sink that uses
 * {@link org.apache.hadoop.fs.FileSystem} to write the metrics logs.  Every
 * roll interval a new directory will be created under the path specified by the
 * <code>basepath</code> property. All metrics will be logged to a file in the
 * current interval's directory in a file named &lt;hostname&gt;.log, where
 * &lt;hostname&gt; is the name of the host on which the metrics logging
 * process is running. The base path is set by the
 * <code>&lt;prefix&gt;.sink.&lt;instance&gt;.basepath</code> property.  The
 * time zone used to create the current interval's directory name is GMT.  If
 * the <code>basepath</code> property isn't specified, it will default to
 * &quot;/tmp&quot;, which is the temp directory on whatever default file
 * system is configured for the cluster.</p>
 *
 * <p>The <code>&lt;prefix&gt;.sink.&lt;instance&gt;.ignore-error</code>
 * property controls whether an exception is thrown when an error is encountered
 * writing a log file.  The default value is <code>true</code>.  When set to
 * <code>false</code>, file errors are quietly swallowed.</p>
 *
 * <p>The <code>roll-interval</code> property sets the amount of time before
 * rolling the directory. The default value is 1 hour. The roll interval may
 * not be less than 1 minute. The property's value should be given as
 * <i>number unit</i>, where <i>number</i> is an integer value, and
 * <i>unit</i> is a valid unit.  Valid units are <i>minute</i>, <i>hour</i>,
 * and <i>day</i>.  The units are case insensitive and may be abbreviated or
 * plural. If no units are specified, hours are assumed. For example,
 * &quot;2&quot;, &quot;2h&quot;, &quot;2 hour&quot;, and
 * &quot;2 hours&quot; are all valid ways to specify two hours.</p>
 *
 * <p>The <code>roll-offset-interval-millis</code> property sets the upper
 * bound on a random time interval (in milliseconds) that is used to delay
 * before the initial roll.  All subsequent rolls will happen an integer
 * number of roll intervals after the initial roll, hence retaining the original
 * offset. The purpose of this property is to insert some variance in the roll
 * times so that large clusters using this sink on every node don't cause a
 * performance impact on HDFS by rolling simultaneously.  The default value is
 * 30000 (30s).  When writing to HDFS, as a rule of thumb, the roll offset in
 * millis should be no less than the number of sink instances times 5.
 *
 * <p>The primary use of this class is for logging to HDFS.  As it uses
 * {@link org.apache.hadoop.fs.FileSystem} to access the target file system,
 * however, it can be used to write to the local file system, Amazon S3, or any
 * other supported file system.  The base path for the sink will determine the
 * file system used.  An unqualified path will write to the default file system
 * set by the configuration.</p>
 *
 * <p>Not all file systems support the ability to append to files.  In file
 * systems without the ability to append to files, only one writer can write to
 * a file at a time.  To allow for concurrent writes from multiple daemons on a
 * single host, the <code>source</code> property is used to set unique headers
 * for the log files.  The property should be set to the name of
 * the source daemon, e.g. <i>namenode</i>.  The value of the
 * <code>source</code> property should typically be the same as the property's
 * prefix.  If this property is not set, the source is taken to be
 * <i>unknown</i>.</p>
 *
 * <p>Instead of appending to an existing file, by default the sink
 * will create a new file with a suffix of &quot;.&lt;n&gt;&quot;, where
 * <i>n</i> is the next lowest integer that isn't already used in a file name,
 * similar to the Hadoop daemon logs.  NOTE: the file with the <b>highest</b>
 * sequence number is the <b>newest</b> file, unlike the Hadoop daemon logs.</p>
 *
 * <p>For file systems that allow append, the sink supports appending to the
 * existing file instead. If the <code>allow-append</code> property is set to
 * true, the sink will instead append to the existing file on file systems that
 * support appends. By default, the <code>allow-append</code> property is
 * false.</p>
 *
 * <p>Note that when writing to HDFS with <code>allow-append</code> set to true,
 * there is a minimum acceptable number of data nodes.  If the number of data
 * nodes drops below that minimum, the append will succeed, but reading the
 * data will fail with an IOException in the DataStreamer class.  The minimum
 * number of data nodes required for a successful append is generally 2 or
 * 3.</p>
 *
 * <p>Note also that when writing to HDFS, the file size information is not
 * updated until the file is closed (at the end of the interval) even though
 * the data is being written successfully. This is a known HDFS limitation that
 * exists because of the performance cost of updating the metadata.  See
 * <a href="https://issues.apache.org/jira/browse/HDFS-5478">HDFS-5478</a>.</p>
 *
 * <p>When using this sink in a secure (Kerberos) environment, two additional
 * properties must be set: <code>keytab-key</code> and
 * <code>principal-key</code>. <code>keytab-key</code> should contain the key by
 * which the keytab file can be found in the configuration, for example,
 * <code>yarn.nodemanager.keytab</code>. <code>principal-key</code> should
 * contain the key by which the principal can be found in the configuration,
 * for example, <code>yarn.nodemanager.principal</code>.
 */
StatsD (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/StatsDSink.java)/**
   * Class that sends UDP packets to StatsD daemon.
   *
   */
StatsDSink (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/StatsDSink.java)/**
 * A metrics sink that writes metrics to a StatsD daemon.
 * This sink will produce metrics of the form
 * '[hostname].servicename.context.name.metricname:value|type'
 * where hostname is optional. This is useful when sending to
 * a daemon that is running on the localhost and will add the
 * hostname to the metric (such as the
 * <a href="https://collectd.org/">CollectD</a> StatsD plugin).
 * <br>
 * To configure this plugin, you will need to add the following
 * entries to your hadoop-metrics2.properties file:
 * <br>
 * <pre>
 * *.sink.statsd.class=org.apache.hadoop.metrics2.sink.StatsDSink
 * [prefix].sink.statsd.server.host=
 * [prefix].sink.statsd.server.port=
 * [prefix].sink.statsd.skip.hostname=true|false (optional)
 * [prefix].sink.statsd.service.name=NameNode (name you want for service)
 * </pre>
 */
JvmMetrics (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/source/JvmMetrics.java)/**
 * JVM and logging related metrics.
 * Mostly used by various servers as a part of the metrics they export.
 */
Contracts (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/Contracts.java)/**
 * Additional helpers (besides guava Preconditions) for programming by contract
 */
MBeans (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/MBeans.java)/**
 * This util class provides a method to register an MBean using
 * our standard naming convention as described in the doc
 *  for {link {@link #register(String, String, Object)}.
 */
NameValuePair (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/Metrics2Util.java)/**
   * A pair of a name and its corresponding value. Defines a custom
   * comparator so the TopN PriorityQueue sorts based on the count.
   */
TopN (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/Metrics2Util.java)/**
   * A fixed-size priority queue, used to retrieve top-n of offered entries.
   */
Metrics2Util (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/Metrics2Util.java)/**
 * Utility class to simplify creation of hadoop metrics2 source/sink.
 */
Record (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/MetricsCache.java)/**
   * Cached record
   */
MetricsCache (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/MetricsCache.java)/**
 * A metrics cache for sinks that don't support sparse updates.
 */
Quantile (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/Quantile.java)/**
 * Specifies a quantile (with error bounds) to be watched by a
 * {@link SampleQuantiles} object.
 */
SampleItem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java)/**
   * Describes a measured value passed to the estimator, tracking additional
   * metadata required by the CKMS algorithm.
   */
SampleQuantiles (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java)/**
 * Implementation of the Cormode, Korn, Muthukrishnan, and Srivastava algorithm
 * for streaming calculation of targeted high-percentile epsilon-approximate
 * quantiles.
 * 
 * This is a generalization of the earlier work by Greenwald and Khanna (GK),
 * which essentially allows different error bounds on the targeted quantiles,
 * which allows for far more efficient calculation of high-percentiles.
 * 
 * See: Cormode, Korn, Muthukrishnan, and Srivastava
 * "Effective Computation of Biased Quantiles over Data Streams" in ICDE 2005
 * 
 * Greenwald and Khanna,
 * "Space-efficient online computation of quantile summaries" in SIGMOD 2001
 * 
 */
MinMax (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/SampleStat.java)/**
   * Helper to keep running min/max
   */
SampleStat (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/SampleStat.java)/**
 * Helper to compute running sample stats
 */
Servers (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/Servers.java)/**
 * Helpers to handle server addresses
 */
AbstractDNSToSwitchMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java)/**
 * This is a base class for DNS to Switch mappings. <p> It is not mandatory to
 * derive {@link DNSToSwitchMapping} implementations from it, but it is strongly
 * recommended, as it makes it easy for the Hadoop developers to add new methods
 * to this base class that are automatically picked up by all implementations.
 * <p>
 *
 * This class does not extend the <code>Configured</code>
 * base class, and should not be changed to do so, as it causes problems
 * for subclasses. The constructor of the <code>Configured</code> calls
 * the  {@link #setConf(Configuration)} method, which will call into the
 * subclasses before they have been fully constructed.
 *
 */
CachedDNSToSwitchMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/CachedDNSToSwitchMapping.java)/**
 * A cached implementation of DNSToSwitchMapping that takes an
 * raw DNSToSwitchMapping and stores the resolved network location in 
 * a cache. The following calls to a resolved network location
 * will get its location from the cache. 
 *
 */
ConnectTimeoutException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/ConnectTimeoutException.java)/**
 * Thrown by {@link NetUtils#connect(java.net.Socket, java.net.SocketAddress, int)}
 * if it times out while connecting to the remote host.
 */
DNS (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/DNS.java)/**
 * 
 * A class that provides direct and reverse lookup functionalities, allowing
 * the querying of specific network interfaces or nameservers.
 * 
 * 
 */
DNSDomainNameResolver (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/DNSDomainNameResolver.java)/**
 * DNSDomainNameResolver wraps up the default DNS service for forward/reverse
 * DNS lookup. It also provides a function to resolve a host name to all of
 * fully qualified domain names belonging to the IPs from this host name
 */
DNSToSwitchMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/DNSToSwitchMapping.java)/**
 * An interface that must be implemented to allow pluggable
 * DNS-name/IP-address to RackID resolvers.
 *
 */
DNSToSwitchMappingWithDependency (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/DNSToSwitchMappingWithDependency.java)/**
 * An interface that must be implemented to allow pluggable
 * DNS-name/IP-address to RackID resolvers.
 *
 */
DomainNameResolver (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/DomainNameResolver.java)/**
 * This interface provides methods for the failover proxy to get IP addresses
 * of the associated servers (NameNodes, RBF routers etc). Implementations will
 * use their own service discovery mechanism, DNS, Zookeeper etc
 */
DomainNameResolverFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/DomainNameResolverFactory.java)/**
 * This class creates the DomainNameResolver instance based on the config.
 * It can either create the default resolver for the whole resolving for
 * hadoop or create individual resolver per nameservice or yarn.
 */
InnerNodeImpl (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/InnerNodeImpl.java)/** InnerNode represents a switch/router of a data center or rack.
 * Different from a leaf node, it has non-null children.
 */
NetworkTopology (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java)/** The class represents a cluster of computer with a tree hierarchical
 * network topology.
 * For example, a cluster may be consists of many data centers filled 
 * with racks of computers.
 * In a network topology, leaves represent data nodes (computers) and inner
 * nodes represent switches/routers that manage traffic in/out of data centers
 * or racks.  
 * 
 */
InnerNodeWithNodeGroup (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java)/** InnerNodeWithNodeGroup represents a switch/router of a data center, rack
   * or physical host. Different from a leaf node, it has non-null children.
   */
NetworkTopologyWithNodeGroup (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java)/**
 * The class extends NetworkTopology to represents a cluster of computer with
 *  a 4-layers hierarchical network topology.
 * In this network topology, leaves represent data nodes (computers) and inner
 * nodes represent switches/routers that manage traffic in/out of data centers,
 * racks or physical host (with virtual switch).
 * 
 * @see NetworkTopology
 */
RawScriptBasedMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/ScriptBasedMapping.java)/**
   * This is the uncached script mapping that is fed into the cache managed
   * by the superclass {@link CachedDNSToSwitchMapping}
   */
ScriptBasedMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/ScriptBasedMapping.java)/**
 * This class implements the {@link DNSToSwitchMapping} interface using a 
 * script configured via the
 * {@link CommonConfigurationKeys#NET_TOPOLOGY_SCRIPT_FILE_NAME_KEY} option.
 * <p>
 * It contains a static class <code>RawScriptBasedMapping</code> that performs
 * the work: reading the configuration parameters, executing any defined
 * script, handling errors and such like. The outer
 * class extends {@link CachedDNSToSwitchMapping} to cache the delegated
 * queries.
 * <p>
 * This DNS mapper's {@link #isSingleSwitch()} predicate returns
 * true if and only if a script is defined.
 */
RawScriptBasedMappingWithDependency (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java)/**
   * This is the uncached script mapping that is fed into the cache managed
   * by the superclass {@link CachedDNSToSwitchMapping}
   */
ScriptBasedMappingWithDependency (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java)/**
 * This class extends ScriptBasedMapping class and implements 
 * the {@link DNSToSwitchMappingWithDependency} interface using 
 * a script configured via the 
 * {@link CommonConfigurationKeys#NET_DEPENDENCY_SCRIPT_FILE_NAME_KEY} option.
 * <p>
 * It contains a static class <code>RawScriptBasedMappingWithDependency</code>
 * that performs the getDependency work.
 */
SocketInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/SocketInputStream.java)/**
 * This implements an input stream that can have a timeout while reading.
 * This sets non-blocking flag on the socket channel.
 * So after create this object, read() on 
 * {@link Socket#getInputStream()} and write() on 
 * {@link Socket#getOutputStream()} for the associated socket will throw 
 * IllegalBlockingModeException. 
 * Please use {@link SocketOutputStream} for writing.
 */
SocketInputWrapper (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/SocketInputWrapper.java)/**
 * A wrapper stream around a socket which allows setting of its timeout. If the
 * socket has a channel, this uses non-blocking IO via the package-private
 * {@link SocketInputStream} implementation. Otherwise, timeouts are managed by
 * setting the underlying socket timeout itself.
 */
SelectorPool (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/SocketIOWithTimeout.java)/**
   * This maintains a pool of selectors. These selectors are closed
   * once they are idle (unused) for a few seconds.
   */
SocketIOWithTimeout (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/SocketIOWithTimeout.java)/**
 * This supports input and output streams for a socket channels. 
 * These streams can have a timeout.
 */
SocketOutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/SocketOutputStream.java)/**
 * This implements an output stream that can have a timeout while writing.
 * This sets non-blocking flag on the socket channel.
 * So after creating this object , read() on 
 * {@link Socket#getInputStream()} and write() on 
 * {@link Socket#getOutputStream()} on the associated socket will throw 
 * llegalBlockingModeException.
 * Please use {@link SocketInputStream} for reading.
 */
SocksSocketFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/SocksSocketFactory.java)/**
 * Specialized SocketFactory to create sockets with a SOCKS proxy
 */
StandardSocketFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/StandardSocketFactory.java)/**
 * Specialized SocketFactory to create sockets with a SOCKS proxy
 */
TableMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/TableMapping.java)/**
 * <p>
 * Simple {@link DNSToSwitchMapping} implementation that reads a 2 column text
 * file. The columns are separated by whitespace. The first column is a DNS or
 * IP address and the second column specifies the rack where the address maps.
 * </p>
 * <p>
 * This class uses the configuration parameter {@code
 * net.topology.table.file.name} to locate the mapping file.
 * </p>
 * <p>
 * Calls to {@link #resolve(List)} will look up the address as defined in the
 * mapping file. If no entry corresponding to the address is found, the value
 * {@code /default-rack} is returned.
 * </p>
 */
DomainInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/unix/DomainSocket.java)/**
   * Input stream for UNIX domain sockets.
   */
DomainOutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/unix/DomainSocket.java)/**
   * Output stream for UNIX domain sockets.
   */
DomainSocket (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/unix/DomainSocket.java)/**
 * The implementation of UNIX domain sockets in Java.
 * 
 * See {@link DomainSocket} for more information about UNIX domain sockets.
 */
NotificationHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java)/**
   * Handler for {DomainSocketWatcher#notificationSockets[1]}
   */
FdSet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java)/**
   * The FdSet is a set of file descriptors that gets passed to poll(2).
   * It contains a native memory segment, so that we don't have to copy
   * in the poll0 function.
   */
DomainSocketWatcher (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java)/**
 * The DomainSocketWatcher watches a set of domain sockets to see when they
 * become readable, or closed.  When one of those events happens, it makes a
 * callback.
 *
 * See {@link DomainSocket} for more information about UNIX domain sockets.
 */
AccessControlException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AccessControlException.java)/**
 * An exception class for access control related issues.
 */
AbstractJavaKeyStoreProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java)/**
 * Abstract class for implementing credential providers that are based on
 * Java Keystores as the underlying credential store.
 *
 * The password for the keystore is taken from the HADOOP_CREDSTORE_PASSWORD
 * environment variable with a default of 'none'.
 *
 * It is expected that for access to credential protected resource to copy the
 * creds from the original provider into the job's Credentials object, which is
 * accessed via the UserProvider. Therefore, these providers won't be directly
 * used by MapReduce tasks.
 */
CredentialEntry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/CredentialProvider.java)/**
   * The combination of both the alias and the actual credential value.
   */
CredentialProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/CredentialProvider.java)/**
 * A provider of credentials or password for Hadoop applications. Provides an
 * abstraction to separate credential storage from users of them. It
 * is intended to support getting or storing passwords in a variety of ways,
 * including third party bindings.
 * 
 * <code>CredentialProvider</code> implementations must be thread safe.
 */
CredentialProviderFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/CredentialProviderFactory.java)/**
 * A factory to create a list of CredentialProvider based on the path given in a
 * Configuration. It uses a service loader interface to find the available
 * CredentialProviders and create them based on the list of URIs.
 */
PasswordReader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/CredentialShell.java)/** To facilitate testing since Console is a final class. */
CredentialShell (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/CredentialShell.java)/**
 * This program is the CLI utility for the CredentialProvider facilities in
 * Hadoop.
 */
Factory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/JavaKeyStoreProvider.java)/**
   * The factory to create JksProviders, which is used by the ServiceLoader.
   */
JavaKeyStoreProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/JavaKeyStoreProvider.java)/**
 * CredentialProvider based on Java's KeyStore file format. The file may be
 * stored in any Hadoop FileSystem using the following name mangling:
 * jceks://hdfs@nn1.example.com/my/creds.jceks {@literal ->}
 * hdfs://nn1.example.com/my/creds.jceks jceks://file/home/larry/creds.jceks
 * {@literal ->} file:///home/larry/creds.jceks
 */
Factory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/LocalJavaKeyStoreProvider.java)/**
   * The factory to create JksProviders, which is used by the ServiceLoader.
   */
LocalJavaKeyStoreProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/LocalJavaKeyStoreProvider.java)/**
 * CredentialProvider based on Java's KeyStore file format. The file may be
 * stored only on the local filesystem using the following name mangling:
 * localjceks://file/home/larry/creds.jceks {@literal ->}
 * file:///home/larry/creds.jceks
 */
UserProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/alias/UserProvider.java)/**
 * A CredentialProvider for UGIs. It uses the credentials object associated
 * with the current user to find credentials. This provider is created using a
 * URI of "user:///".
 */
AnnotatedSecurityInfo (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AnnotatedSecurityInfo.java)/**
 * Constructs SecurityInfo from Annotations provided in protocol interface.
 */
ProxyUserAuthenticationFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java)/**
 * AuthenticationFilter which adds support to perform operations
 * using end user instead of proxy user. Fetches the end user from
 * doAs Query Parameter.
 */
ProxyUserAuthenticationFilterInitializer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java)/**
 * Filter initializer to initialize
 * {@link ProxyUserAuthenticationFilter} which adds support
 * to perform operations using end user instead of proxy user.
 */
AuthenticationFilterInitializer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java)/**
 * Initializes hadoop-auth AuthenticationFilter which provides support for
 * Kerberos HTTP SPNEGO authentication.
 * <p>
 * It enables anonymous access, simple/pseudo and Kerberos HTTP SPNEGO
 * authentication  for Hadoop JobTracker, NameNode, DataNodes and
 * TaskTrackers.
 * <p>
 * Refer to the <code>core-default.xml</code> file, after the comment
 * 'HTTP Authentication' for details on the configuration options.
 * All related configuration properties have 'hadoop.http.authentication.'
 * as prefix.
 */
AccessControlList (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/AccessControlList.java)/**
 * Class representing a configured access control list.
 */
AuthorizationException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/AuthorizationException.java)/**
 * An exception class for authorization-related issues.
 * 
 * This class <em>does not</em> provide the stack trace for security purposes.
 */
PolicyProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/PolicyProvider.java)/**
 * {@link PolicyProvider} provides the {@link Service} definitions to the
 * security {@link Policy} in effect for Hadoop.
 *
 */
RefreshAuthorizationPolicyProtocol (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/RefreshAuthorizationPolicyProtocol.java)/**
 * Protocol which is used to refresh the authorization policy in use currently.
 */
Service (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/Service.java)/**
 * An abstract definition of <em>service</em> as related to 
 * Service Level Authorization for Hadoop.
 * 
 * Each service defines it's configuration key and also the necessary
 * {@link Permission} required to access the service.
 */
ServiceAuthorizationManager (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java)/**
 * An authorization manager which handles service-level authorization
 * for incoming service requests.
 */
CompositeGroupsMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/CompositeGroupsMapping.java)/**
 * An implementation of {@link GroupMappingServiceProvider} which
 * composites other group mapping providers for determining group membership.
 * This allows to combine existing provider implementations and composite 
 * a virtually new provider without customized development to deal with complex situation. 
 */
Credentials (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Credentials.java)/**
 * A class that provides the facilities of reading and writing
 * secret keys and Tokens.
 */
FastSaslClientFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/FastSaslClientFactory.java)/**
 * Class for dealing with caching SASL client factories.
 */
FastSaslServerFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/FastSaslServerFactory.java)/**
 * Class for dealing with caching SASL server factories.
 */
GroupMappingServiceProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/GroupMappingServiceProvider.java)/**
 * An interface for the implementation of a user-to-groups mapping service
 * used by {@link Groups}.
 */
TimerToTickerAdapter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Groups.java)/**
   * Convert millisecond times from hadoop's timer to guava's nanosecond ticker.
   */
GroupCacheLoader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Groups.java)/**
   * Deals with loading data into the cache.
   */
Groups (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Groups.java)/**
 * A user-to-groups mapping service.
 * 
 * {@link Groups} allows for server to get the various group memberships
 * of a given user via the {@link #getGroups(String)} call, thus ensuring 
 * a consistent user-to-groups mapping and protects against vagaries of 
 * different mappings on servers and clients in a Hadoop cluster. 
 */
HadoopKerberosName (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/HadoopKerberosName.java)/**
 * This class implements parsing and handling of Kerberos principal names. In 
 * particular, it splits them apart and translates them down into local
 * operating system names.
 */
HttpInteraction (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java)/**
   * Defines the minimal API requirements for the filter to execute its
   * filtering logic.  This interface exists to facilitate integration in
   * components that do not run within a servlet container and therefore cannot
   * rely on a servlet container to dispatch to the {@link #doFilter} method.
   * Applications that do run inside a servlet container will not need to write
   * code that uses this interface.  Instead, they can use typical servlet
   * container configuration mechanisms to insert the filter.
   */
ServletFilterHttpInteraction (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java)/**
   * {@link HttpInteraction} implementation for use in the servlet filter.
   */
RestCsrfPreventionFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java)/**
 * This filter provides protection against cross site request forgery (CSRF)
 * attacks for REST APIs. Enabling this filter on an endpoint results in the
 * requirement of all client to send a particular (configurable) HTTP header
 * with every request. In the absense of this header the filter will reject the
 * attempt as a bad request.
 */
XFrameOptionsResponseWrapper (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/http/XFrameOptionsFilter.java)/**
   * This wrapper allows the rest of the filter pipeline to
   * see the configured value when interrogating the response.
   * It also blocks other filters from setting the value to
   * anything other than what is configured.
   *
   */
XFrameOptionsFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/http/XFrameOptionsFilter.java)/**
 * This filter protects webapps from clickjacking attacks that
 * are possible through use of Frames to embed the resources in another
 * application and intercept clicks to accomplish nefarious things.
 */
IdMappingConstant (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/IdMappingConstant.java)/**
 * Some constants for IdMapping
 */
IdMappingServiceProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/IdMappingServiceProvider.java)/**
 * An interface for the implementation of {@literal <}userId,
 * userName{@literal >} mapping and {@literal <}groupId, groupName{@literal >}
 * mapping.
 */
IngressPortBasedResolver (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/IngressPortBasedResolver.java)/**
 * An implementation of SaslPropertiesResolver. Used on server side,
 * returns SASL properties based on the port the client is connecting
 * to. This should be used along with server side enabling multiple ports
 * TODO: when NN multiple listener is enabled, automatically use this
 * resolver without having to set in config.
 *
 * For configuration, for example if server runs on two ports 9000 and 9001,
 * and we want to specify 9000 to use auth-conf and 9001 to use auth.
 *
 * We need to set the following configuration properties:
 * ingress.port.sasl.configured.ports=9000,9001
 * ingress.port.sasl.prop.9000=privacy
 * ingress.port.sasl.prop.9001=authentication
 *
 * One note is that, if there is misconfiguration that a port, say, 9002 is
 * given in ingress.port.sasl.configured.ports, but it's sasl prop is not
 * set, a default of QOP of privacy (auth-conf) will be used. In addition,
 * if a port is not given even in ingress.port.sasl.configured.ports, but
 * is being checked in getServerProperties(), the default SASL prop will
 * be returned. Both of these two cases are considered misconfiguration.
 */
JniBasedUnixGroupsMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsMapping.java)/**
 * A JNI-based implementation of {@link GroupMappingServiceProvider} 
 * that invokes libC calls to get the group
 * memberships of a given user.
 */
JniBasedUnixGroupsNetgroupMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java)/**
 * A JNI-based implementation of {@link GroupMappingServiceProvider} 
 * that invokes libC calls to get the group
 * memberships of a given user.
 */
KerberosDiagsFailure (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/KDiag.java)/**
   * Diagnostics failures return the exit code 41, "unauthorized".
   *
   * They have a category, initially for testing: the category can be
   * validated without having to match on the entire string.
   */
KDiag (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/KDiag.java)/**
 * Kerberos diagnostics
 *
 * This operation expands some of the diagnostic output of the security code,
 * but not all. For completeness
 *
 * Set the environment variable {@code HADOOP_JAAS_DEBUG=true}
 * Set the log level for {@code org.apache.hadoop.security=DEBUG}
 */
KerberosAuthException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/KerberosAuthException.java)/**
 * Thrown when {@link UserGroupInformation} failed with an unrecoverable error,
 * such as failure in kerberos login/logout, invalid subject etc.
 *
 * Caller should not retry when catching this exception.
 */
LdapSslSocketFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/LdapGroupsMapping.java)/**
   * An private internal socket factory used to create SSL sockets with custom
   * configuration. There is no way to pass a specific instance of a factory to
   * the Java naming services, and the instantiated socket factory is not
   * passed any contextual information, so all information must be encapsulated
   * directly in the class. Static fields are used here to achieve this. This is
   * safe since the only usage of {@link LdapGroupsMapping} is within
   * {@link Groups}, which is a singleton (see the GROUPS field).
   * <p>
   * This has nearly the same behavior as an {@link SSLSocketFactory}. The only
   * additional logic is to configure the key store and trust store.
   * <p>
   * This is public only to be accessible by the Java naming services.
   */
LdapGroupsMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/LdapGroupsMapping.java)/**
 * An implementation of {@link GroupMappingServiceProvider} which
 * connects directly to an LDAP server for determining group membership.
 * 
 * This provider should be used only if it is necessary to map users to
 * groups that reside exclusively in an Active Directory or LDAP installation.
 * The common case for a Hadoop installation will be that LDAP users and groups
 * materialized on the Unix servers, and for an installation like that,
 * ShellBasedUnixGroupsMapping is preferred. However, in cases where
 * those users and groups aren't materialized in Unix, but need to be used for
 * access control, this class may be used to communicate directly with the LDAP
 * server.
 * 
 * It is important to note that resolving group mappings will incur network
 * traffic, and may cause degraded performance, although user-group mappings
 * will be cached via the infrastructure provided by {@link Groups}.
 * 
 * This implementation does not support configurable search limits. If a filter
 * is used for searching users or groups which returns more results than are
 * allowed by the server, an exception will be thrown.
 * 
 * The implementation attempts to resolve group hierarchies,
 * to a configurable limit.
 * If the limit is 0, in order to be considered a member of a group,
 * the user must be an explicit member in LDAP.  Otherwise, it will traverse the
 * group hierarchy n levels up.
 */
NetgroupCache (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/NetgroupCache.java)/**
 * Class that caches the netgroups and inverts group-to-user map
 * to user-to-group map, primarily intended for use with
 * netgroups (as returned by getent netgrgoup) which only returns
 * group to user mapping.
 */
NullGroupsMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/NullGroupsMapping.java)/**
 * This class provides groups mapping for {@link UserGroupInformation} when the
 * user group information will not be used.
 */
ProviderUtils (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ProviderUtils.java)/**
 * Utility methods for both key and credential provider APIs.
 *
 */
RefreshUserMappingsProtocol (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/RefreshUserMappingsProtocol.java)/**
 * Protocol use 
 *
 */
RuleBasedLdapGroupsMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java)/**
 * This class uses {@link LdapGroupsMapping} for group lookup and applies the
 * rule configured on the group names.
 */
SaslInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslInputStream.java)/**
 * A SaslInputStream is composed of an InputStream and a SaslServer (or
 * SaslClient) so that read() methods return data that are read in from the
 * underlying InputStream but have been additionally processed by the SaslServer
 * (or SaslClient) object. The SaslServer (or SaslClient) object must be fully
 * initialized before being used by a SaslInputStream.
 */
SaslOutputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslOutputStream.java)/**
 * A SaslOutputStream is composed of an OutputStream and a SaslServer (or
 * SaslClient) so that write() methods first process the data before writing
 * them out to the underlying OutputStream. The SaslServer (or SaslClient)
 * object must be fully initialized before being used by a SaslOutputStream.
 */
SaslPropertiesResolver (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslPropertiesResolver.java)/**
 * Provides SaslProperties to be used for a connection.
 * The default implementation is to read the values from configuration.
 * This class can be overridden to provide custom SaslProperties. 
 * The custom class can be specified via configuration.
 *
 */
SaslRpcClient (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java)/**
 * A utility class that encapsulates SASL logic for RPC client
 */
SaslDigestCallbackHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcServer.java)/** CallbackHandler for SASL DIGEST-MD5 mechanism */
SaslGssCallbackHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcServer.java)/** CallbackHandler for SASL GSSAPI Kerberos mechanism */
SaslRpcServer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcServer.java)/**
 * A utility class for dealing with SASL on RPC server
 */
StandardHostResolver (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java)/**
   * Uses standard java host resolution
   */
QualifiedHostResolver (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java)/**
   * This an alternate resolver with important properties that the standard
   * java resolver lacks:
   * 1) The hostname is fully qualified.  This avoids security issues if not
   *    all hosts in the cluster do not share the same search domains.  It
   *    also prevents other hosts from performing unnecessary dns searches.
   *    In contrast, InetAddress simply returns the host as given.
   * 2) The InetAddress is instantiated with an exact host and IP to prevent
   *    further unnecessary lookups.  InetAddress may perform an unnecessary
   *    reverse lookup for an IP.
   * 3) A call to getHostName() will always return the qualified hostname, or
   *    more importantly, the IP if instantiated with an IP.  This avoids
   *    unnecessary dns timeouts if the host is not resolvable.
   * 4) Point 3 also ensures that if the host is re-resolved, ex. during a
   *    connection re-attempt, that a reverse lookup to host and forward
   *    lookup to IP is not performed since the reverse/forward mappings may
   *    not always return the same IP.  If the client initiated a connection
   *    with an IP, then that IP is all that should ever be contacted.
   *    
   * NOTE: this resolver is only used if:
   *       hadoop.security.token.service.use_ip=false 
   */
SecurityUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java)/**
 * Security Utils.
 */
ShellBasedIdMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ShellBasedIdMapping.java)/**
 * A simple shell-based implementation of {@link IdMappingServiceProvider} 
 * Map id to user name or group name. It does update every 15 minutes. Only a
 * single instance of this class is expected to be on the server.
 * 
 * The maps are incrementally updated as described below:
 *   1. Initialize the maps as empty. 
 *   2. Incrementally update the maps
 *      - When ShellBasedIdMapping is requested for user or group name given 
 *        an ID, or for ID given a user or group name, do look up in the map
 *        first, if it doesn't exist, find the corresponding entry with shell
 *        command, and insert the entry to the maps.
 *      - When group ID is requested for a given group name, and if the
 *        group name is numerical, the full group map is loaded. Because we
 *        don't have a good way to find the entry for a numerical group name,
 *        loading the full map helps to get in all entries.
 *   3. Periodically refresh the maps for both user and group, e.g,
 *      do step 1.
 *   Note: for testing purpose, step 1 may initial the maps with full mapping
 *   when using constructor
 *   {@link ShellBasedIdMapping#ShellBasedIdMapping(Configuration, boolean)}.
 */
ShellBasedUnixGroupsMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java)/**
 * A simple shell-based implementation of {@link GroupMappingServiceProvider} 
 * that exec's the <code>groups</code> shell command to fetch the group
 * memberships of a given user.
 */
ShellBasedUnixGroupsNetgroupMapping (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java)/**
 * A simple shell-based implementation of {@link GroupMappingServiceProvider} 
 * that exec's the <code>groups</code> shell command to fetch the group
 * memberships of a given user.
 */
DelegatingSSLSocketFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java)/**
 * A {@link SSLSocketFactory} that can delegate to various SSL implementations.
 * Specifically, either OpenSSL or JSSE can be used. OpenSSL offers better
 * performance than JSSE and is made available via the
 * <a href="https://github.com/wildfly/wildfly-openssl">wildlfy-openssl</a>
 * library.
 *
 * <p>
 *   The factory has several different modes of operation:
 *   <ul>
 *     <li>OpenSSL: Uses the wildly-openssl library to delegate to the
 *     system installed OpenSSL. If the wildfly-openssl integration is not
 *     properly setup, an exception is thrown.</li>
 *     <li>Default: Attempts to use the OpenSSL mode, if it cannot load the
 *     necessary libraries, it falls back to the Default_JSEE mode.</li>
 *     <li>Default_JSSE: Delegates to the JSSE implementation of SSL, but
 *     it disables the GCM cipher when running on Java 8.</li>
 *     <li>Default_JSSE_with_GCM: Delegates to the JSSE implementation of
 *     SSL with no modification to the list of enabled ciphers.</li>
 *   </ul>
 * </p>
 */
FileBasedKeyStoresFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java)/**
 * {@link KeyStoresFactory} implementation that reads the certificates from
 * keystore files.
 * <p>
 * if the trust certificates keystore file changes, the {@link TrustManager}
 * is refreshed with the new trust certificate entries (using a
 * {@link ReloadingX509TrustManager} trustmanager).
 */
KeyStoresFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ssl/KeyStoresFactory.java)/**
 * Interface that gives access to {@link KeyManager} and {@link TrustManager}
 * implementations.
 */
ReloadingX509TrustManager (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ssl/ReloadingX509TrustManager.java)/**
 * A {@link TrustManager} implementation that reloads its configuration when
 * the truststore file on disk changes.
 */
SSLFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ssl/SSLFactory.java)/**
 * Factory that creates SSLEngine and SSLSocketFactory instances using
 * Hadoop configuration information.
 * <p>
 * This SSLFactory uses a {@link ReloadingX509TrustManager} instance,
 * which reloads public keys if the truststore file changes.
 * <p>
 * This factory is used to configure HTTPS in Hadoop HTTP based endpoints, both
 * client and server.
 */
SSLHostnameVerifier (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java)/**
 ************************************************************************
 * Copied from the not-yet-commons-ssl project at
 * http://juliusdavies.ca/commons-ssl/
 * This project is not yet in Apache, but it is Apache 2.0 licensed.
 ************************************************************************
 * Interface for checking if a hostname matches the names stored inside the
 * server's X.509 certificate.  Correctly implements
 * javax.net.ssl.HostnameVerifier, but that interface is not recommended.
 * Instead we added several check() methods that take SSLSocket,
 * or X509Certificate, or ultimately (they all end up calling this one),
 * String.  (It's easier to supply JUnit with Strings instead of mock
 * SSLSession objects!)
 * <p>Our check() methods throw exceptions if the name is
 * invalid, whereas javax.net.ssl.HostnameVerifier just returns true/false.
 * <p>
 * We provide the HostnameVerifier.DEFAULT, HostnameVerifier.STRICT, and
 * HostnameVerifier.ALLOW_ALL implementations.  We also provide the more
 * specialized HostnameVerifier.DEFAULT_AND_LOCALHOST, as well as
 * HostnameVerifier.STRICT_IE6.  But feel free to define your own
 * implementations!
 * <p>
 * Inspired by Sebastian Hauer's original StrictSSLProtocolSocketFactory in the
 * HttpClient "contrib" repository.
 */
DelegationTokenInformation (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java)/** Class to encapsulate a token's renew date and password. */
AbstractDelegationTokenSelector (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSelector.java)/**
 * Look through tokens to find the first delegation token that matches the
 * service and return it.
 */
DelegationKey (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/DelegationKey.java)/**
 * Key used for generating and verifying delegation tokens
 */
Token (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java)/**
   * Client side authentication token that handles Delegation Tokens.
   */
DelegationTokenAuthenticatedURL (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java)/**
 * The <code>DelegationTokenAuthenticatedURL</code> is a
 * {@link AuthenticatedURL} sub-class with built-in Hadoop Delegation Token
 * functionality.
 * <p>
 * The authentication mechanisms supported by default are Hadoop Simple
 * authentication (also known as pseudo authentication) and Kerberos SPNEGO
 * authentication.
 * <p>
 * Additional authentication mechanisms can be supported via {@link
 * DelegationTokenAuthenticator} implementations.
 * <p>
 * The default {@link DelegationTokenAuthenticator} is the {@link
 * KerberosDelegationTokenAuthenticator} class which supports
 * automatic fallback from Kerberos SPNEGO to Hadoop Simple authentication via
 * the {@link PseudoDelegationTokenAuthenticator} class.
 * <p>
 * <code>AuthenticatedURL</code> instances are not thread-safe.
 */
DelegationTokenAuthenticationFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java)/**
 *  The <code>DelegationTokenAuthenticationFilter</code> filter is a
 *  {@link AuthenticationFilter} with Hadoop Delegation Token support.
 *  <p>
 *  By default it uses it own instance of the {@link
 *  AbstractDelegationTokenSecretManager}. For situations where an external
 *  <code>AbstractDelegationTokenSecretManager</code> is required (i.e. one that
 *  shares the secret with <code>AbstractDelegationTokenSecretManager</code>
 *  instance running in other services), the external
 *  <code>AbstractDelegationTokenSecretManager</code> must be set as an
 *  attribute in the {@link ServletContext} of the web application using the
 *  {@link #DELEGATION_TOKEN_SECRET_MANAGER_ATTR} attribute name (
 *  'hadoop.http.delegation-token-secret-manager').
 */
DelegationTokenAuthenticationHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java)/**
 * An {@link AuthenticationHandler} that implements Kerberos SPNEGO mechanism
 * for HTTP and supports Delegation Token functionality.
 * <p>
 * In addition to the wrapped {@link AuthenticationHandler} configuration
 * properties, this handler supports the following properties prefixed
 * with the type of the wrapped <code>AuthenticationHandler</code>:
 * <ul>
 * <li>delegation-token.token-kind: the token kind for generated tokens
 * (no default, required property).</li>
 * <li>delegation-token.update-interval.sec: secret manager master key
 * update interval in seconds (default 1 day).</li>
 * <li>delegation-token.max-lifetime.sec: maximum life of a delegation
 * token in seconds (default 7 days).</li>
 * <li>delegation-token.renewal-interval.sec: renewal interval for
 * delegation tokens in seconds (default 1 day).</li>
 * <li>delegation-token.removal-scan-interval.sec: delegation tokens
 * removal scan interval in seconds (default 1 hour).</li>
 * </ul>
 *
 */
DelegationTokenAuthenticator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java)/**
 * {@link Authenticator} wrapper that enhances an {@link Authenticator} with
 * Delegation Token support.
 */
DelegationTokenIdentifier (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenIdentifier.java)/**
 * Concrete delegation token identifier used by {@link DelegationTokenManager},
 * {@link KerberosDelegationTokenAuthenticationHandler} and
 * {@link DelegationTokenAuthenticationFilter}.
 */
DelegationTokenManager (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java)/**
 * Delegation Token Manager used by the
 * {@link KerberosDelegationTokenAuthenticationHandler}.
 *
 */
HttpUserGroupInformation (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/HttpUserGroupInformation.java)/**
 * Util class that returns the remote {@link UserGroupInformation} in scope
 * for the HTTP request.
 */
KerberosDelegationTokenAuthenticationHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticationHandler.java)/**
 * An {@link AuthenticationHandler} that implements Kerberos SPNEGO mechanism
 * for HTTP and supports Delegation Token functionality.
 * <p>
 * In addition to the {@link KerberosAuthenticationHandler} configuration
 * properties, this handler supports:
 * <ul>
 * <li>kerberos.delegation-token.token-kind: the token kind for generated tokens
 * (no default, required property).</li>
 * <li>kerberos.delegation-token.update-interval.sec: secret manager master key
 * update interval in seconds (default 1 day).</li>
 * <li>kerberos.delegation-token.max-lifetime.sec: maximum life of a delegation
 * token in seconds (default 7 days).</li>
 * <li>kerberos.delegation-token.renewal-interval.sec: renewal interval for
 * delegation tokens in seconds (default 1 day).</li>
 * <li>kerberos.delegation-token.removal-scan-interval.sec: delegation tokens
 * removal scan interval in seconds (default 1 hour).</li>
 * </ul>
 */
KerberosDelegationTokenAuthenticator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticator.java)/**
 * The <code>KerberosDelegationTokenAuthenticator</code> provides support for
 * Kerberos SPNEGO authentication mechanism and support for Hadoop Delegation
 * Token operations.
 * <p>
 * It falls back to the {@link PseudoDelegationTokenAuthenticator} if the HTTP
 * endpoint does not trigger a SPNEGO authentication
 */
MultiSchemeDelegationTokenAuthenticationHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java)/**
 * A {@link CompositeAuthenticationHandler} that supports multiple HTTP
 * authentication schemes along with Delegation Token functionality. e.g.
 * server can support multiple authentication mechanisms such as Kerberos
 * (SPENGO) and LDAP. During the authentication phase, server will specify
 * all possible authentication schemes and let client choose the appropriate
 * scheme. Please refer to RFC-2616 and HADOOP-12082 for more details.
 *
 * Internally it uses {@link MultiSchemeAuthenticationHandler} implementation.
 * This handler also provides an option to enable delegation token management
 * functionality for only a specified subset of authentication schemes. This is
 * required to ensure that only schemes with strongest level of security should
 * be used for delegation token management.
 *
 * <p>
 * In addition to the wrapped {@link AuthenticationHandler} configuration
 * properties, this handler supports the following properties prefixed with the
 * type of the wrapped <code>AuthenticationHandler</code>:
 * <ul>
 * <li>delegation-token.token-kind: the token kind for generated tokens (no
 * default, required property).</li>
 * <li>delegation-token.update-interval.sec: secret manager master key update
 * interval in seconds (default 1 day).</li>
 * <li>delegation-token.max-lifetime.sec: maximum life of a delegation token in
 * seconds (default 7 days).</li>
 * <li>delegation-token.renewal-interval.sec: renewal interval for delegation
 * tokens in seconds (default 1 day).</li>
 * <li>delegation-token.removal-scan-interval.sec: delegation tokens removal
 * scan interval in seconds (default 1 hour).</li>
 * <li>delegation.http.schemes: A comma separated list of HTTP authentication
 * mechanisms (e.g. Negotiate, Basic) etc. to be allowed for delegation token
 * management operations.</li>
 * </ul>
 */
PseudoDelegationTokenAuthenticationHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticationHandler.java)/**
 * An {@link AuthenticationHandler} that implements Kerberos SPNEGO mechanism
 * for HTTP and supports Delegation Token functionality.
 * <p>
 * In addition to the {@link KerberosAuthenticationHandler} configuration
 * properties, this handler supports:
 * <ul>
 * <li>simple.delegation-token.token-kind: the token kind for generated tokens
 * (no default, required property).</li>
 * <li>simple.delegation-token.update-interval.sec: secret manager master key
 * update interval in seconds (default 1 day).</li>
 * <li>simple.delegation-token.max-lifetime.sec: maximum life of a delegation
 * token in seconds (default 7 days).</li>
 * <li>simple.delegation-token.renewal-interval.sec: renewal interval for
 * delegation tokens in seconds (default 1 day).</li>
 * <li>simple.delegation-token.removal-scan-interval.sec: delegation tokens
 * removal scan interval in seconds (default 1 hour).</li>
 * </ul>
 */
PseudoDelegationTokenAuthenticator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticator.java)/**
 * The <code>PseudoDelegationTokenAuthenticator</code> provides support for
 * Hadoop's pseudo authentication mechanism that accepts
 * the user name specified as a query string parameter and support for Hadoop
 * Delegation Token operations.
 * <p>
 * This mimics the model of Hadoop Simple authentication trusting the
 * {@link UserGroupInformation#getCurrentUser()} value.
 */
ServletUtils (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/web/ServletUtils.java)/**
 * Servlet utility methods.
 */
JaasConfiguration (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java)/**
   * Creates a programmatic version of a jaas.conf file. This can be used
   * instead of writing a jaas.conf file and setting the system property,
   * "java.security.auth.login.config", to point to that file. It is meant to be
   * used for connecting to ZooKeeper.
   */
SASLOwnerACLProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java)/**
   * Simple implementation of an {@link ACLProvider} that simply returns an ACL
   * that gives all permissions only to a single principal.
   */
ZKDelegationTokenSecretManager (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java)/**
 * An implementation of {@link AbstractDelegationTokenSecretManager} that
 * persists TokenIdentifiers and DelegationKeys in Zookeeper. This class can
 * be used by HA (Highly available) services that consists of multiple nodes.
 * This class ensures that Identifiers and Keys are replicated to all nodes of
 * the service.
 */
DelegationTokenIssuer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/DelegationTokenIssuer.java)/**
 * Class for issuing delegation tokens.
 */
DtFetcher (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/DtFetcher.java)/**
 *  DtFetcher is an interface which permits the abstraction and separation of
 *  delegation token fetch implementaions across different packages and
 *  compilation units.  Resolution of fetcher impl will be done at runtime.
 */
DtFileOperations (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/DtFileOperations.java)/**
 * DtFileOperations is a collection of delegation token file operations.
 */
DtUtilShell (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/DtUtilShell.java)/**
 *  DtUtilShell is a set of command line token file management operations.
 */
InvalidToken (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManager.java)/**
   * The token was invalid and the message explains why.
   */
SecretManager (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManager.java)/**
 * The server-side secret manager for each token type.
 * @param <T> The type of the token identifier
 */
PrivateToken (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java)/**
   * Indicates whether the token is a clone.  Used by HA failover proxy
   * to indicate a token should not be visible to the user via
   * UGI.getCredentials()
   */
TrivialRenewer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java)/**
   * A trivial renewer for token kinds that aren't managed. Sub-classes need
   * to implement getKind for their token kind.
   */
Token (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java)/**
 * The client-side form of the token.
 */
TokenIdentifier (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/TokenIdentifier.java)/**
 * An identifier that identifies a token, may contain public information 
 * about a token, including its kind (or type).
 */
TokenRenewer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/TokenRenewer.java)/**
 * This is the interface for plugins that handle tokens.
 */
TokenSelector (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/TokenSelector.java)/**
 * Select token of type T from tokens for use with named service
 * 
 * @param <T>
 *          T extends TokenIdentifier
 */
UGIExceptionMessages (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UGIExceptionMessages.java)/**
 * Standard strings to use in exception messages
 * in {@link KerberosAuthException} when throwing.
 */
User (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/User.java)/**
 * Save the full and short name of the user as a principal. This allows us to
 * have a single type that we always look for when picking up user names.
 */
UgiMetrics (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java)/** 
   * UgiMetrics maintains UGI activity statistics
   * and publishes them through the metrics interfaces.
   */
HadoopLoginModule (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java)/**
   * A login module that looks at the Kerberos, Unix, or Windows principal and
   * adds the corresponding UserName.
   */
AutoRenewalForUserCredsRunnable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java)/**
   * An abstract class which encapsulates the functionality required to
   * auto renew Kerbeors TGT. The concrete implementations of this class
   * are expected to provide implementation required to perform actual
   * TGT renewal (see {@code TicketCacheRenewalRunnable} and
   * {@code KeytabRenewalRunnable}).
   */
TicketCacheRenewalRunnable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java)/**
   * A concrete implementation of {@code AutoRenewalForUserCredsRunnable} class
   * which performs TGT renewal using kinit command.
   */
KeytabRenewalRunnable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java)/**
   * A concrete implementation of {@code AutoRenewalForUserCredsRunnable} class
   * which performs TGT renewal using specified keytab.
   */
TestingGroups (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java)/**
   * This class is used for storing the groups for testing. It stores a local
   * map that has the translation of usernames to groups.
   */
HadoopConfiguration (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java)/**
   * A JAAS configuration that defines the login modules that we want
   * to use for login.
   */
UserGroupInformation (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java)/**
 * User and group information for Hadoop.
 * This class wraps around a JAAS Subject and provides methods to determine the
 * user's username and groups. It supports both the Windows, Unix and Kerberos 
 * login modules.
 */
WhitelistBasedResolver (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/WhitelistBasedResolver.java)/**
 * An implementation of the SaslPropertiesResolver.
 * Uses a white list of IPs.
 * If the connection's IP address is in the list of IP addresses, the salProperties
 * will be unchanged.
 * If the connection's IP is not in the list of IP addresses, then QOP for the
 * connection will be restricted to "hadoop.rpc.protection.non-whitelist"
 *
 * Uses 3 IPList implementations together to form an aggregate whitelist.
 * 1. ConstantIPList - to check against a set of hardcoded IPs
 * 2. Fixed IP List - to check against a list of IP addresses which are specified externally, but
 * will not change over runtime.
 * 3. Variable IP List - to check against a list of IP addresses which are specified externally and
 * could change during runtime.
 * A connection IP address will checked against these 3 IP Lists in the order specified above.
 * Once a match is found , the IP address is determined to be in whitelist.
 *
 * The behavior can be configured using a bunch of configuration parameters.
 *
 */
AbstractService (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/AbstractService.java)/**
 * This is the base implementation class for services.
 */
CompositeServiceShutdownHook (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/CompositeService.java)/**
   * JVM Shutdown hook for CompositeService which will stop the give
   * CompositeService gracefully in case of JVM shutdown.
   */
CompositeService (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/CompositeService.java)/**
 * Composition of services.
 */
AbstractLaunchableService (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/AbstractLaunchableService.java)/**
 * Subclass of {@link AbstractService} that provides basic implementations
 * of the {@link LaunchableService} methods.
 */
HadoopUncaughtExceptionHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/HadoopUncaughtExceptionHandler.java)/**
 * This class is intended to be installed by calling 
 * {@link Thread#setDefaultUncaughtExceptionHandler(UncaughtExceptionHandler)}
 * in the main entry point. 
 *
 * The base class will always attempt to shut down the process if an Error
 * was raised; the behavior on a standard Exception, raised outside 
 * process shutdown, is simply to log it. 
 *
 * (Based on the class {@code YarnUncaughtExceptionHandler})
 */
ServiceForcedShutdown (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/InterruptEscalator.java)/**
   * Forced shutdown runnable.
   */
Interrupted (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java)/**
   * Callback issues on an interrupt.
   */
InterruptData (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java)/**
   * Interrupt data to pass on.
   */
IrqHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/IrqHandler.java)/**
 * Handler of interrupts that relays them to a registered
 * implementation of {@link IrqHandler.Interrupted}.
 *
 * This class bundles up all the compiler warnings about abuse of sun.misc
 * interrupt handling code into one place.
 */
LaunchableService (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/LaunchableService.java)/**
 * An interface which services can implement to have their
 * execution managed by the ServiceLauncher.
 * <p>
 * The command line options will be passed down before the 
 * {@link Service#init(Configuration)} operation is invoked via an
 * invocation of {@link LaunchableService#bindArgs(Configuration, List)}
 * After the service has been successfully started via {@link Service#start()}
 * the {@link LaunchableService#execute()} method is called to execute the 
 * service. When this method returns, the service launcher will exit, using
 * the return code from the method as its exit option.
 */
LauncherArguments (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/LauncherArguments.java)/**
 * Standard launcher arguments. These are all from
 * the {@code GenericOptionsParser}, simply extracted to constants.
 */
LauncherExitCodes (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/LauncherExitCodes.java)/**
 * Common Exit codes.
 * <p>
 * Codes with a YARN prefix are YARN-related.
 * <p>
 * Many of the exit codes are designed to resemble HTTP error codes,
 * squashed into a single byte. e.g 44 , "not found" is the equivalent
 * of 404. The various 2XX HTTP error codes aren't followed;
 * the Unix standard of "0" for success is used.
 * <pre>
 *    0-10: general command issues
 *   30-39: equivalent to the 3XX responses, where those responses are
 *          considered errors by the application.
 *   40-49: client-side/CLI/config problems
 *   50-59: service-side problems.
 *   60+  : application specific error codes
 * </pre>
 */
MinimalGenericOptionsParser (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/ServiceLauncher.java)/**
   * A generic options parser which does not parse any of the traditional
   * Hadoop options.
   */
ServiceLauncher (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/ServiceLauncher.java)/**
 * A class to launch any YARN service by name.
 *
 * It's designed to be subclassed for custom entry points.
 *
 * Workflow:
 * <ol>
 *   <li>An instance of the class is created. It must be of the type
 *   {@link Service}</li>
 *   <li>If it implements
 *   {@link LaunchableService#bindArgs(Configuration, List)},
 *    it is given the binding args off the CLI after all general configuration
 *    arguments have been stripped.</li>
 *   <li>Its {@link Service#init(Configuration)} and {@link Service#start()}
 *   methods are called.</li>
 *   <li>If it implements it, {@link LaunchableService#execute()}
 *   is called and its return code used as the exit code.</li>
 *   <li>Otherwise: it waits for the service to stop, assuming that the
 *   {@link Service#start()} method spawns one or more thread
 *   to perform work</li>
 *   <li>If any exception is raised and provides an exit code,
 *   that is, it implements {@link ExitCodeProvider},
 *   the return value of {@link ExitCodeProvider#getExitCode()},
 *   becomes the exit code of the command.</li>
 * </ol>
 * Error and warning messages are logged to {@code stderr}.
 * 
 * @param <S> service class to cast the generated service to.
 */
ServiceLaunchException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java)/**
 * A service launch exception that includes an exit code.
 * <p>
 * When caught by the ServiceLauncher, it will convert that
 * into a process exit code.
 * 
 * The {@link #ServiceLaunchException(int, String, Object...)} constructor
 * generates formatted exceptions.
 */
ServiceShutdownHook (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java)/**
 * JVM Shutdown hook for Service which will stop the
 * Service gracefully in case of JVM shutdown.
 * This hook uses a weak reference to the service,
 * and when shut down, calls {@link Service#stop()} if the reference is valid.
 */
LifecycleEvent (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/LifecycleEvent.java)/**
 * A serializable lifecycle event: the time a state
 * transition occurred, and what state was entered.
 */
LoggingStateChangeListener (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/LoggingStateChangeListener.java)/**
 * This is a state change listener that logs events at INFO level
 */
Service (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/Service.java)/**
 * Service LifeCycle.
 */
ServiceListeners (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/ServiceOperations.java)/**
   * Class to manage a list of {@link ServiceStateChangeListener} instances,
   * including a notification loop that is robust against changes to the list
   * during the notification process.
   */
ServiceOperations (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/ServiceOperations.java)/**
 * This class contains a set of methods to work with services, especially
 * to walk them through their lifecycle.
 */
ServiceStateChangeListener (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/ServiceStateChangeListener.java)/**
 * Interface to notify state changes of a service.
 */
ServiceStateException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/ServiceStateException.java)/**
 * Exception that can be raised on state change operations, whose
 * exit code can be explicitly set, determined from that of any nested
 * cause, or a default value of
 * {@link  LauncherExitCodes#EXIT_SERVICE_LIFECYCLE_EXCEPTION}.
 */
ServiceStateModel (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/ServiceStateModel.java)/**
 * Implements the service state model.
 */
SubCommand (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tools/CommandShell.java)/**
   * Base class for any subcommands of this shell command.
   */
CommandShell (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tools/CommandShell.java)/**
 * This program is a CLI utility base class utilizing hadoop Tool class.
 */
GetGroupsBase (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tools/GetGroupsBase.java)/**
 * Base class for the HDFS and MR implementations of tools which fetch and
 * display the groups that users belong to.
 */
GetUserMappingsProtocol (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tools/GetUserMappingsProtocol.java)/**
 * Protocol implemented by the Name Node and Job Tracker which maps users to
 * groups.
 */
TableListing (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tools/TableListing.java)/**
 * This class implements a "table listing" with column headers.
 *
 * Example:
 *
 * NAME   OWNER   GROUP   MODE       WEIGHT
 * pool1  andrew  andrew  rwxr-xr-x     100
 * pool2  andrew  andrew  rwxr-xr-x     100
 * pool3  andrew  andrew  rwxr-xr-x     100
 *
 */
TraceAdmin (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tracing/TraceAdmin.java)/**
 * A command-line tool for viewing and modifying tracing settings.
 */
TraceAdminProtocol (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tracing/TraceAdminProtocol.java)/**
 * Protocol interface that provides tracing.
 */
TracerConfigurationManager (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tracing/TracerConfigurationManager.java)/**
 * This class provides functions for managing the tracer configuration at
 * runtime via an RPC protocol.
 */
TraceUtils (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tracing/TraceUtils.java)/**
 * This class provides utility functions for tracing.
 */
ApplicationClassLoader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ApplicationClassLoader.java)/**
 * A {@link URLClassLoader} for application isolation. Classes from the
 * application JARs are loaded in preference to the parent loader.
 */
AutoCloseableLock (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/AutoCloseableLock.java)/**
 * This is a wrap class of a ReentrantLock. Extending AutoCloseable
 * interface such that the users can use a try-with-resource syntax.
 */
BasicDiskValidator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/BasicDiskValidator.java)/**
 * BasicDiskValidator is a wrapper around {@link DiskChecker}.
 */
BlockingThreadPoolExecutorService (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java)/**
 * This ExecutorService blocks the submission of new tasks when its queue is
 * already full by using a semaphore. Task submissions require permits, task
 * completions release permits.
 * <p>
 * This is inspired by <a href="https://github.com/apache/incubator-s4/blob/master/subprojects/s4-comm/src/main/java/org/apache/s4/comm/staging/BlockingThreadPoolExecutorService.java">
 * this s4 threadpool</a>
 */
BloomFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/BloomFilter.java)/**
 * Implements a <i>Bloom filter</i>, as defined by Bloom in 1970.
 * <p>
 * The Bloom filter is a data structure that was introduced in 1970 and that has been adopted by 
 * the networking research community in the past decade thanks to the bandwidth efficiencies that it
 * offers for the transmission of set membership information between networked hosts.  A sender encodes 
 * the information into a bit vector, the Bloom filter, that is more compact than a conventional 
 * representation. Computation and space costs for construction are linear in the number of elements.  
 * The receiver uses the filter to test whether various elements are members of the set. Though the 
 * filter will occasionally return a false positive, it will never return a false negative. When creating 
 * the filter, the sender can choose its desired point in a trade-off between the false positive rate and the size. 
 * 
 * <p>
 * Originally created by
 * <a href="http://www.one-lab.org">European Commission One-Lab Project 034819</a>.
 * 
 * @see Filter The general behavior of a filter
 * 
 * @see <a href="http://portal.acm.org/citation.cfm?id=362692&dl=ACM&coll=portal">Space/Time Trade-Offs in Hash Coding with Allowable Errors</a>
 */
CountingBloomFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java)/**
 * Implements a <i>counting Bloom filter</i>, as defined by Fan et al. in a ToN
 * 2000 paper.
 * <p>
 * A counting Bloom filter is an improvement to standard a Bloom filter as it
 * allows dynamic additions and deletions of set membership information.  This 
 * is achieved through the use of a counting vector instead of a bit vector.
 * <p>
 * Originally created by
 * <a href="http://www.one-lab.org">European Commission One-Lab Project 034819</a>.
 *
 * @see Filter The general behavior of a filter
 * 
 * @see <a href="http://portal.acm.org/citation.cfm?id=343571.343572">Summary cache: a scalable wide-area web cache sharing protocol</a>
 */
DynamicBloomFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java)/**
 * Implements a <i>dynamic Bloom filter</i>, as defined in the INFOCOM 2006 paper.
 * <p>
 * A dynamic Bloom filter (DBF) makes use of a <code>s * m</code> bit matrix but
 * each of the <code>s</code> rows is a standard Bloom filter. The creation 
 * process of a DBF is iterative. At the start, the DBF is a <code>1 * m</code>
 * bit matrix, i.e., it is composed of a single standard Bloom filter.
 * It assumes that <code>n<sub>r</sub></code> elements are recorded in the 
 * initial bit vector, where <code>n<sub>r</sub> {@literal <=} n</code>
 * (<code>n</code> is the cardinality of the set <code>A</code> to record in
 * the filter).
 * <p>
 * As the size of <code>A</code> grows during the execution of the application,
 * several keys must be inserted in the DBF.  When inserting a key into the DBF,
 * one must first get an active Bloom filter in the matrix.  A Bloom filter is
 * active when the number of recorded keys, <code>n<sub>r</sub></code>, is 
 * strictly less than the current cardinality of <code>A</code>, <code>n</code>.
 * If an active Bloom filter is found, the key is inserted and 
 * <code>n<sub>r</sub></code> is incremented by one. On the other hand, if there
 * is no active Bloom filter, a new one is created (i.e., a new row is added to
 * the matrix) according to the current size of <code>A</code> and the element
 * is added in this new Bloom filter and the <code>n<sub>r</sub></code> value of
 * this new Bloom filter is set to one.  A given key is said to belong to the
 * DBF if the <code>k</code> positions are set to one in one of the matrix rows.
 * <p>
 * Originally created by
 * <a href="http://www.one-lab.org">European Commission One-Lab Project 034819</a>.
 *
 * @see Filter The general behavior of a filter
 * @see BloomFilter A Bloom filter
 * 
 * @see <a href="http://www.cse.fau.edu/~jie/research/publications/Publication_files/infocom2006.pdf">Theory and Network Applications of Dynamic Bloom Filters</a>
 */
Filter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/Filter.java)/**
 * Defines the general behavior of a filter.
 * <p>
 * A filter is a data structure which aims at offering a lossy summary of a set <code>A</code>.  The
 * key idea is to map entries of <code>A</code> (also called <i>keys</i>) into several positions 
 * in a vector through the use of several hash functions.
 * <p>
 * Typically, a filter will be implemented as a Bloom filter (or a Bloom filter extension).
 * <p>
 * It must be extended in order to define the real behavior.
 * 
 * @see Key The general behavior of a key
 * @see HashFunction A hash function
 */
HashFunction (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/HashFunction.java)/**
 * Implements a hash object that returns a certain number of hashed values.
 * 
 * @see Key The general behavior of a key being stored in a filter
 * @see Filter The general behavior of a filter
 */
Key (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/Key.java)/**
 * The general behavior of a key that must be stored in a filter.
 * 
 * @see Filter The general behavior of a filter
 */
RemoveScheme (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/RemoveScheme.java)/**
 * Defines the different remove scheme for retouched Bloom filters.
 * <p>
 * Originally created by
 * <a href="http://www.one-lab.org">European Commission One-Lab Project 034819</a>.
 */
RetouchedBloomFilter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java)/**
 * Implements a <i>retouched Bloom filter</i>, as defined in the CoNEXT 2006 paper.
 * <p>
 * It allows the removal of selected false positives at the cost of introducing
 * random false negatives, and with the benefit of eliminating some random false
 * positives at the same time.
 * 
 * <p>
 * Originally created by
 * <a href="http://www.one-lab.org">European Commission One-Lab Project 034819</a>.
 * 
 * @see Filter The general behavior of a filter
 * @see BloomFilter A Bloom filter
 * @see RemoveScheme The different selective clearing algorithms
 * 
 * @see <a href="http://www-rp.lip6.fr/site_npa/site_rp/_publications/740-rbf_cameraready.pdf">Retouched Bloom Filters: Allowing Networked Applications to Trade Off Selected False Positives Against False Negatives</a>
 */
ChunkedArrayList (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ChunkedArrayList.java)/**
 * Simplified List implementation which stores elements as a list
 * of chunks, each chunk having a maximum size. This improves over
 * using an ArrayList in that creating a large list will never require
 * a large amount of contiguous heap space -- thus reducing the likelihood
 * of triggering a CMS compaction pause due to heap fragmentation.
 * 
 * The first chunks allocated are small, but each additional chunk is
 * 50% larger than the previous, ramping up to a configurable maximum
 * chunk size. Reasonable defaults are provided which should be a good
 * balance between not making any large allocations while still retaining
 * decent performance.
 *
 * This currently only supports a small subset of List operations --
 * namely addition and iteration.
 */
Classpath (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Classpath.java)/**
 * Command-line utility for getting the full classpath needed to launch a Hadoop
 * client application.  If the hadoop script is called with "classpath" as the
 * command, then it simply prints the classpath and exits immediately without
 * launching a JVM.  The output likely will include wildcards in the classpath.
 * If there are arguments passed to the classpath command, then this class gets
 * called.  With the --glob argument, it prints the full classpath with wildcards
 * expanded.  This is useful in situations where wildcard syntax isn't usable.
 * With the --jar argument, it writes the classpath as a manifest in a jar file.
 * This is useful in environments with short limitations on the maximum command
 * line length, where it may not be possible to specify the full classpath in a
 * command.  For example, the maximum command line length on Windows is 8191
 * characters.
 */
BufferCleaner (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/CleanerUtil.java)/**
   * Pass in an implementation of this interface to cleanup ByteBuffers.
   * CleanerUtil implements this to allow unmapping of bytebuffers
   * with private Java APIs.
   */
CleanerUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/CleanerUtil.java)/**
 * sun.misc.Cleaner has moved in OpenJDK 9 and
 * sun.misc.Unsafe#invokeCleaner(ByteBuffer) is the replacement.
 * This class is a hack to use sun.misc.Cleaner in Java 8 and
 * use the replacement in Java 9+.
 * This implementation is inspired by LUCENE-6989.
 */
CloseableReferenceCount (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/CloseableReferenceCount.java)/**
 * A closeable object that maintains a reference count.
 *
 * Once the object is closed, attempting to take a new reference will throw
 * ClosedChannelException.
 */
CombinedIPList (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/CombinedIPList.java)/**
 * Util class to stores ips/hosts/subnets.
 */
IntegerItem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ComparableVersion.java)/**
     * Represents a numeric item in the version item list.
     */
StringItem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ComparableVersion.java)/**
     * Represents a string in the version item list, usually a qualifier.
     */
ListItem (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ComparableVersion.java)/**
     * Represents a version list item. This class is used both for the global item list and for sub-lists (which start
     * with '-(number)' in the version specification).
     */
ComparableVersion (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ComparableVersion.java)/**
 * Generic implementation of version comparison.
 * 
 * <p>Features:
 * <ul>
 * <li>mixing of '<code>-</code>' (dash) and '<code>.</code>' (dot) separators,</li>
 * <li>transition between characters and digits also constitutes a separator:
 *     <code>1.0alpha1 =&gt; [1, 0, alpha, 1]</code></li>
 * <li>unlimited number of version components,</li>
 * <li>version components in the text can be digits or strings,</li>
 * <li>strings are checked for well-known qualifiers and the qualifier ordering is used for version ordering.
 *     Well-known qualifiers (case insensitive) are:<ul>
 *     <li><code>alpha</code> or <code>a</code></li>
 *     <li><code>beta</code> or <code>b</code></li>
 *     <li><code>milestone</code> or <code>m</code></li>
 *     <li><code>rc</code> or <code>cr</code></li>
 *     <li><code>snapshot</code></li>
 *     <li><code>(the empty string)</code> or <code>ga</code> or <code>final</code></li>
 *     <li><code>sp</code></li>
 *     </ul>
 *     Unknown qualifiers are considered after known qualifiers, with lexical order (always case insensitive),
 *   </li>
 * <li>a dash usually precedes a qualifier, and is always less important than something preceded with a dot.</li>
 * </ul><p>
 *
 * @see <a href="https://cwiki.apache.org/confluence/display/MAVENOLD/Versioning">"Versioning" on Maven Wiki</a>
 */
Util (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/concurrent/AsyncGet.java)/** Utility */
AsyncGet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/concurrent/AsyncGet.java)/**
 * This interface defines an asynchronous {@link #get(long, TimeUnit)} method.
 *
 * When the return value is still being computed, invoking
 * {@link #get(long, TimeUnit)} will result in a {@link TimeoutException}.
 * The method should be invoked again and again
 * until the underlying computation is completed.
 *
 * @param <R> The type of the return value.
 * @param <E> The exception type that the underlying implementation may throw.
 */
AsyncGetFuture (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java)/** A {@link Future} implemented using an {@link AsyncGet} object. */
ExecutorHelper (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/concurrent/ExecutorHelper.java)/** Helper functions for Executors. */
HadoopExecutors (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java)/** Factory methods for ExecutorService, ScheduledExecutorService instances.
 * These executor service instances provide additional functionality (e.g
 * logging uncaught exceptions). */
HadoopScheduledThreadPoolExecutor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/concurrent/HadoopScheduledThreadPoolExecutor.java)/** An extension of ScheduledThreadPoolExecutor that provides additional
 * functionality. */
HadoopThreadPoolExecutor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/concurrent/HadoopThreadPoolExecutor.java)/** An extension of ThreadPoolExecutor that provides additional functionality.
 *  */
ConfTest (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ConfTest.java)/**
 * This class validates configuration XML files in ${HADOOP_CONF_DIR} or
 * specified ones.
 */
CpuTimeTracker (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/CpuTimeTracker.java)/**
 * Utility for sampling and computing CPU usage.
 */
CrcComposer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/CrcComposer.java)/**
 * Encapsulates logic for composing multiple CRCs into one or more combined CRCs
 * corresponding to concatenated underlying data ranges. Optimized for composing
 * a large number of CRCs that correspond to underlying chunks of data all of
 * same size.
 */
CrcUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/CrcUtil.java)/**
 * This class provides utilities for working with CRCs.
 */
ChildReaper (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/curator/ChildReaper.java)/**
 * This is a copy of Curator 2.7.1's ChildReaper class, modified to work with
 * Guava 11.0.2.  The problem is the 'paths' Collection, which calls Guava's
 * Sets.newConcurrentHashSet(), which was added in Guava 15.0.
 * <p>
 * Utility to reap empty child nodes of a parent node. Periodically calls getChildren on
 * the node and adds empty nodes to an internally managed {@link Reaper}
 */
SafeTransaction (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/curator/ZKCuratorManager.java)/**
   * Use curator transactions to ensure zk-operations are performed in an all
   * or nothing fashion. This is equivalent to using ZooKeeper#multi.
   */
ZKCuratorManager (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/curator/ZKCuratorManager.java)/**
 * Helper class that provides utility methods specific to ZK operations.
 */
DaemonFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Daemon.java)/**
   * Provide a factory for named daemon threads,
   * for use in ExecutorServices constructors
   */
Daemon (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Daemon.java)/** A thread that has called {@link Thread#setDaemon(boolean) } with true.*/
ChecksumNull (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DataChecksum.java)/**
   * This just provides a dummy implimentation for Checksum class
   * This is used when there is no checksum available or required for 
   * data
   */
Java9Crc32CFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DataChecksum.java)/**
   * Holds constructor handle to let it be initialized on demand.
   */
DataChecksum (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DataChecksum.java)/**
 * This class provides interface and utilities for processing checksums for
 * DFS data transfers.
 */
DirectBufferPool (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DirectBufferPool.java)/**
 * A simple class for pooling direct ByteBuffers. This is necessary
 * because Direct Byte Buffers do not take up much space on the heap,
 * and hence will not trigger GCs on their own. However, they do take
 * native memory, and thus can cause high memory usage if not pooled.
 * The pooled instances are referred to only via weak references, allowing
 * them to be collected when a GC does run.
 *
 * This class only does effective pooling when many buffers will be
 * allocated at the same size. There is no attempt to reuse larger
 * buffers to satisfy smaller allocations.
 */
FileIoProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DiskChecker.java)/**
   * An interface that abstracts operations on {@link FileOutputStream}
   * objects for testability.
   */
DefaultFileIoProvider (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DiskChecker.java)/**
   * The default implementation of {@link FileIoProvider}.
   */
DiskChecker (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DiskChecker.java)/**
 * Class that provides utility functions for checking disk problem
 */
DiskValidator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DiskValidator.java)/**
 * A interface for disk validators.
 *
 * The {@link #checkStatus(File)} operation checks status of a file/dir.
 *
 */
DiskValidatorFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DiskValidatorFactory.java)/**
 * The factory class to create instance of {@link DiskValidator}.
 */
DurationInfo (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DurationInfo.java)/**
 * A duration with logging of final state at info or debug
 * in the {@code close()} call.
 * This allows it to be used in a try-with-resources clause, and have the
 * duration automatically logged.
 */
ExitException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ExitUtil.java)/**
   * An exception raised when a call to {@link #terminate(int)} was
   * called and system exits were blocked.
   */
HaltException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ExitUtil.java)/**
   * An exception raised when a call to {@link #terminate(int)} was
   * called and system halts were blocked.
   */
ExitUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ExitUtil.java)/**
 * Facilitates hooking process termination for tests, debugging
 * and embedding.
 * 
 * Hadoop code that attempts to call {@link System#exit(int)} 
 * or {@link Runtime#halt(int)} MUST invoke it via these methods.
 */
FastNumberFormat (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/FastNumberFormat.java)/**
 * Fast thread-safe version of NumberFormat
 */
FileBasedIPList (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/FileBasedIPList.java)/**
 * FileBasedIPList loads a list of subnets in CIDR format and ip addresses from
 * a file.
 *
 * Given an ip address, isIn  method returns true if ip belongs to one of the
 * subnets.
 *
 * Thread safe.
 */
FindClass (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/FindClass.java)/**
 * This entry point exists for diagnosing classloader problems:
 * is a class or resource present -and if so, where?
 *
 * <p>
 * Actions
 * <br>
 * <ul>
 *   <li><pre>load</pre>: load a class but do not attempt to create it </li>
 *   <li><pre>create</pre>: load and create a class, print its string value</li>
 *   <li><pre>printresource</pre>: load a resource then print it to stdout</li>
 *   <li><pre>resource</pre>: load a resource then print the URL of that
 *   resource</li>
 * </ul>
 *
 * It returns an error code if a class/resource cannot be loaded/found
 * -and optionally a class may be requested as being loaded.
 * The latter action will call the class's constructor -it must support an
 * empty constructor); any side effects from the
 * constructor or static initializers will take place.
 *
 * All error messages are printed to {@link System#out}; errors
 * to {@link System#err}.
 * 
 */
GcTimeAlertHandler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GcTimeMonitor.java)/**
   * The user can provide an instance of a class implementing this interface
   * when initializing a GcTimeMonitor to receive alerts when GC time
   * percentage exceeds the specified threshold.
   */
GcData (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GcTimeMonitor.java)/** Encapsulates data about GC pauses measured at the specific timestamp. */
GcTimeMonitor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GcTimeMonitor.java)/**
 * This class monitors the percentage of time the JVM is paused in GC within
 * the specified observation window, say 1 minute. The user can provide a
 * hook which will be called whenever this percentage exceeds the specified
 * threshold.
 */
GenericOptionsParser (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java)/**
 * <code>GenericOptionsParser</code> is a utility to parse command line
 * arguments generic to the Hadoop framework. 
 * 
 * <code>GenericOptionsParser</code> recognizes several standard command
 * line arguments, enabling applications to easily specify a namenode, a 
 * ResourceManager, additional configuration resources etc.
 * 
 * <h3 id="GenericOptions">Generic Options</h3>
 * 
 * <p>The supported generic options are:
 * <p><blockquote><pre>
 *     -conf &lt;configuration file&gt;     specify a configuration file
 *     -D &lt;property=value&gt;            use value for given property
 *     -fs &lt;local|namenode:port&gt;      specify a namenode
 *     -jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager
 *     -files &lt;comma separated list of files&gt;    specify comma separated
 *                            files to be copied to the map reduce cluster
 *     -libjars &lt;comma separated list of jars&gt;   specify comma separated
 *                            jar files to include in the classpath.
 *     -archives &lt;comma separated list of archives&gt;    specify comma
 *             separated archives to be unarchived on the compute machines.

 * </pre></blockquote><p>
 * 
 * <p>The general command line syntax is:</p>
 * <p><pre><code>
 * bin/hadoop command [genericOptions] [commandOptions]
 * </code></pre><p>
 * 
 * <p>Generic command line arguments <strong>might</strong> modify 
 * <code>Configuration </code> objects, given to constructors.</p>
 * 
 * <p>The functionality is implemented using Commons CLI.</p>
 *
 * <p>Examples:</p>
 * <p><blockquote><pre>
 * $ bin/hadoop dfs -fs darwin:8020 -ls /data
 * list /data directory in dfs with namenode darwin:8020
 * 
 * $ bin/hadoop dfs -D fs.default.name=darwin:8020 -ls /data
 * list /data directory in dfs with namenode darwin:8020
 *     
 * $ bin/hadoop dfs -conf core-site.xml -conf hdfs-site.xml -ls /data
 * list /data directory in dfs with multiple conf files specified.
 *
 * $ bin/hadoop job -D yarn.resourcemanager.address=darwin:8032 -submit job.xml
 * submit a job to ResourceManager darwin:8032
 *
 * $ bin/hadoop job -jt darwin:8032 -submit job.xml
 * submit a job to ResourceManager darwin:8032
 *
 * $ bin/hadoop job -jt local -submit job.xml
 * submit a job to local runner
 * 
 * $ bin/hadoop jar -libjars testlib.jar 
 * -archives test.tgz -files file.txt inputjar args
 * job submission with libjars, files and archives
 * </pre></blockquote><p>
 *
 * @see Tool
 * @see ToolRunner
 */
GenericsUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericsUtil.java)/**
 * Contains utility methods for dealing with Java Generics. 
 */
GSet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GSet.java)/**
 * A {@link GSet} is set,
 * which supports the {@link #get(Object)} operation.
 * The {@link #get(Object)} operation uses a key to lookup an element.
 * 
 * Null element is not supported.
 * 
 * @param <K> The type of the keys.
 * @param <E> The type of the elements, which must be a subclass of the keys.
 */
GSetByHashMap (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GSetByHashMap.java)/**
 * A {@link GSet} implementation by {@link HashMap}.
 */
Hash (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/hash/Hash.java)/**
 * This class represents a common API for hashing functions.
 */
JenkinsHash (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/hash/JenkinsHash.java)/**
 * Produces 32-bit hash for hash table lookup.
 * 
 * <pre>lookup3.c, by Bob Jenkins, May 2006, Public Domain.
 *
 * You can use this free for any purpose.  It's in the public domain.
 * It has no warranty.
 * </pre>
 * 
 * @see <a href="http://burtleburtle.net/bob/c/lookup3.c">lookup3.c</a>
 * @see <a href="http://www.ddj.com/184410284">Hash Functions (and how this
 * function compares to others such as CRC, MD?, etc</a>
 * @see <a href="http://burtleburtle.net/bob/hash/doobs.html">Has update on the
 * Dr. Dobbs Article</a>
 */
MurmurHash (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/hash/MurmurHash.java)/**
 * This is a very fast, non-cryptographic hash suitable for general hash-based
 * lookup.  See http://murmurhash.googlepages.com/ for more details.
 * 
 * <p>The C version of MurmurHash 2.0 found at that site was ported
 * to Java by Andrzej Bialecki (ab at getopt org).</p>
 */
HeapSort (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/HeapSort.java)/**
 * An implementation of the core algorithm of HeapSort.
 */
HostDetails (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/HostsFileReader.java)/**
   * An atomic view of the included and excluded hosts
   */
HttpExceptionUtils (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/HttpExceptionUtils.java)/**
 * HTTP utility class to help propagate server side exception to the client
 * over HTTP as a JSON payload.
 * <p>
 * It creates HTTP Servlet and JAX-RPC error responses including details of the
 * exception that allows a client to recreate the remote exception.
 * <p>
 * It parses HTTP client connections and recreates the exception.
 */
IdentityHashStore (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/IdentityHashStore.java)/**
 * The IdentityHashStore stores (key, value) mappings in an array.
 * It is similar to java.util.HashTable, but much more lightweight.
 * Neither inserting nor removing an element ever leads to any garbage
 * getting created (assuming the array doesn't need to be enlarged).
 *
 * Unlike HashTable, it compares keys using
 * {@link System#identityHashCode(Object)} and the identity operator.
 * This is useful for types like ByteBuffer which have expensive hashCode
 * and equals operators.
 *
 * We use linear probing to resolve collisions.  This avoids the need for
 * the overhead of linked list data structures.  It also means that it is
 * expensive to attempt to remove an element that isn't there, since we
 * have to look at the entire array to be sure that it doesn't exist.
 *
 * @param <K>    The key type to use.
 * @param <V>    THe value type to use.
 */
IdGenerator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/IdGenerator.java)/**
 * Generic ID generator
 * used for generating various types of number sequences.
 */
IndexedSortable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/IndexedSortable.java)/**
 * Interface for collections capable of being sorted by {@link IndexedSorter}
 * algorithms.
 */
IndexedSorter (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/IndexedSorter.java)/**
 * Interface for sort algorithms accepting {@link IndexedSortable} items.
 *
 * A sort algorithm implementing this interface may only
 * {@link IndexedSortable#compare} and {@link IndexedSortable#swap} items
 * for a range of indices to effect a sort across that range.
 */
InstrumentedLock (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/InstrumentedLock.java)/**
 * This is a debugging class that can be used by callers to track
 * whether a specific lock is being held for too long and periodically
 * log a warning and stack trace, if so.
 *
 * The logged warnings are throttled so that logs are not spammed.
 *
 * A new instance of InstrumentedLock can be created for each object
 * that needs to be instrumented.
 */
InstrumentedReadLock (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/InstrumentedReadLock.java)/**
 * This is a wrap class of a <tt>ReadLock</tt>.
 * It extends the class {@link InstrumentedLock}, and can be used to track
 * whether a specific read lock is being held for too long and log
 * warnings if so.
 *
 * The logged warnings are throttled so that logs are not spammed.
 */
InstrumentedReadWriteLock (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/InstrumentedReadWriteLock.java)/**
 * This is a wrap class of a {@link ReentrantReadWriteLock}.
 * It implements the interface {@link ReadWriteLock}, and can be used to
 * create instrumented <tt>ReadLock</tt> and <tt>WriteLock</tt>.
 */
InstrumentedWriteLock (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/InstrumentedWriteLock.java)/**
 * This is a wrap class of a <tt>WriteLock</tt>.
 * It extends the class {@link InstrumentedLock}, and can be used to track
 * whether a specific write lock is being held for too long and log
 * warnings if so.
 *
 * The logged warnings are throttled so that logs are not spammed.
 */
Element (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/IntrusiveCollection.java)/**
   * An element contained in this list.
   *
   * We pass the list itself as a parameter so that elements can belong to
   * multiple lists.  (The element will need to store separate prev and next
   * pointers for each.)
   */
IntrusiveIterator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/IntrusiveCollection.java)/**
   * An iterator over the intrusive collection.
   *
   * Currently, you can remove elements from the list using
   * #{IntrusiveIterator#remove()}, but modifying the collection in other
   * ways during the iteration is not supported.
   */
IntrusiveCollection (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/IntrusiveCollection.java)/**
 * Implements an intrusive doubly-linked list.
 *
 * An intrusive linked list is one in which the elements themselves are
 * responsible for storing the pointers to previous and next elements.
 * This can save a lot of memory if there are many elements in the list or
 * many lists.
 */
InvalidChecksumSizeException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/InvalidChecksumSizeException.java)/**
 * Thrown when bytesPerChecksun field in the meta file is less than
 * or equal to 0 or type is invalid.
 **/
JsonSerialization (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/JsonSerialization.java)/**
 * Support for marshalling objects to and from JSON.
 *
 * It constructs an object mapper as an instance field.
 * and synchronizes access to those methods
 * which use the mapper.
 *
 * This class was extracted from
 * {@code org.apache.hadoop.registry.client.binding.JsonSerDeser},
 * which is now a subclass of this class.
 * @param <T> Type to marshal.
 */
JvmPauseMonitor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/JvmPauseMonitor.java)/**
 * Class which sets up a simple thread which runs in a loop sleeping
 * for a short interval of time. If the sleep takes significantly longer
 * than its target time, it implies that the JVM or host machine has
 * paused processing, which may cause other problems. If such a pause is
 * detected, the thread logs a message.
 */
KMSUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/KMSUtil.java)/**
 * Utils for KMS.
 */
LambdaUtils (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LambdaUtils.java)/**
 * Lambda-expression utilities be they generic or specific to
 * Hadoop datatypes.
 */
Entry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LightWeightCache.java)/**
   * Entries of {@link LightWeightCache}.
   */
LightWeightCache (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LightWeightCache.java)/**
 * A low memory footprint Cache which extends {@link LightWeightGSet}.
 * An entry in the cache is expired if
 * (1) it is added to the cache longer than the creation-expiration period, and
 * (2) it is not accessed for the access-expiration period.
 * When an entry is expired, it may be evicted from the cache.
 * When the size limit of the cache is set, the cache will evict the entries
 * with earliest expiration time, even if they are not expired.
 * 
 * It is guaranteed that number of entries in the cache is less than or equal
 * to the size limit.  However, It is not guaranteed that expired entries are
 * evicted from the cache. An expired entry may possibly be accessed after its
 * expiration time. In such case, the expiration time may be updated.
 *
 * This class does not support null entry.
 *
 * This class is not thread safe.
 *
 * @param <K> Key type for looking up the entries
 * @param <E> Entry type, which must be
 *       (1) a subclass of K, and
 *       (2) implementing {@link Entry} interface, and
 */
LinkedElement (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LightWeightGSet.java)/**
   * Elements of {@link LightWeightGSet}.
   */
LightWeightGSet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LightWeightGSet.java)/**
 * A low memory footprint {@link GSet} implementation,
 * which uses an array for storing the elements
 * and linked lists for collision resolution.
 *
 * No rehash will be performed.
 * Therefore, the internal array will never be resized.
 *
 * This class does not support null element.
 *
 * This class is not thread safe.
 *
 * @param <K> Key type for looking up the elements
 * @param <E> Element type, which must be
 *       (1) a subclass of K, and
 *       (2) implementing {@link LinkedElement} interface.
 */
LightWeightResizableGSet (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LightWeightResizableGSet.java)/**
 * A low memory footprint {@link GSet} implementation,
 * which uses an array for storing the elements
 * and linked lists for collision resolution.
 *
 * If the size of elements exceeds the threshold,
 * the internal array will be resized to double length.
 *
 * This class does not support null element.
 *
 * This class is not thread safe.
 *
 * @param <K> Key type for looking up the elements
 * @param <E> Element type, which must be
 *       (1) a subclass of K, and
 *       (2) implementing {@link LinkedElement} interface.
 */
LimitInputStream (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LimitInputStream.java)/**
 * Copied from guava source code v15 (LimitedInputStream)
 * Guava deprecated LimitInputStream in v14 and removed it in v15. Copying this class here
 * allows to be compatible with guava 11 to 15+.
 * 
 * Originally: org.apache.hadoop.hbase.io.LimitInputStream
 */
LineReader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LineReader.java)/**
 * A class that provides a line reader from an input stream.
 * Depending on the constructor used, lines will either be terminated by:
 * <ul>
 * <li>one of the following: '\n' (LF) , '\r' (CR),
 * or '\r\n' (CR+LF).</li>
 * <li><em>or</em>, a custom byte sequence delimiter</li>
 * </ul>
 * In both cases, EOF also terminates an otherwise unterminated
 * line.
 */
InetAddressFactory (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/MachineList.java)/**
   * InetAddressFactory is used to obtain InetAddress from host.
   * This class makes it easy to simulate host to ip mappings during testing.
   *
   */
MergeSort (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/MergeSort.java)/** An implementation of the core algorithm of MergeSort. */
NativeCodeLoader (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/NativeCodeLoader.java)/**
 * A helper to load the native hadoop code i.e. libhadoop.so.
 * This handles the fallback to either the bundled libhadoop-Linux-i386-32.so
 * or the default java implementations where appropriate.
 *  
 */
NativeCrc32 (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/NativeCrc32.java)/**
 * Wrapper around JNI support code to do checksum computation
 * natively.
 */
NodeHealthMonitorExecutor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/NodeHealthScriptRunner.java)/**
   * Class which is used by the {@link Timer} class to periodically execute the
   * node health script.
   * 
   */
NodeHealthScriptRunner (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/NodeHealthScriptRunner.java)/**
 * 
 * The class which provides functionality of checking the health of the node
 * using the configured node health script and reporting back to the service
 * for which the health checker has been asked to report.
 */
OperationDuration (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/OperationDuration.java)/**
 * Little duration counter.
 */
Options (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Options.java)/**
 * This class allows generic access to variable length type-safe parameter
 * lists.
 */
PrintJarMainClass (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/PrintJarMainClass.java)/**
 * A micro-application that prints the main class name out of a jar file.
 */
PriorityQueue (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/PriorityQueue.java)/** A PriorityQueue maintains a partial ordering of its elements such that the
  least element can always be found in constant time.  Put()'s and pop()'s
  require log(size) time. */
ProgramDriver (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ProgramDriver.java)/** A driver that is used to run programs added to it
 */
Progress (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Progress.java)/** Utility to assist with generation of progress reports.  Applications build
 * a hierarchy of {@link Progress} instances, each modelling a phase of
 * execution.  The root is constructed with {@link #Progress()}.  Nodes for
 * sub-phases are created by calling {@link #addPhase()}.
 */
Progressable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Progressable.java)/**
 * A facility for reporting progress.
 * 
 * <p>Clients and/or applications can use the provided <code>Progressable</code>
 * to explicitly report progress to the Hadoop framework. This is especially
 * important for operations which take significant amount of time since,
 * in-lieu of the reported progress, the framework has to assume that an error
 * has occurred and time-out the operation.</p>
 */
PureJavaCrc32 (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/PureJavaCrc32.java)/**
 * A pure-java implementation of the CRC32 checksum that uses
 * the same polynomial as the built-in native CRC32.
 *
 * This is to avoid the JNI overhead for certain uses of Checksumming
 * where many small pieces of data are checksummed in succession.
 *
 * The current version is ~10x to 1.8x as fast as Sun's native
 * java.util.zip.CRC32 in Java 1.6
 *
 * @see java.util.zip.CRC32
 */
PureJavaCrc32C (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/PureJavaCrc32C.java)/**
 * A pure-java implementation of the CRC32 checksum that uses
 * the CRC32-C polynomial, the same polynomial used by iSCSI
 * and implemented on many Intel chipsets supporting SSE4.2.
 */
QuickSort (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/QuickSort.java)/**
 * An implementation of the core algorithm of QuickSort.
 */
ReadWriteDiskValidator (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ReadWriteDiskValidator.java)/**
 * ReadWriteDiskValidator is the class to check a directory by to create a file,
 * write some bytes into it, read back, and verify if they are identical.
 * Read time and write time are recorded and put into an
 * {@link ReadWriteDiskValidatorMetrics}.
 */
ReadWriteDiskValidatorMetrics (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java)/**
 * The metrics for a directory generated by {@link ReadWriteDiskValidator}.
 */
CopyInCopyOutBuffer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ReflectionUtils.java)/**
   * A pair of input/output buffers that we use to clone writables.
   */
ReflectionUtils (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ReflectionUtils.java)/**
 * General reflection utils
 */
RunJar (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/RunJar.java)/** Run a Hadoop job jar. */
RunnableWithPermitRelease (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java)/**
   * Releases a permit after the task is executed.
   */
CallableWithPermitRelease (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java)/**
   * Releases a permit after the task is completed.
   */
SemaphoredDelegatingExecutor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java)/**
 * This ExecutorService blocks the submission of new tasks when its queue is
 * already full by using a semaphore. Task submissions require permits, task
 * completions release permits.
 * <p>
 * This is a refactoring of {@link BlockingThreadPoolExecutorService}; that code
 * contains the thread pool logic, whereas this isolates the semaphore
 * and submit logic for use with other thread pools and delegation models.
 * <p>
 * This is inspired by <a href="https://github.com/apache/incubator-s4/blob/master/subprojects/s4-comm/src/main/java/org/apache/s4/comm/staging/BlockingThreadPoolExecutorService.java">
 * this s4 threadpool</a>
 */
SequentialNumber (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SequentialNumber.java)/**
 * Sequential number generator.
 * 
 * This class is thread safe.
 */
ServicePlugin (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ServicePlugin.java)/**
 * Service plug-in interface.
 * 
 * Service plug-ins may be used to expose functionality of datanodes or
 * namenodes using arbitrary RPC protocols. Plug-ins are instantiated by the
 * service instance, and are notified of service life-cycle events using the
 * methods defined by this class.
 * 
 * Service plug-ins are started after the service instance is started, and
 * stopped before the service instance is stopped.
 */
ExitCodeException (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Shell.java)/**
   * This is an IOException with exit code added.
   */
ShellCommandExecutor (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Shell.java)/**
   * A simple shell command executor.
   *
   * <code>ShellCommandExecutor</code>should be used in cases where the output
   * of the command needs no explicit parsing and where the command, working
   * directory and the environment remains unchanged. The output of the command
   * is stored as-is and is expected to be small.
   */
ShellTimeoutTimerTask (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Shell.java)/**
   * Timer which is used to timeout scripts spawned off by shell.
   */
Shell (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Shell.java)/**
 * A base class for running a Shell command.
 *
 * <code>Shell</code> can be used to run shell commands like <code>du</code> or
 * <code>df</code>. It also offers facilities to gate commands by
 * time-intervals.
 */
HookEntry (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ShutdownHookManager.java)/**
   * Private structure to store ShutdownHook, its priority and timeout
   * settings.
   */
ShutdownHookManager (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ShutdownHookManager.java)/**
 * The <code>ShutdownHookManager</code> enables running shutdownHook
 * in a deterministic order, higher priority first.
 * <p>
 * The JVM runs ShutdownHooks in a non-deterministic order or in parallel.
 * This class registers a single JVM shutdownHook and run all the
 * shutdownHooks registered to it (to this class) in order based on their
 * priority.
 *
 * Unless a hook was registered with a shutdown explicitly set through
 * {@link #addShutdownHook(Runnable, int, long, TimeUnit)},
 * the shutdown time allocated to it is set by the configuration option
 * {@link CommonConfigurationKeysPublic#SERVICE_SHUTDOWN_TIMEOUT} in
 * {@code core-site.xml}, with a default value of
 * {@link CommonConfigurationKeysPublic#SERVICE_SHUTDOWN_TIMEOUT_DEFAULT}
 * seconds.
 */
ShutdownThreadsHelper (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ShutdownThreadsHelper.java)/**
 * Helper class to shutdown {@link Thread}s and {@link ExecutorService}s.
 */
Handler (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SignalLogger.java)/**
   * Our signal handler.
   */
StopWatch (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StopWatch.java)/**
 * A simplified StopWatch implementation which can measure times in nanoseconds.
 */
StringInterner (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringInterner.java)/**
 * Provides string interning utility methods. For weak interning,
 * we use the standard String.intern() call, that performs very well
 * (no problems with PermGen overflowing, etc.) starting from JDK 7.
 */
StringUtils (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java)/**
 * General string utils
 */
SysInfo (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SysInfo.java)/**
 * Plugin to calculate resource information on the system.
 */
SysInfoLinux (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SysInfoLinux.java)/**
 * Plugin to calculate resource information on Linux systems.
 */
SysInfoWindows (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/SysInfoWindows.java)/**
 * Plugin to calculate resource information on Windows systems.
 */
Time (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Time.java)/**
 * Utility methods for getting the time and computing intervals.
 */
Timer (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Timer.java)/**
 * Utility methods for getting the time and computing intervals.
 *
 * It has the same behavior as {{@link Time}}, with the exception that its
 * functions can be overridden for dependency injection purposes.
 */
Tool (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Tool.java)/**
 * A tool interface that supports handling of generic command-line options.
 * 
 * <p><code>Tool</code>, is the standard for any Map-Reduce tool/application. 
 * The tool/application should delegate the handling of 
 * <a href="{@docRoot}/../hadoop-project-dist/hadoop-common/CommandsManual.html#Generic_Options">
 * standard command-line options</a> to {@link ToolRunner#run(Tool, String[])} 
 * and only handle its custom arguments.</p>
 * 
 * <p>Here is how a typical <code>Tool</code> is implemented:</p>
 * <p><blockquote><pre>
 *     public class MyApp extends Configured implements Tool {
 *     
 *       public int run(String[] args) throws Exception {
 *         // <code>Configuration</code> processed by <code>ToolRunner</code>
 *         Configuration conf = getConf();
 *         
 *         // Create a JobConf using the processed <code>conf</code>
 *         JobConf job = new JobConf(conf, MyApp.class);
 *         
 *         // Process custom command-line options
 *         Path in = new Path(args[1]);
 *         Path out = new Path(args[2]);
 *         
 *         // Specify various job-specific parameters     
 *         job.setJobName("my-app");
 *         job.setInputPath(in);
 *         job.setOutputPath(out);
 *         job.setMapperClass(MyMapper.class);
 *         job.setReducerClass(MyReducer.class);
 *
 *         // Submit the job, then poll for progress until the job is complete
 *         RunningJob runningJob = JobClient.runJob(job);
 *         if (runningJob.isSuccessful()) {
 *           return 0;
 *         } else {
 *           return 1;
 *         }
 *       }
 *       
 *       public static void main(String[] args) throws Exception {
 *         // Let <code>ToolRunner</code> handle generic command-line options 
 *         int res = ToolRunner.run(new Configuration(), new MyApp(), args);
 *         
 *         System.exit(res);
 *       }
 *     }
 * </pre></blockquote><p>
 * 
 * @see GenericOptionsParser
 * @see ToolRunner
 */
ToolRunner (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ToolRunner.java)/**
 * A utility to help run {@link Tool}s.
 * 
 * <p><code>ToolRunner</code> can be used to run classes implementing 
 * <code>Tool</code> interface. It works in conjunction with 
 * {@link GenericOptionsParser} to parse the 
 * <a href="{@docRoot}/../hadoop-project-dist/hadoop-common/CommandsManual.html#Generic_Options">
 * generic hadoop command line arguments</a> and modifies the 
 * <code>Configuration</code> of the <code>Tool</code>. The 
 * application-specific options are passed along without being modified.
 * </p>
 * 
 * @see Tool
 * @see GenericOptionsParser
 */
VersionInfo (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java)/**
 * This class returns build information about Hadoop components.
 */
VersionUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionUtil.java)/**
 * A wrapper class to maven's ComparableVersion class, to comply
 * with maven's version name string convention 
 */
Waitable (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Waitable.java)/**
 * Represents an object that you can wait for.
 */
XMLUtils (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/XMLUtils.java)/**
 * General xml utilities.
 *   
 */
ZKAuthInfo (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ZKUtil.java)/**
   * An authentication token passed to ZooKeeper.addAuthInfo
   */
ZKUtil (/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ZKUtil.java)/**
 * Utilities for working with ZooKeeper.
 */
CLITestHelper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/CLITestHelper.java)/**
 * Tests for the Command Line Interface (CLI)
 */
TestCLI (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/TestCLI.java)/**
 * Tests for the Command Line Interface (CLI)
 */
CLICommand (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/util/CLICommand.java)/**
 * This interface is to generalize types of test command for upstream projects.
 */
CLICommandTypes (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/util/CLICommandTypes.java)/**
 * This interface is to provide command type for test commands enums.
 */
CLITestCmd (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/util/CLITestCmd.java)/**
 * Class to define Test Command along with its type
 */
CLITestData (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/util/CLITestData.java)/**
 *
 * Class to store CLI Test Data
 */
CommandExecutor (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/util/CommandExecutor.java)/**
 *
 * This class execute commands and captures the output
 */
ComparatorBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/util/ComparatorBase.java)/**
 *
 * Comparator interface. To define a new comparator, implement the compare
 * method
 */
ComparatorData (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/util/ComparatorData.java)/**
 *
 * Class to store CLI Test Comparators Data
 */
ExactComparator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/util/ExactComparator.java)/**
 * Comparator for the Command line tests.
 * 
 * This comparator compares the actual to the expected and
 * returns true only if they are the same
 * 
 */
ExactLineComparator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/util/ExactLineComparator.java)/**
 * Comparator for the Command line tests.
 *
 * This comparator searches for an exact line as 'expected'
 * in the string 'actual' and returns true if found
 *
 */
RegexpAcrossOutputComparator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/util/RegexpAcrossOutputComparator.java)/**
 * Comparator for command line tests that attempts to find a regexp
 * within the entire text returned by a command.
 *
 * This comparator differs from RegexpComparator in that it attempts
 * to match the pattern within all of the text returned by the command,
 * rather than matching against each line of the returned text.  This
 * allows matching against patterns that span multiple lines.
 */
RegexpComparator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/util/RegexpComparator.java)/**
 * Comparator for the Command line tests. 
 * 
 * This comparator searches for the regular expression specified in 'expected'
 * in the string 'actual' and returns true if the regular expression match is 
 * done
 * 
 */
TokenComparator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/cli/util/TokenComparator.java)/**
 * Comparator for the Command line tests. 
 * 
 * This comparator compares each token in the expected output and returns true
 * if all tokens are in the actual output
 * 
 */
TestCommonConfigurationFields (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestCommonConfigurationFields.java)/**
 * Unit test class to compare the following Hadoop Configuration classes:
 * <p></p>
 * {@link org.apache.hadoop.fs.AbstractFileSystem}
 * {@link org.apache.hadoop.fs.CommonConfigurationKeys}
 * {@link org.apache.hadoop.fs.CommonConfigurationKeysPublic}
 * {@link org.apache.hadoop.fs.ftp.FtpConfigKeys}
 * {@link org.apache.hadoop.fs.local.LocalConfigKeys}
 * {@link org.apache.hadoop.ha.SshFenceByTcpPort}
 * {@link org.apache.hadoop.http.HttpServer2}
 * {@link org.apache.hadoop.security.LdapGroupsMapping}
 * {@link org.apache.hadoop.security.http.CrossOriginFilter}
 * {@link org.apache.hadoop.security.ssl.SSLFactory}
 * {@link org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil}
 * <p></p>
 * against core-site.xml for missing properties.  Currently only
 * throws an error if the class is missing a property.
 * <p></p>
 * Refer to {@link org.apache.hadoop.conf.TestConfigurationFieldsBase}
 * for how this class works.
 */
TestConfigRedactor (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfigRedactor.java)/**
 * Tests the tool (and the default expression) for deciding which config
 * redact.
 */
TestAppender (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java)/**
   * A simple appender for white box testing.
   */
TestConfigurationFieldsBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfigurationFieldsBase.java)/**
 * Base class for comparing fields in one or more Configuration classes
 * against a corresponding .xml file.  Usage is intended as follows:
 * <p></p>
 * <ol>
 * <li> Create a subclass to TestConfigurationFieldsBase
 * <li> Define <code>initializeMemberVariables</code> method in the
 *      subclass.  In this class, do the following:
 * <p></p>
 *   <ol>
 *   <li> <b>Required</b> Set the variable <code>xmlFilename</code> to
 *        the appropriate xml definition file
 *   <li> <b>Required</b> Set the variable <code>configurationClasses</code>
 *        to an array of the classes which define the constants used by the
 *        code corresponding to the xml files
 *   <li> <b>Optional</b> Set <code>errorIfMissingConfigProps</code> if the
 *        subclass should throw an error in the method
 *        <code>testCompareXmlAgainstConfigurationClass</code>
 *   <li> <b>Optional</b> Set <code>errorIfMissingXmlProps</code> if the
 *        subclass should throw an error in the method
 *        <code>testCompareConfigurationClassAgainstXml</code>
 *   <li> <b>Optional</b> Instantiate and populate strings into one or
 *        more of the following variables:
 *        <br><code>configurationPropsToSkipCompare</code>
 *        <br><code>configurationPrefixToSkipCompare</code>
 *        <br><code>xmlPropsToSkipCompare</code>
 *        <br><code>xmlPrefixToSkipCompare</code>
 *        <br>
 *        in order to get comparisons clean
 *   </ol>
 * </ol>
 * <p></p>
 * The tests to do class-to-file and file-to-class should automatically
 * run.  This class (and its subclasses) are mostly not intended to be
 * overridden, but to do a very specific form of comparison testing.
 */
TestConfServlet (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfServlet.java)/**
 * Basic test case that the ConfServlet can write configuration
 * to its output in XML and JSON format.
 */
ReconfigurableDummy (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestReconfiguration.java)/**
   * a simple reconfigurable class
   */
TestStorageUnit (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestStorageUnit.java)/**
 * Tests that Storage Units work as expected.
 */
TestKMSClientProvider (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/kms/TestKMSClientProvider.java)/**
 * Unit test for {@link KMSClientProvider} class.
 */
DummyCryptoExtensionKeyProvider (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyProviderCryptoExtension.java)/**
   * Dummy class to test that this key provider is chosen to
   * provide CryptoExtension services over the DefaultCryptoExtension.
   */
DummyCachingCryptoExtensionKeyProvider (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyProviderCryptoExtension.java)/**
   * Dummy class to verify that CachingKeyProvider is used to
   * provide CryptoExtension services if the CachingKeyProvider itself
   * implements CryptoExtension.
   */
TestCryptoOutputStreamClosing (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoOutputStreamClosing.java)/**
 * To test proper closing of underlying stream of CryptoOutputStream.
 */
TestCryptoStreamsNormal (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsNormal.java)/**
 * Test crypto streams using normal stream which does not support the 
 * additional interfaces that the Hadoop FileSystem streams implement 
 * (Seekable, PositionedReadable, ByteBufferReadable, HasFileDescriptor, 
 * CanSetDropBehind, CanSetReadahead, HasEnhancedByteBufferAccess, Syncable, 
 * CanSetDropBehind)
 */
AbstractBondedFSContract (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractBondedFSContract.java)/**
 * This is a filesystem contract for any class that bonds to a filesystem
 * through the configuration.
 *
 * It looks for a definition of the test filesystem with the key
 * derived from "fs.contract.test.fs.%s" -if found the value
 * is converted to a URI and used to create a filesystem. If not -the
 * tests are not enabled
 */
AbstractContractAppendTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractAppendTest.java)/**
 * Test append -if supported
 */
AbstractContractConcatTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractConcatTest.java)/**
 * Test concat -if supported
 */
AbstractContractCreateTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractCreateTest.java)/**
 * Test creating files, overwrite options etc.
 */
AbstractContractDeleteTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractDeleteTest.java)/**
 * Test creating files, overwrite options &c
 */
AllPathsFilter (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractGetFileStatusTest.java)/**
   * Accept everything.
   */
NoPathsFilter (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractGetFileStatusTest.java)/**
   * Accept nothing.
   */
MatchesNameFilter (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractGetFileStatusTest.java)/**
   * Path filter which only expects paths whose final name element
   * equals the {@code match} field.
   */
ExtendedFilterFS (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractGetFileStatusTest.java)/**
   * A filesystem filter which exposes the protected method
   * {@link #listLocatedStatus(Path, PathFilter)}.
   */
AbstractContractGetFileStatusTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractGetFileStatusTest.java)/**
 * Test getFileStatus and related listing operations.
 */
AbstractContractMkdirTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractMkdirTest.java)/**
 * Test directory operations
 */
AbstractContractOpenTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractOpenTest.java)/**
 * Test Open operations.
 */
AbstractContractPathHandleTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractPathHandleTest.java)/**
 * Test {@link PathHandle} operations and semantics.
 * @see ContractOptions#SUPPORTS_FILE_REFERENCE
 * @see ContractOptions#SUPPORTS_CONTENT_CHECK
 * @see org.apache.hadoop.fs.FileSystem#getPathHandle(FileStatus, HandleOpt...)
 * @see org.apache.hadoop.fs.FileSystem#open(PathHandle)
 * @see org.apache.hadoop.fs.FileSystem#open(PathHandle, int)
 */
AbstractContractRenameTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractRenameTest.java)/**
 * Test creating files, overwrite options &c
 */
AbstractContractRootDirectoryTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractRootDirectoryTest.java)/**
 * This class does things to the root directory.
 * Only subclass this for tests against transient filesystems where
 * you don't care about the data.
 */
AbstractContractSeekTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractSeekTest.java)/**
 * Test Seek operations
 */
AbstractContractSetTimesTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractSetTimesTest.java)/**
 * Test setTimes -if supported
 */
AbstractContractUnbufferTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java)/**
 * Contract tests for {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer}.
 */
AbstractFSContract (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractFSContract.java)/**
 * Class representing a filesystem contract that a filesystem
 * implementation is expected implement.
 *
 * Part of this contract class is to allow FS implementations to
 * provide specific opt outs and limits, so that tests can be
 * skip unsupported features (e.g. case sensitivity tests),
 * dangerous operations (e.g. trying to delete the root directory),
 * and limit filesize and other numeric variables for scale tests
 */
AbstractFSContractTestBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractFSContractTestBase.java)/**
 * This is the base class for all the contract tests.
 */
ContractOptions (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/ContractOptions.java)/**
 * Options for contract tests: keys for FS-specific values,
 * defaults.
 */
TreeScanResults (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/ContractTestUtils.java)/**
   * Results of recursive directory creation/scan operations.
   */
NanoTimer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/ContractTestUtils.java)/**
   * A simple class for timing operations in nanoseconds, and for
   * printing some useful results in the process.
   */
ContractTestUtils (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/ContractTestUtils.java)/**
 * Utilities used across test cases.
 */
FTPContract (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/ftp/FTPContract.java)/**
 * The contract of FTP; requires the option "test.testdir" to be set
 */
TestFTPContractMkdir (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/ftp/TestFTPContractMkdir.java)/**
 * Test dir operations on a the local FS.
 */
LocalFSContract (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/localfs/LocalFSContract.java)/**
 * The contract of the Local filesystem.
 * This changes its feature set from platform for platform -the default
 * set is updated during initialization.
 *
 * This contract contains some override points, to permit
 * the raw local filesystem and other filesystems to subclass it.
 */
TestLocalFSContractLoaded (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/localfs/TestLocalFSContractLoaded.java)/**
 * just here to make sure that the local.xml resource is actually loading
 */
TestLocalFSContractMkdir (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/localfs/TestLocalFSContractMkdir.java)/**
 * Test dir operations on a the local FS.
 */
TestLocalFSContractMultipartUploader (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/localfs/TestLocalFSContractMultipartUploader.java)/**
 * Test the FileSystemMultipartUploader on local file system.
 */
RawlocalFSContract (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/rawlocal/RawlocalFSContract.java)/**
 * Raw local filesystem. This is the inner OS-layer FS
 * before checksumming is added around it.
 */
TestRawlocalContractMkdir (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/rawlocal/TestRawlocalContractMkdir.java)/**
 * Test dir operations on a the local FS.
 */
FCStatisticsBaseTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FCStatisticsBaseTest.java)/**
 * <p>
 *   Base class to test {@link FileContext} Statistics.
 * </p>
 */
FileContextMainOperationsBaseTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileContextMainOperationsBaseTest.java)/**
 * <p>
 * A collection of tests for the {@link FileContext}.
 * This test should be used for testing an instance of FileContext
 *  that has been initialized to a specific default FileSystem such a
 *  LocalFileSystem, HDFS,S3, etc.
 * </p>
 * <p>
 * To test a given {@link FileSystem} implementation create a subclass of this
 * test and override {@link #setUp()} to initialize the <code>fc</code> 
 * {@link FileContext} instance variable.
 * 
 * Since this a junit 4 you can also do a single setup before 
 * the start of any tests.
 * E.g.
 *     @BeforeClass   public static void clusterSetupAtBegining()
 *     @AfterClass    public static void ClusterShutdownAtEnd()
 * </p>
 */
FileContextPermissionBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileContextPermissionBase.java)/**
 * <p>
 * A collection of permission tests for the {@link FileContext}.
 * This test should be used for testing an instance of FileContext
 *  that has been initialized to a specific default FileSystem such a
 *  LocalFileSystem, HDFS,S3, etc.
 * </p>
 * <p>
 * To test a given {@link FileSystem} implementation create a subclass of this
 * test and override {@link #setUp()} to initialize the <code>fc</code> 
 * {@link FileContext} instance variable.
 * 
 * Since this a junit 4 you can also do a single setup before 
 * the start of any tests.
 * E.g.
 *     @BeforeClass   public static void clusterSetupAtBegining()
 *     @AfterClass    public static void ClusterShutdownAtEnd()
 * </p>
 */
FileContextTestHelper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileContextTestHelper.java)/**
 * Helper class for unit tests.
 */
FileContextTestWrapper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileContextTestWrapper.java)/**
 * Helper class for unit tests.
 */
FileContextURIBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileContextURIBase.java)/**
 * <p>
 * A collection of tests for the {@link FileContext} to test path names passed
 * as URIs. This test should be used for testing an instance of FileContext that
 * has been initialized to a specific default FileSystem such a LocalFileSystem,
 * HDFS,S3, etc, and where path names are passed that are URIs in a different
 * FileSystem.
 * </p>
 * 
 * <p>
 * To test a given {@link FileSystem} implementation create a subclass of this
 * test and override {@link #setUp()} to initialize the <code>fc1</code> and
 * <code>fc2</code>
 * 
 * The tests will do operations on fc1 that use a URI in fc2
 * 
 * {@link FileContext} instance variable.
 * </p>
 */
FileContextUtilBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileContextUtilBase.java)/**
 * <p>
 * A collection of Util tests for the {@link FileContext#util()}.
 * This test should be used for testing an instance of {@link FileContext#util()}
 *  that has been initialized to a specific default FileSystem such a
 *  LocalFileSystem, HDFS,S3, etc.
 * </p>
 * <p>
 * To test a given {@link FileSystem} implementation create a subclass of this
 * test and override {@link #setUp()} to initialize the <code>fc</code> 
 * {@link FileContext} instance variable.
 * 
 * </p>
 */
FileSystemContractBaseTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileSystemContractBaseTest.java)/**
 * <p>
 * A collection of tests for the contract of the {@link FileSystem}.
 * This test should be used for general-purpose implementations of
 * {@link FileSystem}, that is, implementations that provide implementations 
 * of all of the functionality of {@link FileSystem}.
 * </p>
 * <p>
 * To test a given {@link FileSystem} implementation create a subclass of this
 * test and add a @Before method to initialize the <code>fs</code>
 * {@link FileSystem} instance variable.
 * </p>
 */
MockFileSystem (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileSystemTestHelper.java)/**
   * Class to enable easier mocking of a FileSystem
   * Use getRawFileSystem to retrieve the mock
   */
FileSystemTestHelper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileSystemTestHelper.java)/**
 * Helper class for unit tests.
 */
FileSystemTestWrapper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileSystemTestWrapper.java)/**
 * Helper class for unit tests.
 */
FSMainOperationsBaseTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FSMainOperationsBaseTest.java)/**
 * <p>
 * A collection of tests for the {@link FileSystem}.
 * This test should be used for testing an instance of FileSystem
 *  that has been initialized to a specific default FileSystem such a
 *  LocalFileSystem, HDFS,S3, etc.
 * </p>
 * <p>
 * To test a given {@link FileSystem} implementation create a subclass of this
 * test and override {@link #setUp()} to initialize the <code>fSys</code> 
 * {@link FileSystem} instance variable.
 * 
 * Since this a junit 4 you can also do a single setup before 
 * the start of any tests.
 * E.g.
 *     @BeforeClass   public static void clusterSetupAtBegining()
 *     @AfterClass    public static void ClusterShutdownAtEnd()
 * </p>
 */
FSTestWrapper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FSTestWrapper.java)/**
 * Abstraction of filesystem functionality with additional helper methods
 * commonly used in tests. This allows generic tests to be written which apply
 * to the two filesystem abstractions in Hadoop: {@link FileSystem} and
 * {@link FileContext}.
 */
FSWrapper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FSWrapper.java)/**
 * Abstraction of filesystem operations that is essentially an interface
 * extracted from {@link FileContext}.
 */
TestFTPFileSystem (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/ftp/TestFTPFileSystem.java)/**
 * Test basic @{link FTPFileSystem} class methods. Contract tests are in
 * TestFTPContractXXXX.
 */
TestHttpFileSystem (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/http/TestHttpFileSystem.java)/**
 * Testing HttpFileSystem.
 */
TestFutureIO (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/impl/TestFutureIO.java)/**
 * Test behavior of {@link FutureIOSupport}, especially "what thread do things
 * happen in?".
 */
DataGenerator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/loadGenerator/DataGenerator.java)/**
 * This program reads the directory structure and file structure from
 * the input directory and creates the namespace in the file system
 * specified by the configuration in the specified root.
 * All the files are filled with 'a'.
 * 
 * The synopsis of the command is
 * java DataGenerator 
 *   -inDir <inDir>: input directory name where directory/file structures
 *                   are stored. Its default value is the current directory.
 *   -root <root>: the name of the root directory which the new namespace 
 *                 is going to be placed under. 
 *                 Its default value is "/testLoadSpace".
 */
DFSClientThread (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/loadGenerator/LoadGenerator.java)/** A thread sends a stream of requests to the NameNode.
   * At each iteration, it first decides if it is going to read a file,
   * create a file, or listing a directory following the read
   * and write probabilities.
   * When reading, it randomly picks a file in the test space and reads
   * the entire file. When writing, it randomly picks a directory in the
   * test space and creates a file whose name consists of the current 
   * machine's host name and the thread id. The length of the file
   * follows Gaussian distribution with an average size of 2 blocks and
   * the standard deviation of 1 block. The new file is filled with 'a'.
   * Immediately after the file creation completes, the file is deleted
   * from the test space.
   * While listing, it randomly picks a directory in the test space and
   * list the directory content.
   * Between two consecutive operations, the thread pauses for a random
   * amount of time in the range of [0, maxDelayBetweenOps] 
   * if the specified max delay is not zero.
   * A thread runs for the specified elapsed time if the time isn't zero.
   * Otherwise, it runs forever.
   */
LoadGenerator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/loadGenerator/LoadGenerator.java)/** The load generator is a tool for testing NameNode behavior under
 * different client loads. Note there is a subclass of this clas that lets 
 * you run a the load generator as a MapReduce job (see LoadGeneratorMR in the 
 * MapReduce project.
 * 
 * The loadGenerator allows the user to generate different mixes of read, write,
 * and list requests by specifying the probabilities of read and
 * write. The user controls the intensity of the load by
 * adjusting parameters for the number of worker threads and the delay
 * between operations. While load generators are running, the user
 * can profile and monitor the running of the NameNode. When a load
 * generator exits, it print some NameNode statistics like the average
 * execution time of each kind of operations and the NameNode
 * throughput.
 *
 * The program can run in one of two forms. As a regular single process command
 * that runs multiple threads to generate load on the NN or as a Map Reduce
 * program that runs multiple (multi-threaded) map tasks that generate load
 * on the NN; the results summary is generated by a single reduce task.
 * 
 * 
 * The user may either specify constant duration, read and write 
 * probabilities via the command line, or may specify a text file
 * that acts as a script of which read and write probabilities to
 * use for specified durations. If no duration is specified the program
 * runs till killed (duration required if run as MapReduce).
 * 
 * The script takes the form of lines of duration in seconds, read
 * probability and write probability, each separated by white space.
 * Blank lines and lines starting with # (comments) are ignored. If load
 * generator is run as a MapReduce program then the script file needs to be
 * accessible on the the Map task as a HDFS file.
 * 
 * After command line argument parsing and data initialization,
 * the load generator spawns the number of worker threads 
 * as specified by the user.
 * Each thread sends a stream of requests to the NameNode.
 * For each iteration, it first decides if it is going to read a file,
 * create a file, or listing a directory following the read and write 
 * probabilities specified by the user.
 * When reading, it randomly picks a file in the test space and reads
 * the entire file. When writing, it randomly picks a directory in the
 * test space and creates a file whose name consists of the current 
 * machine's host name and the thread id. The length of the file
 * follows Gaussian distribution with an average size of 2 blocks and
 * the standard deviation of 1 block. The new file is filled with 'a'.
 * Immediately after the file creation completes, the file is deleted
 * from the test space.
 * While listing, it randomly picks a directory in the test space and
 * list the directory content.
 * Between two consecutive operations, the thread pauses for a random
 * amount of time in the range of [0, maxDelayBetweenOps] 
 * if the specified max delay is not zero.
 * All threads are stopped when the specified elapsed time has passed 
 * in command-line execution, or all the lines of script have been 
 * executed, if using a script.
 * Before exiting, the program prints the average execution for 
 * each kind of NameNode operations, and the number of requests
 * served by the NameNode.
 *
 * The synopsis of the command is
 * java LoadGenerator
 *   -readProbability <read probability>: read probability [0, 1]
 *                                        with a default value of 0.3333. 
 *   -writeProbability <write probability>: write probability [0, 1]
 *                                         with a default value of 0.3333.
 *   -root <root>: test space with a default value of /testLoadSpace
 *   -maxDelayBetweenOps <maxDelayBetweenOpsInMillis>: 
 *      Max delay in the unit of milliseconds between two operations with a 
 *      default value of 0 indicating no delay.
 *   -numOfThreads <numOfThreads>: 
 *      number of threads to spawn with a default value of 200.
 *   -elapsedTime <elapsedTimeInSecs>: 
 *      the elapsed time of program with a default value of 0 
 *      indicating running forever
 *   -startTime <startTimeInMillis> : when the threads start to run.
 *   -scriptFile <file name>: text file to parse for scripted operation
 */
INode (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/loadGenerator/StructureGenerator.java)/** In memory representation of a directory */
FileINode (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/loadGenerator/StructureGenerator.java)/** In memory representation of a file */
StructureGenerator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/loadGenerator/StructureGenerator.java)/**
 * This program generates a random namespace structure with the following
 * constraints:
 * 1. The number of subdirectories is a random number in [minWidth, maxWidth].
 * 2. The maximum depth of each subdirectory is a random number 
 *    [2*maxDepth/3, maxDepth].
 * 3. Files are randomly placed in the empty directories. The size of each
 *    file follows Gaussian distribution.
 * The generated namespace structure is described by two files in the output
 * directory. Each line of the first file 
 * contains the full name of a leaf directory.  
 * Each line of the second file contains
 * the full name of a file and its size, separated by a blank.
 * 
 * The synopsis of the command is
 * java StructureGenerator
    -maxDepth <maxDepth> : maximum depth of the directory tree; default is 5.
    -minWidth <minWidth> : minimum number of subdirectories per directories; default is 1
    -maxWidth <maxWidth> : maximum number of subdirectories per directories; default is 5
    -numOfFiles <#OfFiles> : the total number of files; default is 10.
    -avgFileSize <avgFileSizeInBlocks>: average size of blocks; default is 1.
    -outDir <outDir>: output directory; default is the current directory.
    -seed <seed>: random number generator seed; default is the current time.
 */
TestAcl (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/permission/TestAcl.java)/**
 * Tests covering basic functionality of the ACL objects.
 */
TestFSSerialization (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/protocolPB/TestFSSerialization.java)/**
 * Verify PB serialization of FS data structures.
 */
MockFileSystem (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/find/MockFileSystem.java)/**
 * A mock {@link FileSystem} for use with the {@link Find} unit tests. Usage:
 * FileSystem mockFs = MockFileSystem.setup(); Methods in the mockFs can then be
 * mocked out by the test script. The {@link Configuration} can be accessed by
 * mockFs.getConf(); The following methods are fixed within the class: -
 * {@link FileSystem#initialize(URI,Configuration)} blank stub -
 * {@link FileSystem#makeQualified(Path)} returns the passed in {@link Path} -
 * {@link FileSystem#getWorkingDirectory} returns new Path("/") -
 * {@link FileSystem#resolvePath(Path)} returns the passed in {@link Path}
 */
TestHelper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/find/TestHelper.java)/** Helper methods for the find expression unit tests. */
TestCopyFromLocal (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestCopyFromLocal.java)/**
 * Test for copyFromLocal.
 */
TestCount (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestCount.java)/**
 * JUnit test class for {@link org.apache.hadoop.fs.shell.Count}
 * 
 */
TestLs (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestLs.java)/**
 * JUnit test class for {@link org.apache.hadoop.fs.shell.Ls}
 *
 */
TestPrintableString (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestPrintableString.java)/**
 * Test {@code PrintableString} class.
 */
TestTail (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestTail.java)/**
 * Test class to verify Tail shell command.
 */
TestTextCommand (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestTextCommand.java)/**
 * This class tests the logic for displaying the binary formats supported
 * by the Text command.
 */
TestEtagChecksum (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/store/TestEtagChecksum.java)/**
 * Unit test of etag operations.
 */
SymlinkBaseTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/SymlinkBaseTest.java)/**
 * Base test for symbolic links
 */
TestChecksumFs (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestChecksumFs.java)/**
 * This class tests the functionality of ChecksumFs.
 */
TestCommandFormat (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestCommandFormat.java)/**
 * This class tests the command line parsing
 */
TestDefaultUri (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestDefaultUri.java)/**
 * Test default URI related APIs in {@link FileSystem}.
 */
UnOverrideDefaultPortFileSystem (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestDelegateToFsCheckPath.java)/**
   * UnOverrideDefaultPortFileSystem does not define default port.
   * The default port defined by AbstractFilesystem is used in this case.
   * (default 0).
   */
OverrideDefaultPortFileSystem (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestDelegateToFsCheckPath.java)/**
   * OverrideDefaultPortFileSystem defines default port.
   */
TestDelegateToFsCheckPath (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestDelegateToFsCheckPath.java)/**
 * The default port of DelegateToFileSystem is set from child file system.
 */
TestDFCachingGetSpaceUsed (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestDFCachingGetSpaceUsed.java)/**
 * Test to make sure df can run and work.
 */
TestDU (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestDU.java)/** This test makes sure that "DU" does not get to run on each call to getUsed */
TestFcLocalFsPermission (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFcLocalFsPermission.java)/**
 * Test permissions for localFs using FileContext API.
 */
TestFcLocalFsUtil (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFcLocalFsUtil.java)/**
 * Test Util for localFs using FileContext API.
 */
TestFileContextDeleteOnExit (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileContextDeleteOnExit.java)/**
 * Tests {@link FileContext.#deleteOnExit(Path)} functionality.
 */
TestFileContextResolveAfs (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileContextResolveAfs.java)/**
 * Tests resolution of AbstractFileSystems for a given path with symlinks.
 */
TestFileSystemStorageStatistics (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemStorageStatistics.java)/**
 * This tests basic operations of {@link FileSystemStorageStatistics} class.
 */
MyFile (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileUtil.java)/**
   * Extend {@link File}. Same as {@link File} except for two things: (1) This
   * treats file1Name as a very special file which is not delete-able
   * irrespective of it's parent-dir's permissions, a peculiar file instance for
   * testing. (2) It returns the files in alphabetically sorted order when
   * listed.
   * 
   */
MustNotImplement (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java)/**
   * FileSystem methods that must not be overwritten by
   * {@link FilterFileSystem}. Either because there is a default implementation
   * already available or because it is not relevant.
   */
TestFsShellList (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFsShellList.java)/**
 * Test FsShell -ls command.
 */
FakeChown (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFsShellReturnCode.java)/**
   * Faked Chown class for {@link testChownUserAndGroupValidity()}.
   *
   * The test only covers argument parsing, so override to skip processing.
   */
FakeChgrp (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFsShellReturnCode.java)/**
   * Faked Chgrp class for {@link testChgrpGroupValidity()}.
   * The test only covers argument parsing, so override to skip processing.
   */
TestFsShellReturnCode (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFsShellReturnCode.java)/**
 * This test validates that chmod, chown, chgrp returning correct exit codes
 * 
 */
TestFsUrlConnectionPath (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFsUrlConnectionPath.java)/**
 * Test case for FsUrlConnection with relativePath and SPACE.
 */
TestGetFileBlockLocations (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestGetFileBlockLocations.java)/**
 * Testing the correctness of FileSystem.getFileBlockLocations.
 */
TestGlobPattern (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestGlobPattern.java)/**
 * Tests for glob patterns
 */
TestHardLink (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestHardLink.java)/**
 * This testing is fairly lightweight.  Assumes HardLink routines will
 * only be called when permissions etc are okay; no negative testing is
 * provided.
 * 
 * These tests all use 
 * "src" as the source directory, 
 * "tgt_one" as the target directory for single-file hardlinking, and
 * "tgt_mult" as the target directory for multi-file hardlinking.
 * 
 * Contents of them are/will be:
 * dir:src: 
 *   files: x1, x2, x3
 * dir:tgt_one:
 *   files: x1 (linked to src/x1), y (linked to src/x2), 
 *          x3 (linked to src/x3), x11 (also linked to src/x1)
 * dir:tgt_mult:
 *   files: x1, x2, x3 (all linked to same name in src/)
 *   
 * NOTICE: This test class only tests the functionality of the OS
 * upon which the test is run! (although you're pretty safe with the
 * unix-like OS's, unless a typo sneaks in.)
 */
MustNotImplement (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestHarFileSystem.java)/**
   * FileSystem methods that must not be overwritten by
   * {@link HarFileSystem}. Either because there is a default implementation
   * already available or because it is not relevant.
   */
TestHarFileSystemBasics (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestHarFileSystemBasics.java)/**
 * This test class checks basic operations with {@link HarFileSystem} including
 * various initialization cases, getters, and modification methods.
 * 
 * NB: to run this test from an IDE make sure the folder
 * "hadoop-common-project/hadoop-common/src/main/resources/" is added as a
 * source path. This will allow the system to pick up the "core-default.xml" and
 * "META-INF/services/..." resources from the class-path in the runtime.
 */
TestListFiles (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestListFiles.java)/**
 * This class tests the FileStatus API.
 */
TestLocalDirAllocator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java)/** This test LocalDirAllocator works correctly;
 * Every test case uses different buffer dirs to
 * enforce the AllocatorPerContext initialization.
 * This test does not run on Cygwin because under Cygwin
 * a directory can be created in a read-only directory
 * which breaks this test.
 */
BuilderWithSupportedKeys (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalFileSystem.java)/**
   * A builder to verify configuration keys are supported.
   */
TestLocalFileSystem (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalFileSystem.java)/**
 * This class tests the local file system via the FileSystem abstraction.
 */
TestLocalFileSystemPermission (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalFileSystemPermission.java)/**
 * This class tests the local file system via the FileSystem abstraction.
 */
TestLocalFsFCStatistics (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalFsFCStatistics.java)/**
 * <p>
 *    Tests the File Context Statistics for {@link LocalFileSystem}
 * </p>
 */
TestLocatedFileStatus (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocatedFileStatus.java)/**
 * This class tests the LocatedFileStatus class.
 */
TestPath (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestPath.java)/**
 * Test Hadoop Filesystem Paths.
 */
TestRawLocalFileSystemContract (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestRawLocalFileSystemContract.java)/**
 * Test filesystem contracts with {@link RawLocalFileSystem}.
 * Root directory related tests from super class will work into target
 * directory since we have no permission to write / on local filesystem.
 */
TestSymlinkLocalFS (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestSymlinkLocalFS.java)/**
 * Test symbolic links using LocalFs.
 */
AuditableEmptier (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestTrash.java)/**
     * A fake emptier that simulates to delete a checkpoint
     * in a fixed interval.
     */
AuditableTrashPolicy (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestTrash.java)/**
   * A fake {@link TrashPolicy} implementation, it keeps a count
   * on number of checkpoints in the trash. It doesn't do anything
   * other than updating the count.
   *
   */
AuditableCheckpoints (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestTrash.java)/**
   * Only counts the number of checkpoints, not do anything more.
   * Declared as an inner static class to share state between
   * testing threads.
   */
TestTrash (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestTrash.java)/**
 * This class tests commands from Trash.
 */
TestTruncatedInputBug (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestTruncatedInputBug.java)/**
 * test for the input truncation bug when mark/reset is used.
 * HADOOP-1489
 */
TestViewFileSystemDelegation (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemDelegation.java)/**
 * Verify that viewfs propagates certain methods to the underlying fs 
 */
TestViewFileSystemDelegationTokenSupport (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemDelegationTokenSupport.java)/**
 * Test ViewFileSystem's support for having delegation tokens fetched and cached
 * for the file system.
 * 
 * Currently this class just ensures that getCanonicalServiceName() always
 * returns <code>null</code> for ViewFileSystem instances.
 */
TestViewFileSystemWithAuthorityLocalFileSystem (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemWithAuthorityLocalFileSystem.java)/**
 * 
 * Test the ViewFsBaseTest using a viewfs with authority: 
 *    viewfs://mountTableName/
 *    ie the authority is used to load a mount table.
 *    The authority name used is "default"
 *
 */
TestViewfsFileStatus (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewfsFileStatus.java)/**
 * The FileStatus is being serialized in MR as jobs are submitted.
 * Since viewfs has overlayed ViewFsFileStatus, we ran into
 * serialization problems. THis test is test the fix.
 */
ViewFileSystemTestSetup (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFileSystemTestSetup.java)/**
 * This class is for  setup and teardown for viewFileSystem so that
 * it can be tested via the standard FileSystem tests.
 * 
 * If tests launched via ant (build.xml) the test root is absolute path
 * If tests launched via eclipse, the test root is 
 * is a test dir below the working directory. (see FileContextTestHelper)
 * 
 * We set a viewFileSystems with 3 mount points: 
 * 1) /<firstComponent>" of testdir  pointing to same in  target fs
 * 2)   /<firstComponent>" of home  pointing to same in  target fs 
 * 3)  /<firstComponent>" of wd  pointing to same in  target fs
 * (note in many cases the link may be the same - viewFileSytem handles this)
 * 
 * We also set the view file system's wd to point to the wd. 
 */
ViewFsBaseTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFsBaseTest.java)/**
 * <p>
 * A collection of tests for the {@link ViewFs}.
 * This test should be used for testing ViewFs that has mount links to 
 * a target file system such  localFs or Hdfs etc.

 * </p>
 * <p>
 * To test a given target file system create a subclass of this
 * test and override {@link #setUp()} to initialize the <code>fcTarget</code> 
 * to point to the file system to which you want the mount targets
 * 
 * Since this a junit 4 you can also do a single setup before 
 * the start of any tests.
 * E.g.
 *     @BeforeClass   public static void clusterSetupAtBegining()
 *     @AfterClass    public static void ClusterShutdownAtEnd()
 * </p>
 */
NullWatcher (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/ClientBaseWithFixes.java)/**
     * In general don't use this. Only use in the special case that you
     * want to ignore results (for whatever reason) in your test. Don't
     * use empty watchers in real code!
     *
     */
ClientBaseWithFixes (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/ClientBaseWithFixes.java)/**
 * Copy-paste of ClientBase from ZooKeeper, but without any of the
 * JMXEnv verification. There seems to be a bug ZOOKEEPER-1438
 * which causes spurious failures in the JMXEnv verification when
 * we run these tests with the upstream ClientBase.
 */
DummyHAService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java)/**
 * Test-only implementation of {@link HAServiceTarget}, which returns
 * a mock implementation.
 */
DummySharedResource (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummySharedResource.java)/**
 * A fake shared resource, for use in automatic failover testing.
 * This simulates a real shared resource like a shared edit log.
 * When the {@link DummyHAService} instances change state or get
 * fenced, they notify the shared resource, which asserts that
 * we never have two HA services who think they're holding the
 * resource at the same time.
 */
DummyZKFCThread (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/MiniZKFCCluster.java)/**
   * Test-thread which runs a ZK Failover Controller corresponding
   * to a given dummy service.
   */
MiniZKFCCluster (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/MiniZKFCCluster.java)/**
 * Harness for starting two dummy ZK FailoverControllers, associated with
 * DummyHAServices. This harness starts two such ZKFCs, designated by
 * indexes 0 and 1, and provides utilities for building tests around them.
 */
TestActiveStandbyElectorRealZK (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestActiveStandbyElectorRealZK.java)/**
 * Test for {@link ActiveStandbyElector} using real zookeeper.
 */
TestHealthMonitorWithDedicatedHealthAddress (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHealthMonitorWithDedicatedHealthAddress.java)/**
 * Repeats all tests of {@link TestHealthMonitor}, but using a separate
 * dedicated health check RPC address.
 */
AlwaysSucceedFencer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java)/**
   * Mock fencing method that always returns true
   */
AlwaysFailFencer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java)/**
   * Identical mock to above, except always returns false
   */
LogAnswer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java)/**
   * An answer simply delegate some basic log methods to real LOG.
   */
RandomlyThrow (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestZKFailoverControllerStress.java)/**
   * Randomly throw an exception half the time the method is called
   */
TestZKFailoverControllerStress (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestZKFailoverControllerStress.java)/**
 * Stress test for ZKFailoverController.
 * Starts multiple ZKFCs for dummy services, and then performs many automatic
 * failovers. While doing so, ensures that a fake "shared resource"
 * (simulating the shared edits dir) is only owned by one service at a time. 
 */
HttpServerFunctionalTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/HttpServerFunctionalTest.java)/**
 * This is a base class for functional tests of the {@link HttpServer2}.
 * The methods are static for other classes to import statically.
 */
JerseyResource (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/resource/JerseyResource.java)/**
 * A simple Jersey resource class TestHttpServer.
 * The servlet simply puts the path and the op parameter in a map
 * and return it in JSON format in the response.
 */
Initializer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestGlobalFilter.java)/** Configuration for RecordingFilter */
RecordingFilter (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestGlobalFilter.java)/** A very simple filter that records accessed uri's */
DummyServletFilter (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestHttpServer.java)/**
   * Dummy filter that mimics as an authentication filter. Obtains user identity
   * from the request parameter user.name. Wraps around the request so that
   * request.getRemoteUser() returns the user identity.
   * 
   */
DummyFilterInitializer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestHttpServer.java)/**
   * FilterInitializer that initialized the DummyFilter.
   *
   */
MyGroupsProvider (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestHttpServer.java)/**
   * Custom user->group mapping service.
   */
TestHttpServerWebapps (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestHttpServerWebapps.java)/**
 * Test webapp loading
 */
TestHttpServerWithSpnego (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestHttpServerWithSpnego.java)/**
 * This class is tested for http server with SPNEGO authentication.
 */
TestIsActiveServlet (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestIsActiveServlet.java)/**
 * Test if the {@link IsActiveServlet} returns the right answer if the
 * underlying service is active.
 */
Initializer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestPathFilter.java)/** Configuration for RecordingFilter */
RecordingFilter (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestPathFilter.java)/** A very simple filter that records accessed uri's */
Initializer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestServletFilter.java)/** Configuration for the filter */
SimpleFilter (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestServletFilter.java)/** A very simple filter which record the uri filtered. */
Initializer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestServletFilter.java)/** Configuration for the filter */
TestSSLHttpServer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/http/TestSSLHttpServer.java)/**
 * This testcase issues SSL certificates configures the HttpServer to serve
 * HTTPS using the created certificates and calls an echo servlet using the
 * corresponding HTTPS URL.
 */
FakeCompressor (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/FakeCompressor.java)/**
 * A fake compressor
 * Its input and output is the same.
 */
FakeDecompressor (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/FakeDecompressor.java)/**
 * A fake decompressor, just like FakeCompressor
 * Its input and output is the same.
 */
TestCompressorDecompressor (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCompressorDecompressor.java)/** 
 * Test for pairs:
 * <pre>
 * SnappyCompressor/SnappyDecompressor
 * Lz4Compressor/Lz4Decompressor
 * BuiltInZlibDeflater/new BuiltInZlibInflater
 *
 *
 * Note: we can't use ZlibCompressor/ZlibDecompressor here 
 * because his constructor can throw exception (if native libraries not found)
 * For ZlibCompressor/ZlibDecompressor pair testing used {@code TestZlibCompressorDecompressor}   
 *
 * </pre>
 *
 */
TestGzipCodec (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestGzipCodec.java)/**
 * Verify resettable compressor.
 */
SimpleBufferAllocator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/BufferAllocator.java)/**
   * A simple buffer allocator that just uses ByteBuffer's
   * allocate/allocateDirect API.
   */
SlicedBufferAllocator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/BufferAllocator.java)/**
   * A buffer allocator that allocates a buffer from an existing large buffer by
   * slice calling, but if no available space just degrades as
   * SimpleBufferAllocator. So please ensure enough space for it.
   */
BufferAllocator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/BufferAllocator.java)/**
 * An abstract buffer allocator used for test.
 */
TestBlock (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/coder/TestErasureCoderBase.java)/**
   * It's just a block for this test purpose. We don't use HDFS block here
   * at all for simple.
   */
TestErasureCoderBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/coder/TestErasureCoderBase.java)/**
 * Erasure coder test base with utilities.
 */
TestHHErasureCoderBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/coder/TestHHErasureCoderBase.java)/**
 * Erasure coder test base with utilities for hitchhiker.
 */
TestRSErasureCoder (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/coder/TestRSErasureCoder.java)/**
 * Test Reed-Solomon encoding and decoding.
 */
TestXORCoder (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/coder/TestXORCoder.java)/**
 * Test XOR encoding and decoding.
 */
RawErasureCoderBenchmark (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureCoderBenchmark.java)/**
 * A benchmark tool to test the performance of different erasure coders.
 * The tool launches multiple threads to encode/decode certain amount of data,
 * and measures the total throughput. It only focuses on performance and doesn't
 * validate correctness of the encoded/decoded results.
 * User can specify the data size each thread processes, as well as the chunk
 * size to use for the coder.
 * Different coders are supported. User can specify the coder by a coder index.
 * The coder is shared among all the threads.
 */
TestCoderUtil (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestCoderUtil.java)/**
 * Test of the utility of raw erasure coder.
 */
TestDummyRawCoder (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestDummyRawCoder.java)/**
 * Test dummy raw coder.
 */
TestNativeRSRawCoder (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestNativeRSRawCoder.java)/**
 * Test native raw Reed-solomon encoding and decoding.
 */
TestNativeXORRawCoder (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestNativeXORRawCoder.java)/**
 * Test NativeXOR encoding and decoding.
 */
TestRawCoderBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestRawCoderBase.java)/**
 * Raw coder test base with utilities.
 */
TestRawErasureCoderBenchmark (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestRawErasureCoderBenchmark.java)/**
 * Tests for the raw erasure coder benchmark tool.
 */
TestRSLegacyRawCoder (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestRSLegacyRawCoder.java)/**
 * Test the legacy raw Reed-solomon coder implemented in Java.
 */
TestRSRawCoder (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestRSRawCoder.java)/**
 * Test the new raw Reed-solomon coder implemented in Java.
 */
TestRSRawCoderBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestRSRawCoderBase.java)/**
 * Test base for raw Reed-solomon coders.
 */
TestRSRawCoderInteroperable1 (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestRSRawCoderInteroperable1.java)/**
 * Test raw Reed-solomon coder implemented in Java.
 */
TestRSRawCoderInteroperable2 (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestRSRawCoderInteroperable2.java)/**
 * Test raw Reed-solomon coder implemented in Java.
 */
TestXORRawCoder (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestXORRawCoder.java)/**
 * Test pure Java XOR encoding and decoding.
 */
TestXORRawCoderBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestXORRawCoderBase.java)/**
 * Test base for raw XOR coders.
 */
TestXORRawCoderInteroperable1 (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestXORRawCoderInteroperable1.java)/**
 * Test raw XOR coder implemented in Java.
 */
TestXORRawCoderInteroperable2 (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/rawcoder/TestXORRawCoderInteroperable2.java)/**
 * Test raw XOR coder implemented in Java.
 */
TestCodecRawCoderMapping (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/TestCodecRawCoderMapping.java)/**
 * Test the codec to raw coder mapping.
 */
TestCodecRegistry (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/TestCodecRegistry.java)/**
 * Test CodecRegistry.
 */
TestCoderBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/erasurecode/TestCoderBase.java)/**
 * Test base of common utilities for tests not only raw coders but also block
 * coders.
 */
KVGenerator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/file/tfile/KVGenerator.java)/**
 * Generate random <key, value> pairs.
 */
NanoTimer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/file/tfile/NanoTimer.java)/**
 * A nano-second timer.
 */
DiscreteRNG (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/file/tfile/RandomDistribution.java)/**
   * Interface for discrete (integer) random distributions.
   */
Flat (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/file/tfile/RandomDistribution.java)/**
   * P(i)=1/(max-min)
   */
Zipf (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/file/tfile/RandomDistribution.java)/**
   * Zipf distribution. The ratio of the probabilities of integer i and j is
   * defined as follows:
   * 
   * P(i)/P(j)=((j-min+1)/(i-min+1))^sigma.
   */
Binomial (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/file/tfile/RandomDistribution.java)/**
   * Binomial distribution.
   * 
   * P(k)=select(n, k)*p^k*(1-p)^(n-k) (k = 0, 1, ..., n)
   * 
   * P(k)=select(max-min-1, k-min)*p^(k-min)*(1-p)^(k-min)*(1-p)^(max-k-1)
   */
RandomDistribution (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/file/tfile/RandomDistribution.java)/**
 * A class that generates random numbers that follow some distribution.
 */
TestTFile (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/file/tfile/TestTFile.java)/**
 * test tfile features.
 * 
 */
TestTFileByteArrays (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/file/tfile/TestTFileByteArrays.java)/**
 * 
 * Byte arrays test case class using GZ compression codec, base class of none
 * and LZO compression classes.
 * 
 */
TestTFileComparators (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/file/tfile/TestTFileComparators.java)/**
 * 
 * Byte arrays test case class using GZ compression codec, base class of none
 * and LZO compression classes.
 * 
 */
TestTFileSeek (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/file/tfile/TestTFileSeek.java)/**
 * test the performance for seek.
 *
 */
Timer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/file/tfile/Timer.java)/**
 * this class is a time class to 
 * measure to measure the time 
 * taken for some event.
 */
Comparator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/RandomDatum.java)/** A WritableComparator optimized for RandomDatum. */
TestConnectionRetryPolicy (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/retry/TestConnectionRetryPolicy.java)/**
 * This class mainly tests behaviors of various retry policies in connection
 * level.
 */
TestDefaultRetryPolicy (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/retry/TestDefaultRetryPolicy.java)/**
 * Test the behavior of the default retry policy.
 */
TestRetryProxy (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/retry/TestRetryProxy.java)/**
 * TestRetryProxy tests the behaviour of the {@link RetryPolicy} class using
 * a certain method of {@link UnreliableInterface} implemented by
 * {@link UnreliableImplementation}.
 *
 * Some methods may be sensitive to the {@link Idempotent} annotation
 * (annotated in {@link UnreliableInterface}).
 */
UnreliableImplementation (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/retry/UnreliableImplementation.java)/**
 * For the usage and purpose of this class see {@link UnreliableInterface}
 * which this class implements.
 *
 * @see UnreliableInterface
 */
UnreliableInterface (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/retry/UnreliableInterface.java)/**
 * The methods of UnreliableInterface could throw exceptions in a
 * predefined way. It is currently used for testing {@link RetryPolicy}
 * and {@link FailoverProxyProvider} classes, but can be potentially used
 * to test any class's behaviour where an underlying interface or class
 * may throw exceptions.
 *
 * Some methods may be annotated with the {@link Idempotent} annotation.
 * In order to test those some methods of UnreliableInterface are annotated,
 * but they are not actually Idempotent functions.
 *
 */
TestArrayFile (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestArrayFile.java)/** Support for flat files of binary key/value pairs. */
TestArrayPrimitiveWritable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestArrayPrimitiveWritable.java)/** Unit tests for {@link ArrayPrimitiveWritable} */
TestArrayWritable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestArrayWritable.java)/** Unit tests for ArrayWritable */
TestBoundedByteArrayOutputStream (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestBoundedByteArrayOutputStream.java)/** Unit tests for BoundedByteArrayOutputStream */
TestBytesWritable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestBytesWritable.java)/**
 * This is the unit test for BytesWritable.
 */
TestEnumSetWritable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestEnumSetWritable.java)/** Unit test for EnumSetWritable */
Foo (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestGenericWritable.java)/** Dummy class for testing {@link GenericWritable} */
Bar (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestGenericWritable.java)/** Dummy class for testing {@link GenericWritable} */
Baz (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestGenericWritable.java)/** Dummy class for testing {@link GenericWritable} */
FooGenericWritable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestGenericWritable.java)/** Dummy class for testing {@link GenericWritable} */
TestGenericWritable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestGenericWritable.java)/**
 * TestCase for {@link GenericWritable} class.
 * @see TestWritable#testWritable(Writable)
 */
TestIOUtils (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestIOUtils.java)/**
 * Test cases for IOUtils.java
 */
TestMapWritable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestMapWritable.java)/**
 * Tests MapWritable
 */
TestMD5Hash (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestMD5Hash.java)/** Unit tests for MD5Hash. */
TestObjectWritableProtos (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestObjectWritableProtos.java)/**
 * Test case for the use of Protocol Buffers within ObjectWritable.
 */
TestSequenceFile (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestSequenceFile.java)/** Support for flat files of binary key/value pairs. */
TestSequenceFileSync (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestSequenceFileSync.java)/** Tests sync based seek reads/write intervals inside SequenceFiles. */
TestSetFile (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestSetFile.java)/** Support for flat files of binary key/value pairs. */
TestSortedMapWritable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestSortedMapWritable.java)/**
 * Tests SortedMapWritable
 */
TestText (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestText.java)/** Unit tests for LargeUTF8. */
TestTextNonUTF8 (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestTextNonUTF8.java)/** Unit tests for NonUTF8. */
TestUTF8 (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestUTF8.java)/** Unit tests for UTF8. */
SimpleVersionedWritable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestVersionedWritable.java)/** Example class used in test cases below. */
SimpleWritable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestWritable.java)/** Example class used in test cases below. */
TestWritable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestWritable.java)/** Unit tests for Writable. */
SimpleWritable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestWritableName.java)/** Example class used in test cases below. */
TestWritableName (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestWritableName.java)/** Unit tests for WritableName. */
MiniServer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/MiniRPCBenchmark.java)/**
   * Primitive RPC server, which
   * allows clients to connect to it.
   */
MiniRPCBenchmark (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/MiniRPCBenchmark.java)/**
 * MiniRPCBenchmark measures time to establish an RPC connection 
 * to a secure RPC server.
 * It sequentially establishes connections the specified number of times, 
 * and calculates the average time taken to connect.
 * The time to connect includes the server side authentication time.
 * The benchmark supports three authentication methods:
 * <ol>
 * <li>simple - no authentication. In order to enter this mode 
 * the configuration file <tt>core-site.xml</tt> should specify
 * <tt>hadoop.security.authentication = simple</tt>.
 * This is the default mode.</li>
 * <li>kerberos - kerberos authentication. In order to enter this mode 
 * the configuration file <tt>core-site.xml</tt> should specify
 * <tt>hadoop.security.authentication = kerberos</tt> and 
 * the argument string should provide qualifying
 * <tt>keytabFile</tt> and <tt>userName</tt> parameters.
 * <li>delegation token - authentication using delegation token.
 * In order to enter this mode the benchmark should provide all the
 * mentioned parameters for kerberos authentication plus the
 * <tt>useToken</tt> argument option.
 * </ol>
 * Input arguments:
 * <ul>
 * <li>numIterations - number of connections to establish</li>
 * <li>keytabFile - keytab file for kerberos authentication</li>
 * <li>userName - principal name for kerberos authentication</li>
 * <li>useToken - should be specified for delegation token authentication</li>
 * <li>logLevel - logging level, see {@link Level}</li>
 * </ul>
 */
RpcServiceWrapper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/RPCCallBenchmark.java)/**
   * Simple interface that can be implemented either by the
   * protobuf or writable implementations.
   */
RPCCallBenchmark (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/RPCCallBenchmark.java)/**
 * Benchmark for protobuf RPC.
 * Run with --help option for usage.
 */
Putter (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestCallQueueManager.java)/**
   * Putter produces FakeCalls
   */
Taker (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestCallQueueManager.java)/**
   * Taker consumes FakeCalls
   */
Putter (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestFairCallQueue.java)/**
   * Putter produces FakeCalls
   */
Taker (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestFairCallQueue.java)/**
   * Taker consumes FakeCalls
   */
TestInvocationHandler (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java)/**
   * A RpcInvocationHandler instance for test. Its invoke function uses the same
   * {@link Client} instance, and will fail the first totalRetry times (by 
   * throwing an IOException).
   */
MockSocket (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java)/**
   * Mock socket class to help inject an exception for HADOOP-7428.
   */
DummyProtocol (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java)/** A dummy protocol */
NetworkTraces (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java)/**
   * Wireshark traces collected from various client versions. These enable
   * us to test that old versions of the IPC stack will receive the correct
   * responses so that they will throw a meaningful error message back
   * to the user.
   */
TestIPC (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPC.java)/** Unit tests for IPC. */
TestIPCServerResponder (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestIPCServerResponder.java)/**
 * This test provokes partial writes in the server, which is 
 * serving multiple clients.
 */
TestMiniRPCBenchmark (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestMiniRPCBenchmark.java)/**
 * Test {@link MiniRPCBenchmark}
 */
TestProcessingDetails (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestProcessingDetails.java)/**
 * Unit tests for ProcessingDetails time unit conversion and output.
 */
TestProtoBufRpc (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestProtoBufRpc.java)/**
 * Test for testing protocol buffer based RPC mechanism.
 * This test depends on test.proto definition of types in src/test/proto
 * and protobuf service definition from src/test/test_rpc_service.proto
 */
TestResponseBuffer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestResponseBuffer.java)/** Unit tests for ResponseBuffer. */
TestRetryCache (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestRetryCache.java)/**
 * Tests for {@link RetryCache}
 */
TestRetryCacheMetrics (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestRetryCacheMetrics.java)/**
 * Tests for {@link RetryCacheMetrics}
 */
TestReuseRpcConnections (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestReuseRpcConnections.java)/**
 * This class mainly tests behaviors of reusing RPC connections for various
 * retry policies.
 */
StoppedProtocol (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestRPC.java)/**
   * A basic interface for testing client-side RPC resource cleanup.
   */
StoppedRpcEngine (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestRPC.java)/**
   * A class used for testing cleanup of client side RPC resources.
   */
StoppedInvocationHandler (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestRPC.java)/**
   * An invocation handler which does nothing when invoking methods, and just
   * counts the number of times close() is called.
   */
TestRPC (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestRPC.java)/** Unit tests for RPC. */
TestRpcBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestRpcBase.java)/** Test facilities for unit tests for RPC. */
TestRPCCompatibility (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestRPCCompatibility.java)/** Unit test for supporting method-name based compatible RPCs. */
TestRPCServerShutdown (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestRPCServerShutdown.java)/** Split from TestRPC. */
TestRPCWaitForProxy (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestRPCWaitForProxy.java)/**
 * tests that the proxy can be interrupted
 */
TestSaslRPC (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestSaslRPC.java)/** Unit tests for using Sasl over RPC. */
TestServer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestServer.java)/**
 * This is intended to be a set of unit tests for the 
 * org.apache.hadoop.ipc.Server class.
 */
DummySocketFactory (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestSocketFactory.java)/**
   * A dummy socket factory class that extends the StandardSocketFactory.
   */
ServerRunnable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestSocketFactory.java)/**
   * Simple tcp server. Server gets a string, transforms it to upper case and returns it.
   */
TestSocketFactory (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestSocketFactory.java)/**
 * test StandardSocketFactory and SocksSocketFactory NetUtils
 *
 */
TestWeightedTimeCostProvider (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestWeightedTimeCostProvider.java)/** Tests for {@link WeightedTimeCostProvider}. */
TestLogger (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/log/TestLog4Json.java)/**
   * This test logger avoids integrating with the main runtimes Logger hierarchy
   * in ways the reader does not want to know.
   */
TestLogLevel (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/log/TestLogLevel.java)/**
 * Test LogLevel.
 */
TestLogThrottlingHelper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/log/TestLogThrottlingHelper.java)/**
 * Tests for {@link LogThrottlingHelper}.
 */
ConfigBuilder (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/ConfigBuilder.java)/**
 * Helper class for building configs, mostly used in tests
 */
ConfigUtil (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/ConfigUtil.java)/**
 * Helpers for config tests and debugging
 */
MetricsLists (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/MetricsLists.java)/**
 * Helper to create metrics list for testing
 */
MetricsRecords (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/MetricsRecords.java)/**
 * Utility class mainly for tests
 */
MockDatagramSocket (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestGangliaMetrics.java)/**
   * This class is used to capture data send to Ganglia servers.
   *
   * Initial attempt was to use mockito to mock and capture but
   * while testing figured out that mockito is keeping the reference
   * to the byte array and since the sink code reuses the byte array
   * hence all the captured byte arrays were pointing to one instance.
   */
TestMetricsConfig (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsConfig.java)/**
 * Test metrics configuration
 */
TestMetricsSource (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSourceAdapter.java)/**
   * Thread safe source: stores a key value pair. Allows thread safe key-value
   * pair reads/writes.
   */
SourceUpdater (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSourceAdapter.java)/**
   * An thread that updates the metrics source every 1 JMX cache TTL
   */
SourceReader (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSourceAdapter.java)/**
   * An thread that reads the metrics source every JMX cache TTL. After each
   * read it updates the metric source to report a new key. The next read must
   * be able to pick up this new key.
   */
TestClosableSink (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSystemImpl.java)/**
   * Class to verify HADOOP-11932. Instead of reading from HTTP, going in loop
   * until closed.
   */
TestMetricsSystemImpl (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSystemImpl.java)/**
 * Test the MetricsSystemImpl class
 */
TestMetricsVisitor (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsVisitor.java)/**
 * Test the metric visitor interface
 */
TestSinkQueue (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestSinkQueue.java)/**
 * Test the half-blocking metrics sink queue
 */
MetricsTestHelper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/lib/MetricsTestHelper.java)/**
 * A helper class that can provide test cases access to package-private
 * methods.
 */
TestMetricsRegistry (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/lib/TestMetricsRegistry.java)/**
 * Test the metric registry class
 */
TestMutableMetrics (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/lib/TestMutableMetrics.java)/**
 * Test metrics record builder interface and mutable metrics
 */
TestMutableRollingAverages (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/lib/TestMutableRollingAverages.java)/**
 * This class tests various cases of the algorithms implemented in
 * {@link MutableRollingAverages}.
 */
GangliaMetricsTestHelper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaMetricsTestHelper.java)/**
 * Helper class in the same package as ganglia sinks to be used by unit tests
 */
MyMetrics1 (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java)/**
   * A sample metric class
   */
MyMetrics2 (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java)/**
   * Another sample metrics class
   */
MockSink (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java)/**
   * This class is a {@link RollingFileSystemSink} wrapper that tracks whether
   * an exception has been thrown during operations.
   */
RollingFileSystemSinkTestBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java)/**
 * This class is a base class for testing the {@link RollingFileSystemSink}
 * class in various contexts. It provides the a number of useful utility
 * methods for classes that extend it.
 */
TestMetrics (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestPrometheusMetricsSink.java)/**
   * Example metric pojo.
   */
TestPrometheusMetricsSink (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestPrometheusMetricsSink.java)/**
 * Test prometheus Sink.
 */
TestRollingFileSystemSink (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSink.java)/**
 * Test that the init() method picks up all the configuration settings
 * correctly.
 */
TestRollingFileSystemSinkWithLocal (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithLocal.java)/**
 * Test the {@link RollingFileSystemSink} class in the context of the local file
 * system.
 */
DummyMXBean (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/util/DummyMXBean.java)/**
 * Sample JMX Bean interface to test JMX registration.
 */
TestMBeans (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/util/TestMBeans.java)/**
 * Test MXBean addition of key/value pairs to registered MBeans.
 */
TestSampleStat (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/util/TestSampleStat.java)/**
 * Test the running sample stat computation
 */
MockDomainNameResolver (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/MockDomainNameResolver.java)/**
 * This mock resolver class returns the predefined resolving/reverse lookup
 * results. By default it uses a default "test.foo.bar" domain with two
 * IP addresses.
 */
StaticMapping (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/StaticMapping.java)/**
 * Implements the {@link DNSToSwitchMapping} via static mappings. Used
 * in testcases that simulate racks, and in the
 * {@link org.apache.hadoop.hdfs.MiniDFSCluster}
 *
 * A shared, static mapping is used; to reset it call {@link #resetMap()}.
 *
 * When an instance of the class has its {@link #setConf(Configuration)}
 * method called, nodes listed in the configuration will be added to the map.
 * These do not get removed when the instance is garbage collected.
 *
 * The switch mapping policy of this class is the same as for the
 * {@link ScriptBasedMapping} -the presence of a non-empty topology script.
 * The script itself is not used.
 */
TestDNS (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestDNS.java)/**
 * Test host name and IP resolution and caching.
 */
TestMockDomainNameResolver (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestMockDomainNameResolver.java)/**
 * This class mainly test the MockDomainNameResolver comes working as expected.
 */
TestSocketIOWithTimeout (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestSocketIOWithTimeout.java)/**
 * This tests timout out from SocketInputStream and
 * SocketOutputStream using pipes.
 * 
 * Normal read and write using these streams are tested by pretty much
 * every DFS unit test.
 */
TestStaticMapping (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestStaticMapping.java)/**
 * Test the static mapping class.
 * Because the map is actually static, this map needs to be reset for every test
 */
TestSwitchMapping (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestSwitchMapping.java)/**
 * Test some other details of the switch mapping
 */
TemporarySocketDirectory (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/unix/TemporarySocketDirectory.java)/**
 * Create a temporary directory in which sockets can be created.
 * When creating a UNIX domain socket, the name
 * must be fairly short (around 110 bytes on most platforms).
 */
Success (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/unix/TestDomainSocket.java)/**
   * A Throwable representing success.
   *
   * We can't use null to represent this, because you cannot insert null into
   * ArrayBlockingQueue.
   */
TestProxyUserAuthenticationFilter (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authentication/server/TestProxyUserAuthenticationFilter.java)/**
 * Test ProxyUserAuthenticationFilter with doAs Request Parameter.
 */
TestDefaultImpersonationProvider (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestDefaultImpersonationProvider.java)/**
 * Test class for @DefaultImpersonationProvider
 */
TestRestCsrfPreventionFilter (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/http/TestRestCsrfPreventionFilter.java)/**
 * This class tests the behavior of the RestCsrfPreventionFilter.
 *
 */
TestXFrameOptionsFilter (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/http/TestXFrameOptionsFilter.java)/**
 * Test the default and customized behaviors of XFrameOptionsFilter.
 *
 */
ManualTestKeytabLogins (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/ManualTestKeytabLogins.java)/**
 * Regression test for HADOOP-6947 which can be run manually in
 * a kerberos environment.
 *
 * To run this test, set up two keytabs, each with a different principal.
 * Then run something like:
 *  <code>
 *  HADOOP_CLASSPATH=build/test/classes bin/hadoop \
 *     org.apache.hadoop.security.ManualTestKeytabLogins \
 *     usera/test@REALM  /path/to/usera-keytab \
 *     userb/test@REALM  /path/to/userb-keytab
 *  </code>
 */
NetUtilsTestResolver (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/NetUtilsTestResolver.java)/**
 * provides a dummy dns search resolver with a configurable search path
 * and host mapping
 */
SecurityUtilTestHelper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/SecurityUtilTestHelper.java)/** helper utils for tests */
TestDelegatingSSLSocketFactory (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/ssl/TestDelegatingSSLSocketFactory.java)/**
 * Tests for {@link DelegatingSSLSocketFactory}.
 */
TestDoAsEffectiveUser (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestDoAsEffectiveUser.java)/**
 * Test do as effective user.
 */
TestFixKerberosTicketOrder (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestFixKerberosTicketOrder.java)/**
 * Testcase for HADOOP-13433 that verifies the logic of fixKerberosTicketOrder.
 */
TestIngressPortBasedResolver (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestIngressPortBasedResolver.java)/**
 * Test class for IngressPortBasedResolver.
 */
DummyLdapCtxFactory (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestLdapGroupsMappingBase.java)/**
   * Ldap Context Factory implementation to be used for testing to check
   * contexts are requested for the expected LDAP server URLs etc.
   */
TestLdapGroupsMappingWithBindUserSwitch (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestLdapGroupsMappingWithBindUserSwitch.java)/**
 * Test functionality for switching bind user information if
 * AuthenticationExceptions are encountered.
 */
TestLdapGroupsMappingWithFailover (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestLdapGroupsMappingWithFailover.java)/**
 * Test failover functionality for switching to different
 * LDAP server URLs upon failures.
 */
TestLdapGroupsMappingWithOneQuery (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestLdapGroupsMappingWithOneQuery.java)/**
 * Test LdapGroupsMapping with one-query lookup enabled.
 * Mockito is used to simulate the LDAP server response.
 */
TestNullGroupsMapping (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestNullGroupsMapping.java)/**
 * Test that the {@link NullGroupsMapping} really does nothing.
 */
TestRaceWhenRelogin (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestRaceWhenRelogin.java)/**
 * Testcase for HADOOP-13433 that confirms that tgt will always be the first
 * ticket after relogin.
 */
TestRuleBasedLdapGroupsMapping (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestRuleBasedLdapGroupsMapping.java)/**
 * Test cases to verify the rules supported by RuleBasedLdapGroupsMapping.
 */
TestUGILoginFromKeytab (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestUGILoginFromKeytab.java)/**
 * Verify UGI login from keytab. Check that the UGI is
 * configured to use keytab to catch regressions like
 * HADOOP-10786.
 */
TestUGIWithExternalKdc (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestUGIWithExternalKdc.java)/**
 * Tests kerberos keytab login using a user-specified external KDC
 *
 * To run, users must specify the following system properties:
 *   externalKdc=true
 *   java.security.krb5.conf
 *   user.principal
 *   user.keytab
 */
TestUGIWithMiniKdc (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestUGIWithMiniKdc.java)/**
 * Test {@link UserGroupInformation} with a minikdc.
 */
DummyLoginConfiguration (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestUserGroupInformation.java)/**
   * UGI should not use the default security conf, else it will collide
   * with other classes that may change the default conf.  Using this dummy
   * class that simply throws an exception will ensure that the tests fail
   * if UGI uses the static default config instead of its own config
   */
TestToken (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/TestToken.java)/** Unit tests for Token */
BrokenLifecycleEvent (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/BreakableService.java)/**
   * The exception explicitly raised on a failure.
   */
BreakableStateChangeListener (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/BreakableStateChangeListener.java)/**
 * A state change listener that logs the number of state change events received,
 * and the last state invoked.
 *
 * It can be configured to fail during a state change event
 */
ExitTrackingServiceLauncher (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/ExitTrackingServiceLauncher.java)/**
 * Service launcher for testing: The exit operation has been overloaded to
 * record the exit exception.
 *
 * It relies on the test runner to have disabled exits in the
 * {@link ExitUtil} class.
 * @param <S> type of service to launch
 */
TestServiceConf (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/TestServiceConf.java)/**
 * Test how configuration files are loaded off the command line.
 */
TestServiceInterruptHandling (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/TestServiceInterruptHandling.java)/**
 * Test service launcher interrupt handling.
 */
TestServiceLauncherCreationFailures (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/TestServiceLauncherCreationFailures.java)/**
 * Explore the ways in which the launcher is expected to (safely) fail.
 */
TestServiceLauncherInnerMethods (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/TestServiceLauncherInnerMethods.java)/**
 * Test the inner launcher methods.
 */
ExceptionInExecuteLaunchableService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/testservices/ExceptionInExecuteLaunchableService.java)/**
 * Raise an exception in the execute() method; the exception type can
 * be configured from the CLI.
 */
FailInConstructorService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/testservices/FailInConstructorService.java)/**
 * Service which fails in its constructor.
 */
FailingStopInStartService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/testservices/FailingStopInStartService.java)/**
 * This service stops during its start operation.
 */
FailInInitService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/testservices/FailInInitService.java)/**
 * Service which fails in its init() operation.
 */
FailInStartService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/testservices/FailInStartService.java)/**
 * Service which fails in its start() operation.
 */
FailureTestService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/testservices/FailureTestService.java)/**
 * Launcher test service that does not take CLI arguments.
 */
InitInConstructorLaunchableService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/testservices/InitInConstructorLaunchableService.java)/**
 * Init in the constructor and make sure that it isn't inited again.
 */
LaunchableRunningService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/testservices/LaunchableRunningService.java)/**
 * A service which implements {@link LaunchableService}.
 * It
 * <ol>
 *   <li>does nothing in its {@link #serviceStart()}</li>
 *   <li>does its sleep+ maybe fail operation in its {@link #execute()}
 *   method</li>
 *   <li>gets the failing flag from the argument {@link #ARG_FAILING} first,
 *   the config file second.</li>
 *   <li>returns 0 for a successful execute</li>
 *   <li>returns a configurable exit code for a failing execute</li>
 *   <li>generates a new configuration in {@link #bindArgs(Configuration, List)}
 *   to verify that these propagate.</li>
 * </ol>
 */
NoArgsAllowedService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/testservices/NoArgsAllowedService.java)/**
 * service that does not allow any arguments.
 */
NullBindLaunchableService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/testservices/NullBindLaunchableService.java)/**
 * An extension of {@link LaunchableRunningService} which returns null from
 * the {@link #bindArgs(Configuration, List)} method.
 */
StoppingInStartLaunchableService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/testservices/StoppingInStartLaunchableService.java)/**
 * Try to stop() in service start; in execute() raise an exception.
 */
StringConstructorOnlyService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/launcher/testservices/StringConstructorOnlyService.java)/**
 * Service that only has one constructor that takes a string.
 * This is the standard base class of a YARN service, so handle it
 * in the launch
 */
ServiceAssert (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/ServiceAssert.java)/**
 * A set of assertions about the state of any service
 */
CompositeServiceImpl (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/TestCompositeService.java)/**
   * This is a composite service that keeps a count of the number of lifecycle
   * events called, and can be set to throw a {@link ServiceTestRuntimeException }
   * during service start or stop
   */
ServiceManager (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/TestCompositeService.java)/**
   * Composite service that makes the addService method public to all
   */
TestGlobalStateChangeListener (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/TestGlobalStateChangeListener.java)/**
 * Test global state changes. It is critical for all tests to clean up the
 * global listener afterwards to avoid interfering with follow-on tests.
 *
 * One listener, {@link #listener} is defined which is automatically
 * unregistered on cleanup. All other listeners must be unregistered in the
 * finally clauses of the tests.
 */
NotifyingListener (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/TestServiceLifecycle.java)/**
   * Listener that wakes up all threads waiting on it
   */
AsyncSelfTerminatingService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/TestServiceLifecycle.java)/**
   * Service that terminates itself after starting and sleeping for a while
   */
SelfTerminatingService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/TestServiceLifecycle.java)/**
   * Service that terminates itself in startup
   */
StartInInitService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/TestServiceLifecycle.java)/**
   * Service that starts itself in init
   */
StopInInitService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/TestServiceLifecycle.java)/**
   * Service that starts itself in init
   */
TestServiceOperations (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/service/TestServiceOperations.java)/**
 * Test miscellaneous service operations through mocked failures.
 */
CoreTestDriver (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/CoreTestDriver.java)/**
 * Driver for core tests.
 */
TeePrintStream (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/GenericTestUtils.java)/**
   * Prints output to one {@link PrintStream} while copying to the other.
   * <p>
   * Closing the main {@link PrintStream} will NOT close the other.
   */
SystemErrCapturer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/GenericTestUtils.java)/**
   * Capture output printed to {@link System#err}.
   * <p>
   * Usage:
   * <pre>
   *   try (SystemErrCapturer capture = new SystemErrCapturer()) {
   *     ...
   *     // Call capture.getOutput() to get the output string
   *   }
   * </pre>
   *
   * TODO: Add lambda support once Java 8 is common.
   * <pre>
   *   SystemErrCapturer.withCapture(capture -> {
   *     ...
   *   })
   * </pre>
   */
DelayAnswer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/GenericTestUtils.java)/**
   * Mockito answer helper that triggers one latch as soon as the
   * method is called, then waits on another before continuing.
   */
DelegateAnswer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/GenericTestUtils.java)/**
   * An Answer implementation that simply forwards all calls through
   * to a delegate.
   *
   * This is useful as the default Answer for a mock object, to create
   * something like a spy on an RPC proxy. For example:
   * <code>
   *    NamenodeProtocol origNNProxy = secondary.getNameNode();
   *    NamenodeProtocol spyNNProxy = Mockito.mock(NameNodeProtocol.class,
   *        new DelegateAnswer(origNNProxy);
   *    doThrow(...).when(spyNNProxy).getBlockLocations(...);
   *    ...
   * </code>
   */
SleepAnswer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/GenericTestUtils.java)/**
   * An Answer implementation which sleeps for a random number of milliseconds
   * between 0 and a configurable value before delegating to the real
   * implementation of the method. This can be useful for drawing out race
   * conditions.
   */
GenericTestUtils (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/GenericTestUtils.java)/**
 * Test provides some very generic helpers which might be used across the tests
 */
HadoopTestBase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/HadoopTestBase.java)/**
 * A base class for JUnit4 tests that sets a default timeout for all tests
 * that subclass this test.
 *
 * Threads are named to the method being executed, for ease of diagnostics
 * in logs and thread dumps.
 */
TimeoutHandler (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/LambdaTestUtils.java)/**
   * Interface to implement for converting a timeout into some form
   * of exception to raise.
   */
GenerateTimeout (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/LambdaTestUtils.java)/**
   * Returns {@code TimeoutException} on a timeout. If
   * there was a inner class passed in, includes it as the
   * inner failure.
   */
FixedRetryInterval (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/LambdaTestUtils.java)/**
   * Retry at a fixed time period between calls.
   */
ProportionalRetryInterval (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/LambdaTestUtils.java)/**
   * Gradually increase the sleep time by the initial interval, until
   * the limit set by {@code maxIntervalMillis} is reached.
   */
FailFastException (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/LambdaTestUtils.java)/**
   * An exception which triggers a fast exist from the
   * {@link #eventually(int, Callable, Callable)} and
   * {@link #await(int, Callable, Callable, TimeoutHandler)} loops.
   */
VoidCallable (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/LambdaTestUtils.java)/**
   * A simple interface for lambdas, which returns nothing; this exists
   * to simplify lambda tests on operations with no return value.
   */
VoidCaller (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/LambdaTestUtils.java)/**
   * Bridge class to make {@link VoidCallable} something to use in anything
   * which takes an {@link Callable}.
   */
PrivilegedOperation (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/LambdaTestUtils.java)/**
   * A lambda-invoker for doAs use; invokes the callable provided
   * in the constructor.
   * @param <T> return type.
   */
PrivilegedVoidOperation (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/LambdaTestUtils.java)/**
   * VoidCaller variant of {@link PrivilegedOperation}: converts
   * a void-returning closure to an action which {@code doAs} can call.
   */
LambdaTestUtils (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/LambdaTestUtils.java)/**
 * Class containing methods and associated classes to make the most of Lambda
 * expressions in Hadoop tests.
 *
 * The code has been designed from the outset to be Java-8 friendly, but
 * to still be usable in Java 7.
 *
 * The code is modelled on {@code GenericTestUtils#waitFor(Supplier, int, int)},
 * but also lifts concepts from Scalatest's {@code awaitResult} and
 * its notion of pluggable retry logic (simple, backoff, maybe even things
 * with jitter: test author gets to choose).
 * The {@link #intercept(Class, Callable)} method is also all credit due
 * Scalatest, though it's been extended to also support a string message
 * check; useful when checking the contents of the exception.
 */
MetricsAsserts (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/MetricsAsserts.java)/**
 * Helpers for metrics source tests
 */
MoreAsserts (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/MoreAsserts.java)/**
 * A few more asserts
 */
TestContext (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/MultithreadedTestUtil.java)/**
   * TestContext is used to setup the multithreaded test runner.
   * It lets you add threads, run them, wait upon or stop them.
   */
TestingThread (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/MultithreadedTestUtil.java)/**
   * A thread that can be added to a test context, and properly
   * passes exceptions through.
   */
RepeatingTestThread (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/MultithreadedTestUtil.java)/**
   * A test thread that performs a repeating operation.
   */
MultithreadedTestUtil (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/MultithreadedTestUtil.java)/**
 * A utility to easily test threaded/synchronized code.
 * Utility works by letting you add threads that do some work to a
 * test context object, and then lets you kick them all off to stress test
 * your parallel code.
 *
 * Also propagates thread exceptions back to the runner, to let you verify.
 *
 * An example:
 *
 * <code>
 *  final AtomicInteger threadsRun = new AtomicInteger();
 *
 *  TestContext ctx = new TestContext();
 *  // Add 3 threads to test.
 *  for (int i = 0; i < 3; i++) {
 *    ctx.addThread(new TestingThread(ctx) {
 *      @Override
 *      public void doWork() throws Exception {
 *        threadsRun.incrementAndGet();
 *      }
 *    });
 *  }
 *  ctx.startThreads();
 *  // Set a timeout period for threads to complete.
 *  ctx.waitFor(30000);
 *  assertEquals(3, threadsRun.get());
 * </code>
 *
 * For repetitive actions, use the {@link MultithreadedTestUtil.RepeatingThread}
 * instead.
 *
 * (More examples can be found in {@link TestMultithreadedTestUtil})
 */
PlatformAssumptions (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/PlatformAssumptions.java)/**
 * JUnit assumptions for the environment (OS).
 */
StatUtils (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/StatUtils.java)/**
 * Helper class for stat/permission utility methods. Forks processes to query
 * permission info.
 */
TestLambdaTestUtils (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/TestLambdaTestUtils.java)/**
 * Test the logic in {@link LambdaTestUtils}.
 * This test suite includes Java 8 and Java 7 code; the Java 8 code exists
 * to verify that the API is easily used with Lambda expressions.
 */
TimedOutTestsListener (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/TimedOutTestsListener.java)/**
 * JUnit run listener which prints full thread dump into System.err
 * in case a test is failed due to timeout.
 */
UnitTestcaseTimeLimit (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/UnitTestcaseTimeLimit.java)/**
 * Class for test units to extend in order that their individual tests will
 * be timed out and fail automatically should they run more than 10 seconds.
 * This provides an automatic regression check for tests that begin running
 * longer than expected.
 */
Whitebox (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/Whitebox.java)/**
 * This class was ported from org.mockito.internal.util.reflection.Whitebox
 * since the class was removed in Mockito 2.1. Using this class is a bad
 * practice. Consider refactoring instead of using this.
 */
SetSpanReceiver (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/tracing/SetSpanReceiver.java)/**
 * Span receiver that puts all spans into a single set.
 * This is useful for testing.
 * <p>
 * We're not using HTrace's POJOReceiver here so as that doesn't
 * push all the metrics to a static place, and would make testing
 * SpanReceiverHost harder.
 */
ClassLoaderCheckMain (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/ClassLoaderCheckMain.java)/**
 * Test class used by {@link TestRunJar} to verify that it is loaded by the
 * {@link ApplicationClassLoader}.
 */
ClassLoaderCheckSecond (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/ClassLoaderCheckSecond.java)/**
 * A class {@link ClassLoaderCheckMain} depends on that should be loaded by the
 * system classloader.
 */
ClassLoaderCheckThird (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/ClassLoaderCheckThird.java)/**
 * A class {@link ClassLoaderCheckMain} depends on that should be loaded by the
 * application classloader.
 */
Crc32PerformanceTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/Crc32PerformanceTest.java)/**
 * Performance tests to compare performance of Crc32|Crc32C implementations
 * This can be run from the command line with:
 *
 *   java -cp path/to/test/classes:path/to/common/classes \
 *      'org.apache.hadoop.util.Crc32PerformanceTest'
 *
 *      or
 *
 *  hadoop org.apache.hadoop.util.Crc32PerformanceTest
 *
 * If any argument is provided, this test will run with non-directly buffer.
 *
 * The output is in JIRA table format.
 */
TestChildReaper (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/curator/TestChildReaper.java)/**
 * This is a copy of Curator 2.7.1's TestChildReaper class, with minor
 * modifications to make it work with JUnit (some setup code taken from
 * Curator's BaseClassForTests).  This is to ensure that the ChildReaper
 * class we modified is still correct.
 */
TestZKCuratorManager (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/curator/TestZKCuratorManager.java)/**
 * Test the manager for ZooKeeper Curator.
 */
FakeTimer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/FakeTimer.java)/**
 * FakeTimer can be used for test purposes to control the return values
 * from {{@link Timer}}.
 */
JarFinder (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/JarFinder.java)/**
 * Finds the Jar for a class. If the class is in a directory in the
 * classpath, it creates a Jar on the fly with the contents of the directory
 * and returns the path to that Jar. If a Jar is created, it is created in
 * the system temporary directory.
 */
ExampleTask (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestAsyncDiskService.java)/** An example task for incrementing a counter.  
   */
TestAsyncDiskService (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestAsyncDiskService.java)/**
 * A test for AsyncDiskService.
 */
TestAutoCloseableLock (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestAutoCloseableLock.java)/**
 * A test class for AutoCloseableLock.
 */
TestBasicDiskValidator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestBasicDiskValidator.java)/**
 * The class to test BasicDiskValidator.
 */
TestClasspath (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestClasspath.java)/**
 * Tests covering the classpath command-line utility.
 */
TestCrcComposer (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestCrcComposer.java)/**
 * Unittests for CrcComposer.
 */
TestCrcUtil (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestCrcUtil.java)/**
 * Unittests for CrcUtil.
 */
TestFileIoProvider (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestDiskCheckerWithDiskIo.java)/**
   * A dummy {@link DiskChecker#FileIoProvider} that can throw a programmable
   * number of times.
   */
TestDiskCheckerWithDiskIo (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestDiskCheckerWithDiskIo.java)/**
 * Verify {@link DiskChecker} validation routines that perform
 * Disk IO.
 */
TestDiskValidatorFactory (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestDiskValidatorFactory.java)/**
 * The class to test DiskValidatorFactory.
 */
TestDurationInfo (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestDurationInfo.java)/**
 * The class to test DurationInfo.
 */
TestFastNumberFormat (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestFastNumberFormat.java)/**
 * Test for FastNumberFormat
 */
FailInStaticInit (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestFindClass.java)/**
   * trigger a divide by zero fault in the static init
   */
FailInConstructor (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestFindClass.java)/**
   * trigger a divide by zero fault in the constructor
   */
NoEmptyConstructor (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestFindClass.java)/**
   * A class with no parameterless constructor -expect creation to fail
   */
BadToStringClass (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestFindClass.java)/**
   * This has triggers an NPE in the toString() method; checks the logging
   * code handles this.
   */
PrivateClass (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestFindClass.java)/**
   * This has a private constructor
   * -creating it will trigger an IllegalAccessException
   */
PrivateConstructor (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestFindClass.java)/**
   * This has a private constructor
   * -creating it will trigger an IllegalAccessException
   */
TestFindClass (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestFindClass.java)/**
 * Test the find class logic
 */
GenericClass (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericsUtil.java)/** This class uses generics */
GSetTestCase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGSet.java)/** Test cases */
IntData (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGSet.java)/** Test data set */
IntElement (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGSet.java)/** Elements of {@link LightWeightGSet} in this test */
TestInstrumentedLock (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestInstrumentedLock.java)/**
 * A test class for InstrumentedLock.
 */
TestInstrumentedReadWriteLock (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestInstrumentedReadWriteLock.java)/**
 * A test class for InstrumentedReadLock and InstrumentedWriteLock.
 */
TestJsonSerialization (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestJsonSerialization.java)/**
 * Test the JSON serialization helper.
 */
LightWeightCacheTestCase (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLightWeightCache.java)/**
   * The test case contains two data structures, a cache and a hashMap.
   * The hashMap is used to verify the correctness of the cache.  Note that
   * no automatic eviction is performed in the hashMap.  Thus, we have
   * (1) If an entry exists in cache, it MUST exist in the hashMap.
   * (2) If an entry does not exist in the cache, it may or may not exist in the
   *     hashMap.  If it exists, it must be expired.
   */
IntEntry (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLightWeightCache.java)/** Entries of {@link LightWeightCache} in this test */
TestLightWeightCache (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLightWeightCache.java)/** Testing {@link LightWeightCache} */
TestLightWeightGSet (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLightWeightGSet.java)/** Testing {@link LightWeightGSet} */
TestLightWeightResizableGSet (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLightWeightResizableGSet.java)/** Testing {@link LightWeightResizableGSet} */
Table (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestPureJavaCrc32.java)/**
   * Generate a table to perform checksums based on the same CRC-32 polynomial
   * that java.util.zip.CRC32 uses.
   */
PerformanceTest (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestPureJavaCrc32.java)/**
   * Performance tests to compare performance of the Pure Java implementation
   * to the built-in java.util.zip implementation. This can be run from the
   * command line with:
   *
   *   java -cp path/to/test/classes:path/to/common/classes \
   *      'org.apache.hadoop.util.TestPureJavaCrc32$PerformanceTest'
   *
   * The output is in JIRA table format.
   */
TestPureJavaCrc32 (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestPureJavaCrc32.java)/**
 * Unit test to verify that the pure-Java CRC32 algorithm gives
 * the same results as the built-in implementation.
 */
TestReadWriteDiskValidator (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestReadWriteDiskValidator.java)/**
 * The class to test {@link ReadWriteDiskValidator} and
 * {@link ReadWriteDiskValidatorMetrics}.
 */
Hook (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestShutdownHookManager.java)/**
   * Hooks for testing; save state for ease of asserting on
   * invocation.
   */
TestStringInterner (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringInterner.java)/**
 * 
 * Tests string interning {@link StringInterner}
 */
FakeLinuxResourceCalculatorPlugin (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestSysInfoLinux.java)/**
   * LinuxResourceCalculatorPlugin with a fake timer
   */
TestSysInfoLinux (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestSysInfoLinux.java)/**
 * A JUnit test to test {@link SysInfoLinux}
 * Create the fake /proc/ information and verify the parsing and calculation
 */
TestTime (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestTime.java)/**
 * A JUnit test to test {@link Time}.
 */
TestWinUtils (/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestWinUtils.java)/**
 * Test cases for helper Windows winutils.exe utility.
 */
EagerKeyGeneratorKeyProviderCryptoExtension (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/EagerKeyGeneratorKeyProviderCryptoExtension.java)/**
 * A {@link KeyProviderCryptoExtension} that pre-generates and caches encrypted 
 * keys.
 */
KeyACLs (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KeyAuthorizationKeyProvider.java)/**
   * Interface that needs to be implemented by a client of the
   * <code>KeyAuthorizationKeyProvider</code>.
   */
KeyAuthorizationKeyProvider (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KeyAuthorizationKeyProvider.java)/**
 * A {@link KeyProvider} proxy that checks whether the current user derived via
 * {@link UserGroupInformation}, is authorized to perform the following
 * type of operations on a Key :
 * <ol>
 * <li>MANAGEMENT operations : createKey, rollNewVersion, deleteKey</li>
 * <li>GENERATE_EEK operations : generateEncryptedKey, warmUpEncryptedKeys</li>
 * <li>DECRYPT_EEK operation : decryptEncryptedKey</li>
 * <li>READ operations : getKeyVersion, getKeyVersions, getMetadata,
 * getKeysMetadata, getCurrentKey</li>
 * </ol>
 * The read operations (getCurrentKeyVersion / getMetadata) etc are not checked.
 */
KMS (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMS.java)/**
 * Class providing the REST bindings, via Jersey, for the KMS.
 */
KMSACLs (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSACLs.java)/**
 * Provides access to the <code>AccessControlList</code>s used by KMS,
 * hot-reloading them if the <code>kms-acls.xml</code> file where the ACLs
 * are defined has been updated.
 */
KMSAudit (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSAudit.java)/**
 * Provides convenience methods for audit logging consisting different
 * types of events.
 */
AuditEvent (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSAuditLogger.java)/**
   * Class defining an audit event.
   */
KMSAuditLogger (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSAuditLogger.java)/**
 * Interface defining a KMS audit logger.
 * <p>
 * IMPORTANT WARNING: Audit logs should be strictly backwards-compatible,
 * because there are usually parsing tools highly dependent on the audit log
 * formatting. Different tools have different ways of parsing the audit log, so
 * changing the audit log output in any way is considered incompatible,
 * and will haunt the consumer tools / developers. Don't do it.
 */
KMSAuthenticationFilter (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSAuthenticationFilter.java)/**
 * Authentication filter that takes the configuration from the KMS configuration
 * file.
 */
KMSConfiguration (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSConfiguration.java)/**
 * Utility class to load KMS configuration files.
 */
KMSExceptionsProvider (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSExceptionsProvider.java)/**
 * Jersey provider that converts KMS exceptions into detailed HTTP errors.
 */
KMSJSONWriter (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSJSONWriter.java)/**
 * Jersey provider that converts <code>Map</code>s and <code>List</code>s
 * to their JSON representation.
 */
KMSMDCFilter (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSMDCFilter.java)/**
 * Servlet filter that captures context of the HTTP request to be use in the
 * scope of KMS calls on the server side.
 */
KMSServerJSONUtils (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSServerJSONUtils.java)/**
 * JSON utility methods for the KMS.
 */
KMSWebServer (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSWebServer.java)/**
 * The KMS web server.
 */
SimpleKMSAuditLogger (/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/SimpleKMSAuditLogger.java)/**
 * A simple text format audit logger. This is the default.
 * <p>
 * IMPORTANT WARNING: Audit logs should be strictly backwards-compatible,
 * because there are usually parsing tools highly dependent on the audit log
 * formatting. Different tools have different ways of parsing the audit log, so
 * changing the audit log output in any way is considered incompatible,
 * and will haunt the consumer tools / developers. Don't do it.
 */
OperationStatsBase (/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/KMSBenchmark.java)/**
   * Base class for collecting operation statistics.
   *
   * Overload this class in order to run statistics for a
   * specific kms operation.
   */
StatsDaemon (/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/KMSBenchmark.java)/**
   * One of the threads that perform stats operations.
   */
EncryptKeyStats (/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/KMSBenchmark.java)/**
   * Encrypt key statistics.
   *
   * Each thread encrypts the key.
   */
DecryptKeyStats (/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/KMSBenchmark.java)/**
   * Decrypt key statistics.
   *
   * Each thread decrypts the key.
   */
KMSBenchmark (/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/KMSBenchmark.java)/**
 * Main class for a series of KMS benchmarks.
 *
 * Each benchmark measures throughput and average execution time
 * of a specific kms operation, e.g. encrypt or decrypt of
 * Data Encryption Keys.
 *
 * The benchmark does not involve any other hadoop components
 * except for kms operations. Each operation is executed
 * by calling directly the respective kms operation.
 *
 * For usage, please see <a href="http://hadoop.apache.org/docs/current/
 * hadoop-project-dist/hadoop-common/Benchmarking.html#KMSBenchmark">
 * the documentation</a>.
 * Meanwhile, if you change the usage of this program, please also update the
 * documentation accordingly.
 */
TestKMSAuthenticationFilter (/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAuthenticationFilter.java)/**
 * Test KMS Authentication Filter.
 */
TestKMSMDCFilter (/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSMDCFilter.java)/**
 * Test for {@link KMSMDCFilter}.
 *
 */
MiniKdc (/hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java)/**
 * Mini KDC based on Apache Directory Server that can be embedded in testcases
 * or used from command line as a standalone KDC.
 * <p>
 * <b>From within testcases:</b>
 * <p>
 * MiniKdc sets one System property when started and un-set when stopped:
 * <ul>
 *   <li>sun.security.krb5.debug: set to the debug value provided in the
 *   configuration</li>
 * </ul>
 * Because of this, multiple MiniKdc instances cannot be started in parallel.
 * For example, running testcases in parallel that start a KDC each. To
 * accomplish this a single MiniKdc should be used for all testcases running
 * in parallel.
 * <p>
 * MiniKdc default configuration values are:
 * <ul>
 *   <li>org.name=EXAMPLE (used to create the REALM)</li>
 *   <li>org.domain=COM (used to create the REALM)</li>
 *   <li>kdc.bind.address=localhost</li>
 *   <li>kdc.port=0 (ephemeral port)</li>
 *   <li>instance=DefaultKrbServer</li>
 *   <li>max.ticket.lifetime=86400000 (1 day)</li>
 *   <li>max.renewable.lifetime=604800000 (7 days)</li>
 *   <li>transport=TCP</li>
 *   <li>debug=false</li>
 * </ul>
 * The generated krb5.conf forces TCP connections.
 */
MountdBase (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/mount/MountdBase.java)/**
 * Main class for starting mountd daemon. This daemon implements the NFS
 * mount protocol. When receiving a MOUNT request from an NFS client, it checks
 * the request against the list of currently exported file systems. If the
 * client is permitted to mount the file system, rpc.mountd obtains a file
 * handle for requested directory and returns it to the client.
 */
MountEntry (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/mount/MountEntry.java)/**
 * Represents a mount entry.
 */
MountInterface (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/mount/MountInterface.java)/**
 * This is an interface that should be implemented for handle Mountd related
 * requests. See RFC 1094 for more details.
 */
MountResponse (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/mount/MountResponse.java)/**
 * Helper class for sending MountResponse
 */
FileHandle (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/FileHandle.java)/**
 * This is a file handle use by the NFS clients.
 * Server returns this handle to the client, which is used by the client
 * on subsequent operations to reference the file.
 */
Nfs3Base (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/Nfs3Base.java)/**
 * Nfs server. Supports NFS v3 using {@link RpcProgram}.
 * Only TCP server is supported and UDP is not supported.
 */
Nfs3Constant (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/Nfs3Constant.java)/**
 * Some constants for NFSv3
 */
Nfs3FileAttributes (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/Nfs3FileAttributes.java)/**
 * File attrbutes reported in NFS.
 */
Nfs3Interface (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/Nfs3Interface.java)/**
 * RPC procedures as defined in RFC 1813.
 */
Nfs3Status (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/Nfs3Status.java)/**
 * Success or error status is reported in NFS3 responses.
 */
ACCESS3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/ACCESS3Request.java)/**
 * ACCESS3 Request
 */
COMMIT3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/COMMIT3Request.java)/**
 * COMMIT3 Request
 */
CREATE3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/CREATE3Request.java)/**
 * CREATE3 Request
 */
FSINFO3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/FSINFO3Request.java)/**
 * FSINFO3 Request
 */
FSSTAT3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/FSSTAT3Request.java)/**
 * FSSTAT3 Request
 */
GETATTR3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/GETATTR3Request.java)/**
 * GETATTR3 Request
 */
LINK3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/LINK3Request.java)/**
 * LINK3 Request
 */
LOOKUP3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/LOOKUP3Request.java)/**
 * LOOKUP3 Request
 */
MKDIR3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/MKDIR3Request.java)/**
 * MKDIR3 Request
 */
MKNOD3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/MKNOD3Request.java)/**
 * MKNOD3 Request
 */
NFS3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/NFS3Request.java)/**
 * An NFS request that uses {@link FileHandle} to identify a file.
 */
PATHCONF3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/PATHCONF3Request.java)/**
 * PATHCONF3 Request
 */
READ3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/READ3Request.java)/**
 * READ3 Request
 */
READDIR3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/READDIR3Request.java)/**
 * READDIR3 Request
 */
READDIRPLUS3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/READDIRPLUS3Request.java)/**
 * READDIRPLUS3 Request
 */
READLINK3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/READLINK3Request.java)/**
 * READLINK3 Request
 */
REMOVE3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/REMOVE3Request.java)/**
 * REMOVE3 Request
 */
RENAME3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/RENAME3Request.java)/**
 * RENAME3 Request
 */
RequestWithHandle (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/RequestWithHandle.java)/**
 * An NFS request that uses {@link FileHandle} to identify a file.
 */
RMDIR3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/RMDIR3Request.java)/**
 * RMDIR3 Request
 */
SetAttr3 (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/SetAttr3.java)/**
 * SetAttr3 contains the file attributes that can be set from the client. The
 * fields are the same as the similarly named fields in the NFS3Attributes
 * structure.
 */
SETATTR3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/SETATTR3Request.java)/**
 * SETATTR3 Request
 */
SYMLINK3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/SYMLINK3Request.java)/**
 * SYMLINK3 Request
 */
WRITE3Request (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/request/WRITE3Request.java)/**
 * WRITE3 Request
 */
ACCESS3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/ACCESS3Response.java)/**
 * ACCESS3 Response 
 */
COMMIT3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/COMMIT3Response.java)/**
 * COMMIT3 Response
 */
CREATE3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/CREATE3Response.java)/**
 * CREATE3 Response
 */
FSINFO3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/FSINFO3Response.java)/**
 * FSINFO3 Response
 */
FSSTAT3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/FSSTAT3Response.java)/**
 * FSSTAT3 Response
 */
GETATTR3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/GETATTR3Response.java)/**
 * GETATTR3 Response
 */
LOOKUP3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/LOOKUP3Response.java)/**
 * LOOKUP3 Response
 */
MKDIR3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/MKDIR3Response.java)/**
 * MKDIR3 Response
 */
NFS3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/NFS3Response.java)/**
 * Base class for a NFSv3 response. This class and its subclasses contain
 * the response from NFSv3 handlers.
 */
PATHCONF3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/PATHCONF3Response.java)/**
 * PATHCONF3 Response
 */
READ3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/READ3Response.java)/**
 * READ3 Response
 */
READDIR3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/READDIR3Response.java)/**
 * READDIR3 Response
 */
READDIRPLUS3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/READDIRPLUS3Response.java)/**
 * READDIRPLUS3 Response
 */
READLINK3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/READLINK3Response.java)/**
 * READLINK3 Response
 */
REMOVE3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/REMOVE3Response.java)/**
 * REMOVE3 Response
 */
RENAME3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/RENAME3Response.java)/**
 * RENAME3 Response
 */
RMDIR3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/RMDIR3Response.java)/**
 * RMDIR3 Response
 */
SETATTR3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/SETATTR3Response.java)/**
 * SETATTR3 Response
 */
SYMLINK3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/SYMLINK3Response.java)/**
 * SYMLINK3 Response
 */
WccAttr (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/WccAttr.java)/**
 * WccAttr saves attributes used for weak cache consistency
 */
WccData (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/WccData.java)/**
 * WccData saved information used by client for weak cache consistency
 */
WRITE3Response (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/WRITE3Response.java)/**
 * WRITE3 Response
 */
AnonymousMatch (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/NfsExports.java)/**
   * Matcher covering all client hosts (specified by "*")
   */
CIDRMatch (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/NfsExports.java)/**
   * Matcher using CIDR for client host matching
   */
ExactMatch (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/NfsExports.java)/**
   * Matcher requiring exact string match for client host
   */
RegexMatch (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/NfsExports.java)/**
   * Matcher where client hosts are specified by regular expression
   */
NfsExports (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/NfsExports.java)/**
 * This class provides functionality for loading and checking the mapping 
 * between client hosts and their access privileges.
 */
NfsTime (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/NfsTime.java)/**
 * Class that encapsulates time.
 */
RegistrationClientHandler (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RegistrationClient.java)/**
   * Handler to handle response from the server.
   */
RegistrationClient (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RegistrationClient.java)/**
 * A simple client that registers an RPC program with portmap.
 */
RpcAcceptedReply (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcAcceptedReply.java)/** 
 * Represents RPC message MSG_ACCEPTED reply body. See RFC 1831 for details.
 * This response is sent to a request to indicate success of the request.
 */
RpcCall (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcCall.java)/**
 * Represents an RPC message of type RPC call as defined in RFC 1831
 */
ClientRequest (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcCallCache.java)/**
   * Call that is used to track a client in the {@link RpcCallCache}
   */
RpcCallCache (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcCallCache.java)/**
 * This class is used for handling the duplicate <em>non-idempotenty</em> Rpc
 * calls. A non-idempotent request is processed as follows:
 * <ul>
 * <li>If the request is being processed for the first time, its state is
 * in-progress in cache.</li>
 * <li>If the request is retransimitted and is in-progress state, it is ignored.
 * </li>
 * <li>If the request is retransimitted and is completed, the previous response
 * from the cache is sent back to the client.</li>
 * </ul>
 * <br>
 * A request is identified by the client ID (address of the client) and
 * transaction ID (xid) from the Rpc call.
 * 
 */
RpcDeniedReply (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcDeniedReply.java)/** 
 * Represents RPC message MSG_DENIED reply body. See RFC 1831 for details.
 * This response is sent to a request to indicate failure of the request.
 */
RpcInfo (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcInfo.java)/**
 * RpcInfo records all contextual information of an RPC message. It contains
 * the RPC header, the parameters, and the information of the remote peer.
 */
RpcMessage (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcMessage.java)/**
 * Represent an RPC message as defined in RFC 1831.
 */
RpcProgram (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcProgram.java)/**
 * Class for writing RPC server programs based on RFC 1050. Extend this class
 * and implement {@link #handleInternal} to handle the requests received.
 */
RpcReply (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcReply.java)/**
 * Represents an RPC message of type RPC reply as defined in RFC 1831
 */
RpcResponse (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcResponse.java)/**
 * RpcResponse encapsulates a response to a RPC request. It contains the data
 * that is going to cross the wire, as well as the information of the remote
 * peer.
 */
RpcFrameDecoder (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcUtil.java)/**
   * An RPC client can separate a RPC message into several frames (i.e.,
   * fragments) when transferring it across the wire. RpcFrameDecoder
   * reconstructs a full RPC message from these fragments.
   *
   * RpcFrameDecoder is a stateful pipeline stage. It has to be constructed for
   * each RPC client.
   */
RpcMessageParserStage (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcUtil.java)/**
   * RpcMessageParserStage parses the network bytes and encapsulates the RPC
   * request into a RpcInfo instance.
   */
RpcTcpResponseStage (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcUtil.java)/**
   * RpcTcpResponseStage sends an RpcResponse across the wire with the
   * appropriate fragment header.
   */
RpcUdpResponseStage (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/RpcUtil.java)/**
   * RpcUdpResponseStage sends an RpcResponse as a UDP packet, which does not
   * require a fragment header.
   */
Credentials (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/Credentials.java)/**
 * Base class for all credentials. Currently we only support 3 different types
 * of auth flavors: AUTH_NONE, AUTH_SYS, and RPCSEC_GSS.
 */
CredentialsGSS (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/CredentialsGSS.java)/** Credential used by RPCSEC_GSS */
CredentialsNone (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/CredentialsNone.java)/** Credential used by AUTH_NONE */
CredentialsSys (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/CredentialsSys.java)/** Credential used by AUTH_SYS */
RpcAuthInfo (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/RpcAuthInfo.java)/**
 *  Authentication Info. Base class of Verifier and Credential.
 */
Verifier (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/Verifier.java)/**
 * Base class for verifier. Currently our authentication only supports 3 types
 * of auth flavors: {@link RpcAuthInfo.AuthFlavor#AUTH_NONE}, {@link RpcAuthInfo.AuthFlavor#AUTH_SYS},
 * and {@link RpcAuthInfo.AuthFlavor#RPCSEC_GSS}. Thus for verifier we only need to handle
 * AUTH_NONE and RPCSEC_GSS
 */
VerifierGSS (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/VerifierGSS.java)/** Verifier mapped to RPCSEC_GSS. */
VerifierNone (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/security/VerifierNone.java)/** Verifier used by AUTH_NONE. */
SimpleTcpClient (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/SimpleTcpClient.java)/**
 * A simple TCP based RPC client which just sends a request to a server.
 */
SimpleTcpClientHandler (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/SimpleTcpClientHandler.java)/**
 * A simple TCP based RPC client handler used by {@link SimpleTcpServer}.
 */
SimpleTcpServer (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/SimpleTcpServer.java)/**
 * Simple UDP server implemented using netty.
 */
SimpleUdpClient (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/SimpleUdpClient.java)/**
 * A simple UDP based RPC client which just sends one request to a server.
 */
SimpleUdpServer (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/SimpleUdpServer.java)/**
 * Simple UDP server implemented based on netty.
 */
XDR (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/oncrpc/XDR.java)/**
 * Utility class for building XDR messages based on RFC 4506.
 *
 * Key points of the format:
 *
 * <ul>
 * <li>Primitives are stored in big-endian order (i.e., the default byte order
 * of ByteBuffer).</li>
 * <li>Booleans are stored as an integer.</li>
 * <li>Each field in the message is always aligned by 4.</li>
 * </ul>
 *
 */
Portmap (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/portmap/Portmap.java)/**
 * Portmap service for binding RPC protocols. See RFC 1833 for details.
 */
PortmapMapping (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/portmap/PortmapMapping.java)/**
 * Represents a mapping entry for in the Portmap service for binding RPC
 * protocols. See RFC 1833 for details.
 * 
 * This maps a program to a port number.
 */
PortmapRequest (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/portmap/PortmapRequest.java)/**
 * Helper utility for building portmap request
 */
PortmapResponse (/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/portmap/PortmapResponse.java)/**
 * Helper utility for sending portmap response.
 */
TestCredentialsSys (/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/oncrpc/security/TestCredentialsSys.java)/**
 * Test for {@link CredentialsSys}
 */
TestRpcAuthInfo (/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/oncrpc/security/TestRpcAuthInfo.java)/**
 * Tests for {@link RpcAuthInfo}
 */
TestRpcAcceptedReply (/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/oncrpc/TestRpcAcceptedReply.java)/**
 * Test for {@link RpcAcceptedReply}
 */
TestRpcCall (/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/oncrpc/TestRpcCall.java)/**
 * Tests for {@link RpcCall}
 */
TestRpcCallCache (/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/oncrpc/TestRpcCallCache.java)/**
 * Unit tests for {@link RpcCallCache}
 */
TestRpcDeniedReply (/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/oncrpc/TestRpcDeniedReply.java)/**
 * Test for {@link RpcDeniedReply}
 */
TestRpcMessage (/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/oncrpc/TestRpcMessage.java)/**
 * Test for {@link RpcMessage}
 */
TestRpcReply (/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/oncrpc/TestRpcReply.java)/**
 * Test for {@link RpcReply}
 */
RegistryCli (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/cli/RegistryCli.java)/**
 * Command line for registry operations.
 */
BindFlags (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/api/BindFlags.java)/**
 * Combinable Flags to use when creating a service entry.
 */
DNSOperations (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/api/DNSOperations.java)/**
 * DNS Operations.
 */
DNSOperationsFactory (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/api/DNSOperationsFactory.java)/**
 * A factory for DNS operation service instances.
 */
RegistryConstants (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/api/RegistryConstants.java)/**
 * Constants for the registry, including configuration keys and default
 * values.
 */
RegistryOperations (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/api/RegistryOperations.java)/**
 * Registry Operations
 */
RegistryOperationsFactory (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/api/RegistryOperationsFactory.java)/**
 * A factory for registry operation service instances.
 * <p>
 * <i>Each created instance will be returned initialized.</i>
 * <p>
 * That is, the service will have had <code>Service.init(conf)</code> applied
 * to it possibly after the configuration has been modified to
 * support the specific binding/security mechanism used
 */
JsonSerDeser (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/binding/JsonSerDeser.java)/**
 * Support for marshalling objects to and from JSON.
 *  <p>
 * This extends {@link JsonSerialization} with the notion
 * of a marker field in the JSON file, with
 * <ol>
 *   <li>a fail-fast check for it before even trying to parse.</li>
 *   <li>Specific IOException subclasses for a failure.</li>
 * </ol>
 * The rationale for this is not only to support different things in the,
 * registry, but the fact that all ZK nodes have a size &gt; 0 when examined.
 *
 * @param <T> Type to marshal.
 */
RegistryPathUtils (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/binding/RegistryPathUtils.java)/**
 * Basic operations on paths: manipulating them and creating and validating
 * path elements.
 */
RegistryTypeUtils (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/binding/RegistryTypeUtils.java)/**
 * Static methods to work with registry types primarily endpoints and the
 * list representation of addresses.
 */
ServiceRecordMarshal (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/binding/RegistryUtils.java)/**
   * Static instance of service record marshalling
   */
RegistryUtils (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/binding/RegistryUtils.java)/**
 * Utility methods for working with a registry.
 */
AuthenticationFailedException (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/exceptions/AuthenticationFailedException.java)/**
 * Exception raised when client access wasn't authenticated.
 * That is: the credentials provided were incomplete or invalid.
 */
InvalidPathnameException (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/exceptions/InvalidPathnameException.java)/**
 * A path name was invalid. This is raised when a path string has
 * characters in it that are not permitted.
 */
InvalidRecordException (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/exceptions/InvalidRecordException.java)/**
 * Raised if an attempt to parse a record failed.
 *
 */
NoChildrenForEphemeralsException (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/exceptions/NoChildrenForEphemeralsException.java)/**
 * This is a manifestation of the Zookeeper restrictions about
 * what nodes may act as parents.
 *
 * Children are not allowed under ephemeral nodes. This is an aspect
 * of ZK which isn't directly exposed to the registry API. It may
 * surface if the registry is manipulated outside of the registry API.
 */
NoPathPermissionsException (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/exceptions/NoPathPermissionsException.java)/**
 * Raised on path permission exceptions.
 * <p>
 * This is similar to PathIOException, except that exception doesn't let
 */
NoRecordException (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/exceptions/NoRecordException.java)/**
 * Raised if there is no {@link ServiceRecord} resolved at the end
 * of the specified path.
 * <p>
 * There may be valid data of some form at the end of the path, but it does
 * not appear to be a Service Record.
 */
RegistryIOException (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/exceptions/RegistryIOException.java)/**
 * Base exception for registry operations.
 * <p>
 * These exceptions include the path of the failing operation wherever possible;
 * this can be retrieved via {@link PathIOException#getPath()}.
 */
FSRegistryOperationsService (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/FSRegistryOperationsService.java)/**
 * Filesystem-based implementation of RegistryOperations. This class relies
 * entirely on the configured FS for security and does no extra checks.
 */
RegistryOperationsClient (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/RegistryOperationsClient.java)/**
 * This is the client service for applications to work with the registry.
 *
 * It does not set up the root paths for the registry, is bonded
 * to a user, and can be set to use SASL, anonymous or id:pass auth.
 *
 * For SASL, the client must be operating in the context of an authed user.
 *
 * For id:pass the client must have the relevant id and password, SASL is
 * not used even if the client has credentials.
 *
 * For anonymous, nothing is used.
 *
 * Any SASL-authed client also has the ability to add one or more authentication
 * id:pass pair on all future writes, and to reset them later.
 */
BindingInformation (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/BindingInformation.java)/**
 * Binding information provided by a {@link RegistryBindingSource}
 */
CuratorService (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/CuratorService.java)/**
 * This service binds to Zookeeper via Apache Curator. It is more
 * generic than just the YARN service registry; it does not implement
 * any of the Registry Operations API.
 */
ListenerHandle (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/ListenerHandle.java)/**
 *
 */
PathListener (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/PathListener.java)/**
 *
 */
RegistryBindingSource (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/RegistryBindingSource.java)/**
 * Interface which can be implemented by a registry binding source
 */
RegistryInternalConstants (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/RegistryInternalConstants.java)/**
 * Internal constants for the registry.
 *
 * These are the things which aren't visible to users.
 *
 */
RegistryOperationsService (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/RegistryOperationsService.java)/**
 * The Registry operations service.
 * <p>
 * This service implements the {@link RegistryOperations}
 * API by mapping the commands to zookeeper operations, and translating
 * results and exceptions back into those specified by the API.
 * <p>
 * Factory methods should hide the detail that this has been implemented via
 * the {@link CuratorService} by returning it cast to that
 * {@link RegistryOperations} interface, rather than this implementation class.
 */
JaasConfiguration (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/RegistrySecurity.java)/**
   * Creates a programmatic version of a jaas.conf file. This can be used
   * instead of writing a jaas.conf file and setting the system property,
   * "java.security.auth.login.config", to point to that file. It is meant to be
   * used for connecting to ZooKeeper.
   */
UgiInfo (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/RegistrySecurity.java)/**
   * On demand string-ifier for UGI with extra details
   */
AclListInfo (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/RegistrySecurity.java)/**
   * on-demand stringifier for a list of ACLs
   */
ZKPathDumper (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/ZKPathDumper.java)/**
 * This class dumps a registry tree to a string.
 * It does this in the <code>toString()</code> method, so it
 * can be used in a log statement -the operation
 * will only take place if the method is evaluated.
 *
 */
ZookeeperConfigOptions (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/impl/zk/ZookeeperConfigOptions.java)/**
 * Configuration options which are internal to Zookeeper,
 * as well as some other ZK constants
 * <p>
 * Zookeeper options are passed via system properties prior to the ZK
 * Methods/classes being invoked. This implies that:
 * <ol>
 *   <li>There can only be one instance of a ZK client or service class
 *   in a single JVM else their configuration options will conflict.</li>
 *   <li>It is safest to set these properties immediately before
 *   invoking ZK operations.</li>
 * </ol>
 *
 */
AddressTypes (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/types/AddressTypes.java)/**
 * Enum of address types -as integers.
 * Why integers and not enums? Cross platform serialization as JSON
 */
Marshal (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/types/Endpoint.java)/**
   * Static instance of service record marshalling
   */
Endpoint (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/types/Endpoint.java)/**
 * Description of a single service/component endpoint.
 * It is designed to be marshalled as JSON.
 * <p>
 * Every endpoint can have more than one address entry, such as
 * a list of URLs to a replicated service, or a (hostname, port)
 * pair. Each of these address entries is represented as a string list,
 * as that is the only reliably marshallable form of a tuple JSON can represent.
 *
 *
 */
ProtocolTypes (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/types/ProtocolTypes.java)/**
 * some common protocol types
 */
RegistryPathStatus (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/types/RegistryPathStatus.java)/**
 * Output of a <code>RegistryOperations.stat()</code> call
 */
ServiceRecord (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/types/ServiceRecord.java)/**
 * JSON-marshallable description of a single component.
 * It supports the deserialization of unknown attributes, but does
 * not support their creation.
 */
YarnRegistryAttributes (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/client/types/yarn/YarnRegistryAttributes.java)/**
 * YARN specific attributes in the registry.
 */
RegistryConfiguration (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/conf/RegistryConfiguration.java)/**
 * Intermediate configuration class to import the keys from YarnConfiguration
 * in yarn-default.xml and yarn-site.xml. Once hadoop-yarn-registry is totally
 * deprecated, this should be deprecated.
 */
TXTApplicationRecordDescriptor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java)/**
   * An application TXT record descriptor.
   */
SRVApplicationRecordDescriptor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java)/**
   * An application SRV record descriptor.
   */
CNAMEApplicationRecordDescriptor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java)/**
   * An application CNAME record descriptor.
   */
AApplicationRecordDescriptor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java)/**
   * An application A record descriptor.
   */
AAAAApplicationRecordDescriptor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java)/**
   * An application AAAA record descriptor.
   */
ApplicationServiceRecordProcessor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java)/**
 * A processor for generating application DNS records from registry service
 * records.
 */
RecordDescriptor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/BaseServiceRecordProcessor.java)/**
   * A descriptor container the information to be populated into a DNS record.
   *
   * @param <T> the DNS record type/class.
   */
ContainerRecordDescriptor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/BaseServiceRecordProcessor.java)/**
   * A container-based DNS record descriptor.
   *
   * @param <T> the DNS record type/class.
   */
ApplicationRecordDescriptor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/BaseServiceRecordProcessor.java)/**
   * An application-based DNS record descriptor.
   *
   * @param <T> the DNS record type/class.
   */
BaseServiceRecordProcessor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/BaseServiceRecordProcessor.java)/**
 * Provides common service record processing logic.
 */
TXTContainerRecordDescriptor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ContainerServiceRecordProcessor.java)/**
   * A container TXT record descriptor.
   */
PTRContainerRecordDescriptor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ContainerServiceRecordProcessor.java)/**
   * A container PTR record descriptor.
   */
AContainerRecordDescriptor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ContainerServiceRecordProcessor.java)/**
   * A container A record descriptor.
   */
AAAAContainerRecordDescriptor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ContainerServiceRecordProcessor.java)/**
   * A container AAAA record descriptor.
   */
ContainerServiceRecordProcessor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ContainerServiceRecordProcessor.java)/**
 * A processor for generating container DNS records from registry service
 * records.
 */
PrivilegedRegistryDNSStarter (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/PrivilegedRegistryDNSStarter.java)/**
 * This class is used to allow the RegistryDNSServer to run on a privileged
 * port (e.g. 53).
 */
RecordCreator (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/RecordCreatorFactory.java)/**
   * A DNS Record creator.
   *
   * @param <R> the record type
   * @param <T> the record's target type
   */
ARecordCreator (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/RecordCreatorFactory.java)/**
   * An A Record creator.
   */
AAAARecordCreator (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/RecordCreatorFactory.java)/**
   * An AAAA Record creator.
   */
TXTRecordCreator (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/RecordCreatorFactory.java)/**
   * A TXT Record creator.
   */
PTRRecordCreator (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/RecordCreatorFactory.java)/**
   * A PTR Record creator.
   */
SRVRecordCreator (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/RecordCreatorFactory.java)/**
   * A SRV Record creator.
   */
HostPortInfo (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/RecordCreatorFactory.java)/**
   * An object for storing the host and port info used to generate SRV records.
   */
RecordCreatorFactory (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/RecordCreatorFactory.java)/**
 * A factory for creating DNS records.
 */
RegistryCommand (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/RegistryDNS.java)/**
   * An interface representing a registry associated function/command (see
   * command pattern).
   */
CloseableLock (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/RegistryDNS.java)/**
   * An implementation allowing for obtaining and releasing a lock.
   */
RegistryDNS (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/RegistryDNS.java)/**
 * A DNS service reflecting the state of the YARN registry.  Records are created
 * based on service records available in the YARN ZK-based registry.
 */
ManagementCommand (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/RegistryDNSServer.java)/**
   * A registry management command interface.
   */
RegistryDNSServer (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/RegistryDNSServer.java)/**
 * A server/service that starts and manages the lifecycle of a DNS registry
 * instance.
 */
ReverseZoneUtils (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ReverseZoneUtils.java)/**
 * Utilities for configuring reverse zones.
 */
SecureableZone (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/SecureableZone.java)/**
 * A zone implementation geared to support some DNSSEC functionality.
 */
ServiceRecordProcessor (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ServiceRecordProcessor.java)/**
 *  Manage the processing of service records in order to create DNS records.
 */
ZoneSelector (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/dns/ZoneSelector.java)/**
 * A selector that returns the zone associated with a provided name.
 */
SelectByYarnPersistence (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/integration/SelectByYarnPersistence.java)/**
 * Select an entry by the YARN persistence policy
 */
AddingCompositeService (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/services/AddingCompositeService.java)/**
 * Composite service that exports the add/remove methods.
 * <p>
 * This allows external classes to add services to these methods, after which
 * they follow the same lifecyce.
 * <p>
 * It is essential that any service added is in a state where it can be moved
 * on with that of the parent services. Specifically, do not add an uninited
 * service to a parent that is already inited as the <code>start</code>
 * operation will then fail
 *
 */
DeleteCompletionCallback (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/services/DeleteCompletionCallback.java)/**
 * Curator callback for delete operations completing.
 * <p>
 * This callback logs at debug and increments the event counter.
 */
MicroZookeeperService (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/services/MicroZookeeperService.java)/**
 * This is a small, localhost Zookeeper service instance that is contained
 * in a YARN service...it's been derived from Apache Twill.
 * <p>
 * It implements {@link RegistryBindingSource} and provides binding information,
 * <i>once started</i>. Until {@link #start()} is called, the hostname and
 * port may be undefined. Accordingly, the service raises an exception in this
 * condition.
 * <p>
 * If you wish to chain together a registry service with this one under
 * the same {@code CompositeService}, this service must be added
 * as a child first.
 * <p>
 * It also sets the configuration parameter
 * {@link RegistryConstants#KEY_REGISTRY_ZK_QUORUM}
 * to its connection string. Any code with access to the service configuration
 * can view it.
 */
MicroZookeeperServiceKeys (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/services/MicroZookeeperServiceKeys.java)/**
 * Service keys for configuring the {@link MicroZookeeperService}.
 * These are not used in registry clients or the RM-side service,
 * so are kept separate.
 */
NodeSelector (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/services/RegistryAdminService.java)/**
   * Comparator used for purge logic
   */
AsyncPurge (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/services/RegistryAdminService.java)/**
   * An async registry purge action taking
   * a selector which decides what to delete
   */
RegistryAdminService (/hadoop-common-project/hadoop-registry/src/main/java/org/apache/hadoop/registry/server/services/RegistryAdminService.java)/**
 * Administrator service for the registry. This is the one with
 * permissions to create the base directories and those for users.
 *
 * It also includes support for asynchronous operations, so that
 * zookeeper connectivity problems do not hold up the server code
 * performing the actions.
 *
 * Any action queued via {@link #submit(Callable)} will be
 * run asynchronously. The {@link #createDirAsync(String, List, boolean)}
 * is an example of such an an action
 *
 * A key async action is the depth-first tree purge, which supports
 * pluggable policies for deleting entries. The method
 * {@link #purge(String, NodeSelector, PurgePolicy, BackgroundCallback)}
 * implements the recursive purge operation the class
 * {{AsyncPurge}} provides the asynchronous scheduling of this.
 */
TestMarshalling (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/client/binding/TestMarshalling.java)/**
 * Test record marshalling
 */
TestRegistryOperationUtils (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/client/binding/TestRegistryOperationUtils.java)/**
 * Tests for the {@link RegistryUtils} class
 */
CuratorEventCatcher (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/client/impl/CuratorEventCatcher.java)/**
 * This is a little event catcher for curator asynchronous
 * operations.
 */
TestCuratorService (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/client/impl/TestCuratorService.java)/**
 * Test the curator service
 */
TestFSRegistryOperationsService (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/client/impl/TestFSRegistryOperationsService.java)/**
 * FSRegistryOperationsService test, using the local filesystem.
 */
TestMicroZookeeperService (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/client/impl/TestMicroZookeeperService.java)/**
 * Simple tests to look at the micro ZK service itself
 */
RegistryTestHelper (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/RegistryTestHelper.java)/**
 * This is a set of static methods to aid testing the registry operations.
 * The methods can be imported statically or the class used as a base
 * class for tests.
 */
AbstractSecureRegistryTest (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/secure/AbstractSecureRegistryTest.java)/**
 * Add kerberos tests. This is based on the (JUnit3) KerberosSecurityTestcase
 * and its test case, <code>TestMiniKdc</code>
 */
TestRegistrySecurityHelper (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/secure/TestRegistrySecurityHelper.java)/**
 * Test for registry security operations
 */
TestSecureLogins (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/secure/TestSecureLogins.java)/**
 * Verify that logins work
 */
TestSecureRegistry (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/secure/TestSecureRegistry.java)/**
 * Verify that the Mini ZK service can be started up securely
 */
TestRegistryDNS (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/server/dns/TestRegistryDNS.java)/**
 *
 */
TestReverseZoneUtils (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/server/dns/TestReverseZoneUtils.java)/**
 * Tests for the reverse zone utilities.
 */
TestSecureRegistryDNS (/hadoop-common-project/hadoop-registry/src/test/java/org/apache/hadoop/registry/server/dns/TestSecureRegistryDNS.java)/**
 *
 */
ContainerOperationClient (/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/client/ContainerOperationClient.java)/**
 * This class provides the client-facing APIs of container operations.
 */
HddsClientUtils (/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/client/HddsClientUtils.java)/**
 * Utility methods for Ozone and Container Clients.
 *
 * The methods to retrieve SCM service endpoints assume there is a single
 * SCM service instance. This will change when we switch to replicated service
 * instances for redundancy.
 */
ClientCredentialInterceptor (/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/ClientCredentialInterceptor.java)/**
 * GRPC client interceptor for ozone block token.
 */
BlockInputStream (/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java)/**
 * An {@link InputStream} called from KeyInputStream to read a block from the
 * container.
 * This class encapsulates all state management for iterating
 * through the sequence of chunks through {@link ChunkInputStream}.
 */
BlockOutputStream (/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java)/**
 * An {@link OutputStream} used by the REST service in combination with the
 * SCMClient to write the value of a key to a sequence
 * of container chunks.  Writes are buffered locally and periodically written to
 * the container as a new chunk.  In order to preserve the semantics that
 * replacement of a pre-existing key is atomic, each instance of the stream has
 * an internal unique identifier.  This unique identifier and a monotonically
 * increasing chunk index form a composite key that is used as the chunk name.
 * After all data is written, a putKey call creates or updates the corresponding
 * container key, and this call includes the full list of chunks that make up
 * the key data.  The list of chunks is updated all at once.  Therefore, a
 * concurrent reader never can see an intermediate state in which different
 * chunks of data from different versions of the key data are interleaved.
 * This class encapsulates all state management for buffering and writing
 * through to the container.
 */
BufferPool (/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BufferPool.java)/**
 * This class creates and manages pool of n buffers.
 */
ChunkInputStream (/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java)/**
 * An {@link InputStream} called from BlockInputStream to read a chunk from the
 * container. Each chunk may contain multiple underlying {@link ByteBuffer}
 * instances.
 */
CommitWatcher (/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/CommitWatcher.java)/**
 * This class executes watchForCommit on ratis pipeline and releases
 * buffers once data successfully gets replicated.
 */
XceiverClientGrpc (/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java)/**
 * A Client for the storageContainer protocol for read object data.
 */
ScmClientConfig (/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java)/**
   * Configuration for HDDS client.
   */
XceiverClientManager (/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java)/**
 * XceiverClientManager is responsible for the lifecycle of XceiverClient
 * instances.  Callers use this class to acquire an XceiverClient instance
 * connected to the desired container pipeline.  When done, the caller also uses
 * this class to release the previously acquired XceiverClient instance.
 *
 *
 * This class caches connection to container for reuse purpose, such that
 * accessing same container frequently will be through the same connection
 * without reestablishing connection. But the connection will be closed if
 * not being used for a period of time.
 */
XceiverClientMetrics (/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientMetrics.java)/**
 * The client metrics for the Storage Container protocol.
 */
XceiverClientRatis (/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientRatis.java)/**
 * An abstract implementation of {@link XceiverClientSpi} using Ratis.
 * The underlying RPC mechanism can be chosen via the constructor.
 */
DummyBlockInputStream (/hadoop-hdds/client/src/test/java/org/apache/hadoop/hdds/scm/storage/TestBlockInputStream.java)/**
   * A dummy BlockInputStream to mock read block call to DN.
   */
TestBlockInputStream (/hadoop-hdds/client/src/test/java/org/apache/hadoop/hdds/scm/storage/TestBlockInputStream.java)/**
 * Tests for {@link BlockInputStream}'s functionality.
 */
DummyChunkInputStream (/hadoop-hdds/client/src/test/java/org/apache/hadoop/hdds/scm/storage/TestChunkInputStream.java)/**
   * A dummy ChunkInputStream to mock read chunk calls to DN.
   */
TestChunkInputStream (/hadoop-hdds/client/src/test/java/org/apache/hadoop/hdds/scm/storage/TestChunkInputStream.java)/**
 * Tests for {@link ChunkInputStream}'s functionality.
 */
GenericCli (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/cli/GenericCli.java)/**
 * This is a generic parent class for all the ozone related cli tools.
 */
GenericParentCommand (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/cli/GenericParentCommand.java)/**
 * Interface to access the higher level parameters.
 */
HddsVersionProvider (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/cli/HddsVersionProvider.java)/**
 * Version provider for the CLI interface.
 */
MissingSubcommandException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/cli/MissingSubcommandException.java)/**
 * Exception to throw if subcommand is not selected but required.
 */
ContainerBlockID (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/client/ContainerBlockID.java)/**
 * BlockID returned by SCM during allocation of block (containerID + localID).
 */
OzoneQuota (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/client/OzoneQuota.java)/**
 * represents an OzoneQuota Object that can be applied to
 * a storage volume.
 */
BadFormatException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/conf/HddsConfServlet.java)/**
   * Exception for signal bad content type.
   */
HddsConfServlet (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/conf/HddsConfServlet.java)/**
 * A servlet to print out the running configuration data.
 */
XMLConfiguration (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/conf/OzoneConfiguration.java)/**
   * Class to marshall/un-marshall configuration from xml files.
   */
Property (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/conf/OzoneConfiguration.java)/**
   * Class to marshall/un-marshall configuration properties from xml files.
   */
OzoneConfiguration (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/conf/OzoneConfiguration.java)/**
 * Configuration for ozone.
 */
FunctionWithServiceException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/function/FunctionWithServiceException.java)/**
 * Functional interface like java.util.function.Function but with
 * checked exception.
 */
HddsConfigKeys (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/HddsConfigKeys.java)/**
 * This class contains constants for configuration keys and default values
 * used in hdds.
 */
HddsIdFactory (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/HddsIdFactory.java)/**
 * HDDS Id generator.
 */
HddsUtils (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/HddsUtils.java)/**
 * HDDS specific stateless utility functions.
 */
Builder (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/protocol/DatanodeDetails.java)/**
   * Builder class for building DatanodeDetails.
   */
Port (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/protocol/DatanodeDetails.java)/**
   * Container to hold DataNode Port details.
   */
DatanodeDetails (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/protocol/DatanodeDetails.java)/**
 * DatanodeDetails class contains details about DataNode like:
 * - UUID of the DataNode.
 * - IP and Hostname details.
 * - Port details to which the DataNode will be listening.
 */
SCMSecurityProtocol (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/protocol/SCMSecurityProtocol.java)/**
 * The protocol used to perform security related operations with SCM.
 */
SCMSecurityProtocolClientSideTranslatorPB (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/protocolPB/SCMSecurityProtocolClientSideTranslatorPB.java)/**
 * This class is the client-side translator that forwards requests for
 * {@link SCMSecurityProtocol} to the {@link SCMSecurityProtocolPB} proxy.
 */
ContainerCommandRequestMessage (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/ratis/ContainerCommandRequestMessage.java)/**
 * Implementing the {@link Message} interface
 * for {@link ContainerCommandRequestProto}.
 */
RatisHelper (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/ratis/RatisHelper.java)/**
 * Ratis helper methods.
 */
ByteStringConversion (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/ByteStringConversion.java)/**
 * Helper class to create a conversion function from ByteBuffer to ByteString
 * based on the property
 * {@link OzoneConfigKeys#OZONE_UNSAFEBYTEOPERATIONS_ENABLED} in the
 * Ozone configuration.
 */
ScmClient (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/client/ScmClient.java)/**
 * The interface to call into underlying container layer.
 *
 * Written as interface to allow easy testing: implement a mock container layer
 * for standalone testing of CBlock API without actually calling into remote
 * containers. Actual container layer can simply re-implement this.
 *
 * NOTE this is temporarily needed class. When SCM containers are full-fledged,
 * this interface will likely be removed.
 */
Builder (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/common/helpers/AllocatedBlock.java)/**
   * Builder for AllocatedBlock.
   */
AllocatedBlock (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/common/helpers/AllocatedBlock.java)/**
 * Allocated block wraps the result returned from SCM#allocateBlock which
 * contains a Pipeline and the key.
 */
BlockNotCommittedException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/common/helpers/BlockNotCommittedException.java)/**
 * Exceptions thrown when a block is yet to be committed on the datanode.
 */
ContainerNotOpenException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/common/helpers/ContainerNotOpenException.java)/**
 * Exceptions thrown when a write/update opearation is done on non-open
 * container.
 */
ContainerWithPipeline (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/common/helpers/ContainerWithPipeline.java)/**
 * Class wraps ozone container info.
 */
DeleteBlockResult (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/common/helpers/DeleteBlockResult.java)/**
 * Class wraps storage container manager block deletion results.
 */
ExcludeList (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/common/helpers/ExcludeList.java)/**
 * This class contains set of dns and containers which ozone client provides
 * to be handed over to SCM when block allocation request comes.
 */
InvalidContainerStateException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/common/helpers/InvalidContainerStateException.java)/**
 * Exceptions thrown when a container is in invalid state while doing a I/O.
 */
StorageContainerException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/common/helpers/StorageContainerException.java)/**
 * Exceptions thrown from the Storage Container.
 */
ContainerException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/ContainerException.java)/**
 * Signals that ContainerException of some sort has occurred. This is parent
 * of all the exceptions thrown by ContainerManager.
 */
ContainerID (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/ContainerID.java)/**
 * Container ID is an integer that is a value between 1..MAX_CONTAINER ID.
 * <p>
 * We are creating a specific type for this to avoid mixing this with
 * normal integers in code.
 */
Builder (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/ContainerInfo.java)/**
   * Builder class for ContainerInfo.
   */
ContainerInfo (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/ContainerInfo.java)/**
 * Class wraps ozone container info.
 */
ContainerNotFoundException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/ContainerNotFoundException.java)/**
 * Signals that a container is missing from ContainerManager.
 */
ContainerReplicaNotFoundException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/ContainerReplicaNotFoundException.java)/**
 * Signals that a ContainerReplica is missing from the Container in
 * ContainerManager.
 */
ContainerPlacementPolicy (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/ContainerPlacementPolicy.java)/**
 * A ContainerPlacementPolicy support choosing datanodes to build replication
 * pipeline with specified constraints.
 */
SCMException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/exceptions/SCMException.java)/**
 * Exception thrown by SCM.
 */
Factory (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/InnerNode.java)/** A factory interface to get new InnerNode instance. */
InnerNode (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/InnerNode.java)/**
 * The interface defines an inner node in a network topology.
 * An inner node represents network topology entities, such as data center,
 * rack, switch or logical group.
 */
InnerNodeImpl (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/InnerNodeImpl.java)/**
 * A thread safe class that implements InnerNode interface.
 */
NetConstants (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/NetConstants.java)/**
 * Class to hold network topology related constants and configurations.
 */
NetUtils (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/NetUtils.java)/**
 * Utility class to facilitate network topology functions.
 */
InvalidTopologyException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/NetworkTopology.java)/** Exception for invalid network topology detection. */
NetworkTopology (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/NetworkTopology.java)/**
 * The interface defines a network topology.
 */
NetworkTopologyImpl (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/NetworkTopologyImpl.java)/**
 * The class represents a cluster of computers with a tree hierarchical
 * network topology. In the network topology, leaves represent data nodes
 * (computers) and inner nodes represent datacenter/core-switches/routers that
 * manages traffic in/out of data centers or racks.
 */
Node (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/Node.java)/**
 * The interface defines a node in a network topology.
 * A node may be a leave representing a data node or an inner
 * node representing a data center or rack.
 * Each node has a name and its location in the network is
 * decided by a string with syntax similar to a file name.
 * For example, a data node's name is hostname:port# and if it's located at
 * rack "orange" in data center "dog", the string representation of its
 * network location will be /dog/orange.
 */
NodeImpl (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/NodeImpl.java)/**
 * A thread safe class that implements interface Node.
 */
Builder (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/NodeSchema.java)/**
   * Builder for NodeSchema.
   */
NodeSchema (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/NodeSchema.java)/**
 * Network topology schema to housekeeper relevant information.
 */
NodeSchemaLoadResult (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/NodeSchemaLoader.java)/**
   * Class to house keep the result of parsing a network topology schema file.
   */
NodeSchemaLoader (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/net/NodeSchemaLoader.java)/**
 * A Network topology layer schema loading tool that loads user defined network
 * layer schema data from a XML configuration file.
 */
Builder (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/pipeline/Pipeline.java)/**
   * Builder class for Pipeline.
   */
Pipeline (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/pipeline/Pipeline.java)/**
 * Represents a group of datanodes which store a container.
 */
PipelineID (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineID.java)/**
 * ID for the pipeline, the ID is based on UUID.
 */
PipelineNotFoundException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineNotFoundException.java)/**
 * Signals that a pipeline is missing from PipelineManager.
 */
UnknownPipelineStateException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/pipeline/UnknownPipelineStateException.java)/**
 * Signals that a pipeline state is not recognized.
 */
LocatedContainer (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/protocol/LocatedContainer.java)/**
 * Holds the nodes that currently host the container for an object key hash.
 */
ScmBlockLocationProtocol (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/protocol/ScmBlockLocationProtocol.java)/**
 * ScmBlockLocationProtocol is used by an HDFS node to find the set of nodes
 * to read/write a block.
 */
ScmLocatedBlock (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/protocol/ScmLocatedBlock.java)/**
 * Holds the nodes that currently host the block for a block key.
 */
StorageContainerLocationProtocol (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/protocol/StorageContainerLocationProtocol.java)/**
 * ContainerLocationProtocol is used by an HDFS node to find the set of nodes
 * that currently host a container.
 */
ScmBlockLocationProtocolClientSideTranslatorPB (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/protocolPB/ScmBlockLocationProtocolClientSideTranslatorPB.java)/**
 * This class is the client-side translator to translate the requests made on
 * the {@link ScmBlockLocationProtocol} interface to the RPC server
 * implementing {@link ScmBlockLocationProtocolPB}.
 */
ScmBlockLocationProtocolPB (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/protocolPB/ScmBlockLocationProtocolPB.java)/**
 * Protocol used from an HDFS node to StorageContainerManager.  This extends the
 * Protocol Buffers service interface to add Hadoop-specific annotations.
 */
StorageContainerLocationProtocolClientSideTranslatorPB (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/protocolPB/StorageContainerLocationProtocolClientSideTranslatorPB.java)/**
 * This class is the client-side translator to translate the requests made on
 * the {@link StorageContainerLocationProtocol} interface to the RPC server
 * implementing {@link StorageContainerLocationProtocolPB}.
 */
StorageContainerLocationProtocolPB (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/protocolPB/StorageContainerLocationProtocolPB.java)/**
 * Protocol used from an HDFS node to StorageContainerManager.  This extends the
 * Protocol Buffers service interface to add Hadoop-specific annotations.
 */
ScmConfigKeys (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/ScmConfigKeys.java)/**
 * This class contains constants for configuration keys used in SCM.
 */
Builder (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/ScmInfo.java)/**
   * Builder for ScmInfo.
   */
ScmInfo (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/ScmInfo.java)/**
 * ScmInfo wraps the result returned from SCM#getScmInfo which
 * contains clusterId and the SCM Id.
 */
CheckedBiFunction (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/storage/CheckedBiFunction.java)/**
 * Defines a functional interface having two inputs which throws IOException.
 */
ContainerProtocolCalls (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/storage/ContainerProtocolCalls.java)/**
 * Implementation of all container protocol calls performed by Container
 * clients.
 */
XceiverClientReply (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientReply.java)/**
 * This class represents the reply from XceiverClient.
 */
XceiverClientSpi (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientSpi.java)/**
 * A Client for the storageContainer protocol.
 */
SCMSecurityException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/exception/SCMSecurityException.java)/**
 * Root Security Exception call for all Certificate related Execptions.
 */
BlockTokenException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/token/BlockTokenException.java)/**
 * Block Token Exceptions from the SCM Security layer.
 */
BlockTokenVerifier (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/token/BlockTokenVerifier.java)/**
 * Verify token and return a UGI with token if authenticated.
 */
Renewer (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/token/OzoneBlockTokenIdentifier.java)/**
   * Default TrivialRenewer.
   */
OzoneBlockTokenIdentifier (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/token/OzoneBlockTokenIdentifier.java)/**
 * Block token identifier for Ozone/HDDS. Ozone block access token is similar
 * to HDFS block access token, which is meant to be lightweight and
 * short-lived. No need to renew or revoke a block access token. when a
 * cached block access token expires, the client simply get a new one.
 * Block access token should be cached only in memory and never write to disk.
 */
OzoneBlockTokenSelector (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/token/OzoneBlockTokenSelector.java)/**
 * A block token selector for Ozone.
 */
TokenVerifier (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/token/TokenVerifier.java)/**
 * Ozone GRPC token header verifier.
 */
BaseApprover (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/authority/BaseApprover.java)/**
 * A base approver class for certificate approvals.
 */
CertificateApprover (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/authority/CertificateApprover.java)/**
 * Certificate Approver interface is used to inspectCSR a certificate.
 */
CertificateServer (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/authority/CertificateServer.java)/**
 * Interface for Certificate Authority. This can be extended to talk to
 * external CAs later or HSMs later.
 */
CertificateStore (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/authority/CertificateStore.java)/**
 * This interface allows the DefaultCA to be portable and use different DB
 * interfaces later. It also allows us define this interface in the SCM layer
 * by which we don't have to take a circular dependency between hdds-common
 * and the SCM.
 *
 * With this interface, DefaultCA server read and write DB or persistence
 * layer and we can write to SCM's Metadata DB.
 */
DefaultApprover (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/authority/DefaultApprover.java)/**
 * Default Approver used the by the DefaultCA.
 */
DefaultCAServer (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/authority/DefaultCAServer.java)/**
 * The default CertificateServer used by SCM. This has no dependencies on any
 * external system, this allows us to bootstrap a CertificateServer from
 * Scratch.
 * <p>
 * Details =======
 * <p>
 * The Default CA server is one of the many possible implementations of an SCM
 * Certificate Authority.
 * <p>
 * A certificate authority needs the Root Certificates and its private key to
 * operate.  The init function of the DefaultCA Server detects four possible
 * states the System can be in.
 * <p>
 * 1.  Success - This means that the expected Certificates and Keys are in
 * place, and the CA was able to read those files into memory.
 * <p>
 * 2. Missing Keys - This means that private keys are missing. This is an error
 * state which SCM CA cannot recover from. The cluster might have been
 * initialized earlier and for some reason, we are not able to find the private
 * keys for the CA. Eventually we will have 2 ways to recover from this state,
 * first one is to copy the SCM CA private keys from a backup. Second one is to
 * rekey the whole cluster. Both of these are improvements we will support in
 * future.
 * <p>
 * 3. Missing Certificate - Similar to Missing Keys, but the root certificates
 * are missing.
 * <p>
 * 4. Initialize - We don't have keys or certificates. DefaultCA assumes that
 * this is a system bootup and will generate the keys and certificates
 * automatically.
 * <p>
 * The init() follows the following logic,
 * <p>
 * 1. Compute the Verification Status -- Success, Missing Keys, Missing Certs or
 * Initialize.
 * <p>
 * 2. ProcessVerificationStatus - Returns a Lambda, based on the Verification
 * Status.
 * <p>
 * 3. Invoke the Lambda function.
 * <p>
 * At the end of the init function, we have functional CA. This function can be
 * invoked as many times since we will regenerate the keys and certs only if
 * both of them are missing.
 */
DefaultCAProfile (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/authority/PKIProfiles/DefaultCAProfile.java)/**
 * CA Profile, this is needed when SCM does HA.
 * A place holder class indicating what we need to do when we support issuing
 * CA certificates to other SCMs in HA mode.
 */
DefaultProfile (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/authority/PKIProfiles/DefaultProfile.java)/**
 * Ozone PKI profile.
 * <p>
 * This PKI profile is invoked by SCM CA to make sure that certificates issued
 * by SCM CA are constrained
 */
PKIProfile (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/authority/PKIProfiles/PKIProfile.java)/**
 * Base class for profile rules. Generally profiles are documents that define
 * the PKI policy. In HDDS/Ozone world, we have chosen to make PKIs
 * executable code. So if an end-user wants to use a custom profile or one of
 * the existing profile like the list below, they are free to implement a
 * custom profile.
 *
 *     PKIX - Internet PKI profile.
 *     FPKI - (US) Federal PKI profile.
 *     MISSI - US DoD profile.
 *     ISO 15782 - Banking - Certificate Management Part 1: Public Key
 *         Certificates.
 *     TeleTrust/MailTrusT - German MailTrusT profile for TeleTrusT (it
 *     really is
 *         capitalised that way).
 *     German SigG Profile - Profile to implement the German digital
 *     signature law
 *     ISIS Profile - Another German profile.
 *     Australian Profile - Profile for the Australian PKAF
 *     SS 61 43 31 Electronic ID Certificate - Swedish profile.
 *     FINEID S3 - Finnish profile.
 *     ANX Profile - Automotive Network Exchange profile.
 *     Microsoft Profile - This isn't a real profile, but windows uses this.
 */
CertificateClient (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/client/CertificateClient.java)/**
 * Certificate client provides and interface to certificate operations that
 * needs to be performed by all clients in the Ozone eco-system.
 */
DefaultCertificateClient (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/client/DefaultCertificateClient.java)/**
 * Default Certificate client implementation. It provides certificate
 * operations that needs to be performed by certificate clients in the Ozone
 * eco-system.
 */
DNCertificateClient (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/client/DNCertificateClient.java)/**
 * Certificate client for DataNodes.
 */
OMCertificateClient (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/client/OMCertificateClient.java)/**
 * Certificate client for OzoneManager.
 */
CertificateCodec (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificate/utils/CertificateCodec.java)/**
 * A class used to read and write X.509 certificates  PEM encoded Streams.
 */
Builder (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificates/utils/CertificateSignRequest.java)/**
   * Builder class for Certificate Sign Request.
   */
CertificateSignRequest (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificates/utils/CertificateSignRequest.java)/**
 * A certificate sign request object that wraps operations to build a
 * PKCS10CertificationRequest to CertificateServer.
 */
Builder (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificates/utils/SelfSignedCertificate.java)/**
   * Builder class for Root Certificates.
   */
SelfSignedCertificate (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/certificates/utils/SelfSignedCertificate.java)/**
 * A Self Signed Certificate with CertificateServer basic constraint can be used
 * to bootstrap a certificate infrastructure, if no external certificate is
 * provided.
 */
CertificateException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/exceptions/CertificateException.java)/**
 * Certificate Exceptions from the SCM Security layer.
 */
HDDSKeyGenerator (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/keys/HDDSKeyGenerator.java)/**
 * A class to generate Key Pair for use with Certificates.
 */
KeyCodec (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/keys/KeyCodec.java)/**
 * We store all Key material in good old PEM files. This helps in avoiding
 * dealing will persistent Java KeyStore issues. Also when debugging, general
 * tools like OpenSSL can be used to read and decode these files.
 */
SecurityUtil (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/keys/SecurityUtil.java)/**
 * Utility functions for Security modules for Ozone.
 */
SecurityConfig (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/security/x509/SecurityConfig.java)/**
 * A class that deals with all Security related configs in HDDS.
 * <p>
 * This class allows security configs to be read and used consistently across
 * all of security related code base.
 */
GrpcClientInterceptor (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/tracing/GrpcClientInterceptor.java)/**
 * Interceptor to add the tracing id to the outgoing call header.
 */
GrpcServerInterceptor (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/tracing/GrpcServerInterceptor.java)/**
 * Interceptor to add the tracing id to the outgoing call header.
 */
StringFormat (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/tracing/StringCodec.java)/**
   * The format to save the context as text.
   * <p>
   * Using the mutable StringBuilder instead of plain String.
   */
StringCodec (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/tracing/StringCodec.java)/**
 * A jaeger codec to save the current tracing context as a string.
 */
TraceAllMethod (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/tracing/TraceAllMethod.java)/**
 * A Java proxy invocation handler to trace all the methods of the delegate
 * class.
 *
 * @param <T>
 */
TracingUtil (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/tracing/TracingUtil.java)/**
 * Utility class to collect all the tracing helper methods.
 */
PeriodicalTask (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/BackgroundService.java)/**
   * Run one or more background tasks concurrently.
   * Wait until all tasks to return the result.
   */
BackgroundService (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/BackgroundService.java)/**
 * An abstract class for a background service in ozone.
 * A background service schedules multiple child tasks in parallel
 * in a certain period. In each interval, it waits until all the tasks
 * finish execution and then schedule next interval.
 */
BackgroundTask (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/BackgroundTask.java)/**
 * A task thread to run by {@link BackgroundService}.
 */
BackgroundTaskQueue (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/BackgroundTaskQueue.java)/**
 * A priority queue that stores a number of {@link BackgroundTask}.
 */
EmptyTaskResult (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/BackgroundTaskResult.java)/**
   * An empty task result implementation.
   */
BackgroundTaskResult (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/BackgroundTaskResult.java)/**
 * Result of a {@link BackgroundTask}.
 */
SingleOperation (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/BatchOperation.java)/**
   * A SingleOperation represents a PUT or DELETE operation
   * and the data the operation needs to manipulates.
   */
BatchOperation (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/BatchOperation.java)/**
 * An utility class to store a batch of DB write operations.
 */
BatchOperation (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/BatchOperation.java)/**
 * Class represents a batch operation, collects multiple db operation.
 */
ByteArrayKeyValue (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/ByteArrayKeyValue.java)/**
 * Key value for raw Table implementations.
 */
CacheKey (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/cache/CacheKey.java)/**
 * CacheKey for the RocksDB table.
 * @param <KEY>
 */
CacheResult (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/cache/CacheResult.java)/**
 * CacheResult which is returned as response for Key exist in cache or not.
 * @param <CACHEVALUE>
 */
CacheValue (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/cache/CacheValue.java)/**
 * CacheValue for the RocksDB Table.
 * @param <VALUE>
 */
EpochEntry (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/cache/EpochEntry.java)/**
 * Class used which describes epoch entry. This will be used during deletion
 * entries from cache for partial table cache.
 * @param <CACHEKEY>
 */
TableCacheImpl (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/cache/TableCacheImpl.java)/**
 * Cache implementation for the table. Depending on the cache clean up policy
 * this cache will be full cache or partial cache.
 *
 * If cache cleanup policy is set as {@link CacheCleanupPolicy#MANUAL},
 * this will be a partial cache.
 *
 * If cache cleanup policy is set as {@link CacheCleanupPolicy#NEVER},
 * this will be a full cache.
 */
Codec (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/Codec.java)/**
 * Codec interface to marshall/unmarshall data to/from a byte[] based
 * key/value store.
 *
 * @param <T> Unserialized type
 */
CodecRegistry (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/CodecRegistry.java)/**
 * Collection of available codecs.
 */
DBCheckpoint (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/DBCheckpoint.java)/**
 * Generic DB Checkpoint interface.
 */
DBConfigFromFile (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/DBConfigFromFile.java)/**
 * A Class that controls the standard config options of RocksDB.
 * <p>
 * Important : Some of the functions in this file are magic functions designed
 * for the use of OZONE developers only. Due to that this information is
 * documented in this files only and is *not* intended for end user consumption.
 * Please do not use this information to tune your production environments.
 * Please remember the SpiderMan principal; with great power comes great
 * responsibility.
 */
DBStore (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/DBStore.java)/**
 * The DBStore interface provides the ability to create Tables, which store
 * a specific type of Key-Value pair. Some DB interfaces like LevelDB will not
 * be able to do this. In those case a Table creation will map to a default
 * store.
 *
 */
DBStoreBuilder (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/DBStoreBuilder.java)/**
 * DBStore Builder.
 */
DBUpdatesWrapper (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/DBUpdatesWrapper.java)/**
 * Wrapper class to hold DB data read from the RocksDB log file.
 */
IntegerCodec (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/IntegerCodec.java)/**
 * Codec to convert Integer to/from byte array.
 */
LongCodec (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/LongCodec.java)/**
 * Codec to convert Long to/from byte array.
 */
RDBBatchOperation (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/RDBBatchOperation.java)/**
 * Batch operation implementation for rocks db.
 */
RDBCheckpointManager (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/RDBCheckpointManager.java)/**
 * RocksDB Checkpoint Manager, used to create and cleanup checkpoints.
 */
RDBStore (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStore.java)/**
 * RocksDB Store that supports creating Tables in DB.
 */
RDBStoreIterator (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStoreIterator.java)/**
 * RocksDB store iterator.
 */
RDBTable (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/RDBTable.java)/**
 * RocksDB implementation of ozone metadata store. This class should be only
 * used as part of TypedTable as it's underlying implementation to access the
 * metadata store content. All other user's using Table should use TypedTable.
 */
RocksDBCheckpoint (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/RocksDBCheckpoint.java)/**
 * Class to hold information and location of a RocksDB Checkpoint.
 */
RocksDBConfiguration (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/RocksDBConfiguration.java)/**
 * Holds configuration items for OM RocksDB.
 */
SequenceNumberNotFoundException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/SequenceNumberNotFoundException.java)/**
 * Thrown if RocksDB is unable to find requested data from WAL file.
 */
StringCodec (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/StringCodec.java)/**
 * Codec to convert String to/from byte array.
 */
KeyValue (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/Table.java)/**
   * Class used to represent the key and value pair of a db entry.
   */
Table (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/Table.java)/**
 * Interface for key-value store that stores ozone metadata. Ozone metadata is
 * stored as key value pairs, both key and value are arbitrary byte arrays. Each
 * Table Stores a certain kind of keys and values. This allows a DB to have
 * different kind of tables.
 */
TableConfig (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/TableConfig.java)/**
 * Class that maintains Table Configuration.
 */
TableIterator (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/TableIterator.java)/**
 * Iterator for MetaDataStore DB.
 *
 * @param <T>
 */
TypedKeyValue (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/TypedTable.java)/**
   * Key value implementation for strongly typed tables.
   */
TypedTableIterator (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/TypedTable.java)/**
   * Table Iterator implementation for strongly typed tables.
   */
TypedTable (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/db/TypedTable.java)/**
 * Strongly typed table implementation.
 * <p>
 * Automatically converts values and keys using a raw byte[] based table
 * implementation and registered converters.
 *
 * @param <KEY>   type of the keys in the store.
 * @param <VALUE> type of the values in the store.
 */
EntryConsumer (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/EntryConsumer.java)/**
 * A consumer for metadata store key-value entries.
 * Used by {@link MetadataStore} class.
 */
HddsVersionInfo (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/HddsVersionInfo.java)/**
 * This class returns build information about Hadoop components.
 */
LevelDBStore (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/LevelDBStore.java)/**
 * LevelDB interface.
 */
LevelDBStoreIterator (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/LevelDBStoreIterator.java)/**
 * LevelDB store iterator.
 */
MetadataKeyFilter (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/MetadataKeyFilters.java)/**
   * Interface for levelDB key filters.
   */
KeyPrefixFilter (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/MetadataKeyFilters.java)/**
   * Utility class to filter key by a string prefix. This filter
   * assumes keys can be parsed to a string.
   */
MetadataKeyFilters (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/MetadataKeyFilters.java)/**
 * An utility class to filter levelDB keys.
 */
KeyValue (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/MetadataStore.java)/**
   * Class used to represent the key and value pair of a db entry.
   */
MetadataStore (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/MetadataStore.java)/**
 * Interface for key-value store that stores ozone metadata.
 * Ozone metadata is stored as key value pairs, both key and value
 * are arbitrary byte arrays.
 */
MetadataStoreBuilder (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/MetadataStoreBuilder.java)/**
 * Builder for metadata store.
 */
MetaStoreIterator (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/MetaStoreIterator.java)/**
 * Iterator for MetaDataStore DB.
 * @param <T>
 */
RetriableTask (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/RetriableTask.java)/**
 * {@code Callable} implementation that retries a delegate task according to
 * the specified {@code RetryPolicy}.  Sleeps between retries in the caller
 * thread.
 *
 * @param <V> the result type of method {@code call}
 */
RocksDBStore (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/RocksDBStore.java)/**
 * RocksDB implementation of ozone metadata store.
 */
RocksDBStoreIterator (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/RocksDBStoreIterator.java)/**
 * RocksDB store iterator.
 */
RocksDBStoreMBean (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/RocksDBStoreMBean.java)/**
 * Adapter JMX bean to publish all the Rocksdb metrics.
 */
Scheduler (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/Scheduler.java)/**
 * This class encapsulates ScheduledExecutorService.
 */
UniqueId (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/UniqueId.java)/**
 * This class uses system current time milliseconds to generate unique id.
 */
VersionInfo (/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/utils/VersionInfo.java)/**
 * This class returns build information about Hadoop components.
 */
Auditable (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/audit/Auditable.java)/**
 * Interface to make an entity auditable.
 */
AuditAction (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/audit/AuditAction.java)/**
 * Interface to define AuditAction.
 */
AuditLogger (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/audit/AuditLogger.java)/**
 * Class to define Audit Logger for Ozone.
 */
Builder (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/audit/AuditMessage.java)/**
   * Builder class for AuditMessage.
   */
AuditMessage (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/audit/AuditMessage.java)/**
 * Defines audit message structure.
 */
Auditor (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/audit/Auditor.java)/**
 * Interface to mark an actor as Auditor.
 */
Builder (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/BlockGroup.java)/**
   * BlockGroup instance builder.
   */
BlockGroup (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/BlockGroup.java)/**
 * A group of blocks relations relevant, e.g belong to a certain object key.
 */
Checksum (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/Checksum.java)/**
 * Class to compute and verify checksums for chunks.
 *
 * This class is not thread safe.
 */
CrcIntTable (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChecksumByteBuffer.java)/**
   * An abstract class implementing {@link ChecksumByteBuffer}
   * with a 32-bit checksum and a lookup table.
   */
ChecksumByteBuffer (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChecksumByteBuffer.java)/**
 * A sub-interface of {@link Checksum}
 * with a method to update checksum from a {@link ByteBuffer}.
 */
ChecksumData (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChecksumData.java)/**
 * Java class that represents Checksum ProtoBuf class. This helper class allows
 * us to convert to and from protobuf to normal java.
 */
DeleteBlockGroupResult (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/DeleteBlockGroupResult.java)/**
 * Result to delete a group of blocks.
 */
InconsistentStorageStateException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/InconsistentStorageStateException.java)/**
 * The exception is thrown when file system state is inconsistent
 * and is not recoverable.
 */
OzoneChecksumException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/OzoneChecksumException.java)/** Thrown for checksum errors. */
PureJavaCrc32ByteBuffer (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/PureJavaCrc32ByteBuffer.java)/**
 * Similar to {@link org.apache.hadoop.util.PureJavaCrc32}
 * except that this class implement {@link ChecksumByteBuffer}.
 */
PureJavaCrc32CByteBuffer (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/PureJavaCrc32CByteBuffer.java)/**
 * Similar to {@link org.apache.hadoop.util.PureJavaCrc32C}
 * except that this class implement {@link ChecksumByteBuffer}.
 */
InvalidStateTransitionException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/statemachine/InvalidStateTransitionException.java)/**
 * Class wraps invalid state transition exception.
 */
StateMachine (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/statemachine/StateMachine.java)/**
 * Template class that wraps simple event driven state machine.
 * @param <STATE> states allowed
 * @param <EVENT> events allowed
 */
Storage (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/Storage.java)/**
 * Storage information file. This Class defines the methods to check
 * the consistency of the storage dir and the version file.
 * <p>
 * Local storage information is stored in a separate file VERSION.
 * It contains type of the node,
 * the storage layout version, the SCM id, and
 * the OM/SCM state creation time.
 *
 */
StorageInfo (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/StorageInfo.java)/**
 * Common class for storage information. This class defines the common
 * properties and functions to set them , write them into the version file
 * and read them from the version file.
 *
 */
BlockData (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/container/common/helpers/BlockData.java)/**
 * Helper class to convert Protobuf to Java classes.
 */
ChunkInfo (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/container/common/helpers/ChunkInfo.java)/**
 * Java class that represents ChunkInfo ProtoBuf class. This helper class allows
 * us to convert to and from protobuf to normal java.
 */
ContainerCommandRequestPBHelper (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/container/common/helpers/ContainerCommandRequestPBHelper.java)/**
 * Utilities for converting protobuf classes to Java classes.
 */
Lease (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lease/Lease.java)/**
 * This class represents the lease created on a resource. Callback can be
 * registered on the lease which will be executed in case of timeout.
 *
 * @param <T> Resource type for which the lease can be associated
 */
LeaseAlreadyExistException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lease/LeaseAlreadyExistException.java)/**
 * This exception represents that there is already a lease acquired on the
 * same resource.
 */
LeaseCallbackExecutor (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lease/LeaseCallbackExecutor.java)/**
 * This class is responsible for executing the callbacks of a lease in case of
 * timeout.
 */
LeaseException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lease/LeaseException.java)/**
 * This exception represents all lease related exceptions.
 */
LeaseExpiredException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lease/LeaseExpiredException.java)/**
 * This exception represents that the lease that is being accessed has expired.
 */
LeaseMonitor (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lease/LeaseManager.java)/**
   * Monitors the leases and expires them based on the timeout, also
   * responsible for executing the callbacks of expired leases.
   */
LeaseManager (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lease/LeaseManager.java)/**
 * LeaseManager is someone who can provide you leases based on your
 * requirement. If you want to return the lease back before it expires,
 * you can give it back to Lease Manager. He is the one responsible for
 * the lifecycle of leases. The resource for which lease is created
 * should have proper {@code equals} method implementation, resource
 * equality is checked while the lease is created.
 *
 * @param <T> Type of leases that this lease manager can create
 */
LeaseManagerNotRunningException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lease/LeaseManagerNotRunningException.java)/**
 * This exception represents that there LeaseManager service is not running.
 */
LeaseNotFoundException (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lease/LeaseNotFoundException.java)/**
 * This exception represents that the lease that is being accessed does not
 * exist.
 */
ActiveLock (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lock/ActiveLock.java)/**
 * Lock implementation which also maintains counter.
 */
LockManager (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lock/LockManager.java)/**
 * Manages the locks on a given resource. A new lock is created for each
 * and every unique resource. Uniqueness of resource depends on the
 * {@code equals} implementation of it.
 */
PooledLockFactory (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lock/PooledLockFactory.java)/**
 * Pool factory to create {@code ActiveLock} instances.
 */
OzoneConfigKeys (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConfigKeys.java)/**
 * This class contains constants for configuration keys used in Ozone.
 */
OzoneConsts (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConsts.java)/**
 * Set of constants used in Ozone implementation.
 */
OzoneSecurityUtil (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneSecurityUtil.java)/**
 * Ozone security Util class.
 */
MetricName (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/protocolPB/ProtocolMessageMetrics.java)/**
   * Simple metrics info implementation.
   */
ProtocolMessageMetrics (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/protocolPB/ProtocolMessageMetrics.java)/**
 * Metrics to count all the subtypes of a specific message.
 */
JsonUtils (/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/web/utils/JsonUtils.java)/**
 * JSON Utility functions used in ozone.
 */
SimpleConfiguration (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/conf/SimpleConfiguration.java)/**
 * Example configuration to test the configuration injection.
 */
TestOzoneConfiguration (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/conf/TestOzoneConfiguration.java)/**
 * Test class for OzoneConfiguration.
 */
TestContainerCommandRequestMessage (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/ratis/TestContainerCommandRequestMessage.java)/** Testing {@link ContainerCommandRequestMessage}. */
TestSCMExceptionResultCodes (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/scm/exceptions/TestSCMExceptionResultCodes.java)/**
 * Test Result code mappping between SCMException and the protobuf definitions.
 */
TestNetworkTopologyImpl (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/scm/net/TestNetworkTopologyImpl.java)/** Test the network topology functions. */
TestNodeSchemaLoader (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/scm/net/TestNodeSchemaLoader.java)/** Test the node schema loader. */
TestNodeSchemaManager (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/scm/net/TestNodeSchemaManager.java)/** Test the node schema loader. */
TestYamlSchemaLoader (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/scm/net/TestYamlSchemaLoader.java)/** Test the node schema loader. */
TestOzoneBlockTokenIdentifier (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/security/token/TestOzoneBlockTokenIdentifier.java)/**
 * Test class for OzoneManagerDelegationToken.
 */
MockApprover (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/security/x509/certificate/authority/MockApprover.java)/**
 * A test approver class that makes testing easier.
 */
MockCAStore (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/security/x509/certificate/authority/MockCAStore.java)/**
 *
 */
TestDefaultCAServer (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/security/x509/certificate/authority/TestDefaultCAServer.java)/**
 * Tests the Default CA Server.
 */
TestDefaultProfile (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/security/x509/certificate/authority/TestDefaultProfile.java)/**
 * Tests for the default PKI Profile.
 */
TestCertificateClientInit (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/security/x509/certificate/client/TestCertificateClientInit.java)/**
 * Test class for {@link DefaultCertificateClient}.
 */
TestDefaultCertificateClient (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/security/x509/certificate/client/TestDefaultCertificateClient.java)/**
 * Test class for {@link DefaultCertificateClient}.
 */
TestCertificateCodec (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/security/x509/certificate/utils/TestCertificateCodec.java)/**
 * Tests the Certificate codecs.
 */
TestCertificateSignRequest (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/security/x509/certificates/TestCertificateSignRequest.java)/**
 * Certificate Signing Request.
 */
TestRootCertificate (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/security/x509/certificates/TestRootCertificate.java)/**
 * Test Class for Root Certificate generation.
 */
TestHDDSKeyGenerator (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/security/x509/keys/TestHDDSKeyGenerator.java)/**
 * Test class for HDDS Key Generator.
 */
TestKeyCodec (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/security/x509/keys/TestKeyCodec.java)/**
 * Test class for HDDS pem writer.
 */
TestHddsUtils (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/TestHddsUtils.java)/**
 * Testing HddsUtils.
 */
TestTableCacheImpl (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/utils/db/cache/TestTableCacheImpl.java)/**
 * Class tests partial table cache.
 */
TestDBConfigFromFile (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/utils/db/TestDBConfigFromFile.java)/**
 * DBConf tests.
 */
TestDBStoreBuilder (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/utils/db/TestDBStoreBuilder.java)/**
 * Tests RDBStore creation.
 */
TestRDBStore (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/utils/db/TestRDBStore.java)/**
 * RDBStore Tests.
 */
TestRDBTableStore (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/utils/db/TestRDBTableStore.java)/**
 * Tests for RocksDBTable Store.
 */
TestTypedRDBTableStore (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/utils/db/TestTypedRDBTableStore.java)/**
 * Tests for RocksDBTable Store.
 */
TestHddsIdFactory (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/utils/TestHddsIdFactory.java)/**
 * Test the JMX interface for the rocksdb metastore implementation.
 */
TestMetadataStore (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/utils/TestMetadataStore.java)/**
 * Test class for ozone metadata store.
 */
TestRetriableTask (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/utils/TestRetriableTask.java)/**
 * Tests for {@link RetriableTask}.
 */
TestRocksDBStoreMBean (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/utils/TestRocksDBStoreMBean.java)/**
 * Test the JMX interface for the rocksdb metastore implementation.
 */
BufferedMetricsRecordBuilderImpl (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/utils/TestRocksDBStoreMBean.java)/**
   * Test class to buffer a single snapshot of metrics.
   */
BufferedMetricsCollector (/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/utils/TestRocksDBStoreMBean.java)/**
 * Test class to buffer a single MetricsRecordBuilder instance.
 */
DummyEntity (/hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/audit/DummyEntity.java)/**
 * DummyEntity that implements Auditable for test purpose.
 */
TestOzoneAuditLogger (/hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/audit/TestOzoneAuditLogger.java)/**
 * Test Ozone Audit Logger.
 */
TestChecksum (/hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/common/TestChecksum.java)/**
 * Tests for {@link Checksum} class.
 */
TestChecksumByteBuffer (/hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/common/TestChecksumByteBuffer.java)/**
 * Test {@link ChecksumByteBuffer} implementations.
 */
TestStateMachine (/hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/common/TestStateMachine.java)/**
 * This class is to test ozone common state machine.
 */
DummyResource (/hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/lease/TestLeaseManager.java)/**
   * Dummy resource on which leases can be acquired.
   */
TestLeaseManager (/hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/lease/TestLeaseManager.java)/**
 * Test class to check functionality and consistency of LeaseManager.
 */
TestLockManager (/hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/lock/TestLockManager.java)/**
 * Test-cases to test LockManager.
 */
ConfigFileAppender (/hadoop-hdds/config/src/main/java/org/apache/hadoop/hdds/conf/ConfigFileAppender.java)/**
 * Simple DOM based config file writer.
 * <p>
 * This class can init/load existing ozone-default-generated.xml fragments
 * and append new entries and write to the file system.
 */
ConfigFileGenerator (/hadoop-hdds/config/src/main/java/org/apache/hadoop/hdds/conf/ConfigFileGenerator.java)/**
 * Annotation processor to generate config fragments from Config annotations.
 */
ConfigurationException (/hadoop-hdds/config/src/main/java/org/apache/hadoop/hdds/conf/ConfigurationException.java)/**
 * Exception to throw in case of a configuration problem.
 */
ConfigurationExample (/hadoop-hdds/config/src/test/java/org/apache/hadoop/hdds/conf/ConfigurationExample.java)/**
 * Example configuration to test the configuration injection.
 */
TestConfigFileAppender (/hadoop-hdds/config/src/test/java/org/apache/hadoop/hdds/conf/TestConfigFileAppender.java)/**
 * Test the utility which loads/writes the config file fragments.
 */
HddsServerUtil (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/hdds/scm/HddsServerUtil.java)/**
 * Hdds stateless helper functions for server side components.
 */
VersionInfo (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/hdds/scm/VersionInfo.java)/**
 * This is a class that tracks versions of SCM.
 */
DataNodeLayoutVersion (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/DataNodeLayoutVersion.java)/**
 * Datanode layout version which describes information about the layout version
 * on the datanode.
 */
ContainerMetrics (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/helpers/ContainerMetrics.java)/**
 *
 * This class is for maintaining  the various Storage Container
 * DataNode statistics and publishing them through the metrics interfaces.
 * This also registers the JMX MBean for RPC.
 * <p>
 * This class has a number of metrics variables that are publicly accessible;
 * these variables (objects) have methods to update their values;
 *  for example:
 *  <p> {@link #numOps}.inc()
 *
 */
ContainerUtils (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/helpers/ContainerUtils.java)/**
 * A set of helper functions to create proper responses.
 */
DatanodeDetailsYaml (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/helpers/DatanodeIdYaml.java)/**
   * Datanode details bean to be written to the yaml file.
   */
DatanodeIdYaml (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/helpers/DatanodeIdYaml.java)/**
 * Class for creating datanode.id file in yaml format.
 */
DatanodeVersionFile (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/helpers/DatanodeVersionFile.java)/**
 * This is a utility class which helps to create the version file on datanode
 * and also validate the content of the version file.
 */
DeletedContainerBlocksSummary (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/helpers/DeletedContainerBlocksSummary.java)/**
 * A helper class to wrap the info about under deletion container blocks.
 */
ContainerData (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/ContainerData.java)/**
 * ContainerData is the in-memory representation of container metadata and is
 * represented on disk by the .container file.
 */
ContainerDataRepresenter (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/ContainerDataYaml.java)/**
   * Representer class to define which fields need to be stored in yaml file.
   */
ContainerDataConstructor (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/ContainerDataYaml.java)/**
   * Constructor class for KeyValueData, which will be used by Yaml.
   */
ContainerSet (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/ContainerSet.java)/**
 * Class that manages Containers created on the datanode.
 */
HddsDispatcher (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/HddsDispatcher.java)/**
 * Ozone Container dispatcher takes a call from the netty server and routes it
 * to the right handler function.
 */
BlockDataMap (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/OpenContainerBlockMap.java)/**
   * Map: localId {@literal ->} BlockData.
   *
   * In order to support {@link #getAll()}, the update operations are
   * synchronized.
   */
OpenContainerBlockMap (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/OpenContainerBlockMap.java)/**
 * Map: containerId {@literal ->} (localId {@literal ->} {@link BlockData}).
 * The outer container map does not entail locking for a better performance.
 * The inner {@link BlockDataMap} is synchronized.
 *
 * This class will maintain list of open keys per container when closeContainer
 * command comes, it should autocommit all open keys of a open container before
 * marking the container as closed.
 */
RandomContainerDeletionChoosingPolicy (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/RandomContainerDeletionChoosingPolicy.java)/**
 * Randomly choosing containers for block deletion.
 */
Builder (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/StorageLocationReport.java)/**
   * Builder class for building StorageLocationReport.
   */
StorageLocationReport (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/StorageLocationReport.java)/**
 * Storage location stats of datanodes that provide back store for containers.
 *
 */
TopNOrderedContainerDeletionChoosingPolicy (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/TopNOrderedContainerDeletionChoosingPolicy.java)/**
 * TopN Ordered choosing policy that choosing containers based on pending
 * deletion blocks' number.
 */
BlockIterator (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/BlockIterator.java)/**
 * Block Iterator for container. Each container type need to implement this
 * interface.
 * @param <T>
 */
Container (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/Container.java)/**
 * Interface for Container Operations.
 */
ContainerDeletionChoosingPolicy (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/ContainerDeletionChoosingPolicy.java)/**
 * This interface is used for choosing desired containers for
 * block deletion.
 */
ContainerDispatcher (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/ContainerDispatcher.java)/**
 * Dispatcher acts as the bridge between the transport layer and
 * the actual container layer. This layer is capable of transforming
 * protobuf objects into corresponding class and issue the function call
 * into the lower layers.
 *
 * The reply from the request is dispatched to the client.
 */
ContainerLocationManager (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/ContainerLocationManager.java)/**
 * Returns physical path locations, where the containers will be created.
 */
ContainerLocationManagerMXBean (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/ContainerLocationManagerMXBean.java)/**
 * Returns physical path locations, where the containers will be created.
 */
ContainerPacker (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/ContainerPacker.java)/**
 * Service to pack/unpack ContainerData container data to/from a single byte
 * stream.
 */
Handler (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/Handler.java)/**
 * Dispatcher sends ContainerCommandRequests to Handler. Each Container Type
 * should have an implementation for Handler.
 */
StorageLocationReportMXBean (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/StorageLocationReportMXBean.java)/**
 * Contract to define properties available on the JMX interface.
 */
VolumeChoosingPolicy (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/VolumeChoosingPolicy.java)/**
 * This interface specifies the policy for choosing volumes to store replicas.
 */
CommandStatusReportPublisher (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/report/CommandStatusReportPublisher.java)/**
 * Publishes CommandStatusReport which will be sent to SCM as part of
 * heartbeat. CommandStatusReport consist of the following information:
 * - type       : type of command.
 * - status     : status of command execution (PENDING, EXECUTED, FAILURE).
 * - cmdId      : Command id.
 * - msg        : optional message.
 */
ContainerReportPublisher (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/report/ContainerReportPublisher.java)/**
 * Publishes ContainerReport which will be sent to SCM as part of heartbeat.
 * ContainerReport consist of the following information about each containers:
 *   - containerID
 *   - size
 *   - used
 *   - keyCount
 *   - readCount
 *   - writeCount
 *   - readBytes
 *   - writeBytes
 *   - finalHash
 *   - LifeCycleState
 *
 */
NodeReportPublisher (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/report/NodeReportPublisher.java)/**
 * Publishes NodeReport which will be sent to SCM as part of heartbeat.
 * NodeReport consist of:
 *   - NodeIOStats
 *   - VolumeReports
 */
PipelineReportPublisher (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/report/PipelineReportPublisher.java)/**
 * Publishes Pipeline which will be sent to SCM as part of heartbeat.
 * PipelineReport consist of the following information about each containers:
 *   - pipelineID
 *
 */
Builder (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/report/ReportManager.java)/**
   * Builder to construct {@link ReportManager}.
   */
ReportManager (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/report/ReportManager.java)/**
 * ReportManager is responsible for managing all the {@link ReportPublisher}
 * and also provides {@link ScheduledExecutorService} to ReportPublisher
 * which should be used for scheduling the reports.
 */
ReportPublisher (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/report/ReportPublisher.java)/**
 * Abstract class responsible for scheduling the reports based on the
 * configured interval. All the ReportPublishers should extend this class.
 */
ReportPublisherFactory (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/report/ReportPublisherFactory.java)/**
 * Factory class to construct {@link ReportPublisher} for a report.
 */
CloseContainerCommandHandler (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/CloseContainerCommandHandler.java)/**
 * Handler for close container command received from SCM.
 */
Builder (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/CommandDispatcher.java)/**
   * Helper class to construct command dispatcher.
   */
CommandDispatcher (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/CommandDispatcher.java)/**
 * Dispatches command to the correct handler.
 */
CommandHandler (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/CommandHandler.java)/**
 * Generic interface for handlers.
 */
DeleteBlocksCommandHandler (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/DeleteBlocksCommandHandler.java)/**
 * Handle block deletion commands.
 */
DeleteContainerCommandHandler (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/DeleteContainerCommandHandler.java)/**
 * Handler to process the DeleteContainerCommand from SCM.
 */
ReplicateContainerCommandHandler (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/ReplicateContainerCommandHandler.java)/**
 * Command handler to copy containers from sources.
 */
DatanodeStateMachine (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/DatanodeStateMachine.java)/**
 * State Machine Class.
 */
EndpointStateMachine (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/EndpointStateMachine.java)/**
 * Endpoint is used as holder class that keeps state around the RPC endpoint.
 */
EndpointStateMachineMBean (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/EndpointStateMachineMBean.java)/**
 * JMX representation of an EndpointStateMachine.
 */
SCMConnectionManager (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/SCMConnectionManager.java)/**
 * SCMConnectionManager - Acts as a class that manages the membership
 * information of the SCMs that we are working with.
 */
SCMConnectionManagerMXBean (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/SCMConnectionManagerMXBean.java)/**
 * JMX information about the connected SCM servers.
 */
StateContext (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/StateContext.java)/**
 * Current Context of State Machine.
 */
InitDatanodeState (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/datanode/InitDatanodeState.java)/**
 * Init Datanode State is the task that gets run when we are in Init State.
 */
RunningDatanodeState (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/datanode/RunningDatanodeState.java)/**
 * Class that implements handshake with SCM.
 */
DatanodeState (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/DatanodeState.java)/**
 * State Interface that allows tasks to maintain states.
 */
Builder (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/HeartbeatEndpointTask.java)/**
   * Builder class for HeartbeatEndpointTask.
   */
HeartbeatEndpointTask (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/HeartbeatEndpointTask.java)/**
 * Heartbeat class for SCMs.
 */
Builder (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/RegisterEndpointTask.java)/**
   * Builder class for RegisterEndPoint task.
   */
RegisterEndpointTask (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/RegisterEndpointTask.java)/**
 * Register a datanode with SCM.
 */
VersionEndpointTask (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java)/**
 * Task that returns version.
 */
GrpcXceiverService (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/GrpcXceiverService.java)/**
 * Grpc Service for handling Container Commands on datanode.
 */
CSMMetrics (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/CSMMetrics.java)/**
 * This class is for maintaining Container State Machine statistics.
 */
Builder (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/DispatcherContext.java)/**
   * Builder class for building DispatcherContext.
   */
DispatcherContext (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/DispatcherContext.java)/**
 * DispatcherContext class holds transport protocol specific context info
 * required for execution of container commands over the container dispatcher.
 */
RatisServerConfiguration (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/RatisServerConfiguration.java)/**
 * Holds configuration items for Ratis/Raft server.
 */
XceiverServerRatis (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/XceiverServerRatis.java)/**
 * Creates a ratis server endpoint that acts as the communication layer for
 * Ozone containers.
 */
ServerCredentialInterceptor (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ServerCredentialInterceptor.java)/**
 * Grpc Server Interceptor for Ozone Block token.
 */
XceiverServer (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/XceiverServer.java)/**
 * A server endpoint that acts as the communication layer for Ozone containers.
 */
XceiverServerGrpc (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/XceiverServerGrpc.java)/**
 * Creates a Grpc server endpoint that acts as the communication layer for
 * Ozone containers.
 */
XceiverServerSpi (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/XceiverServerSpi.java)/** A server endpoint that acts as the communication layer for Ozone
 * containers. */
ContainerCache (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/ContainerCache.java)/**
 * container cache is a LRUMap that maintains the DB handles.
 */
HddsVolumeUtil (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java)/**
 * A util class for {@link HddsVolume}.
 */
ReferenceCountedDB (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/ReferenceCountedDB.java)/**
 * Class to implement reference counting over instances handed by Container
 * Cache.
 * Enable DEBUG log below will enable us quickly locate the leaked reference
 * from caller stack. When JDK9 StackWalker is available, we can switch to
 * StackWalker instead of new Exception().printStackTrace().
 */
TrustedFuture (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/AbstractFuture.java)/**
   * A less abstract subclass of AbstractFuture. This can be used to optimize
   * setFuture by ensuring that {@link #get} calls exactly the implementation
   * of {@link AbstractFuture#get}.
   */
Waiter (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/AbstractFuture.java)/**
   * Waiter links form a Treiber stack, in the {@link #waiters} field.
   */
Listener (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/AbstractFuture.java)/**
   * Listeners also form a stack through the {@link #listeners} field.
   */
Failure (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/AbstractFuture.java)/**
   * A special value to represent failure, when {@link #setException} is
   * called successfully.
   */
Cancellation (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/AbstractFuture.java)/**
   * A special value to represent cancellation and the 'wasInterrupted' bit.
   */
SetFuture (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/AbstractFuture.java)/**
   * A special value that encodes the 'setFuture' state.
   */
UnsafeAtomicHelper (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/AbstractFuture.java)/**
   * {@link AtomicHelper} based on {@link sun.misc.Unsafe}.
   * <p>
   * <p>Static initialization of this class will fail if the
   * {@link sun.misc.Unsafe} object cannot be accessed.
   */
SafeAtomicHelper (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/AbstractFuture.java)/**
   * {@link AtomicHelper} based on {@link AtomicReferenceFieldUpdater}.
   */
SynchronizedHelper (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/AbstractFuture.java)/**
   * {@link AtomicHelper} based on {@code synchronized} and volatile writes.
   * <p>
   * <p>This is an implementation of last resort for when certain basic VM
   * features are broken (like AtomicReferenceFieldUpdater).
   */
AsyncChecker (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/AsyncChecker.java)/**
 * A class that can be used to schedule an asynchronous check on a given
 * {@link Checkable}. If the check is successfully scheduled then a
 * {@link ListenableFuture} is returned.
 *
 */
Builder (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/HddsVolume.java)/**
   * Builder for HddsVolume.
   */
HddsVolume (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/HddsVolume.java)/**
 * HddsVolume represents volume in a datanode. {@link VolumeSet} maintains a
 * list of HddsVolumes, one for each volume in the Datanode.
 * {@link VolumeInfo} in encompassed by this class.
 * <p>
 * The disk layout per volume is as follows:
 * <p>../hdds/VERSION
 * <p>{@literal ../hdds/<<scmUuid>>/current/<<containerDir>>/<<containerID
 * >>/metadata}
 * <p>{@literal ../hdds/<<scmUuid>>/current/<<containerDir>>/<<containerID
 * >>/<<dataDir>>}
 * <p>
 * Each hdds volume has its own VERSION file. The hdds volume will have one
 * scmUuid directory for each SCM it is a part of (currently only one SCM is
 * supported).
 *
 * During DN startup, if the VERSION file exists, we verify that the
 * clusterID in the version file matches the clusterID from SCM.
 */
Callback (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/HddsVolumeChecker.java)/**
   * A callback interface that is supplied the result of running an
   * async disk check on multiple volumes.
   */
ResultHandler (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/HddsVolumeChecker.java)/**
   * A callback to process the results of checking a volume.
   */
HddsVolumeChecker (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/HddsVolumeChecker.java)/**
 * A class that encapsulates running disk checks against each HDDS volume and
 * allows retrieving a list of failed volumes.
 */
RoundRobinVolumeChoosingPolicy (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/RoundRobinVolumeChoosingPolicy.java)/**
 * Choose volumes in round-robin order.
 * The caller should synchronize access to the list of volumes.
 */
LastCheckResult (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/ThrottledAsyncChecker.java)/**
   * Status of running a check. It can either be a result or an
   * exception, depending on whether the check completed or threw.
   */
ThrottledAsyncChecker (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/ThrottledAsyncChecker.java)/**
 * An implementation of {@link AsyncChecker} that skips checking recently
 * checked objects. It will enforce at least minMsBetweenChecks
 * milliseconds between two successive checks of any one object.
 *
 * It is assumed that the total number of Checkable objects in the system
 * is small, (not more than a few dozen) since the checker uses O(Checkables)
 * storage and also potentially O(Checkables) threads.
 *
 * minMsBetweenChecks should be configured reasonably
 * by the caller to avoid spinning up too many threads frequently.
 */
Fire (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/TimeoutFuture.java)/**
   * A runnable that is called when the delegate or the timer completes.
   */
TimeoutFuture (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/TimeoutFuture.java)/**
 * Implementation of {@code Futures#withTimeout}.
 * <p>
 * <p>Future that delegates to another but will finish early (via a
 * {@link TimeoutException} wrapped in an {@link ExecutionException}) if the
 * specified duration expires. The delegate future is interrupted and
 * cancelled if it times out.
 */
Builder (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeInfo.java)/**
   * Builder for VolumeInfo.
   */
VolumeInfo (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeInfo.java)/**
 * Stores information about a disk/volume.
 */
VolumeIOStats (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeIOStats.java)/**
 * This class is used to track Volume IO stats for each HDDS Volume.
 */
VolumeSet (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java)/**
 * VolumeSet to manage HDDS volumes in a DataNode.
 */
VolumeUsage (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeUsage.java)/**
 * Class that wraps the space df of the Datanode Volumes used by SCM
 * containers.
 */
BlockUtils (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/BlockUtils.java)/**
 * Utils functions to help block functions.
 */
ChunkUtils (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/ChunkUtils.java)/**
 * Utility methods for chunk operations for KeyValue container.
 */
KeyValueContainerLocationUtil (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerLocationUtil.java)/**
 * Class which provides utility methods for container locations.
 */
SmallFileUtils (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/SmallFileUtils.java)/**
 * File Utils are helper routines used by putSmallFile and getSmallFile
 * RPCs.
 */
BlockManagerImpl (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/BlockManagerImpl.java)/**
 * This class is for performing block related operations on the KeyValue
 * Container.
 */
ChunkManagerDummyImpl (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/ChunkManagerDummyImpl.java)/**
 * Implementation of ChunkManager built for running performance tests.
 * Chunks are not written to disk, Reads are returned with zero-filled buffers
 */
ChunkManagerFactory (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/ChunkManagerFactory.java)/**
 * Select an appropriate ChunkManager implementation as per config setting.
 * Ozone ChunkManager is a Singleton
 */
ChunkManagerImpl (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/ChunkManagerImpl.java)/**
 * This class is for performing chunk related operations.
 */
BlockManager (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/interfaces/BlockManager.java)/**
 * BlockManager is for performing key related operations on the container.
 */
KeyValueBlockIterator (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueBlockIterator.java)/**
 * Block Iterator for KeyValue Container. This block iterator returns blocks
 * which match with the {@link MetadataKeyFilters.KeyPrefixFilter}. If no
 * filter is specified, then default filter used is
 * {@link MetadataKeyFilters#getNormalKeyFilter()}
 */
KeyValueContainer (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java)/**
 * Class to perform KeyValue Container operations. Any modifications to
 * KeyValueContainer object should ideally be done via api exposed in
 * KeyValueHandler class.
 */
KeyValueContainerData (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainerData.java)/**
 * This class represents the KeyValueContainer metadata, which is the
 * in-memory representation of container metadata and is represented on disk
 * by the .container file.
 */
KeyValueHandler (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java)/**
 * Handler for KeyValue Container type.
 */
TarContainerPacker (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/TarContainerPacker.java)/**
 * Compress/uncompress KeyValueContainer data to a tar.gz archive.
 */
ContainerController (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/ContainerController.java)/**
 * Control plane for container management in datanode.
 */
ContainerDataScanner (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/ContainerDataScanner.java)/**
 * VolumeScanner scans a single volume.  Each VolumeScanner has its own thread.
 * <p>They are all managed by the DataNode's BlockScanner.
 */
ContainerDataScrubberMetrics (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/ContainerDataScrubberMetrics.java)/**
 * This class captures the container data scrubber metrics on the data-node.
 **/
ContainerMetadataScanner (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/ContainerMetadataScanner.java)/**
 * This class is responsible to perform metadata verification of the
 * containers.
 */
ContainerMetadataScrubberMetrics (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/ContainerMetadataScrubberMetrics.java)/**
 * This class captures the container meta-data scrubber metrics on the
 * data-node.
 **/
ContainerReader (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/ContainerReader.java)/**
 * Class used to read .container files from Volume and build container map.
 *
 * Layout of the container directory on disk is as follows:
 *
 * <p>../hdds/VERSION
 * <p>{@literal ../hdds/<<scmUuid>>/current/<<containerDir>>/<<containerID
 * >/metadata/<<containerID>>.container}
 * <p>{@literal ../hdds/<<scmUuid>>/current/<<containerDir>>/<<containerID
 * >/<<dataPath>>}
 * <p>
 * Some ContainerTypes will have extra metadata other than the .container
 * file. For example, KeyValueContainer will have a .db file. This .db file
 * will also be stored in the metadata folder along with the .container file.
 * <p>
 * {@literal ../hdds/<<scmUuid>>/current/<<containerDir>>/<<KVcontainerID
 * >/metadata/<<KVcontainerID>>.db}
 * <p>
 * Note that the {@literal <<dataPath>>} is dependent on the ContainerType.
 * For KeyValueContainers, the data is stored in a "chunks" folder. As such,
 * the {@literal <<dataPath>>} layout for KeyValueContainers is:
 * <p>{@literal ../hdds/<<scmUuid>>/current/<<containerDir>>/<<KVcontainerID
 * >/chunks/<<chunksFile>>}
 *
 */
ContainerScrubberConfiguration (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/ContainerScrubberConfiguration.java)/**
 * This class defines configuration parameters for container scrubber.
 **/
OzoneContainer (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/OzoneContainer.java)/**
 * Ozone main class sets up the network servers and initializes the container
 * layer.
 */
ContainerDownloader (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/ContainerDownloader.java)/**
 * Service to download container data from other datanodes.
 * <p>
 * The implementation of this interface should copy the raw container data in
 * compressed form to working directory.
 * <p>
 * A smart implementation would use multiple sources to do parallel download.
 */
ContainerReplicationSource (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/ContainerReplicationSource.java)/**
 * Contract to prepare provide the container in binary form..
 * <p>
 * Prepare will be called when container is closed. An implementation could
 * precache any binary representation of a container and store the pre packede
 * images.
 */
ContainerReplicator (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/ContainerReplicator.java)/**
 * Service to do the real replication task.
 *
 * An implementation should download the container and im
 */
ContainerStreamingOutput (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/ContainerStreamingOutput.java)/**
 * JAX-RS streaming output to return the binary container data.
 */
DownloadAndImportReplicator (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/DownloadAndImportReplicator.java)/**
 * Default replication implementation.
 * <p>
 * This class does the real job. Executes the download and import the container
 * to the container set.
 */
StreamDownloader (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/GrpcReplicationClient.java)/**
   * Grpc stream observer to ComletableFuture adapter.
   */
GrpcReplicationClient (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/GrpcReplicationClient.java)/**
 * Client to read container data from Grpc.
 */
GrpcReplicationService (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/GrpcReplicationService.java)/**
 * Service to make containers available for replication.
 */
OnDemandContainerReplicationSource (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/OnDemandContainerReplicationSource.java)/**
 * A naive implementation of the replication source which creates a tar file
 * on-demand without pre-create the compressed archives.
 */
ReplicationSupervisor (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/ReplicationSupervisor.java)/**
 * Single point to schedule the downloading tasks based on priorities.
 */
ReplicationTask (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/ReplicationTask.java)/**
 * The task to download a container from the sources.
 */
SimpleContainerDownloader (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/SimpleContainerDownloader.java)/**
 * Simple ContainerDownloaderImplementation to download the missing container
 * from the first available datanode.
 * <p>
 * This is not the most effective implementation as it uses only one source
 * for he container download.
 */
HddsDatanodeHttpServer (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/HddsDatanodeHttpServer.java)/**
 * Simple http server to provide basic monitoring for hdds datanode.
 * <p>
 * This server is used to access default /conf /prom /prof endpoints.
 */
HddsDatanodeStopService (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/HddsDatanodeStopService.java)/**
 * Interface which declares a method to stop HddsDatanodeService.
 */
CloseContainerCommand (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/commands/CloseContainerCommand.java)/**
 * Asks datanode to close a container.
 */
CommandForDatanode (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/commands/CommandForDatanode.java)/**
 * Command for the datanode with the destination address.
 */
CommandStatusBuilder (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/commands/CommandStatus.java)/**
   * Builder class for CommandStatus.
   */
CommandStatus (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/commands/CommandStatus.java)/**
 * A class that is used to communicate status of datanode commands.
 */
DeleteBlockCommandStatusBuilder (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/commands/DeleteBlockCommandStatus.java)/**
   * Builder for DeleteBlockCommandStatus.
   */
DeleteBlockCommandStatus (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/commands/DeleteBlockCommandStatus.java)/**
 * Command status to report about block deletion.
 */
DeleteBlocksCommand (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/commands/DeleteBlocksCommand.java)/**
 * A SCM command asks a datanode to delete a number of blocks.
 */
DeleteContainerCommand (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/commands/DeleteContainerCommand.java)/**
 * SCM command which tells the datanode to delete a container.
 */
Builder (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/commands/RegisteredCommand.java)/**
   * A builder class to verify all values are sane.
   */
RegisteredCommand (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/commands/RegisteredCommand.java)/**
 * Response to Datanode Register call.
 */
ReplicateContainerCommand (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/commands/ReplicateContainerCommand.java)/**
 * SCM command to request replication of a container.
 */
ReregisterCommand (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/commands/ReregisterCommand.java)/**
 * Informs a datanode to register itself with SCM again.
 */
SCMCommand (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/commands/SCMCommand.java)/**
 * A class that acts as the base class to convert between Java and SCM
 * commands in protobuf format.
 * @param <T>
 */
StorageContainerDatanodeProtocol (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/StorageContainerDatanodeProtocol.java)/**
 * The protocol spoken between datanodes and SCM. For specifics please the
 * Protoc file that defines this protocol.
 */
StorageContainerNodeProtocol (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/StorageContainerNodeProtocol.java)/**
 * The protocol spoken between datanodes and SCM.
 *
 * Please note that the full protocol spoken between a datanode and SCM is
 * separated into 2 interfaces. One interface that deals with node state and
 * another interface that deals with containers.
 *
 * This interface has functions that deals with the state of datanode.
 */
Builder (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/VersionResponse.java)/**
   * Builder class.
   */
VersionResponse (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocol/VersionResponse.java)/**
 * Version response class.
 */
StorageContainerDatanodeProtocolClientSideTranslatorPB (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocolPB/StorageContainerDatanodeProtocolClientSideTranslatorPB.java)/**
 * This class is the client-side translator to translate the requests made on
 * the {@link StorageContainerDatanodeProtocol} interface to the RPC server
 * implementing {@link StorageContainerDatanodeProtocolPB}.
 */
StorageContainerDatanodeProtocolServerSideTranslatorPB (/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/protocolPB/StorageContainerDatanodeProtocolServerSideTranslatorPB.java)/**
 * This class is the server-side translator that forwards requests received on
 * {@link StorageContainerDatanodeProtocolPB} to the {@link
 * StorageContainerDatanodeProtocol} server implementation.
 */
ContainerTestUtils (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/ContainerTestUtils.java)/**
 * Helper utility to test containers.
 */
TestDatanodeVersionFile (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/helpers/TestDatanodeVersionFile.java)/**
 * This class tests {@link DatanodeVersionFile}.
 */
TestContainerDataYaml (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestContainerDataYaml.java)/**
 * This class tests create/read .container files.
 */
TestContainerSet (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestContainerSet.java)/**
 * Class used to test ContainerSet operations.
 */
TestHddsDispatcher (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestHddsDispatcher.java)/**
 * Test-cases to verify the functionality of HddsDispatcher.
 */
TestHandler (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/interfaces/TestHandler.java)/**
 * Tests Handler interface.
 */
TestReportManager (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/report/TestReportManager.java)/**
 * Test cases to test {@link ReportManager}.
 */
DummyReportPublisher (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/report/TestReportPublisher.java)/**
   * Dummy report publisher for testing.
   */
TestReportPublisher (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/report/TestReportPublisher.java)/**
 * Test cases to test {@link ReportPublisher}.
 */
TestReportPublisherFactory (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/report/TestReportPublisherFactory.java)/**
 * Test cases to test ReportPublisherFactory.
 */
ScmTestMock (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/ScmTestMock.java)/**
 * SCM RPC mock class.
 */
SCMTestUtils (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/SCMTestUtils.java)/**
 * Test Endpoint class.
 */
TestCloseContainerCommandHandler (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/TestCloseContainerCommandHandler.java)/**
 * Test cases to verify CloseContainerCommandHandler in datanode.
 */
TestHeartbeatEndpointTask (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/states/endpoint/TestHeartbeatEndpointTask.java)/**
 * This class tests the functionality of HeartbeatEndpointTask.
 */
TestChunkLayOutVersion (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestChunkLayOutVersion.java)/**
 * This class tests ChunkLayOutVersion.
 */
TestContainerCache (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestContainerCache.java)/**
 * Test ContainerCache with evictions.
 */
TestDatanodeLayOutVersion (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestDatanodeLayOutVersion.java)/**
 * This class tests DatanodeLayOutVersion.
 */
TestDatanodeStateMachine (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestDatanodeStateMachine.java)/**
 * Tests the datanode state machine class and its states.
 */
TestKeyValueContainerData (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestKeyValueContainerData.java)/**
 * This class is used to test the KeyValueContainerData.
 */
TestHddsVolume (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestHddsVolume.java)/**
 * Unit tests for {@link HddsVolume}.
 */
DummyChecker (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestHddsVolumeChecker.java)/**
   * A checker to wraps the result of {@link HddsVolume#check} in
   * an ImmediateFuture.
   */
TestHddsVolumeChecker (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestHddsVolumeChecker.java)/**
 * Tests for {@link HddsVolumeChecker}.
 */
TestRoundRobinVolumeChoosingPolicy (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestRoundRobinVolumeChoosingPolicy.java)/**
 * Tests {@link RoundRobinVolumeChoosingPolicy}.
 */
TestVolumeSet (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestVolumeSet.java)/**
 * Tests {@link VolumeSet} operations.
 */
DummyChecker (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestVolumeSetDiskChecks.java)/**
   * A no-op checker that fails the given number of volumes and succeeds
   * the rest.
   */
TestVolumeSetDiskChecks (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestVolumeSetDiskChecks.java)/**
 * Verify that {@link VolumeSet} correctly checks for failed disks
 * during initialization.
 */
TestChunkUtils (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/helpers/TestChunkUtils.java)/**
 * Tests for {@link ChunkUtils}.
 */
TestBlockManagerImpl (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestBlockManagerImpl.java)/**
 * This class is used to test key related operations on the container.
 */
TestChunkManagerImpl (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestChunkManagerImpl.java)/**
 * This class is used to test ChunkManager operations.
 */
TestKeyValueBlockIterator (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java)/**
 * This class is used to test KeyValue container block iterator.
 */
TestKeyValueContainer (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueContainer.java)/**
 * Class to test KeyValue Container operations.
 */
TestKeyValueContainerCheck (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueContainerCheck.java)/**
 * Basic sanity test for the KeyValueContainerCheck class.
 */
TestKeyValueContainerMarkUnhealthy (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueContainerMarkUnhealthy.java)/**
 * Tests unhealthy container functionality in the {@link KeyValueContainer}
 * class.
 */
TestKeyValueHandler (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java)/**
 * Unit tests for {@link KeyValueHandler}.
 */
TestKeyValueHandlerWithUnhealthyContainer (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandlerWithUnhealthyContainer.java)/**
 * Test that KeyValueHandler fails certain operations when the
 * container is unhealthy.
 */
TestTarContainerPacker (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestTarContainerPacker.java)/**
 * Test the tar/untar for a given container.
 */
TestContainerScrubberMetrics (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestContainerScrubberMetrics.java)/**
 * This test verifies the container scrubber metrics functionality.
 */
TestOzoneContainer (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestOzoneContainer.java)/**
 * This class is used to test OzoneContainer.
 */
TestReplicationSupervisor (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/replication/TestReplicationSupervisor.java)/**
 * Test the replication supervisor.
 */
BlockDeletingServiceTestImpl (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/testutils/BlockDeletingServiceTestImpl.java)/**
 * A test class implementation for {@link BlockDeletingService}.
 */
TestHddsDatanodeService (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/TestHddsDatanodeService.java)/**
 * Test class for {@link HddsDatanodeService}.
 */
TestHddsSecureDatanodeInit (/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/TestHddsSecureDatanodeInit.java)/**
 * Test class for {@link HddsDatanodeService}.
 */
BaseHttpServer (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java)/**
 * Base class for HTTP server of the Ozone related components.
 */
Event (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/events/Event.java)/**
 * Identifier of an async event.
 *
 * @param <PAYLOAD> THe message payload type of this event.
 */
EventExecutor (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/events/EventExecutor.java)/**
 * Executors defined the  way how an EventHandler should be called.
 * <p>
 * Executors are used only by the EventQueue and they do the thread separation
 * between the caller and the EventHandler.
 * <p>
 * Executors should guarantee that only one thread is executing one
 * EventHandler at the same time.
 *
 * @param <PAYLOAD> the payload type of the event.
 */
EventHandler (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/events/EventHandler.java)/**
 * Processor to react on an event.
 *
 * EventExecutors should guarantee that the implementations are called only
 * from one thread.
 *
 * @param <PAYLOAD>
 */
EventPublisher (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/events/EventPublisher.java)/**
 * Client interface to send a new event.
 */
EventQueue (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/events/EventQueue.java)/**
 * Simple async event processing utility.
 * <p>
 * Event queue handles a collection of event handlers and routes the incoming
 * events to one (or more) event handler.
 */
EventWatcher (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/events/EventWatcher.java)/**
 * Event watcher the (re)send a message after timeout.
 * <p>
 * Event watcher will send the tracked payload/event after a timeout period
 * unless a confirmation from the original event (completion event) is arrived.
 *
 * @param <TIMEOUT_PAYLOAD>    The type of the events which are tracked.
 * @param <COMPLETION_PAYLOAD> The type of event which could cancel the
 *                             tracking.
 */
EventWatcherMetrics (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/events/EventWatcherMetrics.java)/**
 * Metrics for any event watcher.
 */
IdentifiableEventPayload (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/events/IdentifiableEventPayload.java)/**
 * Event with an additional unique identifier.
 *
 */
SingleThreadExecutor (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/events/SingleThreadExecutor.java)/**
 * Simple EventExecutor to call all the event handler one-by-one.
 *
 * @param <T>
 */
TypedEvent (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/events/TypedEvent.java)/**
 * Basic event implementation to implement custom events.
 *
 * @param <T>
 */
LogStreamServlet (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/LogStreamServlet.java)/**
 * Servlet to stream the current logs to the response.
 */
OzoneProtocolMessageDispatcher (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/OzoneProtocolMessageDispatcher.java)/**
 * Dispatch message after tracing and message logging for insight.
 * <p>
 * This is a generic utility to dispatch message in ServerSide translators.
 * <p>
 * It logs the message type/content on DEBUG/TRACING log for insight and create
 * a new span based on the tracing information.
 */
ProfileServlet (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/ProfileServlet.java)/**
 * Servlet that runs async-profiler as web-endpoint.
 * <p>
 * Source: https://github.com/apache/hive/blob/master/common/src/java/org
 * /apache/hive/http/ProfileServlet.java
 * <p>
 * Following options from async-profiler can be specified as query paramater.
 * //  -e event          profiling event: cpu|alloc|lock|cache-misses etc.
 * //  -d duration       run profiling for <duration> seconds (integer)
 * //  -i interval       sampling interval in nanoseconds (long)
 * //  -j jstackdepth    maximum Java stack depth (integer)
 * //  -b bufsize        frame buffer size (long)
 * //  -t                profile different threads separately
 * //  -s                simple class names instead of FQN
 * //  -o fmt[,fmt...]   output format:
 * summary|traces|flat|collapsed|svg|tree|jfr
 * //  --width px        SVG width pixels (integer)
 * //  --height px       SVG frame height pixels (integer)
 * //  --minwidth px     skip frames smaller than px (double)
 * //  --reverse         generate stack-reversed FlameGraph / Call tree
 * Example:
 * - To collect 30 second CPU profile of current process (returns FlameGraph
 * svg)
 * curl "http://localhost:10002/prof"
 * - To collect 1 minute CPU profile of current process and output in tree
 * format (html)
 * curl "http://localhost:10002/prof?output=tree&duration=60"
 * - To collect 30 second heap allocation profile of current process (returns
 * FlameGraph svg)
 * curl "http://localhost:10002/prof?event=alloc"
 * - To collect lock contention profile of current process (returns
 * FlameGraph svg)
 * curl "http://localhost:10002/prof?event=lock"
 * Following event types are supported (default is 'cpu') (NOTE: not all
 * OS'es support all events)
 * // Perf events:
 * //    cpu
 * //    page-faults
 * //    context-switches
 * //    cycles
 * //    instructions
 * //    cache-references
 * //    cache-misses
 * //    branches
 * //    branch-misses
 * //    bus-cycles
 * //    L1-dcache-load-misses
 * //    LLC-load-misses
 * //    dTLB-load-misses
 * //    mem:breakpoint
 * //    trace:tracepoint
 * // Java events:
 * //    alloc
 * //    lock
 */
PrometheusMetricsSink (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/PrometheusMetricsSink.java)/**
 * Metrics sink for prometheus exporter.
 * <p>
 * Stores the metric data in-memory and return with it on request.
 */
PrometheusServlet (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/PrometheusServlet.java)/**
 * Servlet to publish hadoop metrics in prometheus format.
 */
ServerUtils (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/ServerUtils.java)/**
 * Generic utilities for all HDDS/Ozone servers.
 */
ServiceRuntimeInfo (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/ServiceRuntimeInfo.java)/**
 * Common runtime information for any service components.
 *
 * Note: it's intentional to not use MXBean or MBean as a suffix  of the name.
 *
 * Most of the services extends the ServiceRuntimeInfoImpl class and also
 * implements a specific MXBean interface which extends this interface.
 *
 * This inheritance from multiple path could confuse the jmx system and
 * some jmx properties could be disappeared.
 *
 * The solution is to always extend this interface and use the jmx naming
 * convention in the new interface..
 */
ServiceRuntimeInfoImpl (/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/ServiceRuntimeInfoImpl.java)/**
 * Helper base class to report the standard version and runtime information.
 *
 */
EventHandlerStub (/hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/server/events/EventHandlerStub.java)/**
 * Dummy class for testing to collect all the received events.
 */
TestEventQueue (/hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/server/events/TestEventQueue.java)/**
 * Testing the basic functionality of the event queue.
 */
TestEventQueueChain (/hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/server/events/TestEventQueueChain.java)/**
 * More realistic event test with sending event from one listener.
 */
TestEventWatcher (/hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/server/events/TestEventWatcher.java)/**
 * Test the basic functionality of event watcher.
 */
TestBaseHttpServer (/hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/server/TestBaseHttpServer.java)/**
 * Test Common ozone/hdds web methods.
 */
TestProfileServlet (/hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/server/TestProfileServlet.java)/**
 * Test prometheus Sink.
 */
TestMetrics (/hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/server/TestPrometheusMetricsSink.java)/**
   * Example metric pojo.
   */
TestPrometheusMetricsSink (/hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/server/TestPrometheusMetricsSink.java)/**
 * Test prometheus Sink.
 */
TestServerUtils (/hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/server/TestServerUtils.java)/**
 * Unit tests for {@link ServerUtils}.
 */
BlockManager (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/BlockManager.java)/**
 *
 *  Block APIs.
 *  Container is transparent to these APIs.
 */
BlockManagerImpl (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/BlockManagerImpl.java)/** Block Manager manages the block access for SCM. */
BlockmanagerMXBean (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/BlockmanagerMXBean.java)/**
 * JMX interface for the block manager.
 */
DatanodeDeletedBlockTransactions (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/DatanodeDeletedBlockTransactions.java)/**
 * A wrapper class to hold info about datanode and all deleted block
 * transactions that will be sent to this datanode.
 */
DeletedBlockLog (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/DeletedBlockLog.java)/**
 * The DeletedBlockLog is a persisted log in SCM to keep tracking
 * container blocks which are under deletion. It maintains info
 * about under-deletion container blocks that notified by OM,
 * and the state how it is processed.
 */
DeletedBlockLogImpl (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/DeletedBlockLogImpl.java)/**
 * A implement class of {@link DeletedBlockLog}, and it uses
 * K/V db to maintain block deletion transactions between scm and datanode.
 * This is a very basic implementation, it simply scans the log and
 * memorize the position that scanned by last time, and uses this to
 * determine where the next scan starts. It has no notion about weight
 * of each transaction so as long as transaction is still valid, they get
 * equally same chance to be retrieved which only depends on the nature
 * order of the transaction ID.
 */
PendingDeleteHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/PendingDeleteHandler.java)/**
 * Event handler for PedingDeleteStatuList events.
 */
PendingDeleteStatus (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/PendingDeleteStatusList.java)/**
   * Status of pending deletes.
   */
PendingDeleteStatusList (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/PendingDeleteStatusList.java)/**
 * Pending Deletes in the block space.
 */
SCMBlockDeletingService (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/SCMBlockDeletingService.java)/**
 * A background service running in SCM to delete blocks. This service scans
 * block deletion log in certain interval and caches block deletion commands
 * in {@link org.apache.hadoop.hdds.scm.node.CommandQueue}, asynchronously
 * SCM HB thread polls cached commands and sends them to datanode for physical
 * processing.
 */
CommandStatusEvent (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/command/CommandStatusReportHandler.java)/**
   * Wrapper event for CommandStatus.
   */
DeleteBlockStatus (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/command/CommandStatusReportHandler.java)/**
   * Wrapper event for DeleteBlock Command.
   */
CommandStatusReportHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/command/CommandStatusReportHandler.java)/**
 * Handles CommandStatusReports from datanode.
 */
AbstractContainerReportHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/AbstractContainerReportHandler.java)/**
 * Base class for all the container report handlers.
 */
CloseContainerEventHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/CloseContainerEventHandler.java)/**
 * In case of a node failure, volume failure, volume out of spapce, node
 * out of space etc, CLOSE_CONTAINER will be triggered.
 * CloseContainerEventHandler is the handler for CLOSE_CONTAINER.
 * When a close container event is fired, a close command for the container
 * should be sent to all the datanodes in the pipeline and containerStateManager
 * needs to update the container state to Closing.
 */
ContainerActionsHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ContainerActionsHandler.java)/**
 * Handles container reports from datanode.
 */
ContainerManager (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ContainerManager.java)/**
 * ContainerManager class contains the mapping from a name to a pipeline
 * mapping. This is used by SCM when allocating new locations and when
 * looking up a key.
 */
ContainerReplicaBuilder (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ContainerReplica.java)/**
   * Used for building ContainerReplica instance.
   */
ContainerReplica (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ContainerReplica.java)/**
 * In-memory state of a container replica.
 */
ContainerReportHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ContainerReportHandler.java)/**
 * Handles container reports from datanode.
 */
ContainerStateManager (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ContainerStateManager.java)/**
 * A container state manager keeps track of container states and returns
 * containers that match various queries.
 * <p>
 * This state machine is driven by a combination of server and client actions.
 * <p>
 * This is how a create container happens: 1. When a container is created, the
 * Server(or SCM) marks that Container as ALLOCATED state. In this state, SCM
 * has chosen a pipeline for container to live on. However, the container is not
 * created yet. This container along with the pipeline is returned to the
 * client.
 * <p>
 * 2. The client when it sees the Container state as ALLOCATED understands that
 * container needs to be created on the specified pipeline. The client lets the
 * SCM know that saw this flag and is initiating the on the data nodes.
 * <p>
 * This is done by calling into notifyObjectCreation(ContainerName,
 * BEGIN_CREATE) flag. When SCM gets this call, SCM puts the container state
 * into CREATING. All this state means is that SCM told Client to create a
 * container and client saw that request.
 * <p>
 * 3. Then client makes calls to datanodes directly, asking the datanodes to
 * create the container. This is done with the help of pipeline that supports
 * this container.
 * <p>
 * 4. Once the creation of the container is complete, the client will make
 * another call to the SCM, this time specifying the containerName and the
 * COMPLETE_CREATE as the Event.
 * <p>
 * 5. With COMPLETE_CREATE event, the container moves to an Open State. This is
 * the state when clients can write to a container.
 * <p>
 * 6. If the client does not respond with the COMPLETE_CREATE event with a
 * certain time, the state machine times out and triggers a delete operation of
 * the container.
 * <p>
 * Please see the function initializeStateMachine below to see how this looks in
 * code.
 * <p>
 * Reusing existing container :
 * <p>
 * The create container call is not made all the time, the system tries to use
 * open containers as much as possible. So in those cases, it looks thru the
 * list of open containers and will return containers that match the specific
 * signature.
 * <p>
 * Please note : Logically there are 3 separate state machines in the case of
 * containers.
 * <p>
 * The Create State Machine -- Commented extensively above.
 * <p>
 * Open/Close State Machine - Once the container is in the Open State,
 * eventually it will be closed, once sufficient data has been written to it.
 * <p>
 * TimeOut Delete Container State Machine - if the container creating times out,
 * then Container State manager decides to delete the container.
 */
IncrementalContainerReportHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/IncrementalContainerReportHandler.java)/**
 * Handles incremental container reports from datanode.
 */
SCMContainerManagerMetrics (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/metrics/SCMContainerManagerMetrics.java)/**
 * Class contains metrics related to ContainerManager.
 */
ContainerPlacementPolicyFactory (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/ContainerPlacementPolicyFactory.java)/**
 * A factory to create container placement instance based on configuration
 * property ozone.scm.container.placement.classname.
 */
SCMCommonPolicy (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMCommonPolicy.java)/**
 * SCM CommonPolicy implements a set of invariants which are common
 * for all container placement policies, acts as the repository of helper
 * functions which are common to placement policies.
 */
SCMContainerPlacementCapacity (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMContainerPlacementCapacity.java)/**
 * Container placement policy that randomly choose datanodes with remaining
 * space to satisfy the size constraints.
 * <p>
 * The Algorithm is as follows, Pick 2 random nodes from a given pool of nodes
 * and then pick the node which lower utilization. This leads to a higher
 * probability of nodes with lower utilization to be picked.
 * <p>
 * For those wondering why we choose two nodes randomly and choose the node
 * with lower utilization. There are links to this original papers in
 * HDFS-11564.
 * <p>
 * A brief summary -- We treat the nodes from a scale of lowest utilized to
 * highest utilized, there are (s * ( s + 1)) / 2 possibilities to build
 * distinct pairs of nodes.  There are s - k pairs of nodes in which the rank
 * k node is less than the couple. So probability of a picking a node is
 * (2 * (s -k)) / (s * (s - 1)).
 * <p>
 * In English, There is a much higher probability of picking less utilized nodes
 * as compared to nodes with higher utilization since we pick 2 nodes and
 * then pick the node with lower utilization.
 * <p>
 * This avoids the issue of users adding new nodes into the cluster and HDFS
 * sending all traffic to those nodes if we only use a capacity based
 * allocation scheme. Unless those nodes are part of the set of the first 2
 * nodes then newer nodes will not be in the running to get the container.
 * <p>
 * This leads to an I/O pattern where the lower utilized nodes are favoured
 * more than higher utilized nodes, but part of the I/O will still go to the
 * older higher utilized nodes.
 * <p>
 * With this algorithm in place, our hope is that balancer tool needs to do
 * little or no work and the cluster will achieve a balanced distribution
 * over time.
 */
SCMContainerPlacementMetrics (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMContainerPlacementMetrics.java)/**
 * This class is for maintaining Topology aware container placement statistics.
 */
SCMContainerPlacementRackAware (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMContainerPlacementRackAware.java)/**
 * Container placement policy that choose datanodes with network topology
 * awareness, together with the space to satisfy the size constraints.
 * <p>
 * This placement policy complies with the algorithm used in HDFS. With default
 * 3 replica, two replica will be on the same rack, the third one will on a
 * different rack.
 * <p>
 * This implementation applies to network topology like "/rack/node". Don't
 * recommend to use this if the network topology has more layers.
 * <p>
 */
SCMContainerPlacementRandom (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMContainerPlacementRandom.java)/**
 * Container placement policy that randomly chooses healthy datanodes.
 * This is very similar to current HDFS placement. That is we
 * just randomly place containers without any considerations of utilization.
 * <p>
 * That means we rely on balancer to achieve even distribution of data.
 * Balancer will need to support containers as a feature before this class
 * can be practically used.
 */
ContainerStat (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/metrics/ContainerStat.java)/**
 * This class represents the SCM container stat.
 */
DatanodeMetric (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/metrics/DatanodeMetric.java)/**
 * DatanodeMetric acts as the basis for all the metric that is used in
 * comparing 2 datanodes.
 */
LongMetric (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/metrics/LongMetric.java)/**
 * An helper class for all metrics based on Longs.
 */
NodeStat (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/metrics/NodeStat.java)/**
 * Interface that defines Node Stats.
 */
SCMMetrics (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/metrics/SCMMetrics.java)/**
 * This class is for maintaining StorageContainerManager statistics.
 */
SCMNodeMetric (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/metrics/SCMNodeMetric.java)/**
 * SCM Node Metric that is used in the placement classes.
 */
SCMNodeStat (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/metrics/SCMNodeStat.java)/**
 * This class represents the SCM node stat.
 */
ReplicationActivityStatus (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/replication/ReplicationActivityStatus.java)/**
 * Event listener to track the current state of replication.
 */
ReplicationActivityStatusMXBean (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/replication/ReplicationActivityStatusMXBean.java)/**
 * JMX interface to monitor replication status.
 */
InflightAction (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java)/**
   * Wrapper class to hold the InflightAction with its start time.
   */
ReplicationManagerConfiguration (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java)/**
   * Configuration used by the Replication Manager.
   */
ReplicationManager (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java)/**
 * Replication Manager (RM) is the one which is responsible for making sure
 * that the containers are properly replicated. Replication Manager deals only
 * with Quasi Closed / Closed container.
 */
SCMContainerManager (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/SCMContainerManager.java)/**
 * ContainerManager class contains the mapping from a name to a pipeline
 * mapping. This is used by SCM when allocating new locations and when
 * looking up a key.
 */
ContainerAttribute (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/states/ContainerAttribute.java)/**
 * Each Attribute that we manage for a container is maintained as a map.
 * <p>
 * Currently we manage the following attributes for a container.
 * <p>
 * 1. StateMap - LifeCycleState -> Set of ContainerIDs
 * 2. TypeMap  - ReplicationType -> Set of ContainerIDs
 * 3. OwnerMap - OwnerNames -> Set of ContainerIDs
 * 4. FactorMap - ReplicationFactor -> Set of ContainerIDs
 * <p>
 * This means that for a cluster size of 750 PB -- we will have around 150
 * Million containers, if we assume 5GB average container size.
 * <p>
 * That implies that these maps will take around 2/3 GB of RAM which will be
 * pinned down in the SCM. This is deemed acceptable since we can tune the
 * container size --say we make it 10GB average size, then we can deal with a
 * cluster size of 1.5 exa bytes with the same metadata in SCMs memory.
 * <p>
 * Please note: **This class is not thread safe**. This used to be thread safe,
 * while bench marking we found that ContainerStateMap would be taking 5
 * locks for a single container insert. If we remove locks in this class,
 * then we are able to perform about 540K operations per second, with the
 * locks in this class it goes down to 246K operations per second. Hence we
 * are going to rely on ContainerStateMap locks to maintain consistency of
 * data in these classes too, since ContainerAttribute is only used by
 * ContainerStateMap class.
 */
ContainerQueryKey (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/states/ContainerQueryKey.java)/**
 * Key for the Caching layer for Container Query.
 */
ContainerState (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/states/ContainerState.java)/**
 * Class that acts as the container state.
 */
ContainerStateMap (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/states/ContainerStateMap.java)/**
 * Container State Map acts like a unified map for various attributes that are
 * used to select containers when we need allocated blocks.
 * <p>
 * This class provides the ability to query 5 classes of attributes. They are
 * <p>
 * 1. LifeCycleStates - LifeCycle States of container describe in which state
 * a container is. For example, a container needs to be in Open State for a
 * client to able to write to it.
 * <p>
 * 2. Owners - Each instance of Name service, for example, Namenode of HDFS or
 * Ozone Manager (OM) of Ozone or CBlockServer --  is an owner. It is
 * possible to have many OMs for a Ozone cluster and only one SCM. But SCM
 * keeps the data from each OM in separate bucket, never mixing them. To
 * write data, often we have to find all open containers for a specific owner.
 * <p>
 * 3. ReplicationType - The clients are allowed to specify what kind of
 * replication pipeline they want to use. Each Container exists on top of a
 * pipeline, so we need to get ReplicationType that is specified by the user.
 * <p>
 * 4. ReplicationFactor - The replication factor represents how many copies
 * of data should be made, right now we support 2 different types, ONE
 * Replica and THREE Replica. User can specify how many copies should be made
 * for a ozone key.
 * <p>
 * The most common access pattern of this class is to select a container based
 * on all these parameters, for example, when allocating a block we will
 * select a container that belongs to user1, with Ratis replication which can
 * make 3 copies of data. The fact that we will look for open containers by
 * default and if we cannot find them we will add new containers.
 */
SCMEvents (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/events/SCMEvents.java)/**
 * Class that acts as the namespace for all SCM Events.
 */
BigIntegerCodec (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/metadata/BigIntegerCodec.java)/**
 * Encode and decode BigInteger.
 */
DeletedBlocksTransactionCodec (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/metadata/DeletedBlocksTransactionCodec.java)/**
 * Codec for Persisting the DeletedBlocks.
 */
LongCodec (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/metadata/LongCodec.java)/**
 * Codec for Persisting the DeletedBlocks.
 */
SCMMetadataStore (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/metadata/SCMMetadataStore.java)/**
 * Generic interface for data stores for SCM.
 * This is similar to the OMMetadataStore class,
 * where we write classes into some underlying storage system.
 */
SCMMetadataStoreRDBImpl (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/metadata/SCMMetadataStoreRDBImpl.java)/**
 * A RocksDB based implementation of SCM Metadata Store.
 * <p>
 * <p>
 * +---------------+------------------+-------------------------+
 * | Column Family |    Key           |          Value          |
 * +---------------+------------------+-------------------------+
 * | DeletedBlocks | TXID(Long)       | DeletedBlockTransaction |
 * +---------------+------------------+-------------------------+
 * | ValidCerts    | Serial (BigInt)  | X509Certificate         |
 * +---------------+------------------+-------------------------+
 * |RevokedCerts   | Serial (BigInt)  | X509Certificate         |
 * +---------------+------------------+-------------------------+
 */
X509CertificateCodec (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/metadata/X509CertificateCodec.java)/**
 * Encodes and Decodes X509Certificate Class.
 */
Commands (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/CommandQueue.java)/**
   * Class that stores commands for a datanode.
   */
CommandQueue (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/CommandQueue.java)/**
 * Command Queue is queue of commands for the datanode.
 * <p>
 * Node manager, container Manager and Ozone managers can queue commands for
 * datanodes into this queue. These commands will be send in the order in which
 * there where queued.
 */
DatanodeInfo (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/DatanodeInfo.java)/**
 * This class extends the primary identifier of a Datanode with ephemeral
 * state, eg last reported time, usage information etc.
 */
DeadNodeHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/DeadNodeHandler.java)/**
 * Handles Dead Node event.
 */
NewNodeHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/NewNodeHandler.java)/**
 * Handles New Node event.
 */
NodeManager (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/NodeManager.java)/**
 * A node manager supports a simple interface for managing a datanode.
 * <p>
 * 1. A datanode registers with the NodeManager.
 * <p>
 * 2. If the node is allowed to register, we add that to the nodes that we need
 * to keep track of.
 * <p>
 * 3. A heartbeat is made by the node at a fixed frequency.
 * <p>
 * 4. A node can be in any of these 4 states: {HEALTHY, STALE, DEAD,
 * DECOMMISSIONED}
 * <p>
 * HEALTHY - It is a datanode that is regularly heartbeating us.
 *
 * STALE - A datanode for which we have missed few heart beats.
 *
 * DEAD - A datanode that we have not heard from for a while.
 *
 * DECOMMISSIONED - Someone told us to remove this node from the tracking
 * list, by calling removeNode. We will throw away this nodes info soon.
 */
NodeManagerMXBean (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/NodeManagerMXBean.java)/**
 *
 * This is the JMX management interface for node manager information.
 */
NodeReportHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/NodeReportHandler.java)/**
 * Handles Node Reports from datanode.
 */
NodeStateManager (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/NodeStateManager.java)/**
 * NodeStateManager maintains the state of all the datanodes in the cluster. All
 * the node state change should happen only via NodeStateManager. It also
 * runs a heartbeat thread which periodically updates the node state.
 * <p>
 * The getNode(byState) functions make copy of node maps and then creates a list
 * based on that. It should be assumed that these get functions always report
 * *stale* information. For example, getting the deadNodeCount followed by
 * getNodes(DEAD) could very well produce totally different count. Also
 * getNodeCount(HEALTHY) + getNodeCount(DEAD) + getNodeCode(STALE), is not
 * guaranteed to add up to the total nodes that we know off. Please treat all
 * get functions in this file as a snap-shot of information that is inconsistent
 * as soon as you read it.
 */
NonHealthyToHealthyNodeHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/NonHealthyToHealthyNodeHandler.java)/**
 * Handles Stale node event.
 */
SCMNodeManager (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java)/**
 * Maintains information about the Datanodes on SCM side.
 * <p>
 * Heartbeats under SCM is very simple compared to HDFS heartbeatManager.
 * <p>
 * The getNode(byState) functions make copy of node maps and then creates a list
 * based on that. It should be assumed that these get functions always report
 * *stale* information. For example, getting the deadNodeCount followed by
 * getNodes(DEAD) could very well produce totally different count. Also
 * getNodeCount(HEALTHY) + getNodeCount(DEAD) + getNodeCode(STALE), is not
 * guaranteed to add up to the total nodes that we know off. Please treat all
 * get functions in this file as a snap-shot of information that is inconsistent
 * as soon as you read it.
 */
SCMNodeMetrics (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeMetrics.java)/**
 * This class maintains Node related metrics.
 */
SCMNodeStorageStatMap (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeStorageStatMap.java)/**
 * This data structure maintains the disk space capacity, disk usage and free
 * space availability per Datanode.
 * This information is built from the DN node reports.
 */
SCMNodeStorageStatMXBean (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeStorageStatMXBean.java)/**
 *
 * This is the JMX management interface for node manager information.
 */
StaleNodeHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/StaleNodeHandler.java)/**
 * Handles Stale node event.
 */
Node2ContainerMap (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/states/Node2ContainerMap.java)/**
 * This data structure maintains the list of containers that is on a datanode.
 * This information is built from the DN container reports.
 */
Node2ObjectsMap (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/states/Node2ObjectsMap.java)/**
 * This data structure maintains the list of containers that is on a datanode.
 * This information is built from the DN container reports.
 */
Node2PipelineMap (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/states/Node2PipelineMap.java)/**
 * This data structure maintains the list of pipelines which the given
 * datanode is a part of. This information will be added whenever a new
 * pipeline allocation happens.
 *
 * <p>TODO: this information needs to be regenerated from pipeline reports
 * on SCM restart
 */
NodeAlreadyExistsException (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/states/NodeAlreadyExistsException.java)/**
 * This exception represents that there is already a node added to NodeStateMap
 * with same UUID.
 */
NodeException (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/states/NodeException.java)/**
 * This exception represents all node related exceptions in NodeStateMap.
 */
NodeNotFoundException (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/states/NodeNotFoundException.java)/**
 * This exception represents that the node that is being accessed does not
 * exist in NodeStateMap.
 */
NodeStateMap (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/states/NodeStateMap.java)/**
 * Maintains the state of datanodes in SCM. This class should only be used by
 * NodeStateManager to maintain the state. If anyone wants to change the
 * state of a node they should call NodeStateManager, do not directly use
 * this class.
 */
ReportResultBuilder (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/states/ReportResult.java)/**
   * Result after processing report for node2Object map.
   * @param <T>
   */
ReportResult (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/states/ReportResult.java)/**
 * A Container/Pipeline Report gets processed by the
 * Node2Container/Node2Pipeline and returns Report Result class.
 */
StorageReportResult (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/StorageReportResult.java)/**
 * A Container Report gets processsed by the Node2Container and returns the
 * Report Result class.
 */
BackgroundPipelineCreator (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/BackgroundPipelineCreator.java)/**
 * Implements api for running background pipeline creation jobs.
 */
InsufficientDatanodesException (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/InsufficientDatanodesException.java)/**
 * Exception thrown when there are not enough Datanodes to create a pipeline.
 */
PipelineActionHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineActionHandler.java)/**
 * Handles pipeline actions from datanode.
 */
PipelineFactory (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineFactory.java)/**
 * Creates pipeline based on replication type.
 */
PipelineManager (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineManager.java)/**
 * Interface which exposes the api for pipeline management.
 */
PipelineManagerMXBean (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineManagerMXBean.java)/**
 * This is the JMX management interface for information related to
 * PipelineManager.
 */
PipelineProvider (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineProvider.java)/**
 * Interface for creating pipelines.
 */
PipelineReportHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineReportHandler.java)/**
 * Handles Pipeline Reports from datanode.
 */
PipelineStateManager (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateManager.java)/**
 * Manages the state of pipelines in SCM. All write operations like pipeline
 * creation, removal and updates should come via SCMPipelineManager.
 * PipelineStateMap class holds the data structures related to pipeline and its
 * state. All the read and write operations in PipelineStateMap are protected
 * by a read write lock.
 */
PipelineStateMap (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelineStateMap.java)/**
 * Holds the data structures which maintain the information about pipeline and
 * its state.
 * Invariant: If a pipeline exists in PipelineStateMap, both pipelineMap and
 * pipeline2container would have a non-null mapping for it.
 */
RatisPipelineProvider (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/RatisPipelineProvider.java)/**
 * Implements Api for creating ratis pipelines.
 */
RatisPipelineUtils (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/RatisPipelineUtils.java)/**
 * Utility class for Ratis pipelines. Contains methods to create and destroy
 * ratis pipelines.
 */
SCMPipelineManager (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/SCMPipelineManager.java)/**
 * Implements api needed for management of pipelines. All the write operations
 * for pipelines must come via PipelineManager. It synchronises all write
 * and read operations via a ReadWriteLock.
 */
SCMPipelineMetrics (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/SCMPipelineMetrics.java)/**
 * This class maintains Pipeline related metrics.
 */
SimplePipelineProvider (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/SimplePipelineProvider.java)/**
 * Implements Api for creating stand alone pipelines.
 */
ScmBlockLocationProtocolServerSideTranslatorPB (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/protocol/ScmBlockLocationProtocolServerSideTranslatorPB.java)/**
 * This class is the server-side translator that forwards requests received on
 * {@link StorageContainerLocationProtocolPB} to the
 * {@link StorageContainerLocationProtocol} server implementation.
 */
SCMSecurityProtocolServerSideTranslatorPB (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/protocol/SCMSecurityProtocolServerSideTranslatorPB.java)/**
 * This class is the server-side translator that forwards requests received on
 * {@link SCMSecurityProtocolPB} to the {@link
 * SCMSecurityProtocol} server implementation.
 */
StorageContainerLocationProtocolServerSideTranslatorPB (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/protocol/StorageContainerLocationProtocolServerSideTranslatorPB.java)/**
 * This class is the server-side translator that forwards requests received on
 * {@link StorageContainerLocationProtocolPB} to the
 * {@link StorageContainerLocationProtocol} server implementation.
 */
ContainerSafeModeRule (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/ContainerSafeModeRule.java)/**
 * Class defining Safe mode exit criteria for Containers.
 */
DataNodeSafeModeRule (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/DataNodeSafeModeRule.java)/**
 * Class defining Safe mode exit criteria according to number of DataNodes
 * registered with SCM.
 */
HealthyPipelineSafeModeRule (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/HealthyPipelineSafeModeRule.java)/**
 * Class defining Safe mode exit criteria for Pipelines.
 *
 * This rule defines percentage of healthy pipelines need to be reported.
 * Once safe mode exit happens, this rules take care of writes can go
 * through in a cluster.
 */
OneReplicaPipelineSafeModeRule (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/OneReplicaPipelineSafeModeRule.java)/**
 * This rule covers whether we have at least one datanode is reported for each
 * pipeline. This rule is for all open containers, we have at least one
 * replica available for read when we exit safe mode.
 */
Precheck (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/Precheck.java)/**
 * Precheck for SCM operations.
 * */
SafeModeExitRule (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/SafeModeExitRule.java)/**
 * Abstract class for SafeModeExitRules. When a new rule is added, the new
 * rule should extend this abstract class.
 *
 * Each rule Should do:
 * 1. Should add a handler for the event it is looking for during the
 * initialization of the rule.
 * 2. Add the rule in ScmSafeModeManager to list of the rules.
 *
 *
 * @param <T>
 */
SafeModeHandler (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/SafeModeHandler.java)/**
 * Class to handle the activities needed to be performed after exiting safe
 * mode.
 */
SafeModeMetrics (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/SafeModeMetrics.java)/**
 * This class is used for maintaining SafeMode metric information, which can
 * be used for monitoring during SCM startup when SCM is still in SafeMode.
 */
SafeModePrecheck (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/SafeModePrecheck.java)/**
 * Safe mode pre-check for SCM operations.
 * */
SafeModeRestrictedOps (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/SafeModeRestrictedOps.java)/**
 * Operations restricted in SCM safe mode.
 */
SafeModeStatus (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/SCMSafeModeManager.java)/**
   * Class used during SafeMode status event.
   */
SCMSafeModeManager (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/SCMSafeModeManager.java)/**
 * StorageContainerManager enters safe mode on startup to allow system to
 * reach a stable state before becoming fully functional. SCM will wait
 * for certain resources to be reported before coming out of safe mode.
 *
 * SafeModeExitRule defines format to define new rules which must be satisfied
 * to exit Safe mode.
 *
 * Current SafeMode rules:
 * 1. ContainerSafeModeRule:
 * On every new datanode registration, it fires
 * {@link SCMEvents#NODE_REGISTRATION_CONT_REPORT}.  This rule handles this
 * event. This rule process this report, increment the
 * containerWithMinReplicas count when this reported replica is in the
 * containerMap. Then validates if cutoff threshold for containers is meet.
 *
 * 2. DatanodeSafeModeRule:
 * On every new datanode registration, it fires
 * {@link SCMEvents#NODE_REGISTRATION_CONT_REPORT}. This rule handles this
 * event. This rule process this report, and check if this is new node, add
 * to its reported node list. Then validate it cutoff threshold for minimum
 * number of datanode registered is met or not.
 *
 * 3. HealthyPipelineSafeModeRule:
 * Once the pipelineReportHandler processes the
 * {@link SCMEvents#PIPELINE_REPORT}, it fires
 * {@link SCMEvents#PROCESSED_PIPELINE_REPORT}. This rule handles this
 * event. This rule processes this report, and check if pipeline is healthy
 * and increments current healthy pipeline count. Then validate it cutoff
 * threshold for healthy pipeline is met or not.
 *
 * 4. OneReplicaPipelineSafeModeRule:
 * Once the pipelineReportHandler processes the
 * {@link SCMEvents#PIPELINE_REPORT}, it fires
 * {@link SCMEvents#PROCESSED_PIPELINE_REPORT}. This rule handles this
 * event. This rule processes this report, and add the reported pipeline to
 * reported pipeline set. Then validate it cutoff threshold for one replica
 * per pipeline is met or not.
 *
 */
ScmUtils (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ScmUtils.java)/**
 * SCM utility class.
 */
SCMBlockProtocolServer (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMBlockProtocolServer.java)/**
 * SCM block protocol is the protocol used by Namenode and OzoneManager to get
 * blocks from the SCM.
 */
SCMCertStore (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMCertStore.java)/**
 * A Certificate Store class that persists certificates issued by SCM CA.
 */
SCMClientProtocolServer (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMClientProtocolServer.java)/**
 * The RPC server that listens to requests from clients.
 */
SCMConfigurator (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMConfigurator.java)/**
 * This class acts as an SCM builder Class. This class is important for us
 * from a resilience perspective of SCM. This class will allow us swap out
 * different managers and replace with out on manager in the testing phase.
 * <p>
 * At some point in the future, we will make all these managers dynamically
 * loadable, so other developers can extend SCM by replacing various managers.
 * <p>
 * TODO: Add different config keys, so that we can load different managers at
 * run time. This will make it easy to extend SCM without having to replace
 * whole SCM each time.
 * <p>
 * Different Managers supported by this builder are:
 * NodeManager scmNodeManager;
 * PipelineManager pipelineManager;
 * ContainerManager containerManager;
 * BlockManager scmBlockManager;
 * ReplicationManager replicationManager;
 * SCMSafeModeManager scmSafeModeManager;
 * CertificateServer certificateServer;
 * SCMMetadata scmMetadataStore.
 *
 * If any of these are *not* specified then the default version of these
 * managers are used by SCM.
 *
 */
SCMContainerMetrics (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMContainerMetrics.java)/**
 * Metrics source to report number of containers in different states.
 */
ReportFromDatanode (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeHeartbeatDispatcher.java)/**
   * Wrapper class for events with the datanode origin.
   */
NodeReportFromDatanode (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeHeartbeatDispatcher.java)/**
   * Node report event payload with origin.
   */
ContainerReportFromDatanode (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeHeartbeatDispatcher.java)/**
   * Container report event payload with origin.
   */
IncrementalContainerReportFromDatanode (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeHeartbeatDispatcher.java)/**
   * Incremental Container report event payload with origin.
   */
ContainerActionsFromDatanode (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeHeartbeatDispatcher.java)/**
   * Container action event payload with origin.
   */
PipelineReportFromDatanode (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeHeartbeatDispatcher.java)/**
   * Pipeline report event payload with origin.
   */
PipelineActionsFromDatanode (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeHeartbeatDispatcher.java)/**
   * Pipeline action event payload with origin.
   */
CommandStatusReportFromDatanode (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeHeartbeatDispatcher.java)/**
   * Container report event payload with origin.
   */
SCMDatanodeHeartbeatDispatcher (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeHeartbeatDispatcher.java)/**
 * This class is responsible for dispatching heartbeat from datanode to
 * appropriate EventHandler at SCM.
 */
NodeRegistrationContainerReport (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeProtocolServer.java)/**
   * Wrapper class for events with the datanode origin.
   */
SCMDatanodeProtocolServer (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeProtocolServer.java)/**
 * Protocol Handler for Datanode Protocol.
 */
SCMMXBean (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMMXBean.java)/**
 *
 * This is the JMX management interface for scm information.
 */
SCMPolicyProvider (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMPolicyProvider.java)/**
 * {@link PolicyProvider} for SCM protocols.
 */
SCMSecurityProtocolServer (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMSecurityProtocolServer.java)/**
 * The protocol used to perform security related operations with SCM.
 */
SCMStarterInterface (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMStarterInterface.java)/**
 * This interface is used by the StorageContainerManager to allow the
 * dependencies to be injected to the CLI class.
 */
SCMStorageConfig (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMStorageConfig.java)/**
 * SCMStorageConfig is responsible for management of the
 * StorageDirectories used by the SCM.
 */
StorageContainerManager (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java)/**
 * StorageContainerManager is the main entry point for the service that
 * provides information about
 * which SCM nodes host containers.
 *
 * <p>DataNodes report to StorageContainerManager using heartbeat messages.
 * SCM allocates containers
 * and returns a pipeline.
 *
 * <p>A client once it gets a pipeline (a list of datanodes) will connect to
 * the datanodes and create a container, which then can be used to store data.
 */
StorageContainerManagerHttpServer (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManagerHttpServer.java)/**
 * HttpServer2 wrapper for the Ozone Storage Container Manager.
 */
SCMStarterHelper (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManagerStarter.java)/**
   * This static class wraps the external dependencies needed for this command
   * to execute its tasks. This allows the dependency to be injected for unit
   * testing.
   */
RetriableDatanodeEventWatcher (/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/ozone/protocol/commands/RetriableDatanodeEventWatcher.java)/**
 * EventWatcher for start events and completion events with payload of type
 * RetriablePayload and RetriableCompletionPayload respectively.
 */
TestBlockManager (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/block/TestBlockManager.java)/**
 * Tests for SCM Block Manager.
 */
TestDeletedBlockLog (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/block/TestDeletedBlockLog.java)/**
 * Tests for DeletedBlockLog.
 */
TestCommandStatusReportHandler (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/command/TestCommandStatusReportHandler.java)/**
 * Unit test for command status report handler.
 */
NodeData (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/MockNodeManager.java)/**
   * A class to declare some values for the nodes so that our tests
   * won't fail.
   */
MockNodeManager (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/MockNodeManager.java)/**
 * Test Helper for testing container Mapping.
 */
DummyImpl (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/TestContainerPlacementFactory.java)/**
   * A dummy container placement implementation for test.
   */
TestContainerPlacementFactory (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/TestContainerPlacementFactory.java)/**
 * Test for scm container placement factory.
 */
TestSCMContainerPlacementCapacity (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/TestSCMContainerPlacementCapacity.java)/**
 * Test for the scm container placement.
 */
TestSCMContainerPlacementRackAware (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/TestSCMContainerPlacementRackAware.java)/**
 * Test for the scm container rack aware placement.
 */
TestSCMContainerPlacementRandom (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/TestSCMContainerPlacementRandom.java)/**
 * Test for the random container placement.
 */
TestContainerAttribute (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/states/TestContainerAttribute.java)/**
 * Test ContainerAttribute management.
 */
TestCloseContainerEventHandler (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/TestCloseContainerEventHandler.java)/**
 * Tests the closeContainerEventHandler class.
 */
TestContainerActionsHandler (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/TestContainerActionsHandler.java)/**
 * Tests ContainerActionsHandler.
 */
TestContainerReportHandler (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/TestContainerReportHandler.java)/**
 * Test the behaviour of the ContainerReportHandler.
 */
TestContainerStateManager (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/TestContainerStateManager.java)/**
 * Testing ContainerStatemanager.
 */
TestIncrementalContainerReportHandler (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/TestIncrementalContainerReportHandler.java)/**
 * Test cases to verify the functionality of IncrementalContainerReportHandler.
 */
TestReplicationManager (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/TestReplicationManager.java)/**
 * Test cases to verify the functionality of ReplicationManager.
 */
TestSCMContainerManager (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/TestSCMContainerManager.java)/**
 * Tests for Container ContainerManager.
 */
HddsServerUtilTest (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/HddsServerUtilTest.java)/**
 * Test the HDDS server side utilities.
 */
HddsTestUtils (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/HddsTestUtils.java)/**
 * Stateless helper functions for Hdds tests.
 */
HddsWhiteboxTestUtils (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/HddsWhiteboxTestUtils.java)/**
 * This class includes some functions copied from Mockito's
 * Whitebox class for portability reasons.
 *
 * Whitebox methods are accessed differently in different
 * versions of Hadoop. Specifically the availability of the class
 * changed from Apache Hadoop 3.1.0 to Hadoop 3.2.0.
 *
 * Duplicating the test code is ugly but it allows building
 * HDDS portably.
 */
TestNode2ContainerMap (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/states/TestNode2ContainerMap.java)/**
 * Test classes for Node2ContainerMap.
 */
TestContainerPlacement (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestContainerPlacement.java)/**
 * Test for different container placement policy.
 */
TestDeadNodeHandler (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestDeadNodeHandler.java)/**
 * Test DeadNodeHandler.
 */
TestNodeReportHandler (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestNodeReportHandler.java)/**
 * Test for the Node Report Handler.
 */
TestSCMNodeManager (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestSCMNodeManager.java)/**
 * Test the SCM Node Manager class.
 */
TestSCMNodeStorageStatMap (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestSCMNodeStorageStatMap.java)/**
 * Test Node Storage Map.
 */
TestStatisticsUpdate (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestStatisticsUpdate.java)/**
 * Verifies the statics in NodeManager.
 */
MockRatisPipelineProvider (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/pipeline/MockRatisPipelineProvider.java)/**
 * Mock Ratis Pipeline Provider for Mock Nodes.
 */
TestHealthyPipelineSafeModeRule (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/safemode/TestHealthyPipelineSafeModeRule.java)/**
 * This class tests HealthyPipelineSafeMode rule.
 */
TestOneReplicaPipelineSafeModeRule (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/safemode/TestOneReplicaPipelineSafeModeRule.java)/**
 * This class tests OneReplicaPipelineSafeModeRule.
 */
TestSafeModeHandler (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/safemode/TestSafeModeHandler.java)/**
 * Tests SafeModeHandler behavior.
 */
TestSCMSafeModeManager (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/safemode/TestSCMSafeModeManager.java)/** Test class for SCMSafeModeManager.
 */
TestSCMBlockProtocolServer (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestSCMBlockProtocolServer.java)/**
 * Test class for @{@link SCMBlockProtocolServer}.
 */
TestSCMClientProtocolServer (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestSCMClientProtocolServer.java)/**
 * Test class for @{@link SCMClientProtocolServer}.
 * */
TestSCMContainerMetrics (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestSCMContainerMetrics.java)/**
 * Test metrics that represent container states.
 */
TestSCMDatanodeHeartbeatDispatcher (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestSCMDatanodeHeartbeatDispatcher.java)/**
 * This class tests the behavior of SCMDatanodeHeartbeatDispatcher.
 */
TestSCMSecurityProtocolServer (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestSCMSecurityProtocolServer.java)/**
 * Test class for {@link SCMSecurityProtocolServer}.
 * */
TestStorageContainerManagerStarter (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestStorageContainerManagerStarter.java)/**
 * This class is used to test the StorageContainerManagerStarter using a mock
 * class to avoid starting any services and hence just test the CLI component.
 */
TestHddsServerUtils (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/TestHddsServerUtils.java)/**
 * Unit tests for {@link HddsServerUtil}.
 */
TestStorageContainerManagerHttpServer (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/TestStorageContainerManagerHttpServer.java)/**
 * Test http server os SCM with various HTTP option.
 */
TestUtils (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/TestUtils.java)/**
 * Stateless helper functions to handler scm/datanode connection.
 */
TestEndPoint (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/ozone/container/common/TestEndPoint.java)/**
 * Tests the endpoints.
 */
TestContainerPlacement (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/ozone/container/placement/TestContainerPlacement.java)/**
 * Asserts that allocation strategy works as expected.
 */
TestDatanodeMetrics (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/ozone/container/placement/TestDatanodeMetrics.java)/**
 * Tests that test Metrics that support placement.
 */
ReplicationNodeManagerMock (/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/ozone/container/testutils/ReplicationNodeManagerMock.java)/**
 * A Node Manager to test replication.
 */
CloseSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/container/CloseSubcommand.java)/**
 * The handler of close container command.
 */
ContainerCommands (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/container/ContainerCommands.java)/**
 * Subcommand to group container related operations.
 */
CreateSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/container/CreateSubcommand.java)/**
 * This is the handler that process container creation command.
 */
DeleteSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/container/DeleteSubcommand.java)/**
 * This is the handler that process delete container command.
 */
InfoSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/container/InfoSubcommand.java)/**
 * This is the handler that process container info command.
 */
ListSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/container/ListSubcommand.java)/**
 * This is the handler that process container list command.
 */
ActivatePipelineSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/pipeline/ActivatePipelineSubcommand.java)/**
 * Handler of activate pipeline command.
 */
ClosePipelineSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/pipeline/ClosePipelineSubcommand.java)/**
 * Handler of close pipeline command.
 */
DeactivatePipelineSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/pipeline/DeactivatePipelineSubcommand.java)/**
 * Handler of deactivate pipeline command.
 */
ListPipelinesSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/pipeline/ListPipelinesSubcommand.java)/**
 * Handler of list pipelines command.
 */
PipelineCommands (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/pipeline/PipelineCommands.java)/**
 * Subcommand to group pipeline related operations.
 */
ReplicationManagerCommands (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/ReplicationManagerCommands.java)/**
 * Subcommand to group replication manager related operations.
 */
ReplicationManagerStartSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/ReplicationManagerStartSubcommand.java)/**
 * This is the handler that process safe mode check command.
 */
ReplicationManagerStatusSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/ReplicationManagerStatusSubcommand.java)/**
 * This is the handler that process safe mode check command.
 */
ReplicationManagerStopSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/ReplicationManagerStopSubcommand.java)/**
 * This is the handler that process safe mode check command.
 */
SafeModeCheckSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SafeModeCheckSubcommand.java)/**
 * This is the handler that process safe mode check command.
 */
SafeModeCommands (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SafeModeCommands.java)/**
 * Subcommand to group safe mode related operations.
 */
SafeModeExitSubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SafeModeExitSubcommand.java)/**
 * This is the handler that process safe mode exit command.
 */
SCMCLI (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SCMCLI.java)/**
 * Container subcommand.
 */
TopologySubcommand (/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/TopologySubcommand.java)/**
 * Handler of printTopology command.
 */
Null (/hadoop-hdfs-project/hadoop-hdfs/dev-support/jdiff/Null.java)/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
DeprecatedUTF8 (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DeprecatedUTF8.java)/**
 * A simple wrapper around {@link org.apache.hadoop.io.UTF8}.
 * This class should be used only when it is absolutely necessary
 * to use {@link org.apache.hadoop.io.UTF8}. The only difference is that 
 * using this class does not require "@SuppressWarning" annotation to avoid 
 * javac warning. Instead the deprecation is implied in the class name.
 * 
 * This should be treated as package private class to HDFS.
 */
DFSConfigKeys (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java)/** 
 * This class contains constants for configuration keys and default values
 * used in hdfs.
 */
ServiceComparator (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java)/**
   * Comparator for sorting DataNodeInfo[] based on
   * decommissioned and entering_maintenance states.
   */
ServiceAndStaleComparator (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java)/**
   * Comparator for sorting DataNodeInfo[] based on
   * stale, decommissioned and entering_maintenance states.
   * Order: live {@literal ->} stale {@literal ->} entering_maintenance
   * {@literal ->} decommissioned
   */
ConfiguredNNAddress (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java)/**
   * Represent one of the NameNodes configured in the cluster.
   */
HdfsDtFetcher (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HdfsDtFetcher.java)/**
 *  DtFetcher is an interface which permits the abstraction and separation of
 *  delegation token fetch implementaions across different packages and
 *  compilation units.  Resolution of fetcher impl will be done at runtime.
 */
HDFSPolicyProvider (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HDFSPolicyProvider.java)/**
 * {@link PolicyProvider} for HDFS protocols.
 */
NameNodeProxies (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/NameNodeProxies.java)/**
 * Create proxy objects to communicate with a remote NN. All remote access to an
 * NN should be funneled through this class. Most of the time you'll want to use
 * {@link NameNodeProxies#createProxy(Configuration, URI, Class)}, which will
 * create either an HA- or non-HA-enabled client proxy as appropriate.
 */
DFSNetworkTopology (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java)/**
 * The HDFS specific network topology class. The main purpose of doing this
 * subclassing is to add storage-type-aware chooseRandom method. All the
 * remaining parts should be the same.
 *
 * Currently a placeholder to test storage type info.
 */
DFSTopologyNodeImpl (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java)/**
 * The HDFS-specific representation of a network topology inner node. The
 * difference is this class includes the information about the storage type
 * info of this subtree. This info will be used when selecting subtrees
 * in block placement.
 */
CacheDirective (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirective.java)/**
 * Namenode class that tracks state related to a cached path.
 *
 * This is an implementation class, not part of the public API.
 */
BlackListBasedTrustedChannelResolver (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/BlackListBasedTrustedChannelResolver.java)/**
 * Implements {@link TrustedChannelResolver}
 * to trust ips/host/subnets based on a blackList.
 */
Receiver (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Receiver.java)/** Receiver */
InvalidMagicNumberException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/InvalidMagicNumberException.java)/**
 * Indicates that SASL protocol negotiation expected to read a pre-defined magic
 * number, but the expected value was not seen.
 */
PasswordFunction (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java)/**
   * The SASL handshake for encrypted vs. general-purpose uses different logic
   * for determining the password.  This interface is used to parameterize that
   * logic.  It's similar to a Guava Function, but we need to let it throw
   * exceptions.
   */
SaslServerCallbackHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java)/**
   * Sets user name and password when asked by the server-side SASL object.
   */
SaslDataTransferServer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java)/**
 * Negotiates SASL for DataTransferProtocol on behalf of a server.  There are
 * two possible supported variants of SASL negotiation: either a general-purpose
 * negotiation supporting any quality of protection, or a specialized
 * negotiation that enforces privacy as the quality of protection using a
 * cryptographically strong encryption key.
 *
 * This class is used in the DataNode for handling inbound connections.
 */
PathComponentTooLongException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/FSLimitException.java)/**
   * Path component length is too long
   */
MaxDirectoryItemsExceededException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/FSLimitException.java)/**
   * Directory has too many items
   */
LayoutFlags (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/LayoutFlags.java)/**
 * LayoutFlags represent features which the FSImage and edit logs can either
 * support or not, independently of layout version.
 * 
 * Note: all flags starting with 'test' are reserved for unit test purposes.
 */
LayoutFeature (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/LayoutVersion.java)/**
   * The interface to be implemented by NameNode and DataNode layout features 
   */
FeatureInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/LayoutVersion.java)/** Feature information. */
LayoutVersion (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/LayoutVersion.java)/**
 * This class tracks changes in the layout version of HDFS.
 * 
 * Layout version is changed for following reasons:
 * <ol>
 * <li>The layout of how namenode or datanode stores information 
 * on disk changes.</li>
 * <li>A new operation code is added to the editlog.</li>
 * <li>Modification such as format of a record, content of a record 
 * in editlog or fsimage.</li>
 * </ol>
 * <br>
 * <b>How to update layout version:<br></b>
 * When a change requires new layout version, please add an entry into
 * {@link Feature} with a short enum name, new layout version and description
 * of the change. Please see {@link Feature} for further details.
 * <br>
 */
RecoveryInProgressException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/RecoveryInProgressException.java)/**
 * Exception indicating that a replica is already being recovery.
 */
RollingUpgradeException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/RollingUpgradeException.java)/**
 * Exception related to rolling upgrade.
 */
SnapshotException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotException.java)/** Snapshot related exception. */
SnapshotInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotInfo.java)/**
 * SnapshotInfo maintains information for a snapshot
 */
UnregisteredNodeException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/UnregisteredNodeException.java)/**
 * This exception is thrown when a node that has not previously 
 * registered is trying to access the name node.
 */
AliasMapProtocolPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/AliasMapProtocolPB.java)/**
 * Protocol between the Namenode and the Datanode to read the AliasMap
 * used for Provided storage.
 */
AliasMapProtocolServerSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/AliasMapProtocolServerSideTranslatorPB.java)/**
 * AliasMapProtocolServerSideTranslatorPB is responsible for translating RPC
 * calls and forwarding them to the internal InMemoryAliasMap.
 */
ClientDatanodeProtocolServerSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientDatanodeProtocolServerSideTranslatorPB.java)/**
 * Implementation for protobuf service that forwards requests
 * received on {@link ClientDatanodeProtocolPB} to the
 * {@link ClientDatanodeProtocol} server implementation.
 */
ClientNamenodeProtocolServerSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java)/**
 * This class is used on the server side. Calls come across the wire for the
 * for protocol {@link ClientNamenodeProtocolPB}.
 * This class translates the PB data types
 * to the native data types used inside the NN as specified in the generic
 * ClientProtocol.
 */
DatanodeLifelineProtocolClientSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeLifelineProtocolClientSideTranslatorPB.java)/**
 * This class is the client side translator to translate the requests made on
 * {@link DatanodeLifelineProtocol} interfaces to the RPC server implementing
 * {@link DatanodeLifelineProtocolPB}.
 */
DatanodeLifelineProtocolPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeLifelineProtocolPB.java)/**
 * Protocol used by a DataNode to send lifeline messages to a NameNode.
 */
DatanodeLifelineProtocolServerSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeLifelineProtocolServerSideTranslatorPB.java)/**
 * Implementation for protobuf service that forwards requests
 * received on {@link DatanodeLifelineProtocolPB} to the
 * {@link DatanodeLifelineProtocol} server implementation.
 */
DatanodeProtocolClientSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java)/**
 * This class is the client side translator to translate the requests made on
 * {@link DatanodeProtocol} interfaces to the RPC server implementing
 * {@link DatanodeProtocolPB}.
 */
InMemoryAliasMapProtocolClientSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InMemoryAliasMapProtocolClientSideTranslatorPB.java)/**
 * This class is the client side translator to translate requests made to the
 * {@link InMemoryAliasMapProtocol} interface to the RPC server implementing
 * {@link AliasMapProtocolPB}.
 */
InterDatanodeProtocolServerSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InterDatanodeProtocolServerSideTranslatorPB.java)/**
 * Implementation for protobuf service that forwards requests
 * received on {@link InterDatanodeProtocolPB} to the
 * {@link InterDatanodeProtocol} server implementation.
 */
InterDatanodeProtocolTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InterDatanodeProtocolTranslatorPB.java)/**
 * This class is the client side translator to translate the requests made on
 * {@link InterDatanodeProtocol} interfaces to the RPC server implementing
 * {@link InterDatanodeProtocolPB}.
 */
JournalProtocolPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/JournalProtocolPB.java)/**
 * Protocol used to journal edits to a remote node. Currently,
 * this is used to publish edits from the NameNode to a BackupNode.
 * 
 * Note: This extends the protocolbuffer service based interface to
 * add annotations required for security.
 */
JournalProtocolServerSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/JournalProtocolServerSideTranslatorPB.java)/**
 * Implementation for protobuf service that forwards requests
 * received on {@link JournalProtocolPB} to the 
 * {@link JournalProtocol} server implementation.
 */
JournalProtocolTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/JournalProtocolTranslatorPB.java)/**
 * This class is the client side translator to translate the requests made on
 * {@link JournalProtocol} interfaces to the RPC server implementing
 * {@link JournalProtocolPB}.
 */
NamenodeProtocolPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolPB.java)/**
 * Protocol that a secondary NameNode uses to communicate with the NameNode.
 * It's used to get part of the name node state
 * 
 * Note: This extends the protocolbuffer service based interface to
 * add annotations required for security.
 */
NamenodeProtocolServerSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolServerSideTranslatorPB.java)/**
 * Implementation for protobuf service that forwards requests
 * received on {@link NamenodeProtocolPB} to the
 * {@link NamenodeProtocol} server implementation.
 */
NamenodeProtocolTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolTranslatorPB.java)/**
 * This class is the client side translator to translate the requests made on
 * {@link NamenodeProtocol} interfaces to the RPC server implementing
 * {@link NamenodeProtocolPB}.
 */
PBHelper (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java)/**
 * Utilities for converting protobuf classes to and from implementation classes
 * and other helper utilities to help in dealing with protobuf.
 * 
 * Note that when converting from an internal type to protobuf type, the
 * converter never return null for protobuf type. The check for internal type
 * being null must be done before calling the convert() method.
 *
 * For those helper methods that convert HDFS client-side data structures from
 * and to protobuf, see {@link PBHelperClient}.
 */
ReconfigurationProtocolServerSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolServerSideTranslatorPB.java)/**
 * This class is used on the server side. Calls come across the wire for the
 * for protocol {@link ReconfigurationProtocolPB}.
 * This class translates the PB data types
 * to the native data types used inside the NN/DN as specified in the generic
 * ReconfigurationProtocol.
 */
ReconfigurationProtocolServerSideUtils (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolServerSideUtils.java)/**
 * This is a server side utility class that handles
 * common logic to to parameter reconfiguration.
 */
AsyncLogger (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/AsyncLogger.java)/**
 * Interface for a remote log which is only communicated with asynchronously.
 * This is essentially a wrapper around {@link QJournalProtocol} with the key
 * differences being:
 * 
 * <ul>
 * <li>All methods return {@link ListenableFuture}s instead of synchronous
 * objects.</li>
 * <li>The {@link RequestInfo} objects are created by the underlying
 * implementation.</li>
 * </ul>
 */
AsyncLoggerSet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/AsyncLoggerSet.java)/**
 * Wrapper around a set of Loggers, taking care of fanning out
 * calls to the underlying loggers and constructing corresponding
 * {@link QuorumCall} instances.
 */
IPCLoggerChannel (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java)/**
 * Channel to a remote JournalNode using Hadoop IPC.
 * All of the calls are run on a separate thread, and return
 * {@link ListenableFuture} instances to wait for their result.
 * This allows calls to be bound together using the {@link QuorumCall}
 * class.
 */
IPCLoggerChannelMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannelMetrics.java)/**
 * The metrics for a journal from the writer's perspective.
 */
QuorumCall (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumCall.java)/**
 * Represents a set of calls for which a quorum of results is needed.
 * @param <KEY> a key used to identify each of the outgoing calls
 * @param <RESULT> the type of the call result
 */
QuorumException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumException.java)/**
 * Exception thrown when too many exceptions occur while gathering
 * responses to a quorum call. 
 */
QuorumJournalManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java)/**
 * A JournalManager that writes to a set of remote JournalNodes,
 * requiring a quorum of nodes to ack each write.
 */
QuorumOutputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumOutputStream.java)/**
 * EditLogOutputStream implementation that writes to a quorum of
 * remote journals.
 */
SegmentRecoveryComparator (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/SegmentRecoveryComparator.java)/**
 * Compares responses to the prepareRecovery RPC. This is responsible for
 * determining the correct length to recover.
 */
JournalNotFormattedException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocol/JournalNotFormattedException.java)/**
 * Exception indicating that a call has been made to a JournalNode
 * which is not yet formatted.
 */
QJournalProtocol (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocol.java)/**
 * Protocol used to communicate between {@link QuorumJournalManager}
 * and each {@link JournalNode}.
 * 
 * This is responsible for sending edits as well as coordinating
 * recovery of the nodes.
 */
InterQJournalProtocolPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/InterQJournalProtocolPB.java)/**
 * Protocol used to communicate between journal nodes for journal sync.
 * Note: This extends the protocolbuffer service based interface to
 * add annotations required for security.
 */
InterQJournalProtocolServerSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/InterQJournalProtocolServerSideTranslatorPB.java)/**
 * Implementation for protobuf service that forwards requests
 * received on {@link InterQJournalProtocolPB} to the
 * {@link InterQJournalProtocol} server implementation.
 */
InterQJournalProtocolTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/InterQJournalProtocolTranslatorPB.java)/**
 * This class is the client side translator to translate the requests made on
 * {@link InterQJournalProtocol} interfaces to the RPC server implementing
 * {@link InterQJournalProtocolPB}.
 */
QJournalProtocolPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolPB.java)/**
 * Protocol used to journal edits to a JournalNode participating
 * in the quorum journal.
 * Note: This extends the protocolbuffer service based interface to
 * add annotations required for security.
 */
QJournalProtocolServerSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolServerSideTranslatorPB.java)/**
 * Implementation for protobuf service that forwards requests
 * received on {@link JournalProtocolPB} to the 
 * {@link JournalProtocol} server implementation.
 */
QJournalProtocolTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolTranslatorPB.java)/**
 * This class is the client side translator to translate the requests made on
 * {@link JournalProtocol} interfaces to the RPC server implementing
 * {@link JournalProtocolPB}.
 */
GetJournalEditServlet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java)/**
 * This servlet is used in two cases:
 * <ul>
 * <li>The QuorumJournalManager, when reading edits, fetches the edit streams
 * from the journal nodes.</li>
 * <li>During edits synchronization, one journal node will fetch edits from
 * another journal node.</li>
 * </ul>
 */
JNStorage (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java)/**
 * A {@link Storage} implementation for the {@link JournalNode}.
 * 
 * The JN has a storage directory for each namespace for which it stores
 * metadata. There is only a single directory per JN in the current design.
 */
Journal (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java)/**
 * A JournalNode can manage journals for several clusters at once.
 * Each such journal is entirely independent despite being hosted by
 * the same JVM.
 */
JournaledEditsCache (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournaledEditsCache.java)/**
 * An in-memory cache of edits in their serialized form. This is used to serve
 * the {@link Journal#getJournaledEdits(long, int)} call, used by the
 * QJM when {@value DFSConfigKeys#DFS_HA_TAILEDITS_INPROGRESS_KEY} is
 * enabled.
 *
 * <p>When a batch of edits is received by the JournalNode, it is put into this
 * cache via {@link #storeEdits(byte[], long, long, int)}. Edits must be
 * stored contiguously; if a batch of edits is stored that does not align with
 * the previously stored edits, the cache will be cleared before storing new
 * edits to avoid gaps. This decision is made because gaps are only handled
 * when in recovery mode, which the cache is not intended to be used for.
 *
 * <p>Batches of edits are stored in a {@link TreeMap} mapping the starting
 * transaction ID of the batch to the data buffer. Upon retrieval, the
 * relevant data buffers are concatenated together and a header is added
 * to construct a fully-formed edit data stream.
 *
 * <p>The cache is of a limited size capacity determined by
 * {@value DFSConfigKeys#DFS_JOURNALNODE_EDIT_CACHE_SIZE_KEY}. If the capacity
 * is exceeded after adding a new batch of edits, batches of edits are removed
 * until the total size is less than the capacity, starting from the ones
 * containing the oldest transactions. Transactions range in size, but a
 * decent rule of thumb is that 200 bytes are needed per transaction. Monitoring
 * the {@link JournalMetrics#rpcRequestCacheMissAmount} metric is recommended
 * to determine if the cache is too small; it will indicate both how many
 * cache misses occurred, and how many more transactions would have been
 * needed in the cache to serve the request.
 */
JournalFaultInjector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalFaultInjector.java)/**
 * Used for injecting faults in QuorumJournalManager tests.
 * Calls into this are a no-op in production code. 
 */
JournalMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalMetrics.java)/**
 * The server-side metrics for a journal from the JournalNode's
 * perspective.
 */
JournalNode (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java)/**
 * The JournalNode is a daemon which allows namenodes using
 * the QuorumJournalManager to log and retrieve edits stored
 * remotely. It is a thin wrapper around a local edit log
 * directory with the addition of facilities to participate
 * in the quorum protocol.
 */
JournalNodeHttpServer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeHttpServer.java)/**
 * Encapsulates the HTTP server started by the Journal Service.
 */
JournalNodeMXBean (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeMXBean.java)/**
 * This is the JMX management interface for JournalNode information
 */
JournalNodeSyncer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java)/**
 * A Journal Sync thread runs through the lifetime of the JN. It periodically
 * gossips with other journal nodes to compare edit log manifests and if it
 * detects any missing log segment, it downloads it from the other journal node
 */
BlockKey (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockKey.java)/**
 * Key used for generating and verifying block tokens
 */
BlockPoolTokenSecretManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockPoolTokenSecretManager.java)/**
 * Manages a {@link BlockTokenSecretManager} per block pool. Routes the requests
 * given a block pool Id to corresponding {@link BlockTokenSecretManager}
 */
BlockTokenSecretManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java)/**
 * BlockTokenSecretManager can be instantiated in 2 modes, master mode
 * and worker mode. Master can generate new block keys and export block
 * keys to workers, while workers can only import and use block keys
 * received from master. Both master and worker can generate and verify
 * block tokens. Typically, master mode is used by NN and worker mode
 * is used by DN.
 */
ExportedBlockKeys (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/ExportedBlockKeys.java)/**
 * Object for passing block keys
 */
DelegationTokenSecretManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSecretManager.java)/**
 * A HDFS specific delegation token secret manager.
 * The secret manager is responsible for generating and accepting the password
 * for each token.
 */
CheckedFunction2 (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMap.java)/**
   * CheckedFunction is akin to {@link java.util.function.Function} but
   * specifies an IOException.
   * @param <T1> First argument type.
   * @param <T2> Second argument type.
   * @param <R> Return type.
   */
InMemoryAliasMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMap.java)/**
 * InMemoryAliasMap is an implementation of the InMemoryAliasMapProtocol for
 * use with LevelDB.
 */
IterationResult (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMapProtocol.java)/**
   * The result of a read from the in-memory aliasmap. It contains the
   * a list of FileRegions that are returned, along with the next block
   * from which the read operation must continue.
   */
InMemoryAliasMapProtocol (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMapProtocol.java)/**
 * Protocol used by clients to read/write data about aliases of
 * provided blocks for an in-memory implementation of the
 * {@link org.apache.hadoop.hdfs.server.common.blockaliasmap.BlockAliasMap}.
 */
InMemoryLevelDBAliasMapServer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryLevelDBAliasMapServer.java)/**
 * InMemoryLevelDBAliasMapServer is the entry point from the Namenode into
 * the {@link InMemoryAliasMap}.
 */
Node (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/BalancingPolicy.java)/**
   * Cluster is balanced if each node is balanced.
   */
Pool (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/BalancingPolicy.java)/**
   * Cluster is balanced if each pool in each node is balanced.
   */
BalancingPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/BalancingPolicy.java)/**
 * Balancing policy.
 * Since a datanode may contain multiple block pools,
 * {@link Pool} implies {@link Node}
 * but NOT the other way around
 */
PendingMove (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java)/** This class keeps track of a scheduled reportedBlock move */
DBlock (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java)/** A class for keeping track of block locations in the dispatcher. */
Task (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java)/** The class represents a desired move. */
StorageGroup (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java)/** A group of storages in a datanode with the same storage type. */
DDatanode (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java)/** A class that keeps track of a datanode. */
Source (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java)/** A node that can be the sources of a block move */
Dispatcher (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java)/** Dispatching block replica moves between datanodes. */
BlockKeyUpdater (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java)/**
   * Periodically updates access keys.
   */
KeyManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java)/**
 * The class provides utilities for key and token management.
 */
Matcher (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Matcher.java)/** A matcher interface for matching nodes. */
Locations (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/MovedBlocks.java)/** A class for keeping track of a block and its locations */
MovedBlocks (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/MovedBlocks.java)/**
 * This window makes sure to keep blocks that have been moved within a fixed
 * time interval (default is 1.5 hour). Old window has blocks that are older;
 * Current window has blocks that are more recent; Cleanup method triggers the
 * check if blocks in the old window are more than the fixed time interval. If
 * yes, purge the old window and then move blocks in current window to old
 * window.
 * 
 * @param <L> Location type
 */
NameNodeConnector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java)/**
 * The class provides utilities for accessing a NameNode.
 */
AvailableSpaceBlockPlacementPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java)/**
 * Space balanced block placement policy.
 */
BlockCollection (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockCollection.java)/** 
 * This interface is used by the block manager to expose a
 * few characteristics of a collection of Block/BlockUnderConstruction.
 */
BlockIdManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockIdManager.java)/**
 * BlockIdManager allocates the generation stamps and the block ID. The
 * {@link FSNamesystem} is responsible for persisting the allocations in the
 * {@link FSEditLog}.
 */
BlockInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java)/**
 * For a given block (or an erasure coding block group), BlockInfo class
 * maintains 1) the {@link BlockCollection} it is part of, and 2) datanodes
 * where the replicas of the block, or blocks belonging to the erasure coding
 * block group, are stored.
 */
BlockInfoContiguous (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguous.java)/**
 * Subclass of {@link BlockInfo}, used for a block with replication scheme.
 */
StorageAndBlockIndex (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped.java)/**
   * This class contains datanode storage information and block index in the
   * block group.
   */
BlockInfoStriped (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped.java)/**
 * Subclass of {@link BlockInfo}, presenting a block group in erasure coding.
 *
 * We still use a storage array to store DatanodeStorageInfo for each block in
 * the block group. For a (m+k) block group, the first (m+k) storage units
 * are sorted and strictly mapped to the corresponding block.
 *
 * Normally each block belonging to group is stored in only one DataNode.
 * However, it is possible that some block is over-replicated. Thus the storage
 * array's size can be larger than (m+k). Thus currently we use an extra byte
 * array to record the block index for each entry.
 */
StatefulBlockInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java)/**
   * StatefulBlockInfo is used to build the "toUC" list, which is a list of
   * updates to the information about under-construction blocks.
   * Besides the block in question, it provides the ReplicaState
   * reported by the datanode in the block report. 
   */
RedundancyMonitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java)/**
   * Periodically calls computeBlockRecoveryWork().
   */
StorageInfoDefragmenter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java)/**
   * Runnable that monitors the fragmentation of the StorageInfo TreeSet and
   * compacts it when it falls under a certain threshold.
   */
BlockManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java)/**
 * Keeps information related to the blocks stored in the Hadoop cluster.
 * For block state management, it tries to maintain the  safety
 * property of "# of live replicas == # of expected redundancy" under
 * any events such as decommission, namenode failover, datanode failure.
 *
 * The motivation of maintenance mode is to allow admins quickly repair nodes
 * without paying the cost of decommission. Thus with maintenance mode,
 * # of live replicas doesn't have to be equal to # of expected redundancy.
 * If any of the replica is in maintenance mode, the safety property
 * is extended as follows. These property still apply for the case of zero
 * maintenance replicas, thus we can use these safe property for all scenarios.
 * a. # of live replicas &gt;= # of min replication for maintenance.
 * b. # of live replicas &lt;= # of expected redundancy.
 * c. # of live replicas and maintenance replicas &gt;= # of expected
 * redundancy.
 *
 * For regular replication, # of min live replicas for maintenance is determined
 * by DFS_NAMENODE_MAINTENANCE_REPLICATION_MIN_KEY. This number has to &lt;=
 * DFS_NAMENODE_REPLICATION_MIN_KEY.
 * For erasure encoding, # of min live replicas for maintenance is
 * BlockInfoStriped#getRealDataBlockNum.
 *
 * Another safety property is to satisfy the block placement policy. While the
 * policy is configurable, the replicas the policy is applied to are the live
 * replicas + maintenance replicas.
 */
BlockManagerFaultInjector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerFaultInjector.java)/**
 * Used to inject certain faults for testing.
 */
SafeModeMonitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerSafeMode.java)/**
   * Periodically check whether it is time to leave safe mode.
   * This thread starts when the threshold level is reached.
   */
BlockManagerSafeMode (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerSafeMode.java)/**
 * Block manager safe mode info.
 *
 * During name node startup, counts the number of <em>safe blocks</em>, those
 * that have at least the minimal number of replicas, and calculates the ratio
 * of safe blocks to the total number of blocks in the system, which is the size
 * of blocks. When the ratio reaches the {@link #threshold} and enough live data
 * nodes have registered, it needs to wait for the safe mode {@link #extension}
 * interval. After the extension period has passed, it will not leave safe mode
 * until the safe blocks ratio reaches the {@link #threshold} and enough live
 * data node registered.
 */
BlockPlacementPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicy.java)/** 
 * This interface is used for choosing the desired number of targets
 * for placing block replicas.
 */
BlockPlacementPolicyDefault (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java)/**
 * The class is responsible for choosing the desired number of targets
 * for placing block replicas.
 * The replica placement strategy is that if the writer is on a datanode,
 * the 1st replica is placed on the local machine by default
 * (By passing the {@link org.apache.hadoop.fs.CreateFlag#NO_LOCAL_WRITE} flag
 * the client can request not to put a block replica on the local datanode.
 * Subsequent replicas will still follow default block placement policy.).
 * If the writer is not on a datanode, the 1st replica is placed on a random
 * node.
 * The 2nd replica is placed on a datanode that is on a different rack.
 * The 3rd replica is placed on a datanode which is on a different node of the
 * rack as the second replica.
 */
BlockPlacementPolicyRackFaultTolerant (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyRackFaultTolerant.java)/**
 * The class is responsible for choosing the desired number of targets
 * for placing block replicas.
 * The strategy is that it tries its best to place the replicas to most racks.
 */
BlockPlacementPolicyWithNodeGroup (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java)/** The class is responsible for choosing the desired number of targets
 * for placing block replicas on environment with node-group layer.
 * The replica placement strategy is adjusted to:
 * If the writer is on a datanode, the 1st replica is placed on the local 
 *     node(or local node-group or on local rack), otherwise a random datanode.
 * The 2nd replica is placed on a datanode that is on a different rack with 1st
 *     replica node. 
 * The 3rd replica is placed on a datanode which is on a different node-group
 *     but the same rack as the second replica node.
 */
BlockPlacementPolicyWithUpgradeDomain (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithUpgradeDomain.java)/**
 * The class is responsible for choosing the desired number of targets
 * for placing block replicas that honors upgrade domain policy.
 * Here is the replica placement strategy. If the writer is on a datanode,
 * the 1st replica is placed on the local machine,
 * otherwise a random datanode. The 2nd replica is placed on a datanode
 * that is on a different rack. The 3rd replica is placed on a datanode
 * which is on a different node of the rack as the second replica.
 * All 3 replicas have unique upgrade domains.
 */
BlockPlacementStatusWithNodeGroup (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementStatusWithNodeGroup.java)/**
 * An implementation of @see BlockPlacementStatus for
 * @see BlockPlacementPolicyWithNodeGroup
 */
BlockPlacementStatusWithUpgradeDomain (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementStatusWithUpgradeDomain.java)/**
 * An implementation of @see BlockPlacementStatus for
 * @see BlockPlacementPolicyWithUpgradeDomain
 */
BlockReconstructionWork (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReconstructionWork.java)/**
 * This class is used internally by
 * {@link BlockManager#computeReconstructionWorkForBlocks} to represent a
 * task to reconstruct a block through replication or erasure coding.
 * Reconstruction is done by transferring data from srcNodes to targets
 */
BlockReportLeaseManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java)/**
 * The BlockReportLeaseManager manages block report leases.<p/>
 *
 * DataNodes request BR leases from the NameNode by sending a heartbeat with
 * the requestBlockReportLease field set.  The NameNode may choose to respond
 * with a non-zero lease ID.  If so, that DataNode can send a block report with
 * the given lease ID for the next few minutes.  The NameNode will accept
 * these full block reports.<p/>
 *
 * BR leases limit the number of incoming full block reports to the NameNode
 * at any given time.  For compatibility reasons, the NN will always accept
 * block reports sent with a lease ID of 0 and queue them for processing
 * immediately.  Full block reports which were manually triggered will also
 * have a lease ID of 0, bypassing the rate-limiting.<p/>
 *
 * Block report leases expire after a certain amount of time.  This mechanism
 * is in place so that a DN which dies while holding a lease does not
 * permanently decrease the number of concurrent block reports which the NN is
 * willing to accept.<p/>
 *
 * When considering which DNs to grant a BR lease, the NameNode gives priority
 * to the DNs which have gone the longest without sending a full block
 * report.<p/>
 */
BlocksMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java)/**
 * This class maintains the map from a block to its metadata.
 * block's metadata currently includes blockCollection it belongs to and
 * the datanodes that store the block.
 */
BlockStatsMXBean (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStatsMXBean.java)/**
 * This is an interface used to retrieve statistic information related to
 * block management.
 */
BlockStoragePolicySuite (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java)/** A collection of block storage policies. */
BlockToMarkCorrupt (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockToMarkCorrupt.java)/**
 * BlockToMarkCorrupt is used to build the "toCorrupt" list, which is a
 * list of blocks that should be considered corrupt due to a block report.
 */
BlockUnderConstructionFeature (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockUnderConstructionFeature.java)/**
 * Represents the under construction feature of a Block.
 * This is usually the last block of a file opened for write or append.
 */
CacheReplicationMonitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java)/**
 * Scans the namesystem, scheduling blocks to be cached as appropriate.
 *
 * The CacheReplicationMonitor does a full scan when the NameNode first
 * starts up, and at configurable intervals afterwards.
 */
CombinedHostFileManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java)/**
 * This class manages datanode configuration using a json file.
 * Please refer to {@link CombinedHostsFileReader} for the json format.
 * <p>
 * Entries may or may not specify a port.  If they don't, we consider
 * them to apply to every DataNode on that host. The code canonicalizes the
 * entries into IP addresses.
 * <p>
 * The code ignores all entries that the DNS fails to resolve their IP
 * addresses. This is okay because by default the NN rejects the registrations
 * of DNs when it fails to do a forward and reverse lookup. Note that DNS
 * resolutions are only done during the loading time to minimize the latency.
 */
Monitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java)/**
   * Checks to see if datanodes have finished DECOMMISSION_INPROGRESS or
   * ENTERING_MAINTENANCE state.
   * <p/>
   * Since this is done while holding the namesystem lock,
   * the amount of work per monitor tick is limited.
   */
DatanodeAdminManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java)/**
 * Manages decommissioning and maintenance state for DataNodes. A background
 * monitor thread periodically checks the status of DataNodes that are
 * decommissioning or entering maintenance state.
 * <p>
 * A DataNode can be decommissioned in a few situations:
 * <ul>
 * <li>If a DN is dead, it is decommissioned immediately.</li>
 * <li>If a DN is alive, it is decommissioned after all of its blocks
 * are sufficiently replicated. Merely under-replicated blocks do not
 * block decommissioning as long as they are above a replication
 * threshold.</li>
 * </ul>
 * In the second case, the DataNode transitions to a DECOMMISSION_INPROGRESS
 * state and is tracked by the monitor thread. The monitor periodically scans
 * through the list of insufficiently replicated blocks on these DataNodes to
 * determine if they can be DECOMMISSIONED. The monitor also prunes this list
 * as blocks become replicated, so monitor scans will become more efficient
 * over time.
 * <p>
 * DECOMMISSION_INPROGRESS nodes that become dead do not progress to
 * DECOMMISSIONED until they become live again. This prevents potential
 * durability loss for singly-replicated blocks (see HDFS-6791).
 * <p>
 * DataNodes can also be put under maintenance state for any short duration
 * maintenance operations. Unlike decommissioning, blocks are not always
 * re-replicated for the DataNodes to enter maintenance state. When the
 * blocks are replicated at least dfs.namenode.maintenance.replication.min,
 * DataNodes transition to IN_MAINTENANCE state. Otherwise, just like
 * decommissioning, DataNodes transition to ENTERING_MAINTENANCE state and
 * wait for the blocks to be sufficiently replicated and then transition to
 * IN_MAINTENANCE state. The block replication factor is relaxed for a maximum
 * of maintenance expiry time. When DataNodes don't transition or join the
 * cluster back by expiry time, blocks are re-replicated just as in
 * decommissioning case as to avoid read or write performance degradation.
 * <p>
 * This class depends on the FSNamesystem lock for synchronization.
 */
BlockTargetPair (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java)/** Block and targets pair */
BlockQueue (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java)/** A BlockTargetPair queue. */
CachedBlocksList (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java)/**
   * A list of CachedBlock objects on this datanode.
   */
LeavingServiceStatus (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java)/** Leaving service status. */
DatanodeDescriptor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java)/**
 * This class extends the DatanodeInfo class with ephemeral information (eg
 * health, capacity, what blocks are associated with the Datanode) that is
 * private to the Namenode, ie this class is not exposed to clients.
 */
DatanodeManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java)/**
 * Manage datanodes, include decommission and other activities.
 */
DatanodeStatistics (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStatistics.java)/** Datanode statistics */
DatanodeStats (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStats.java)/**
 * Datanode statistics.
 * For decommissioning/decommissioned nodes, only used capacity is counted.
 */
DatanodeStorageInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java)/**
 * A Datanode has one or more storages. A storage in the Datanode is represented
 * by this class.
 */
ExcessRedundancyMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ExcessRedundancyMap.java)/**
 * Maps a datnode to the set of excess redundancy details.
 *
 * This class is thread safe.
 */
FSClusterStats (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/FSClusterStats.java)/**
 * This interface is used for retrieving the load related statistics of
 * the cluster.
 */
Monitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java)/** Periodically check heartbeat and update block key */
HeartbeatManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java)/**
 * Manage the heartbeats received from datanodes.
 * The datanode list and statistics are synchronized
 * by the heartbeat manager lock.
 */
Host2NodesMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/Host2NodesMap.java)/** A map from host names to datanode descriptors. */
HostConfigManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HostConfigManager.java)/**
 * This interface abstracts how datanode configuration is managed.
 *
 * Each implementation defines its own way to persist the configuration.
 * For example, it can use one JSON file to store the configs for all
 * datanodes; or it can use one file to store in-service datanodes and another
 * file to store decommission-requested datanodes.
 *
 * These files control which DataNodes the NameNode expects to see in the
 * cluster.
 */
HostFileManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HostFileManager.java)/**
 * This class manages the include and exclude files for HDFS.
 * <p>
 * These files control which DataNodes the NameNode expects to see in the
 * cluster.  Loosely speaking, the include file, if it exists and is not
 * empty, is a list of everything we expect to see.  The exclude file is
 * a list of everything we want to ignore if we do see it.
 * <p>
 * Entries may or may not specify a port.  If they don't, we consider
 * them to apply to every DataNode on that host. The code canonicalizes the
 * entries into IP addresses.
 * <p>
 * The code ignores all entries that the DNS fails to resolve their IP
 * addresses. This is okay because by default the NN rejects the registrations
 * of DNs when it fails to do a forward and reverse lookup. Note that DNS
 * resolutions are only done during the loading time to minimize the latency.
 */
HostSet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HostSet.java)/**
 * The HostSet allows efficient queries on matching wildcard addresses.
 * <p>
 * For InetSocketAddress A and B with the same host address,
 * we define a partial order between A and B, A &lt;= B iff A.getPort() == B
 * .getPort() || B.getPort() == 0.
 */
InvalidateBlocks (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java)/**
 * Keeps a Collection for every named machine containing blocks
 * that have recently been invalidated and are thought to live
 * on the machine in question.
 */
LowRedundancyBlocks (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/LowRedundancyBlocks.java)/**
 * Keep prioritized queues of low redundant blocks.
 * Blocks have redundancy priority, with priority
 * {@link #QUEUE_HIGHEST_PRIORITY} indicating the highest priority.
 * </p>
 * Having a prioritised queue allows the {@link BlockManager} to select
 * which blocks to replicate first -it tries to give priority to data
 * that is most at risk or considered most valuable.
 *
 * <p/>
 * The policy for choosing which priority to give added blocks
 * is implemented in {@link #getPriority(BlockInfo, int, int, int, int)}.
 * </p>
 * <p>The queue order is as follows:</p>
 * <ol>
 *   <li>{@link #QUEUE_HIGHEST_PRIORITY}: the blocks that should be redundant
 *   first. That is blocks with only one copy, or blocks with zero live
 *   copies but a copy in a node being decommissioned. These blocks
 *   are at risk of loss if the disk or server on which they
 *   remain fails.</li>
 *   <li>{@link #QUEUE_VERY_LOW_REDUNDANCY}: blocks that are very
 *   under-replicated compared to their expected values. Currently
 *   that means the ratio of the ratio of actual:expected means that
 *   there is <i>less than</i> 1:3.</li>. These blocks may not be at risk,
 *   but they are clearly considered "important".
 *   <li>{@link #QUEUE_LOW_REDUNDANCY}: blocks that are also under
 *   replicated, and the ratio of actual:expected is good enough that
 *   they do not need to go into the {@link #QUEUE_VERY_LOW_REDUNDANCY}
 *   queue.</li>
 *   <li>{@link #QUEUE_REPLICAS_BADLY_DISTRIBUTED}: there are as least as
 *   many copies of a block as required, but the blocks are not adequately
 *   distributed. Loss of a rack/switch could take all copies off-line.</li>
 *   <li>{@link #QUEUE_WITH_CORRUPT_BLOCKS} This is for blocks that are corrupt
 *   and for which there are no-non-corrupt copies (currently) available.
 *   The policy here is to keep those corrupt blocks replicated, but give
 *   blocks that are not corrupt higher priority.</li>
 * </ol>
 */
NumberReplicas (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/NumberReplicas.java)/**
 * A immutable object that stores the number of live replicas and
 * the number of decommissioned Replicas.
 */
OutOfLegacyGenerationStampsException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/OutOfLegacyGenerationStampsException.java)/**
 * This exception is thrown when the name node runs out of V1 (legacy)
 * generation stamps.
 *
 */
PendingDataNodeMessages (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingDataNodeMessages.java)/**
 * In the Standby Node, we can receive messages about blocks
 * before they are actually available in the namespace, or while
 * they have an outdated state in the namespace. In those cases,
 * we queue those block-related messages in this structure.
 * */
PendingBlockInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReconstructionBlocks.java)/**
   * An object that contains information about a block that
   * is being reconstructed. It records the timestamp when the
   * system started reconstructing the most recent copy of this
   * block. It also records the list of Datanodes where the
   * reconstruction requests are in progress.
   */
PendingReconstructionBlocks (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReconstructionBlocks.java)/***************************************************
 * PendingReconstructionBlocks does the bookkeeping of all
 * blocks that gains stronger redundancy.
 *
 * It does the following:
 * 1)  record blocks that gains stronger redundancy at this instant.
 * 2)  a coarse grain timer to track age of reconstruction request
 * 3)  a thread that periodically identifies reconstruction-requests
 *     that never made it.
 *
 ***************************************************/
BlockRecoveryAttempt (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingRecoveryBlocks.java)/**
   * Tracks timeout for block recovery attempt of a given block.
   */
PendingRecoveryBlocks (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingRecoveryBlocks.java)/**
 * PendingRecoveryBlocks tracks recovery attempts for each block and their
 * timeouts to ensure we do not have multiple recoveries at the same time
 * and retry only after the timeout for a recovery has expired.
 */
ProvidedBlocksBuilder (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java)/**
   * Builder used for creating {@link LocatedBlocks} when a block is provided.
   */
ProvidedDescriptor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java)/**
   * An abstract DatanodeDescriptor to track datanodes with provided storages.
   * NOTE: never resolved through registerDatanode, so not in the topology.
   */
ProvidedDatanodeStorageInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java)/**
   * The DatanodeStorageInfo used for the provided storage.
   */
ProvidedBlockList (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java)/**
   * Used to emulate block reports for provided blocks.
   */
ProvidedStorageMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java)/**
 * This class allows us to manage and multiplex between storages local to
 * datanodes, and provided storage.
 */
ReplicaUnderConstruction (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ReplicaUnderConstruction.java)/**
 * ReplicaUnderConstruction contains information about replicas (or blocks
 * belonging to a block group) while they are under construction.
 *
 * The GS, the length and the state of the replica is as reported by the
 * datanode.
 *
 * It is not guaranteed, but expected, that datanodes actually have
 * corresponding replicas.
 */
SequentialBlockGroupIdGenerator (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SequentialBlockGroupIdGenerator.java)/**
 * Generate the next valid block group ID by incrementing the maximum block
 * group ID allocated so far, with the first 2^10 block group IDs reserved.
 * HDFS-EC introduces a hierarchical protocol to name blocks and groups:
 * Contiguous: {reserved block IDs | flag | block ID}
 * Striped: {reserved block IDs | flag | block group ID | index in group}
 *
 * Following n bits of reserved block IDs, The (n+1)th bit in an ID
 * distinguishes contiguous (0) and striped (1) blocks. For a striped block,
 * bits (n+2) to (64-m) represent the ID of its block group, while the last m
 * bits represent its index of the group. The value m is determined by the
 * maximum number of blocks in a group (MAX_BLOCKS_IN_GROUP).
 *
 * Note that the {@link #nextValue()} methods requires external lock to
 * guarantee IDs have no conflicts.
 */
SequentialBlockIdGenerator (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SequentialBlockIdGenerator.java)/**
 * Generate the next valid block ID by incrementing the maximum block
 * ID allocated so far, starting at 2^30+1.
 *
 * Block IDs used to be allocated randomly in the past. Hence we may
 * find some conflicts while stepping through the ID space sequentially.
 * However given the sparsity of the ID space, conflicts should be rare
 * and can be skipped over when detected.
 */
DiskLatency (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SlowDiskTracker.java)/**
   * This structure is a thin wrapper over disk latencies.
   */
SlowDiskTracker (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SlowDiskTracker.java)/**
 * This class aggregates information from {@link SlowDiskReports} received via
 * heartbeats.
 */
ReportForJson (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerTracker.java)/**
   * This structure is a thin wrapper over reports to make Json
   * [de]serialization easy.
   */
SlowPeerTracker (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerTracker.java)/**
 * This class aggregates information from {@link SlowPeerReports} received via
 * heartbeats.
 */
StorageTypeStats (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/StorageTypeStats.java)/**
 * Statistics per StorageType.
 *
 */
UnresolvedTopologyException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/UnresolvedTopologyException.java)/**
 * This exception is thrown if resolving topology path 
 * for a node fails. 
 */
BlockAlias (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/BlockAlias.java)/**
 * Interface used to load provided blocks.
 */
ImmutableIterator (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.java)/**
   * ImmutableIterator is an Iterator that does not support the remove
   * operation. This could inherit {@link java.util.Enumeration} but Iterator
   * is supported by more APIs and Enumeration's javadoc even suggests using
   * Iterator instead.
   */
Options (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.java)/**
     * reader options.
     */
Reader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.java)/**
   * An abstract class that is used to read {@link BlockAlias}es
   * for provided blocks.
   */
Options (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.java)/**
     * writer options.
     */
Writer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.java)/**
   * An abstract class used as a writer for the provided block map.
   */
BlockAliasMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.java)/**
 * An abstract class used to read and write block maps for provided blocks.
 */
InMemoryLevelDBAliasMapClient (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java)/**
 * InMemoryLevelDBAliasMapClient is the client for the InMemoryAliasMapServer.
 * This is used by the Datanode and fs2img to store and retrieve FileRegions
 * based on the given Block.
 */
LevelDBOptions (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.java)/**
   * Class specifying reader options for the {@link LevelDBFileRegionAliasMap}.
   */
Options (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.java)/**
     * Options for {@link LevelDBReader}.
     */
LevelDBReader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.java)/**
   * This class is used as a reader for block maps which
   * are stored as LevelDB files.
   */
Options (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.java)/**
     * Interface for Writer options.
     */
LevelDBWriter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.java)/**
   * This class is used as a writer for block maps which
   * are stored as LevelDB files.
   */
LevelDBFileRegionAliasMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.java)/**
 * A LevelDB based implementation of {@link BlockAliasMap}.
 */
ReaderOptions (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.java)/**
   * Class specifying reader options for the {@link TextFileRegionAliasMap}.
   */
WriterOptions (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.java)/**
   * Class specifying writer options for the {@link TextFileRegionAliasMap}.
   */
Options (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.java)/**
     * Options for {@link TextReader}.
     */
TextReader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.java)/**
   * This class is used as a reader for block maps which
   * are stored as delimited text files.
   */
Options (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.java)/**
     * Interface for Writer options.
     */
TextWriter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.java)/**
   * This class is used as a writer for block maps which
   * are stored as delimited text files.
   */
TextFileRegionAliasMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.java)/**
 * This class is used for block maps stored as text files,
 * with a specified delimiter.
 */
ECTopologyVerifier (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/ECTopologyVerifier.java)/**
 * Class for verifying whether the cluster setup can support
 * all enabled EC policies.
 *
 * Scenarios when the verification fails:
 * 1. not enough data nodes compared to EC policy's highest data+parity number
 * 2. not enough racks to satisfy BlockPlacementPolicyRackFaultTolerant
 */
FileRegion (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/FileRegion.java)/**
 * This class is used to represent provided blocks that are file regions,
 * i.e., can be described using (path, offset, length).
 */
GenerationStamp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/GenerationStamp.java)/****************************************************************
 * A GenerationStamp is a Hadoop FS primitive, identified by a long.
 ****************************************************************/
HostRestrictingAuthorizationFilter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java)/**
 * An HTTP filter that can filter requests based on Hosts.
 */
InconsistentFSStateException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/InconsistentFSStateException.java)/**
 * The exception is thrown when file system state is inconsistent 
 * and is not recoverable. 
 * 
 */
IncorrectVersionException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/IncorrectVersionException.java)/**
 * The exception is thrown when external version does not match 
 * current version of the application.
 * 
 */
MetricsLoggerTask (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/MetricsLoggerTask.java)/**
 * MetricsLoggerTask can be used as utility to dump metrics to log.
 */
BlockDispatcher (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/sps/BlockDispatcher.java)/**
 * Dispatching block replica moves between datanodes to satisfy the storage
 * policy.
 */
BlockMovementAttemptFinished (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/sps/BlockMovementAttemptFinished.java)/**
 * This class represents status from a block movement task. This will have the
 * information of the task which was successful or failed due to errors.
 */
BlocksMovementsStatusHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/sps/BlocksMovementsStatusHandler.java)/**
 * Blocks movements status handler, which can be used to collect details of the
 * completed block movements.
 */
BlockStorageMovementTracker (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/sps/BlockStorageMovementTracker.java)/**
 * This class is used to track the completion of block movement future tasks.
 */
StorageDirType (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java)/**
   * An interface to denote storage directory type
   * Implementations can define a type for storage directory by implementing
   * this interface.
   */
StorageDirectory (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java)/**
   * One of the storage directories.
   */
FormatConfirmable (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java)/**
   * Interface for classes which need to have the user confirm their
   * formatting during NameNode -format and other similar operations.
   * 
   * This is currently a storage directory or journal manager.
   */
Storage (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java)/**
 * Storage information file.
 * <p>
 * Local storage information is stored in a separate file VERSION.
 * It contains type of the node, 
 * the storage layout version, the namespace id, and 
 * the fs state creation time.
 * <p>
 * Local storage can reside in multiple directories. 
 * Each directory should contain the same VERSION file as the others.
 * During startup Hadoop servers (name-node and data-nodes) read their local 
 * storage information from them.
 * <p>
 * The servers hold a lock for each storage directory while they run so that 
 * other nodes were not able to startup sharing the same storage.
 * The locks are released when the servers stop (normally or abnormally).
 * 
 */
StorageErrorReporter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/StorageErrorReporter.java)/**
 * Interface which implementations of {@link JournalManager} can use to report
 * errors on underlying storage directories. This avoids a circular dependency
 * between journal managers and the storage which instantiates them.
 */
StorageInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/StorageInfo.java)/**
 * Common class for storage information.
 * 
 * TODO namespaceID should be long and computed as hash(address + port)
 */
TokenVerifier (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/TokenVerifier.java)/**
 * Interface to verify delegation tokens passed through WebHDFS.
 * Implementations are intercepted by JspHelper that pass delegation token
 * for verification.
 */
AbstractBlockChecksumComputer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java)/**
   * The abstract block checksum computer.
   */
BlockChecksumComputer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java)/**
   * The abstract base block checksum computer, mainly for replicated blocks.
   */
ReplicatedBlockChecksumComputer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java)/**
   * Replicated block checksum computer.
   */
BlockGroupNonStripedChecksumComputer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java)/**
   * Non-striped block group checksum computer for striped blocks.
   */
BlockChecksumHelper (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java)/**
 * Utilities for Block checksum computing, for both replicated and striped
 * blocks.
 */
BlockPoolManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java)/**
 * Manages the BPOfferService objects for the data node.
 * Creation, removal, starting, stopping, shutdown on BPOfferService
 * objects must be done via APIs in this class.
 */
BlockPoolSliceStorage (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java)/**
 * Manages storage for the set of BlockPoolSlices which share a particular 
 * block pool id, on this DataNode.
 * 
 * This class supports the following functionality:
 * <ul>
 * <li> Formatting a new block pool storage</li>
 * <li> Recovering a storage state to a consistent state (if possible)</li>
 * <li> Taking a snapshot of the block pool during upgrade</li>
 * <li> Rolling back a block pool to a previous snapshot</li>
 * <li> Finalizing block storage by deletion of a snapshot</li>
 * </ul>
 * 
 * @see Storage
 */
PacketResponder (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java)/**
   * Processes responses from downstream datanodes in the pipeline
   * and sends back replies to the originator.
   */
Packet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java)/**
   * This information is cached by the Datanode in the ackQueue.
   */
BlockReceiver (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java)/** A class that receives a block and writes to its own disk, meanwhile
 * may copies it to another site. If a throttler is provided,
 * streaming throttling is also supported.
 **/
BlockRecord (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java)/** A convenient class used in block recovery. */
RecoveryTaskContiguous (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java)/** A block recovery task for a contiguous block. */
RecoveryTaskStriped (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java)/**
   * blk_0  blk_1  blk_2  blk_3  blk_4  blk_5  blk_6  blk_7  blk_8
   *  64k    64k    64k    64k    64k    64k    64k    64k    64k   &lt;--
   *  stripe_0
   *  64k    64k    64k    64k    64k    64k    64k    64k    64k
   *  64k    64k    64k    64k    64k    64k    64k    61k    &lt;--
   *  startStripeIdx
   *  64k    64k    64k    64k    64k    64k    64k
   *  64k    64k    64k    64k    64k    64k    59k
   *  64k    64k    64k    64k    64k    64k
   *  64k    64k    64k    64k    64k    64k                &lt;--
   *  last full stripe
   *  64k    64k    13k    64k    55k     3k              &lt;--
   *  target last stripe
   *  64k    64k           64k     1k
   *  64k    64k           58k
   *  64k    64k
   *  64k    19k
   *  64k                                               &lt;--
   *  total visible stripe
   *
   *  Due to different speed of streamers, the internal blocks in a block group
   *  could have different lengths when the block group isn't ended normally.
   *  The purpose of this class is to recover the UnderConstruction block group,
   *  so all internal blocks end at the same stripe.
   *
   * The steps:
   * 1. get all blocks lengths from DataNodes.
   * 2. calculate safe length, which is at the target last stripe.
   * 3. decode and feed blk_6~8, make them end at last full stripe. (the last
   * full stripe means the last decodable stripe.)
   * 4. encode the target last stripe, with the remaining sequential data. In
   * this case, the sequential data is 64k+64k+13k. Feed blk_6~8 the parity cells.
   * Overwrite the parity cell if have to.
   * 5. truncate the stripes from visible stripe, to target last stripe.
   * TODO: implement step 3,4
   */
BlockRecoveryWorker (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java)/**
 * This class handles the block recovery work commands.
 */
Conf (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java)/**
   * The cached scanner configuration.
   */
BlockSender (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java)/**
 * Reads a block from the disk and sends it to a recipient.
 * 
 * Data sent from the BlockeSender in the following format:
 * <br><b>Data format:</b> <pre>
 *    +--------------------------------------------------+
 *    | ChecksumHeader | Sequence of data PACKETS...     |
 *    +--------------------------------------------------+ 
 * </pre>   
 * <b>ChecksumHeader format:</b> <pre>
 *    +--------------------------------------------------+
 *    | 1 byte CHECKSUM_TYPE | 4 byte BYTES_PER_CHECKSUM |
 *    +--------------------------------------------------+ 
 * </pre>   
 * An empty packet is sent to mark the end of block and read completion.
 * 
 * PACKET Contains a packet header, checksum and data. Amount of data
 * carried is set by BUFFER_SIZE.
 * <pre>
 *   +-----------------------------------------------------+
 *   | Variable length header. See {@link PacketHeader}    |
 *   +-----------------------------------------------------+
 *   | x byte checksum data. x is defined below            |
 *   +-----------------------------------------------------+
 *   | actual data ......                                  |
 *   +-----------------------------------------------------+
 * 
 *   Data is made of Chunks. Each chunk is of length <= BYTES_PER_CHECKSUM.
 *   A checksum is calculated for each chunk.
 *  
 *   x = (length of data + BYTE_PER_CHECKSUM - 1)/BYTES_PER_CHECKSUM *
 *       CHECKSUM_SIZE
 *  
 *   CHECKSUM_SIZE depends on CHECKSUM_TYPE (usually, 4 for CRC32) 
 *  </pre>
 *  
 *  The client reads data until it receives a packet with 
 *  "LastPacketInBlock" set to true or with a zero length. If there is 
 *  no checksum error, it replies to DataNode with OP_STATUS_CHECKSUM_OK.
 */
BPOfferService (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java)/**
 * One instance per block-pool/namespace on the DN, which handles the
 * heartbeats to the active and standby NNs for that namespace.
 * This class manages an instance of {@link BPServiceActor} for each NN,
 * and delegates calls to both NNs. 
 * It also maintains the state about which of the NNs is considered active.
 */
Scheduler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java)/**
   * Utility class that wraps the timestamp computations for scheduling
   * heartbeats and block reports.
   */
BPServiceActor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java)/**
 * A thread per active or standby namenode to perform:
 * <ul>
 * <li> Pre-registration handshake with namenode</li>
 * <li> Registration with namenode</li>
 * <li> Send periodic heartbeats to the namenode</li>
 * <li> Handle commands received from the namenode</li>
 * </ul>
 */
BPServiceActorAction (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActorAction.java)/**
 * Base class for BPServiceActor class
 * Issued by BPOfferSerivce class to tell BPServiceActor 
 * to take several actions.
 */
TrustedFuture (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java)/**
   * A less abstract subclass of AbstractFuture. This can be used to optimize
   * setFuture by ensuring that {@link #get} calls exactly the implementation
   * of {@link AbstractFuture#get}.
   */
Waiter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java)/**
   * Waiter links form a Treiber stack, in the {@link #waiters} field.
   */
Listener (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java)/**
   * Listeners also form a stack through the {@link #listeners} field.
   */
Failure (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java)/**
   * A special value to represent failure, when {@link #setException} is
   * called successfully.
   */
Cancellation (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java)/**
   * A special value to represent cancellation and the 'wasInterrupted' bit.
   */
SetFuture (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java)/**
   * A special value that encodes the 'setFuture' state.
   */
UnsafeAtomicHelper (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java)/**
   * {@link AtomicHelper} based on {@link sun.misc.Unsafe}.
   * <p>
   * <p>Static initialization of this class will fail if the
   * {@link sun.misc.Unsafe} object cannot be accessed.
   */
SafeAtomicHelper (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java)/**
   * {@link AtomicHelper} based on {@link AtomicReferenceFieldUpdater}.
   */
SynchronizedHelper (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java)/**
   * {@link AtomicHelper} based on {@code synchronized} and volatile writes.
   * <p>
   * <p>This is an implementation of last resort for when certain basic VM
   * features are broken (like AtomicReferenceFieldUpdater).
   */
AsyncChecker (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/AsyncChecker.java)/**
 * A class that can be used to schedule an asynchronous check on a given
 * {@link Checkable}. If the check is successfully scheduled then a
 * {@link ListenableFuture} is returned.
 *
 */
Checkable (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/Checkable.java)/**
 * A Checkable is an object whose health can be probed by invoking its
 * {@link #check} method.
 *
 * e.g. a {@link Checkable} instance may represent a single hardware
 * resource.
 */
Callback (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java)/**
   * A callback interface that is supplied the result of running an
   * async disk check on multiple volumes.
   */
ResultHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java)/**
   * A callback to process the results of checking a volume.
   */
DatasetVolumeChecker (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java)/**
 * A class that encapsulates running disk checks against each volume of an
 * {@link FsDatasetSpi} and allows retrieving a list of failed volumes.
 *
 * This splits out behavior that was originally implemented across
 * DataNode, FsDatasetImpl and FsVolumeList.
 */
StorageLocationChecker (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/StorageLocationChecker.java)/**
 * A utility class that encapsulates checking storage locations during DataNode
 * startup.
 *
 * Some of this code was extracted from the DataNode class.
 */
LastCheckResult (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/ThrottledAsyncChecker.java)/**
   * Status of running a check. It can either be a result or an
   * exception, depending on whether the check completed or threw.
   */
ThrottledAsyncChecker (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/ThrottledAsyncChecker.java)/**
 * An implementation of {@link AsyncChecker} that skips checking recently
 * checked objects. It will enforce at least {@link minMsBetweenChecks}
 * milliseconds between two successive checks of any one object.
 *
 * It is assumed that the total number of Checkable objects in the system
 * is small, (not more than a few dozen) since the checker uses O(Checkables)
 * storage and also potentially O(Checkables) threads.
 *
 * {@link minMsBetweenChecks} should be configured reasonably
 * by the caller to avoid spinning up too many threads frequently.
 */
Fire (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/TimeoutFuture.java)/**
   * A runnable that is called when the delegate or the timer completes.
   */
TimeoutFuture (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/TimeoutFuture.java)/**
 * Implementation of {@code Futures#withTimeout}.
 * <p>
 * <p>Future that delegates to another but will finish early (via a
 * {@link TimeoutException} wrapped in an {@link ExecutionException}) if the
 * specified duration expires. The delegate future is interrupted and
 * cancelled if it times out.
 */
ChangedVolumes (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java)/**
   * Contains the StorageLocations for changed data volumes.
   */
DataTransfer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java)/**
   * Used for transferring a block of data.  This class
   * sends a piece of data to another DataNode.
   */
DataNode (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java)/**********************************************************
 * DataNode is a class (and program) that stores a set of
 * blocks for a DFS deployment.  A single deployment can
 * have one or many DataNodes.  Each DataNode communicates
 * regularly with a single NameNode.  It also communicates
 * with client code and other DataNodes from time to time.
 *
 * DataNodes store a series of named blocks.  The DataNode
 * allows client code to read these blocks, or to write new
 * block data.  The DataNode may also, in response to instructions
 * from its NameNode, delete blocks or copy blocks to/from other
 * DataNodes.
 *
 * The DataNode maintains just one critical table:
 *   block{@literal ->} stream of bytes (of BLOCK_SIZE or less)
 *
 * This info is stored on a local disk.  The DataNode
 * reports the table's contents to the NameNode upon startup
 * and every so often afterwards.
 *
 * DataNodes spend their lives in an endless loop of asking
 * the NameNode for something to do.  A NameNode cannot connect
 * to a DataNode directly; a NameNode simply returns values from
 * functions invoked by a DataNode.
 *
 * DataNodes maintain an open server socket so that client code 
 * or other DataNodes can read/write data.  The host/port for
 * this server is reported to the NameNode, which then sends that
 * information to clients or other DataNodes that might be interested.
 *
 **********************************************************/
DataNodeFaultInjector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNodeFaultInjector.java)/**
 * Used for injecting faults in DFSClient and DFSOutputStream tests.
 * Calls into this are a no-op in production code. 
 */
DataNodeMXBean (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNodeMXBean.java)/**
 * 
 * This is the JMX management interface for data node information.
 * End users shouldn't be implementing these interfaces, and instead
 * access this information through the JMX APIs.
 */
DatanodeUtil (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DatanodeUtil.java)/** Provide utility methods for Datanode. */
VolumeBuilder (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java)/**
   * VolumeBuilder holds the metadata (e.g., the storage directories) of the
   * prepared volume returned from
   * {@link #prepareVolume(DataNode, StorageLocation, List)}.
   * Calling {@link VolumeBuilder#build()}
   * to add the metadata to {@link DataStorage} so that this prepared volume can
   * be active.
   */
DataStorage (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java)/** 
 * Data storage information file.
 * <p>
 * @see Storage
 */
DataXceiver (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java)/**
 * Thread for processing incoming/outgoing data stream.
 */
BlockBalanceThrottler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java)/**
   * A manager to make sure that cluster balancing does not take too much
   * resources.
   *
   * It limits the number of block moves for balancing and the total amount of
   * bandwidth they can use.
   */
DataXceiverServer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java)/**
 * Server used for receiving/sending a block of data. This is created to listen
 * for requests from clients or other DataNodes. This small server does not use
 * the Hadoop IPC mechanism.
 */
Stats (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java)/**
   * Stats tracked for reporting and testing, per blockpool
   */
ScanInfoVolumeReport (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java)/**
   * Helper class for compiling block info reports from report compiler threads.
   * Contains a volume, a set of block pool IDs, and a collection of ScanInfo
   * objects. If a block pool exists but has no ScanInfo objects associated with
   * it, there will be no mapping for that particular block pool.
   */
BlockPoolReport (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java)/**
   * Helper class for compiling block info reports per block pool.
   */
ReportCompiler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java)/**
   * The ReportCompiler class encapsulates the process of searching a datanode's
   * disks for block information. It operates by performing a DFS of the volume
   * to discover block information.
   *
   * When the ReportCompiler discovers block information, it create a new
   * ScanInfo object for it and adds that object to its report list. The report
   * list is returned by the {@link #call()} method.
   */
DirectoryScanner (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java)/**
 * Periodically scans the data directories for block and block metadata files.
 * Reconciles the differences with block information maintained in the dataset.
 */
BlockMover (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java)/**
   * BlockMover supports moving blocks across Volumes.
   */
VolumePair (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java)/**
   * Holds source and dest volumes UUIDs and their BasePaths
   * that disk balancer will be operating against.
   */
DiskBalancerMover (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java)/**
   * Actual DataMover class for DiskBalancer.
   * <p>
   */
DiskBalancer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java)/**
 * Worker class for Disk Balancer.
 * <p>
 * Here is the high level logic executed by this class. Users can submit disk
 * balancing plans using submitPlan calls. After a set of sanity checks the plan
 * is admitted and put into workMap.
 * <p>
 * The executePlan launches a thread that picks up work from workMap and hands
 * it over to the BlockMover#copyBlocks function.
 * <p>
 * Constraints :
 * <p>
 * Only one plan can be executing in a datanode at any given time. This is
 * ensured by checking the future handle of the worker thread in submitPlan.
 */
DiskFileCorruptException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskFileCorruptException.java)/**
 * When kernel report a "Input/output error", we use this exception to
 * represents some corruption(e.g. bad disk track) happened on some disk file.
 */
DNConf (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DNConf.java)/**
 * Simple class encapsulating all of the configuration that the DataNode
 * loads at startup time.
 */
ErasureCodingWorker (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java)/**
 * ErasureCodingWorker handles the erasure coding reconstruction work commands.
 * These commands would be issued from Namenode as part of Datanode's heart beat
 * response. BPOfferService delegates the work to this class for handling EC
 * commands.
 */
StripedBlockChecksumCompositeCrcReconstructor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumCompositeCrcReconstructor.java)/**
 * Computes striped composite CRCs over reconstructed chunk CRCs.
 */
StripedBlockChecksumMd5CrcReconstructor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumMd5CrcReconstructor.java)/**
 * Computes running MD5-of-CRC over reconstructed chunk CRCs.
 */
StripedBlockChecksumReconstructor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumReconstructor.java)/**
 * StripedBlockChecksumReconstructor reconstruct one or more missed striped
 * block in the striped block group, the minimum number of live striped blocks
 * should be no less than data block number. Then checksum will be recalculated
 * using the newly reconstructed block.
 */
StripedBlockReader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReader.java)/**
 * StripedBlockReader is used to read block data from one source DN, it contains
 * a block reader, read buffer and striped block index.
 * Only allocate StripedBlockReader once for one source, and the StripedReader
 * has the same array order with sources. Typically we only need to allocate
 * minimum number (minRequiredSources) of StripedReader, and allocate
 * new for new source DN if some existing DN invalid or slow.
 * If some source DN is corrupt, set the corresponding blockReader to
 * null and will never read from it again.
 */
StripedBlockReconstructor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java)/**
 * StripedBlockReconstructor reconstruct one or more missed striped block in
 * the striped block group, the minimum number of live striped blocks should
 * be no less than data block number.
 */
StripedBlockWriter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockWriter.java)/**
 * A striped block writer that writes reconstructed data to the remote target
 * datanode.
 */
StripedReader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReader.java)/**
 * Manage striped readers that performs reading of block data from remote to
 * serve input data for the erasure decoding.
 */
StripedReconstructionInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructionInfo.java)/**
 * Stores striped block info that can be used for block reconstruction.
 */
StripedReconstructor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructor.java)/**
 * StripedReconstructor reconstruct one or more missed striped block in the
 * striped block group, the minimum number of live striped blocks should be
 * no less than data block number.
 *
 * | <- Striped Block Group -> |
 *  blk_0      blk_1       blk_2(*)   blk_3   ...   <- A striped block group
 *    |          |           |          |
 *    v          v           v          v
 * +------+   +------+   +------+   +------+
 * |cell_0|   |cell_1|   |cell_2|   |cell_3|  ...
 * +------+   +------+   +------+   +------+
 * |cell_4|   |cell_5|   |cell_6|   |cell_7|  ...
 * +------+   +------+   +------+   +------+
 * |cell_8|   |cell_9|   |cell10|   |cell11|  ...
 * +------+   +------+   +------+   +------+
 *  ...         ...       ...         ...
 *
 *
 * We use following steps to reconstruct striped block group, in each round, we
 * reconstruct <code>bufferSize</code> data until finish, the
 * <code>bufferSize</code> is configurable and may be less or larger than
 * cell size:
 * step1: read <code>bufferSize</code> data from minimum number of sources
 *        required by reconstruction.
 * step2: decode data for targets.
 * step3: transfer data to targets.
 *
 * In step1, try to read <code>bufferSize</code> data from minimum number
 * of sources , if there is corrupt or stale sources, read from new source
 * will be scheduled. The best sources are remembered for next round and
 * may be updated in each round.
 *
 * In step2, typically if source blocks we read are all data blocks, we
 * need to call encode, and if there is one parity block, we need to call
 * decode. Notice we only read once and reconstruct all missed striped block
 * if they are more than one.
 *
 * In step3, send the reconstructed data to targets by constructing packet
 * and send them directly. Same as continuous block replication, we
 * don't check the packet ack. Since the datanode doing the reconstruction work
 * are one of the source datanodes, so the reconstructed data are sent
 * remotely.
 *
 * There are some points we can do further improvements in next phase:
 * 1. we can read the block file directly on the local datanode,
 *    currently we use remote block reader. (Notice short-circuit is not
 *    a good choice, see inline comments).
 * 2. We need to check the packet ack for EC reconstruction? Since EC
 *    reconstruction is more expensive than continuous block replication,
 *    it needs to read from several other datanodes, should we make sure the
 *    reconstructed result received by targets?
 */
StripedWriter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedWriter.java)/**
 * Manage striped writers that writes to a target with reconstructed data.
 */
ErrorReportAction (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ErrorReportAction.java)/**
 * A ErrorReportAction is an instruction issued by BPOfferService to 
 * BPServiceActor about a particular block encapsulated in errorMessage.
 */
FaultInjectorFileIoEvents (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FaultInjectorFileIoEvents.java)/**
 * Injects faults in the metadata and data related operations on datanode
 * volumes.
 */
WrappedFileInputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FileIoProvider.java)/**
   * A thin wrapper over {@link FileInputStream} that allows
   * instrumenting disk IO.
   */
WrappedFileOutputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FileIoProvider.java)/**
   * A thin wrapper over {@link FileOutputStream} that allows
   * instrumenting disk IO.
   */
WrappedRandomAccessFile (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FileIoProvider.java)/**
   * A thin wrapper over {@link FileInputStream} that allows
   * instrumenting IO.
   */
FileIoProvider (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FileIoProvider.java)/**
 * This class abstracts out various file IO operations performed by the
 * DataNode and invokes profiling (for collecting stats) and fault injection
 * (for testing) event hooks before and after each file IO.
 *
 * Behavior can be injected into these events by enabling the
 * profiling and/or fault injection event hooks through
 * {@link DFSConfigKeys#DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY}
 * and {@link DFSConfigKeys#DFS_DATANODE_ENABLE_FILEIO_FAULT_INJECTION_KEY}.
 * These event hooks are disabled by default.
 *
 * Most functions accept an optional {@link FsVolumeSpi} parameter for
 * instrumentation/logging.
 *
 * Some methods may look redundant, especially the multiple variations of
 * move/rename/list. They exist to retain behavior compatibility for existing
 * code.
 */
FinalizedProvidedReplica (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedProvidedReplica.java)/**
 * This class is used for provided replicas that are finalized.
 */
FinalizedReplica (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java)/**
 * This class describes a replica that has been finalized.
 */
Builder (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSCachingGetSpaceUsed.java)/**
   * The builder class.
   */
FSCachingGetSpaceUsed (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSCachingGetSpaceUsed.java)/**
 * Fast and accurate class to tell how much space HDFS is using.
 */
AvailableSpaceVolumeList (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java)/**
   * Used to keep track of the list of volumes we're choosing from.
   */
AvailableSpaceVolumePair (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java)/**
   * Used so that we only check the available space on a given volume once, at
   * the beginning of
   * {@link AvailableSpaceVolumeChoosingPolicy#chooseVolume}.
   */
AvailableSpaceVolumeChoosingPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java)/**
 * A DN volume choosing policy which takes into account the amount of free
 * space on each of the available volumes when considering where to assign a
 * new replica allocation. By default this policy prefers assigning replicas to
 * those volumes with more available free space, so as to over time balance the
 * available space of all the volumes within a DN.
 * Use fine-grained locks to enable choosing volumes of different storage
 * types concurrently.
 */
DataNodeVolumeMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/DataNodeVolumeMetrics.java)/**
 * This class is for maintaining Datanode Volume IO related statistics and
 * publishing them through the metrics interfaces.
 */
Factory (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java)/**
   * A factory for creating {@link FsDatasetSpi} objects.
   */
FsVolumeReferences (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java)/**
   * It behaviors as an unmodifiable list of FsVolume. Individual FsVolume can
   * be obtained by using {@link #get(int)}.
   *
   * This also holds the reference counts for these volumes. It releases all the
   * reference counts in {@link #close()}.
   */
FsDatasetSpi (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java)/**
 * This is a service provider interface for the underlying storage that
 * stores replicas for a data node.
 * The default implementation stores replicas on local drives. 
 */
FsVolumeReference (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeReference.java)/**
 * This holds volume reference count as AutoClosable resource.
 * It increases the reference count by one in the constructor, and decreases
 * the reference count by one in {@link #close()}.
 *
 * <pre>
 *  {@code
 *    try (FsVolumeReference ref = volume.obtainReference()) {
 *      // Do IOs on the volume
 *      volume.createRwb(...);
 *      ...
 *    }
 *  }
 * </pre>
 */
BlockIterator (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.java)/**
   * BlockIterator will return ExtendedBlock entries from a block pool in
   * this volume.  The entries will be returned in sorted order.<p>
   *
   * BlockIterator objects themselves do not always have internal
   * synchronization, so they can only safely be used by a single thread at a
   * time.<p>
   *
   * Closing the iterator does not save it.  You must call save to save it.
   */
ScanInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.java)/**
   * Tracks the files and other information related to a block on the disk
   * Missing file is indicated by setting the corresponding member
   * to null.
   *
   * Because millions of these structures may be created, we try to save
   * memory here.  So instead of storing full paths, we store path suffixes.
   * The block file, if it exists, will have a path like this:
   * {@literal <volume_base_path>/<block_path>}
   * So we don't need to store the volume path, since we already know what the
   * volume is.
   *
   * The metadata file, if it exists, will have a path like this:
   * {@literal <volume_base_path>/<block_path>_<genstamp>.meta}
   * So if we have a block file, there isn't any need to store the block path
   * again.
   *
   * The accessor functions take care of these manipulations.
   */
VolumeCheckContext (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.java)/**
   * Context for the {@link #check} call.
   */
FsVolumeSpi (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.java)/**
 * This is an interface for the underlying volume.
 */
AddBlockPoolException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/AddBlockPoolException.java)/**
 * This exception collects all IOExceptions thrown when adding block pools and
 * scanning volumes. It keeps the information about which volume is associated
 * with an exception.
 *
 */
AddReplicaProcessor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java)/**
   * Recursive action for add replica in map.
   */
BlockPoolSlice (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java)/**
 * A block pool slice represents a portion of a block pool stored on a volume.
 * Taken together, all BlockPoolSlices sharing a block pool ID across a
 * cluster represent a single block pool.
 *
 * This class is synchronized by {@link FsVolumeImpl}.
 */
PageRounder (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CacheStats.java)/**
   * Used to count operating system page size.
   */
UsedBytesCount (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CacheStats.java)/**
   * Counts used bytes for memory.
   */
CacheStats (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CacheStats.java)/**
 * Keeps statistics for the memory cache.
 */
ReplicaFileDeleteTask (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java)/** A task for deleting a block file and its associated meta file, as well
   *  as decrement the dfs usage of the volume.
   *  Optionally accepts a trash directory. If one is specified then the files
   *  are moved to trash instead of being deleted. If none is specified then the
   *  files are deleted immediately.
   */
FsDatasetAsyncDiskService (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java)/**
 * This class is a container of multiple thread pools, each for a volume,
 * so that we can schedule async disk operations easily.
 * 
 * Examples of async disk operations are deletion of block files.
 * We don't want to create a new thread for each of the deletion request, and
 * we don't want to do all deletions in the heartbeat thread since deletion
 * can be slow, and we don't want to use a single thread pool because that
 * is inefficient when we have more than 1 volume.  AsyncDiskService is the
 * solution for these.
 * Another example of async disk operation is requesting sync_file_range().
 * 
 * This class and {@link org.apache.hadoop.util.AsyncDiskService} are similar.
 * They should be combined.
 */
Value (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java)/**
   * MappableBlocks that we know about.
   */
CachingTask (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java)/**
   * Background worker that mmaps, mlocks, and checksums a block
   */
FsDatasetCache (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java)/**
 * Manages caching for an FsDatasetImpl by using the mmap(2) and mlock(2)
 * system calls to lock blocks into memory. Block checksums are verified upon
 * entry into the cache.
 */
FsDatasetFactory (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetFactory.java)/**
 * A factory for creating {@link FsDatasetImpl} objects.
 */
VolumeInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java)/**
   * Class for representing the Datanode volume information
   */
FsDatasetImpl (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java)/**************************************************
 * FSDataset manages a set of data blocks.  Each block
 * has a unique name and an extent on disk.
 *
 ***************************************************/
FsDatasetUtil (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java)/** Utility methods. */
BlockIteratorImpl (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java)/**
   * A BlockIterator implementation for FsVolumeImpl.
   */
FsVolumeImpl (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java)/**
 * The underlying volume used to store replica.
 *
 * It uses the {@link FsDatasetImpl} object for synchronization.
 */
FsVolumeImplBuilder (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImplBuilder.java)/**
 * This class is to be used as a builder for {@link FsVolumeImpl} objects.
 */
MappableBlock (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlock.java)/**
 * Represents an HDFS block that is mapped by the DataNode.
 */
MappableBlockLoader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlockLoader.java)/**
 * Maps block to DataNode cache region.
 */
MappableBlockLoaderFactory (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MappableBlockLoaderFactory.java)/**
 * Creates MappableBlockLoader.
 */
MemoryMappableBlockLoader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MemoryMappableBlockLoader.java)/**
 * Maps block to memory.
 */
MemoryMappedBlock (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MemoryMappedBlock.java)/**
 * Represents an HDFS block that is mapped to memory by the DataNode.
 */
NativePmemMappableBlockLoader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/NativePmemMappableBlockLoader.java)/**
 * Map block to persistent memory with native PMDK libs.
 */
NativePmemMappedBlock (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/NativePmemMappedBlock.java)/**
 * Represents an HDFS block that is mapped to persistent memory by the DataNode.
 */
PmemMappableBlockLoader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemMappableBlockLoader.java)/**
 * Maps block to persistent memory by using mapped byte buffer.
 */
PmemMappedBlock (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemMappedBlock.java)/**
 * Represents an HDFS block that is mapped to persistent memory by DataNode
 * with mapped byte buffer. PMDK is NOT involved in this implementation.
 */
UsedBytesCount (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemVolumeManager.java)/**
   * Counts used bytes for persistent memory.
   */
PmemVolumeManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemVolumeManager.java)/**
 * Manage the persistent memory volumes.
 */
ProvidedVolumeDF (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java)/**
   * Class to keep track of the capacity usage statistics for provided volumes.
   */
ProvidedVolumeImpl (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java)/**
 * This class is used to create provided volumes.
 */
RamDiskAsyncLazyPersistService (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java)/**
 * This class is a container of multiple thread pools, one for each non-RamDisk
 * volume with a maximum thread count of 1 so that we can schedule async lazy
 * persist operations easily with volume arrival and departure handled.
 *
 * This class and {@link org.apache.hadoop.util.AsyncDiskService} are similar.
 * They should be combined.
 */
RamDiskReplicaLruTracker (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaLruTracker.java)/**
 * An implementation of RamDiskReplicaTracker that uses an LRU
 * eviction scheme.
 */
ReplicaCachingGetSpaceUsed (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReplicaCachingGetSpaceUsed.java)/**
 * Fast and accurate class to tell how much space HDFS is using. This class gets
 * hdfs used space from FsDatasetImpl#volumeMap#ReplicaInfos that uses an in
 * memory way.
 *
 * Getting hdfs used space by ReplicaCachingGetSpaceUsed impl only includes
 * block and meta files, but DU impl is blockpool dir based statistics that will
 * include additional files, e.g. tmp dir, scanner.cursor file. Getting space
 * used by DU impl will be greater than by ReplicaCachingGetSpaceUsed impl, but
 * the latter is more accurate.
 *
 * Setting fs.getspaceused.classname to
 * org.apache.hadoop.hdfs.server.datanode.fsdataset
 * impl.ReplicaCachingGetSpaceUsed in your core-site.xml if we want to enable.
 */
ReplicaMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReplicaMap.java)/**
 * Maintains the replica map. 
 */
Builder (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.java)/**
   * Used for creating instances of ReservedSpaceCalculator.
   */
ReservedSpaceCalculatorAbsolute (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.java)/**
   * Based on absolute number of reserved bytes.
   */
ReservedSpaceCalculatorPercentage (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.java)/**
   * Based on percentage of total capacity in the storage.
   */
ReservedSpaceCalculatorConservative (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.java)/**
   * Calculates absolute and percentage based reserved space and
   * picks the one that will yield more reserved space.
   */
ReservedSpaceCalculatorAggressive (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.java)/**
   * Calculates absolute and percentage based reserved space and
   * picks the one that will yield less reserved space.
   */
ReservedSpaceCalculator (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.java)/**
 * Used for calculating file system space reserved for non-HDFS data.
 */
VolumeFailureInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/VolumeFailureInfo.java)/**
 * Tracks information about failure of a data volume.
 */
LengthInputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/LengthInputStream.java)/**
 * An input stream with length.
 */
ReplicaInputStreams (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/ReplicaInputStreams.java)/**
 * Contains the input streams for the data and checksum of a replica.
 */
ReplicaOutputStreams (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/ReplicaOutputStreams.java)/**
 * Contains the output streams for the data and checksum of a replica.
 */
RoundRobinVolumeChoosingPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/RoundRobinVolumeChoosingPolicy.java)/**
 * Choose volumes with the same storage type in round-robin order.
 * Use fine-grained locks to synchronize volume choosing.
 */
VolumeChoosingPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/VolumeChoosingPolicy.java)/**
 * This interface specifies the policy for choosing volumes to store replicas.
 */
IncrementalBlockReportManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/IncrementalBlockReportManager.java)/**
 * Manage Incremental Block Reports (IBRs).
 */
LocalReplica (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplica.java)/**
 * This class is used for all replicas which are on local storage media
 * and hence, are backed by files.
 */
LocalReplicaInPipeline (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java)/**
 * This class defines a replica in a pipeline, which
 * includes a persistent replica being written to by a dfs client or
 * a temporary replica being replicated by a source datanode or
 * being copied for the balancing purpose.
 *
 * The base class implements a temporary replica
 */
DataNodeDiskMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java)/**
 * This class detects and maintains DataNode disk outliers and their
 * latencies for different ops (metadata, read, write).
 */
DataNodeMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java)/**
 *
 * This class is for maintaining  the various DataNode statistics
 * and publishing them through the metrics interfaces.
 * This also registers the JMX MBean for RPC.
 * <p>
 * This class has a number of metrics variables that are publicly accessible;
 * these variables (objects) have methods to update their values;
 *  for example:
 *  <p> {@link #blocksRead}.inc()
 *
 */
DataNodePeerMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodePeerMetrics.java)/**
 * This class maintains DataNode peer metrics (e.g. numOps, AvgTime, etc.) for
 * various peer operations.
 */
FSDatasetMBean (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/FSDatasetMBean.java)/**
 * 
 * This Interface defines the methods to get the status of a the FSDataset of
 * a data node.
 * It is also used for publishing via JMX (hence we follow the JMX naming
 * convention.) 
 *  * Note we have not used the MetricsDynamicMBeanBase to implement this
 * because the interface for the FSDatasetMBean is stable and should
 * be published as an interface.
 * 
 * <p>
 * Data Node runtime statistic  info is report in another MBean
 * @see org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics
 *
 */
OutlierDetector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/OutlierDetector.java)/**
 * A utility class to help detect resources (nodes/ disks) whose aggregate
 * latency is an outlier within a given set.
 *
 * We use the median absolute deviation for outlier detection as
 * described in the following publication:
 *
 * Leys, C., et al., Detecting outliers: Do not use standard deviation
 * around the mean, use absolute deviation around the median.
 * http://dx.doi.org/10.1016/j.jesp.2013.03.013
 *
 * We augment the above scheme with the following heuristics to be even
 * more conservative:
 *
 *  1. Skip outlier detection if the sample size is too small.
 *  2. Never flag resources whose aggregate latency is below a low threshold.
 *  3. Never flag resources whose aggregate latency is less than a small
 *     multiple of the median.
 */
ProfilingFileIoEvents (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ProfilingFileIoEvents.java)/**
 * Profiles the performance of the metadata and data related operations on
 * datanode volumes.
 */
ProvidedReplica (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ProvidedReplica.java)/**
 * This abstract class is used as a base class for provided replicas.
 */
Replica (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/Replica.java)/** 
 * This represents block replicas which are stored in DataNode.
 */
ReplicaAlreadyExistsException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaAlreadyExistsException.java)/**
 * Exception indicating that the target block already exists 
 * and is not set to be recovered/overwritten.  
 */
ReplicaBeingWritten (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaBeingWritten.java)/** This class represents replicas being written. 
 * Those are the replicas that
 * are created in a pipeline initiated by a dfs client.
 */
ReplicaBuilder (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaBuilder.java)/**
 * This class is to be used as a builder for {@link ReplicaInfo} objects.
 * The state of the replica is used to determine which object is instantiated.
 */
ReplicaHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaHandler.java)/**
 * This class includes a replica being actively written and the reference to
 * the fs volume where this replica is located.
 */
ReplicaInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaInfo.java)/**
 * This class is used by datanodes to maintain meta data of its replicas.
 * It provides a general interface for meta information of a replica.
 */
ReplicaInPipeline (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaInPipeline.java)/** 
 * This defines the interface of a replica in Pipeline that's being written to
 */
ReplicaUnderRecovery (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaUnderRecovery.java)/**
 * This class represents replicas that are under block recovery
 * It has a recovery id that is equal to the generation stamp 
 * that the replica will be bumped to after recovery
 * The recovery id is used to handle multiple concurrent block recoveries.
 * A recovery with higher recovery id preempts recoveries with a lower id.
 *
 */
ReplicaWaitingToBeRecovered (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaWaitingToBeRecovered.java)/**
 * This class represents a replica that is waiting to be recovered.
 * After a datanode restart, any replica in "rbw" directory is loaded
 * as a replica waiting to be recovered.
 * A replica waiting to be recovered does not provision read nor
 * participates in any pipeline recovery. It will become outdated if its
 * client continues to write or be recovered as a result of
 * lease recovery.
 */
ReportBadBlockAction (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReportBadBlockAction.java)/**
 * ReportBadBlockAction is an instruction issued by {{BPOfferService}} to
 * {{BPServiceActor}} to report bad block to namenode
 *
 */
SecureResources (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/SecureDataNodeStarter.java)/**
   * Stash necessary resources needed for datanode operation in a secure env.
   */
SecureDataNodeStarter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/SecureDataNodeStarter.java)/**
 * Utility class to start a datanode in a secure cluster, first obtaining 
 * privileged resources before main startup and handing them to the datanode.
 */
ShortCircuitRegistry (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java)/**
 * Manages client short-circuit memory segments on the DataNode.
 *
 * DFSClients request shared memory segments from the DataNode.  The 
 * ShortCircuitRegistry generates and manages these segments.  Each segment
 * has a randomly generated 128-bit ID which uniquely identifies it.  The
 * segments each contain several "slots."
 *
 * Before performing a short-circuit read, DFSClients must request a pair of
 * file descriptors from the DataNode via the REQUEST_SHORT_CIRCUIT_FDS
 * operation.  As part of this operation, DFSClients pass the ID of the shared
 * memory segment they would like to use to communicate information about this
 * replica, as well as the slot number within that segment they would like to
 * use.  Slot allocation is always done by the client.
 *
 * Slots are used to track the state of the block on the both the client and
 * datanode. When this DataNode mlocks a block, the corresponding slots for the
 * replicas are marked as "anchorable".  Anchorable blocks can be safely read
 * without verifying the checksum.  This means that BlockReaderLocal objects
 * using these replicas can skip checksumming.  It also means that we can do
 * zero-copy reads on these replicas (the ZCR interface has no way of
 * verifying checksums.)
 * 
 * When a DN needs to munlock a block, it needs to first wait for the block to
 * be unanchored by clients doing a no-checksum read or a zero-copy read. The 
 * DN also marks the block's slots as "unanchorable" to prevent additional 
 * clients from initiating these operations in the future.
 * 
 * The counterpart of this class on the client is {@link DfsClientShmManager}.
 */
CheckContext (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/StorageLocation.java)/**
   * Class to hold the parameters for running a {@link #check}.
   */
StorageLocation (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/StorageLocation.java)/**
 * Encapsulates the URI and storage medium that together describe a
 * storage directory.
 * The default storage medium is assumed to be DISK, if none is specified.
 *
 */
UnexpectedReplicaStateException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/UnexpectedReplicaStateException.java)/**
 * Exception indicating that the replica is in an unexpected state
 */
VolumeScanner (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java)/**
 * VolumeScanner scans a single volume.  Each VolumeScanner has its own thread.
 * <p>They are all managed by the DataNode's BlockScanner.
 */
MapBasedFilterConfig (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.java)/**
   * Since the DataNode HTTP server is not implemented in terms of the
   * servlet API, it
   * takes some extra effort to obtain an instance of the filter.  This
   * method provides
   * a minimal {@link FilterConfig} implementation backed by a {@link Map}.
   * Call this from
   * your filter handler to initialize a servlet filter.
   */
DatanodeHttpServer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.java)/**
 * Data node HTTP Server Class.
 */
NettyHttpInteraction (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/RestCsrfPreventionFilterHandler.java)/**
   * {@link HttpInteraction} implementation for use in a Netty pipeline.
   */
RestCsrfPreventionFilterHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/RestCsrfPreventionFilterHandler.java)/**
 * Netty handler that integrates with the {@link RestCsrfPreventionFilter}.  If
 * the filter determines that the request is allowed, then this handler forwards
 * the request to the next handler in the Netty pipeline.  Otherwise, this
 * handler drops the request and immediately sends an HTTP 400 response.
 */
SimpleHttpProxyHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/SimpleHttpProxyHandler.java)/**
 * Dead simple session-layer HTTP proxy. It gets the HTTP responses
 * inside the context, assuming that the remote peer is reasonable fast and
 * the response is small. The upper layer should be filtering out malicious
 * inputs.
 */
DataNodeUGIProvider (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/DataNodeUGIProvider.java)/**
 * Create UGI from the request for the WebHDFS requests for the DNs. Note that
 * the DN does not authenticate the UGI -- the NN will authenticate them in
 * subsequent operations.
 */
CancelCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/CancelCommand.java)/**
 * Cancels a running plan.
 */
Command (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java)/**
 * Common interface for command handling.
 */
ExecuteCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/ExecuteCommand.java)/**
 * executes a given plan.
 */
HelpCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/HelpCommand.java)/**
 * Help Command prints out detailed help about each command.
 */
PlanCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java)/**
 * Class that implements Plan Command.
 * <p>
 * Plan command reads the Cluster Info and creates a plan for specified data
 * node or a set of Data nodes.
 * <p>
 * It writes the output to a default location unless changed by the user.
 */
QueryCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java)/**
 * Gets the current status of disk balancer command.
 */
ReportCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/ReportCommand.java)/**
 * Executes the report command.
 *
 * This command will report volume information for a specific DataNode or top X
 * DataNode(s) benefiting from running DiskBalancer.
 *
 * This is done by reading the cluster info, sorting the DiskbalancerNodes by
 * their NodeDataDensity and printing out the info.
 */
ClusterConnector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/ClusterConnector.java)/**
 * ClusterConnector interface hides all specifics about how we communicate to
 * the HDFS cluster. This interface returns data in classes that diskbalancer
 * understands.
 */
ConnectorFactory (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/ConnectorFactory.java)/**
 * Connector factory creates appropriate connector based on the URL.
 */
DBNameNodeConnector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/DBNameNodeConnector.java)/**
 * DBNameNodeConnector connects to Namenode and extracts information from a
 * given cluster.
 */
JsonNodeConnector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/JsonNodeConnector.java)/**
 * A connector that understands JSON data cluster models.
 */
DiskBalancerCluster (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.java)/**
 * DiskBalancerCluster represents the nodes that we are working against.
 * <p>
 * Please Note :
 * Semantics of inclusionList and exclusionLists.
 * <p>
 * If a non-empty inclusionList is specified then the diskBalancer assumes that
 * the user is only interested in processing that list of nodes. This node list
 * is checked against the exclusionList and only the nodes in inclusionList but
 * not in exclusionList is processed.
 * <p>
 * if inclusionList is empty, then we assume that all live nodes in the nodes is
 * to be processed by diskBalancer. In that case diskBalancer will avoid any
 * nodes specified in the exclusionList but will process all nodes in the
 * cluster.
 * <p>
 * In other words, an empty inclusionList is means all the nodes otherwise
 * only a given list is processed and ExclusionList is always honored.
 */
DiskBalancerDataNode (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerDataNode.java)/**
 * DiskBalancerDataNode represents a DataNode that exists in the cluster. It
 * also contains a metric called nodeDataDensity which allows us to compare
 * between a set of Nodes.
 */
DiskBalancerVolume (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolume.java)/**
 * DiskBalancerVolume represents a volume in the DataNode.
 */
DiskBalancerVolumeSet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolumeSet.java)/**
 * DiskBalancerVolumeSet is a collection of storage devices on the
 * data node which are of similar StorageType.
 */
DiskBalancerConstants (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/DiskBalancerConstants.java)/**
 * Constants used by Disk Balancer.
 */
DiskBalancerException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/DiskBalancerException.java)/**
 * Disk Balancer Exceptions.
 */
GreedyPlanner (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java)/**
 * Greedy Planner is a simple planner that computes the largest possible move at
 * any point of time given a volumeSet.
 * <p>
 * This is done by choosing the disks with largest  amount of data above and
 * below the idealStorage and then a move is scheduled between them.
 */
MoveStep (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/MoveStep.java)/**
 * Ignore fields with default values. In most cases Throughtput, diskErrors
 * tolerancePercent and bandwidth will be the system defaults.
 * So we will avoid serializing them into JSON.
 */
NodePlan (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/NodePlan.java)/**
 * NodePlan is a set of volumeSetPlans.
 */
Planner (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/Planner.java)/**
 * Planner interface allows different planners to be created.
 */
PlannerFactory (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/PlannerFactory.java)/**
 * Returns a planner based on the user defined tags.
 */
Step (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/Step.java)/**
 * A step in the plan.
 */
AclFeature (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AclFeature.java)/**
 * Feature that represents the ACLs of the inode.
 */
AclStorage (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AclStorage.java)/**
 * AclStorage contains utility methods that define how ACL data is stored in the
 * namespace.
 *
 * If an inode has an ACL, then the ACL bit is set in the inode's
 * {@link FsPermission} and the inode also contains an {@link AclFeature}.  For
 * the access ACL, the owner and other entries are identical to the owner and
 * other bits stored in FsPermission, so we reuse those.  The access mask entry
 * is stored into the group permission bits of FsPermission.  This is consistent
 * with other file systems' implementations of ACLs and eliminates the need for
 * special handling in various parts of the codebase.  For example, if a user
 * calls chmod to change group permission bits on a file with an ACL, then the
 * expected behavior is to change the ACL's mask entry.  By saving the mask entry
 * into the group permission bits, chmod continues to work correctly without
 * special handling.  All remaining access entries (named users and named groups)
 * are stored as explicit {@link AclEntry} instances in a list inside the
 * AclFeature.  Additionally, all default entries are stored in the AclFeature.
 *
 * The methods in this class encapsulate these rules for reading or writing the
 * ACL entries to the appropriate location.
 *
 * The methods in this class assume that input ACL entry lists have already been
 * validated and sorted according to the rules enforced by
 * {@link AclTransformation}.
 */
ValidatedAclSpec (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AclTransformation.java)/**
   * An ACL spec that has been pre-validated and sorted.
   */
AclTransformation (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AclTransformation.java)/**
 * AclTransformation defines the operations that can modify an ACL.  All ACL
 * modifications take as input an existing ACL and apply logic to add new
 * entries, modify existing entries or remove old entries.  Some operations also
 * accept an ACL spec: a list of entries that further describes the requested
 * change.  Different operations interpret the ACL spec differently.  In the
 * case of adding an ACL to an inode that previously did not have one, the
 * existing ACL can be a "minimal ACL" containing exactly 3 entries for owner,
 * group and other, all derived from the {@link FsPermission} bits.
 *
 * The algorithms implemented here require sorted lists of ACL entries.  For any
 * existing ACL, it is assumed that the entries are sorted.  This is because all
 * ACL creation and modification is intended to go through these methods, and
 * they all guarantee correct sort order in their outputs.  However, an ACL spec
 * is considered untrusted user input, so all operations pre-sort the ACL spec as
 * the first step.
 */
AuditLogger (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/AuditLogger.java)/**
 * Interface defining an audit logger.
 */
BackupImage (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java)/**
 * Extension of FSImage for the backup node.
 * This class handles the setup of the journaling 
 * spool on the backup namenode.
 */
BackupJournalManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupJournalManager.java)/**
 * A JournalManager implementation that uses RPCs to log transactions
 * to a BackupNode.
 */
BackupNode (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java)/**
 * BackupNode.
 * <p>
 * Backup node can play two roles.
 * <ol>
 * <li>{@link NamenodeRole#CHECKPOINT} node periodically creates checkpoints, 
 * that is downloads image and edits from the active node, merges them, and
 * uploads the new image back to the active.</li>
 * <li>{@link NamenodeRole#BACKUP} node keeps its namespace in sync with the
 * active node, and periodically creates checkpoints by simply saving the
 * namespace image to local disk(s).</li>
 * </ol>
 */
CachedBlock (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CachedBlock.java)/**
 * Represents a cached block.
 */
CacheManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java)/**
 * The Cache Manager handles caching on DataNodes.
 *
 * This class is instantiated by the FSNamesystem.
 * It maintains the mapping of cached blocks to datanodes via processing
 * datanode cache reports. Based on these reports and addition and removal of
 * caching directives, we will schedule caching and uncaching work.
 */
CachePool (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CachePool.java)/**
 * A CachePool describes a set of cache resources being managed by the NameNode.
 * User caching requests are billed to the cache pool specified in the request.
 *
 * This is an internal class, only used on the NameNode.  For identifying or
 * describing a cache pool to clients, please use CachePoolInfo.
 * 
 * CachePools must be accessed under the FSNamesystem lock.
 */
CheckableNameNodeResource (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CheckableNameNodeResource.java)/**
 * Implementers of this class represent a NN resource whose availability can be
 * checked. A resource can be either "required" or "redundant". All required
 * resources must be available for the NN to continue operating. The NN will
 * continue to operate as long as *any* redundant resource is available.
 */
Checkpointer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java)/**
 * The Checkpointer is responsible for supporting periodic checkpoints 
 * of the HDFS metadata.
 *
 * The Checkpointer is a daemon that periodically wakes up
 * up (determined by the schedule specified in the configuration),
 * triggers a periodic checkpoint and then goes back to sleep.
 *
 * The start of a checkpoint is triggered by one of the two factors:
 * (1) time or (2) the size of the edits file.
 */
CheckpointFaultInjector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CheckpointFaultInjector.java)/**
 * Utility class to faciliate some fault injection tests for the checkpointing
 * process.
 */
CheckpointSignature (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CheckpointSignature.java)/**
 * A unique signature intended to identify checkpoint transactions.
 */
ContentCounts (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ContentCounts.java)/**
 * The counter to be computed for content types such as file, directory and symlink,
 * and the storage type usage such as SSD, DISK, ARCHIVE.
 */
DefaultAuditLogger (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/DefaultAuditLogger.java)/**
 * This class provides an interface for Namenode and Router to Audit events
 * information. This class can be extended and can be used when no access logger
 * is defined in the config file.
 */
DefaultINodeAttributesProvider (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/DefaultINodeAttributesProvider.java)/**
 * A default implementation of the INodeAttributesProvider
 *
 */
DfsServlet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/DfsServlet.java)/**
 * A base class for the servlets in DFS.
 */
DirectoryWithQuotaFeature (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/DirectoryWithQuotaFeature.java)/**
 * Quota feature for {@link INodeDirectory}. 
 */
ECTopologyVerifierResult (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ECTopologyVerifierResult.java)/**
 * Result of the verification whether the current cluster setup can
 * support all enabled EC policies.
 */
ByteBufferInputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogBackupInputStream.java)/**
   * A ByteArrayInputStream, which lets modify the underlying byte array.
   */
EditLogBackupInputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogBackupInputStream.java)/**
 * An implementation of the abstract class {@link EditLogInputStream},
 * which is used to updates HDFS meta-data state on a backup node.
 * 
 * @see org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol#journal
 * (org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,
 *  int, int, byte[])
 */
EditLogBackupOutputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogBackupOutputStream.java)/**
 * An implementation of the abstract class {@link EditLogOutputStream},
 * which streams edits to a backup node.
 * 
 * @see org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol#journal
 * (org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,
 *  int, int, byte[])
 */
LogHeaderCorruptException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileInputStream.java)/**
   * Exception indicating that the header of an edits log file is
   * corrupted. This can be because the header is not present,
   * or because the header data is invalid (eg claims to be
   * over a newer version than the running NameNode)
   */
EditLogFileInputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileInputStream.java)/**
 * An implementation of the abstract class {@link EditLogInputStream}, which
 * reads edits from a file. That file may be either on the local disk or
 * accessible via a URL.
 */
EditLogFileOutputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java)/**
 * An implementation of the abstract class {@link EditLogOutputStream}, which
 * stores edits in a local file.
 */
EditLogInputException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogInputException.java)/**
 * Thrown when there's a failure to read an edit log op from disk when loading
 * edits.
 */
EditLogInputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogInputStream.java)/**
 * A generic abstract class to support reading edits log data from 
 * persistent storage.
 * 
 * It should stream bytes from the storage exactly as they were written
 * into the #{@link EditLogOutputStream}.
 */
EditLogOutputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogOutputStream.java)/**
 * A generic abstract class to support journaling of edits logs into 
 * a persistent storage.
 */
EditsDoubleBuffer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditsDoubleBuffer.java)/**
 * A double-buffer for edits. New edits are written into the first buffer
 * while the second is available to be flushed. Each time the double-buffer
 * is flushed, the two internal buffers are swapped. This allows edits
 * to progress concurrently to flushes without allocating new buffers each
 * time.
 */
EncryptionFaultInjector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EncryptionFaultInjector.java)/**
 * Used to inject certain faults for testing.
 */
EncryptionZoneInt (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EncryptionZoneManager.java)/**
   * EncryptionZoneInt is the internal representation of an encryption zone. The
   * external representation of an EZ is embodied in an EncryptionZone and
   * contains the EZ's pathname.
   */
EncryptionZoneManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EncryptionZoneManager.java)/**
 * Manages the list of encryption zones in the filesystem.
 * <p>
 * The EncryptionZoneManager has its own lock, but relies on the FSDirectory
 * lock being held for many operations. The FSDirectory lock should not be
 * taken if the manager lock is already held.
 */
ErasureCodingPolicyManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java)/**
 * This manages erasure coding policies predefined and activated in the system.
 * It loads customized policies and syncs with persisted ones in
 * NameNode image.
 *
 * This class is instantiated by the FSNamesystem.
 */
EditLogFile (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java)/**
   * Record of an edit log that has been located and had its filename parsed.
   */
FileJournalManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java)/**
 * Journal manager for the common case of edits files being written
 * to a storage directory.
 * 
 * Note: this class is not thread-safe and should be externally
 * synchronized.
 */
FileUnderConstructionFeature (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileUnderConstructionFeature.java)/**
 * Feature for under-construction file.
 */
FsckServlet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FsckServlet.java)/**
 * This class is used in Namesystem's web server to do fsck on namenode.
 */
FSDirAppendOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAppendOp.java)/**
 * Helper class to perform append operation.
 */
FSDirConcatOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java)/**
 * Restrictions for a concat operation:
 * <pre>
 * 1. the src file and the target file are in the same dir
 * 2. all the source files are not in snapshot
 * 3. any source file cannot be the same with the target file
 * 4. source files cannot be under construction or empty
 * 5. source file's preferred block size cannot be greater than the target file
 * </pre>
 */
InitQuotaTask (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java)/**
   * parallel initialization using fork-join.
   */
FSDirectory (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java)/**
 * Both FSDirectory and FSNamesystem manage the state of the namespace.
 * FSDirectory is a pure in-memory data structure, all of whose operations
 * happen entirely in memory. In contrast, FSNamesystem persists the operations
 * to the disk.
 * @see org.apache.hadoop.hdfs.server.namenode.FSNamesystem
 **/
EDEKCacheLoader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java)/**
   * EDEKCacheLoader is being run in a separate thread to loop through all the
   * EDEKs and warm them up in the KMS cache.
   */
FSDirEncryptionZoneOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java)/**
 * Helper class to perform encryption zone operation.
 */
FSDirErasureCodingOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java)/**
 * Helper class to perform erasure coding related operations.
 */
FSDirSatisfyStoragePolicyOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSatisfyStoragePolicyOp.java)/**
 * Helper class to perform storage policy satisfier related operations.
 */
TruncateResult (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirTruncateOp.java)/**
   * Result of truncate operation.
   */
FSDirTruncateOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirTruncateOp.java)/**
 * Helper class to perform truncate operation.
 */
FSEditLog (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java)/**
 * FSEditLog maintains a log of the namespace modifications.
 * 
 */
PositionTrackingInputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java)/**
   * Stream wrapper that keeps track of the current stream position.
   * 
   * This stream also allows us to set a limit on how many bytes we can read
   * without getting an exception.
   */
AddOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * {@literal @AtMostOnce} for {@link ClientProtocol#create} and
   * {@link ClientProtocol#append}
   */
CloseOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Although {@link ClientProtocol#append} may also log a close op, we do
   * not need to record the rpc ids here since a successful appendFile op will
   * finally log an AddOp.
   */
UpdateBlocksOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * {@literal @AtMostOnce} for {@link ClientProtocol#updatePipeline}, but 
   * {@literal @Idempotent} for some other ops.
   */
SetReplicationOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @Idempotent} for {@link ClientProtocol#setReplication} */
ConcatDeleteOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @AtMostOnce} for {@link ClientProtocol#concat} */
RenameOldOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @AtMostOnce} for {@link ClientProtocol#rename} */
DeleteOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @AtMostOnce} for {@link ClientProtocol#delete} */
MkdirOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @Idempotent} for {@link ClientProtocol#mkdirs} */
SetGenstampV1Op (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * The corresponding operations are either {@literal @Idempotent} (
   * {@link ClientProtocol#updateBlockForPipeline},
   * {@link ClientProtocol#recoverLease}, {@link ClientProtocol#addBlock}) or
   * already bound with other editlog op which records rpc ids (
   * {@link ClientProtocol#create}). Thus no need to record rpc ids here.
   */
SetGenstampV2Op (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** Similar with {@link SetGenstampV1Op} */
AllocateBlockIdOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @Idempotent} for {@link ClientProtocol#addBlock} */
SetPermissionsOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @Idempotent} for {@link ClientProtocol#setPermission} */
SetOwnerOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @Idempotent} for {@link ClientProtocol#setOwner} */
SetQuotaOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @Idempotent} for {@link ClientProtocol#setQuota} */
SetQuotaByStorageTypeOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @Idempotent} for {@link ClientProtocol#setQuota} */
TimesOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @Idempotent} for {@link ClientProtocol#setTimes} */
SymlinkOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @AtMostOnce} for {@link ClientProtocol#createSymlink} */
RenameOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @AtMostOnce} for {@link ClientProtocol#rename2} */
ReassignLeaseOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * {@literal @Idempotent} for {@link ClientProtocol#recoverLease}. In the
   * meanwhile, startFile and appendFile both have their own corresponding
   * editlog op.
   */
GetDelegationTokenOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @Idempotent} for {@link ClientProtocol#getDelegationToken} */
RenewDelegationTokenOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @Idempotent} for {@link ClientProtocol#renewDelegationToken} */
CancelDelegationTokenOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @Idempotent} for {@link ClientProtocol#cancelDelegationToken} */
CreateSnapshotOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Operation corresponding to creating a snapshot.
   * {@literal @AtMostOnce} for {@link ClientProtocol#createSnapshot}.
   */
DeleteSnapshotOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Operation corresponding to delete a snapshot.
   * {@literal @AtMostOnce} for {@link ClientProtocol#deleteSnapshot}.
   */
RenameSnapshotOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Operation corresponding to rename a snapshot.
   * {@literal @AtMostOnce} for {@link ClientProtocol#renameSnapshot}.
   */
AllowSnapshotOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Operation corresponding to allow creating snapshot on a directory
   */
DisallowSnapshotOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Operation corresponding to disallow creating snapshot on a directory
   */
AddCacheDirectiveInfoOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * {@literal @AtMostOnce} for
   * {@link ClientProtocol#addCacheDirective}
   */
ModifyCacheDirectiveInfoOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * {@literal @AtMostOnce} for
   * {@link ClientProtocol#modifyCacheDirective}
   */
RemoveCacheDirectiveInfoOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * {@literal @AtMostOnce} for
   * {@link ClientProtocol#removeCacheDirective}
   */
AddCachePoolOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @AtMostOnce} for {@link ClientProtocol#addCachePool} */
ModifyCachePoolOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @AtMostOnce} for {@link ClientProtocol#modifyCachePool} */
RemoveCachePoolOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @AtMostOnce} for {@link ClientProtocol#removeCachePool} */
BlockTwo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * A class to read in blocks stored in the old format. The only two
   * fields in the block were blockid and length.
   */
AddErasureCodingPolicyOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Operation corresponding to add an erasure coding policy.
   */
EnableErasureCodingPolicyOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Operation corresponding to enable an erasure coding policy.
   */
DisableErasureCodingPolicyOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Operation corresponding to disable an erasure coding policy.
   */
RemoveErasureCodingPolicyOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Operation corresponding to remove an erasure coding policy.
   */
RollingUpgradeOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Operation corresponding to upgrade
   */
SetStoragePolicyOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/** {@literal @Idempotent} for {@link ClientProtocol#setStoragePolicy} */
Writer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Class for writing editlog ops
   */
Reader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Class for reading editlog ops from a stream
   */
LengthPrefixedReader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Reads edit logs which are prefixed with a length.  These edit logs also
   * include a checksum and transaction ID.
   */
ChecksummedReader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Read edit logs which have a checksum and a transaction ID, but not a
   * length.
   */
LegacyReader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
   * Read older edit logs which may or may not have transaction IDs and other
   * features.  This code is used during upgrades and to allow HDFS INotify to
   * read older edit log files.
   */
FSEditLogOp (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java)/**
 * Helper classes for reading the ops from an InputStream.
 * All ops derive from FSEditLogOp and are only
 * instantiated from Reader#readOp()
 */
FSImageSaver (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java)/**
   * FSImageSaver is being run in a separate thread when saving
   * FSImage. There is one thread per each copy of the image.
   *
   * FSImageSaver assumes that it was launched from a thread that holds
   * FSNamesystem lock and waits for the execution of FSImageSaver thread
   * to finish.
   * This way we are guaranteed that the namespace is not being updated
   * while multiple instances of FSImageSaver are traversing it
   * and writing it out.
   */
FSImage (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java)/**
 * FSImage handles checkpointing and logging of the namespace edits.
 * 
 */
FSImageCompression (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageCompression.java)/**
 * Simple container class that handles support for compressed fsimage files.
 */
Loader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java)/**
   * A one-shot class responsible for loading an image. The load() function
   * should be called once, after which the getter methods may be used to retrieve
   * information about the image that was loaded, if loading was successful.
   */
Saver (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java)/**
   * A one-shot class responsible for writing an image file.
   * The write() function should be called once, after which the getter
   * functions may be used to retrieve information about the file that was written.
   *
   * This is replaced by the PB-based FSImage. The class is to maintain
   * compatibility for the external fsimage tool.
   */
FSImageFormat (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java)/**
 * Contains inner classes for reading or writing the on-disk format for
 * FSImages.
 *
 * In particular, the format of the FSImage looks like:
 * <pre>
 * FSImage {
 *   layoutVersion: int, namespaceID: int, numberItemsInFSDirectoryTree: long,
 *   namesystemGenerationStampV1: long, namesystemGenerationStampV2: long,
 *   generationStampAtBlockIdSwitch:long, lastAllocatedBlockId:
 *   long transactionID: long, snapshotCounter: int, numberOfSnapshots: int,
 *   numOfSnapshottableDirs: int,
 *   {FSDirectoryTree, FilesUnderConstruction, SecretManagerState} (can be compressed)
 * }
 *
 * FSDirectoryTree (if {@link Feature#FSIMAGE_NAME_OPTIMIZATION} is supported) {
 *   INodeInfo of root, numberOfChildren of root: int
 *   [list of INodeInfo of root's children],
 *   [list of INodeDirectoryInfo of root's directory children]
 * }
 *
 * FSDirectoryTree (if {@link Feature#FSIMAGE_NAME_OPTIMIZATION} not supported){
 *   [list of INodeInfo of INodes in topological order]
 * }
 *
 * INodeInfo {
 *   {
 *     localName: short + byte[]
 *   } when {@link Feature#FSIMAGE_NAME_OPTIMIZATION} is supported
 *   or
 *   {
 *     fullPath: byte[]
 *   } when {@link Feature#FSIMAGE_NAME_OPTIMIZATION} is not supported
 *   replicationFactor: short, modificationTime: long,
 *   accessTime: long, preferredBlockSize: long,
 *   numberOfBlocks: int (-1 for INodeDirectory, -2 for INodeSymLink),
 *   {
 *     nsQuota: long, dsQuota: long,
 *     {
 *       isINodeSnapshottable: byte,
 *       isINodeWithSnapshot: byte (if isINodeSnapshottable is false)
 *     } (when {@link Feature#SNAPSHOT} is supported),
 *     fsPermission: short, PermissionStatus
 *   } for INodeDirectory
 *   or
 *   {
 *     symlinkString, fsPermission: short, PermissionStatus
 *   } for INodeSymlink
 *   or
 *   {
 *     [list of BlockInfo]
 *     [list of FileDiff]
 *     {
 *       isINodeFileUnderConstructionSnapshot: byte,
 *       {clientName: short + byte[], clientMachine: short + byte[]} (when
 *       isINodeFileUnderConstructionSnapshot is true),
 *     } (when {@link Feature#SNAPSHOT} is supported and writing snapshotINode),
 *     fsPermission: short, PermissionStatus
 *   } for INodeFile
 * }
 *
 * INodeDirectoryInfo {
 *   fullPath of the directory: short + byte[],
 *   numberOfChildren: int, [list of INodeInfo of children INode],
 *   {
 *     numberOfSnapshots: int,
 *     [list of Snapshot] (when NumberOfSnapshots is positive),
 *     numberOfDirectoryDiffs: int,
 *     [list of DirectoryDiff] (NumberOfDirectoryDiffs is positive),
 *     number of children that are directories,
 *     [list of INodeDirectoryInfo of the directory children] (includes
 *     snapshot copies of deleted sub-directories)
 *   } (when {@link Feature#SNAPSHOT} is supported),
 * }
 *
 * Snapshot {
 *   snapshotID: int, root of Snapshot: INodeDirectoryInfo (its local name is
 *   the name of the snapshot)
 * }
 *
 * DirectoryDiff {
 *   full path of the root of the associated Snapshot: short + byte[],
 *   childrenSize: int,
 *   isSnapshotRoot: byte,
 *   snapshotINodeIsNotNull: byte (when isSnapshotRoot is false),
 *   snapshotINode: INodeDirectory (when SnapshotINodeIsNotNull is true), Diff
 * }
 *
 * Diff {
 *   createdListSize: int, [Local name of INode in created list],
 *   deletedListSize: int, [INode in deleted list: INodeInfo]
 * }
 *
 * FileDiff {
 *   full path of the root of the associated Snapshot: short + byte[],
 *   fileSize: long,
 *   snapshotINodeIsNotNull: byte,
 *   snapshotINode: INodeFile (when SnapshotINodeIsNotNull is true), Diff
 * }
 * </pre>
 */
DigestThread (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java)/**
     * Thread to compute the MD5 of a file as this can be in parallel while
     * loading the image without interfering much.
     */
FSImageFormatProtobuf (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java)/**
 * Utility class to read / write fsimage in protobuf format.
 */
FSImagePreTransactionalStorageInspector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java)/**
 * Inspects an FSImage storage directory in the "old" (pre-HDFS-1073) format.
 * This format has the following data files:
 *   - fsimage
 *   - fsimage.ckpt (when checkpoint is being uploaded)
 *   - edits
 *   - edits.new (when logs are "rolled")
 */
TLData (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageSerialization.java)/**
   * Simple container "struct" for threadlocal data.
   */
FSImageSerialization (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageSerialization.java)/**
 * Static utility functions for serializing various pieces of data in the correct
 * format for the FSImage file.
 *
 * Some members are currently public for the benefit of the Offline Image Viewer
 * which is located outside of this package. These members should be made
 * package-protected when the OIV is refactored.
 */
FSImageFile (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageStorageInspector.java)/**
   * Record of an image that has been located and had its filename parsed.
   */
FSImageStorageInspector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageStorageInspector.java)/**
 * Interface responsible for inspecting a set of storage directories and devising
 * a plan to load the namespace from them.
 */
NameNodeResourceMonitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java)/**
   * Periodically calls hasAvailableResources of NameNodeResourceChecker, and if
   * there are found to be insufficient resources available, causes the NN to
   * enter safe mode. If resources are later found to have returned to
   * acceptable levels, this daemon will cause the NN to exit safe mode.
   */
LazyPersistFileScrubber (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java)/**
   * Daemon to periodically scan the namespace for lazyPersist files
   * with missing blocks and unlink them.
   */
FSNamesystemAuditLogger (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java)/**
   * FSNamesystem Default AuditLogger implementation;used when no access logger
   * is defined in the config file. It can also be explicitly listed in the
   * config file.
   */
FSNamesystem (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java)/**
 * FSNamesystem is a container of both transient
 * and persisted name-space state, and does all the book-keeping
 * work on a NameNode.
 *
 * Its roles are briefly described below:
 *
 * 1) Is the container for BlockManager, DatanodeManager,
 *    DelegationTokens, LeaseManager, etc. services.
 * 2) RPC calls that modify or inspect the name-space
 *    should get delegated here.
 * 3) Anything that touches only blocks (eg. block reports),
 *    it delegates to BlockManager.
 * 4) Anything that touches only file information (eg. permissions, mkdirs),
 *    it delegates to FSDirectory.
 * 5) Anything that crosses two of the above components should be
 *    coordinated here.
 * 6) Logs mutations to FSEditLog.
 *
 * This class and its contents keep:
 *
 * 1)  Valid fsname {@literal -->} blocklist  (kept on disk, logged)
 * 2)  Set of all valid blocks (inverted #1)
 * 3)  block {@literal -->} machinelist (kept in memory, rebuilt dynamically
 *     from reports)
 * 4)  machine {@literal -->} blocklist (inverted #2)
 * 5)  LRU cache of updated-heartbeat machines
 */
ReadLockHeldInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java)/**
   * Read lock Held Info.
   */
FSNamesystemLock (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java)/**
 * Mimics a ReentrantReadWriteLock but does not directly implement the interface
 * so more sophisticated locking capabilities and logging/metrics are possible.
 * {@link org.apache.hadoop.hdfs.DFSConfigKeys#DFS_NAMENODE_LOCK_DETAILED_METRICS_KEY}
 * to be true, metrics will be emitted into the FSNamesystem metrics registry
 * for each operation which acquires this lock indicating how long the operation
 * held the lock for. These metrics have names of the form
 * FSN(Read|Write)LockNanosOperationName, where OperationName denotes the name
 * of the operation that initiated the lock hold (this will be OTHER for certain
 * uncategorized operations) and they export the hold time values in
 * nanoseconds. Note that if a thread dies, metrics produced after the
 * most recent snapshot will be lost due to the use of
 * {@link MutableRatesWithAggregation}. However since threads are re-used
 * between operations this should not generally be an issue.
 */
FSPermissionChecker (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java)/** 
 * Class that helps in checking file system permission.
 * The state of this class need not be synchronized as it has data structures that
 * are read-only.
 * 
 * Some of the helper methods are guarded by {@link FSNamesystem#readLock()}.
 */
TraverseInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSTreeTraverser.java)/**
   * Class will represent the additional info required for traverse.
   */
FSTreeTraverser (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSTreeTraverser.java)/**
 * FSTreeTraverser traverse directory recursively and process files
 * in batches.
 */
GlobalStateIdContext (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/GlobalStateIdContext.java)/**
 * This is the server side implementation responsible for passing
 * state alignment info to clients.
 */
ActiveState (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ActiveState.java)/**
 * Active state of the namenode. In this state, namenode provides the namenode
 * service and handles operations of type {@link OperationCategory#WRITE} and
 * {@link OperationCategory#READ}.
 */
AliasMapStorageDirectory (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java)/**
   * A storage directory for aliasmaps. This is primarily used for the
   * StorageDirectory#hasSomeData for formatting aliasmap directories.
   */
BootstrapStandby (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java)/**
 * Tool which allows the standby node's storage directories to be bootstrapped
 * by copying the latest namespace snapshot from the active namenode. This is
 * used when first configuring an HA cluster.
 */
EditLogTailerThread (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java)/**
   * The thread which does the actual work of tailing edits journals and
   * applying the transactions to the FSNS.
   */
MultipleNameNodeProxy (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java)/**
   * Manage the 'active namenode proxy'. This cannot just be the a single proxy since we could
   * failover across a number of NameNodes, rather than just between an active and a standby.
   * <p>
   * We - lazily - get a proxy to one of the configured namenodes and attempt to make the request
   * against it. If it doesn't succeed, either because the proxy failed to be created or the request
   * failed, we try the next NN in the list. We try this up to the configuration maximum number of
   * retries before throwing up our hands. A working proxy is retained across attempts since we
   * expect the active NameNode to switch rarely.
   * <p>
   * This mechanism is <b>very bad</b> for cases where we care about being <i>fast</i>; it just
   * blindly goes and tries namenodes.
   */
EditLogTailer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java)/**
 * EditLogTailer represents a thread which periodically reads from edits
 * journals and applies the transactions contained within to a given
 * FSNamesystem.
 */
HAContext (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/HAContext.java)/**
 * Context that is to be used by {@link HAState} for getting/setting the
 * current state and performing required operations.
 */
HAState (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/HAState.java)/**
 * Namenode base state to implement state machine pattern.
 */
RemoteNameNodeInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/RemoteNameNodeInfo.java)/**
 * Information about a single remote NameNode
 */
StandbyCheckpointer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java)/**
 * Thread which runs inside the NN when it's in Standby state,
 * periodically waking up to take a checkpoint of the namespace.
 * When it takes a checkpoint, it saves it to its local
 * storage and then uploads it to the remote NameNode.
 */
StandbyState (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyState.java)/**
 * Namenode standby state. In this state the namenode acts as warm standby and
 * keeps the following updated:
 * <ul>
 * <li>Namespace by getting the edits.</li>
 * <li>Block location information by receiving block reports and blocks
 * received from the datanodes.</li>
 * </ul>
 * 
 * It does not handle read/write/checkpoint operations.
 */
HdfsAuditLogger (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/HdfsAuditLogger.java)/**
 * Extension of {@link AuditLogger}.
 */
IllegalReservedPathException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/IllegalReservedPathException.java)/**
 * Thrown when upgrading from software release that doesn't support reserved
 * path to software release that supports reserved path, and when there is 
 * reserved path name in the Fsimage.
 */
ImageServlet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java)/**
 * This class is used in Namesystem's jetty to retrieve/upload a file 
 * Typically used by the Secondary NameNode to retrieve image and
 * edit file for periodic checkpointing in Non-HA deployments.
 * Standby NameNode uses to upload checkpoints in HA deployments.
 */
QuotaDelta (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java)/**
   * Information used to record quota usage delta. This data structure is
   * usually passed along with an operation like {@link #cleanSubtree}. Note
   * that after the operation the delta counts should be decremented from the
   * ancestral directories' quota usage.
   */
ReclaimContext (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java)/**
   * Context object to record blocks and inodes that need to be reclaimed
   */
UpdatedReplicationInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java)/**
     * The blocks whose replication factor need to be updated.
     */
BlocksMapUpdateInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java)/**
   * Information used for updating the blocksMap when deleting files.
   */
Feature (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java)/** 
   * INode feature such as {@link FileUnderConstructionFeature}
   * and {@link DirectoryWithQuotaFeature}.
   */
INode (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java)/**
 * We keep an in-memory representation of the file/block hierarchy.
 * This is a base INode class containing common fields for file and 
 * directory inodes.
 */
AccessControlEnforcer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.java)/**
   * The AccessControlEnforcer allows implementations to override the
   * default File System permission checking logic enforced on a file system
   * object
   */
SnapshotCopy (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeAttributes.java)/** A read-only copy of the inode attributes. */
INodeAttributes (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeAttributes.java)/**
 * The attributes of an inode.
 */
SnapshotAndINode (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java)/** A pair of Snapshot and INode objects. */
INodeDirectory (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java)/**
 * Directory INode class.
 */
SnapshotCopy (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryAttributes.java)/** A copy of the inode directory attributes */
INodeDirectoryAttributes (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryAttributes.java)/**
 * The attributes of an inode.
 */
INodeFile (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java)/** I-node for closed file. */
SnapshotCopy (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFileAttributes.java)/** A copy of the inode file attributes */
INodeFileAttributes (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFileAttributes.java)/**
 * The attributes of a file.
 */
INodeId (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeId.java)/**
 * An id which uniquely identifies an inode. Id 1 to 16384 are reserved for
 * potential future usage. The id won't be recycled and is not expected to wrap
 * around in a very long time. Root inode id is always 16385. Id 0 is used for
 * backward compatibility support.
 */
INodeMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeMap.java)/**
 * Storing all the {@link INode}s and maintaining the mapping between INode ID
 * and INode.  
 */
WithCount (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeReference.java)/** An anonymous reference with reference count. */
WithName (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeReference.java)/** A reference with a fixed name. */
INodeReference (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeReference.java)/**
 * An anonymous reference to an inode.
 *
 * This class and its subclasses are used to support multiple access paths.
 * A file/directory may have multiple access paths when it is stored in some
 * snapshots and it is renamed/moved to other locations.
 * 
 * For example,
 * (1) Suppose we have /abc/foo, say the inode of foo is inode(id=1000,name=foo)
 * (2) create snapshot s0 for /abc
 * (3) mv /abc/foo /xyz/bar, i.e. inode(id=1000,name=...) is renamed from "foo"
 *     to "bar" and its parent becomes /xyz.
 * 
 * Then, /xyz/bar and /abc/.snapshot/s0/foo are two different access paths to
 * the same inode, inode(id=1000,name=bar).
 *
 * With references, we have the following
 * - /abc has a child ref(id=1001,name=foo).
 * - /xyz has a child ref(id=1002) 
 * - Both ref(id=1001,name=foo) and ref(id=1002) point to another reference,
 *   ref(id=1003,count=2).
 * - Finally, ref(id=1003,count=2) points to inode(id=1000,name=bar).
 * 
 * Note 1: For a reference without name, e.g. ref(id=1002), it uses the name
 *         of the referred inode.
 * Note 2: getParent() always returns the parent in the current state, e.g.
 *         inode(id=1000,name=bar).getParent() returns /xyz but not /abc.
 */
INodesInPath (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodesInPath.java)/**
 * Contains INodes information resolved from a given path.
 */
INodeSymlink (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeSymlink.java)/**
 * An {@link INode} representing a symbolic link.
 */
INodeWithAdditionalFields (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java)/**
 * {@link INode} with additional fields including id, name, permission,
 * access time and modification time.
 */
InotifyFSEditLogOpTranslator (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/InotifyFSEditLogOpTranslator.java)/**
 * Translates from edit log ops to inotify events.
 */
IsNameNodeActiveServlet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/IsNameNodeActiveServlet.java)/**
 * Used by Load Balancers to find the active NameNode.
 */
CorruptionException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalManager.java)/** 
   * Indicate that a journal is cannot be used to load a certain range of 
   * edits.
   * This exception occurs in the case of a gap in the transactions, or a
   * corrupt edit file.
   */
JournalManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalManager.java)/**
 * A JournalManager is responsible for managing a single place of storing
 * edit logs. It may correspond to multiple files, a backup node, etc.
 * Even when the actual underlying storage is rolled, or failed and restored,
 * each conceptual place of storage corresponds to exactly one instance of
 * this class, which is created when the EditLog is first opened.
 */
JournalAndStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java)/**
   * Container for a JournalManager paired with its currently
   * active stream.
   * 
   * If a Journal gets disabled due to an error writing to its
   * stream, then the stream will be aborted and set to null.
   */
JournalClosure (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java)/**
   * Implementations of this interface encapsulate operations that can be
   * iteratively applied on all the journals. For example see
   * {@link JournalSet#mapJournalsAndReportErrors}.
   */
JournalSetOutputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java)/**
   * An implementation of EditLogOutputStream that applies a requested method on
   * all the journals that are currently active.
   */
JournalSet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java)/**
 * Manages a collection of Journals. None of the methods are synchronized, it is
 * assumed that FSEditLog methods, that use this class, use proper
 * synchronization.
 */
LeaseExpiredException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseExpiredException.java)/**
 * The lease that was being used to create this file has expired.
 */
Lease (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java)/************************************************************
   * A Lease governs all the locks held by a single client.
   * For each client there's a corresponding lease, whose
   * timestamp is updated when the client periodically
   * checks in.  If the client dies and allows its lease to
   * expire, all the corresponding locks can be released.
   *************************************************************/
Monitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java)/******************************************************
   * Monitor checks for leases that have expired,
   * and disposes of them.
   ******************************************************/
LeaseManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java)/**
 * LeaseManager does the lease housekeeping for writing on files.   
 * This class also provides useful static methods for lease recovery.
 * 
 * Lease Recovery Algorithm
 * 1) Namenode retrieves lease information
 * 2) For each file f in the lease, consider the last block b of f
 * 2.1) Get the datanodes which contains b
 * 2.2) Assign one of the datanodes as the primary datanode p

 * 2.3) p obtains a new generation stamp from the namenode
 * 2.4) p gets the block info from each datanode
 * 2.5) p computes the minimum block length
 * 2.6) p updates the datanodes, which have a valid generation stamp,
 *      with the new generation stamp and the minimum block length 
 * 2.7) p acknowledges the namenode the update results

 * 2.8) Namenode updates the BlockInfo
 * 2.9) Namenode removes f from the lease
 *      and removes the lease once all files have been removed
 * 2.10) Namenode commit changes to edit log
 */
LogsPurgeable (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LogsPurgeable.java)/**
 * Interface used to abstract over classes which manage edit logs that may need
 * to be purged.
 */
RequestStopException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/MetaRecoveryContext.java)/** Exception thrown when the user has requested processing to stop. */
MetaRecoveryContext (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/MetaRecoveryContext.java)/** Context data for an ongoing NameNode metadata recovery process. */
ECBlockGroupsMBean (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/metrics/ECBlockGroupsMBean.java)/**
 * This interface defines the methods to get status pertaining to blocks of type
 * {@link org.apache.hadoop.hdfs.protocol.BlockType#STRIPED} in FSNamesystem
 * of a NameNode. It is also used for publishing via JMX.
 * <p>
 * Aggregated status of all blocks is reported in
 * @see FSNamesystemMBean
 * Name Node runtime activity statistic info is reported in
 * @see org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics
 *
 */
FSNamesystemMBean (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/metrics/FSNamesystemMBean.java)/**
 * 
 * This Interface defines the methods to get the status of a the FSNamesystem of
 * a name node.
 * It is also used for publishing via JMX (hence we follow the JMX naming
 * convention.)
 * 
 * Note we have not used the MetricsDynamicMBeanBase to implement this
 * because the interface for the NameNodeStateMBean is stable and should
 * be published as an interface.
 * 
 * <p>
 * Name Node runtime activity statistic  info is reported in
 * @see org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics
 *
 */
NameNodeMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.java)/**
 * This class is for maintaining  the various NameNode activity statistics
 * and publishing them through the metrics interfaces.
 */
ReplicatedBlocksMBean (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/metrics/ReplicatedBlocksMBean.java)/**
 * This interface defines the methods to get status pertaining to blocks of type
 * {@link org.apache.hadoop.hdfs.protocol.BlockType#CONTIGUOUS} in FSNamesystem
 * of a NameNode. It is also used for publishing via JMX.
 * <p>
 * Aggregated status of all blocks is reported in
 * @see FSNamesystemMBean
 * Name Node runtime activity statistic info is reported in
 * @see org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics
 */
UseCount (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameCache.java)/**
   * Class for tracking use count of a name
   */
NameCache (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameCache.java)/**
 * Caches frequently used names to facilitate reuse.
 * (example: byte[] representation of the file name in {@link INode}).
 * 
 * This class is used by initially adding all the file names. Cache
 * tracks the number of times a name is used in a transient map. It promotes 
 * a name used more than {@code useThreshold} to the cache.
 * 
 * One all the names are added, {@link #initialized()} should be called to
 * finish initialization. The transient map where use count is tracked is
 * discarded and cache is ready for use.
 * 
 * <p>
 * This class must be synchronized externally.
 * 
 * @param <K> name to be added to the cache
 */
NameNodeHAContext (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java)/**
   * Class used to expose {@link NameNode} as context to {@link HAState}
   */
NameNode (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java)/**********************************************************
 * NameNode serves as both directory namespace manager and
 * "inode table" for the Hadoop DFS.  There is a single NameNode
 * running in any DFS deployment.  (Well, except when there
 * is a second backup/failover NameNode, or when using federated NameNodes.)
 *
 * The NameNode controls two critical tables:
 *   1)  filename{@literal ->}blocksequence (namespace)
 *   2)  block{@literal ->}machinelist ("inodes")
 *
 * The first table is stored on disk and is very precious.
 * The second table is rebuilt every time the NameNode comes up.
 *
 * 'NameNode' refers to both this class as well as the 'NameNode server'.
 * The 'FSNamesystem' class actually performs most of the filesystem
 * management.  The majority of the 'NameNode' class itself is concerned
 * with exposing the IPC interface and the HTTP server to the outside world,
 * plus some configuration management.
 *
 * NameNode implements the
 * {@link org.apache.hadoop.hdfs.protocol.ClientProtocol} interface, which
 * allows clients to ask for DFS services.
 * {@link org.apache.hadoop.hdfs.protocol.ClientProtocol} is not designed for
 * direct use by authors of DFS client code.  End-users should instead use the
 * {@link org.apache.hadoop.fs.FileSystem} class.
 *
 * NameNode also implements the
 * {@link org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol} interface,
 * used by DataNodes that actually store DFS data blocks.  These
 * methods are invoked repeatedly and automatically by all the
 * DataNodes in a DFS deployment.
 *
 * NameNode also implements the
 * {@link org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol} interface,
 * used by secondary namenodes or rebalancing processes to get partial
 * NameNode state, for example partial blocksMap etc.
 **********************************************************/
NameNodeFormatException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeFormatException.java)/**
 * Thrown when NameNode format fails.
 */
Result (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java)/**
   * FsckResult of checking, plus overall DFS statistics.
   */
NamenodeFsck (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java)/**
 * This class provides rudimentary checking of DFS volumes for errors and
 * sub-optimal conditions.
 * <p>The tool scans all files and directories, starting from an indicated
 *  root path. The following abnormal conditions are detected and handled:</p>
 * <ul>
 * <li>files with blocks that are completely missing from all datanodes.<br>
 * In this case the tool can perform one of the following actions:
 *  <ul>
 *      <li>move corrupted files to /lost+found directory on DFS
 *      ({@link #doMove}). Remaining data blocks are saved as a
 *      block chains, representing longest consecutive series of valid blocks.</li>
 *      <li>delete corrupted files ({@link #doDelete})</li>
 *  </ul>
 *  </li>
 *  <li>detect files with under-replicated or over-replicated blocks</li>
 *  </ul>
 *  Additionally, the tool collects a detailed overall DFS statistics, and
 *  optionally can print detailed statistics on block locations and replication
 *  factors of each file.
 */
NameNodeHttpServer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java)/**
 * Encapsulates the HTTP server started by the NameNode. 
 */
NameNodeMXBean (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeMXBean.java)/**
 * This is the JMX management interface for namenode information.
 * End users shouldn't be implementing these interfaces, and instead
 * access this information through the JMX APIs.
 */
NameNodeResourceChecker (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java)/**
 * 
 * NameNodeResourceChecker provides a method -
 * <code>hasAvailableDiskSpace</code> - which will return true if and only if
 * the NameNode has disk space available on all required volumes, and any volume
 * which is configured to be redundant. Volumes containing file system edits dirs
 * are added by default, and arbitrary extra volumes may be configured as well.
 */
NameNodeResourcePolicy (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeResourcePolicy.java)/**
 * Given a set of checkable resources, this class is capable of determining
 * whether sufficient resources are available for the NN to continue operating.
 */
NameNodeRpcServer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java)/**
 * This class is responsible for handling all of the RPC calls to the NameNode.
 * It is created, started, and stopped by {@link NameNode}.
 */
NameNodeStatusMXBean (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeStatusMXBean.java)/**
 * This is the JMX management interface for NameNode status information.
 * End users shouldn't be implementing these interfaces, and instead
 * access this information through the JMX APIs. *
 */
NameNodeUtils (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeUtils.java)/**
 * Utility functions for the NameNode.
 */
Namesystem (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Namesystem.java)/** Namesystem operations. */
NNStorage (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java)/**
 * NNStorage is responsible for management of the StorageDirectories used by
 * the NameNode.
 */
StoragePurger (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java)/**
   * Interface responsible for disposing of old checkpoints and edit logs.
   */
NNStorageRetentionManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java)/**
 * The NNStorageRetentionManager is responsible for inspecting the storage
 * directories of the NN and enforcing a retention policy on checkpoints
 * and edit logs.
 * 
 * It delegates the actual removal of files to a StoragePurger
 * implementation, which might delete the files or instead copy them to
 * a filer or HDFS for later analysis.
 */
Counts (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Quota.java)/** Counters for quota counts. */
QuotaCounts (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/QuotaCounts.java)/**
 * Counters for namespace, storage space and storage type space quota and usage.
 */
RedundantEditLogInputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java)/**
 * A merged input stream that handles failover between different edit logs.
 *
 * We will currently try each edit log stream exactly once.  In other words, we
 * don't handle the "ping pong" scenario where different edit logs contain a
 * different subset of the available edits.
 */
EDEKReencryptCallable (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java)/**
   * Simply contacts the KMS for re-encryption. No NN locks held.
   */
ReencryptionPendingInodeIdCollector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java)/**
   * ReencryptionPendingInodeIdCollector which throttle based on configured
   * throttle ratio.
   */
ReencryptionHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java)/**
 * Class for handling re-encrypt EDEK operations.
 * <p>
 * For each EZ, ReencryptionHandler walks the tree in a depth-first order,
 * and submits batches of (files + existing edeks) as re-encryption tasks
 * to a thread pool. Each thread in the pool then contacts the KMS to
 * re-encrypt the edeks. ReencryptionUpdater tracks the tasks and updates
 * file xattrs with the new edeks.
 * <p>
 * File renames are disabled in the EZ that's being re-encrypted. Newly created
 * files will have new edeks, because the edek cache is drained upon the
 * submission of a re-encryption command.
 * <p>
 * It is assumed only 1 ReencryptionHandler will be running, because:
 *   1. The bottleneck of the entire re-encryption appears to be on the KMS.
 *   2. Even with multiple handlers, since updater requires writelock and is
 * single-threaded, the performance gain is limited.
 * <p>
 * This class uses the FSDirectory lock for synchronization.
 */
ZoneSubmissionTracker (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java)/**
   * Class to track re-encryption submissions of a single zone. It contains
   * all the submitted futures, and statistics about how far the futures are
   * processed.
   */
ReencryptionTask (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java)/**
   * Class representing the task for one batch of a re-encryption command. It
   * also contains statistics about how far this single batch has been executed.
   */
FileEdekInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java)/**
   * Class that encapsulates re-encryption details of a file. It contains the
   * file inode, stores the initial edek of the file, and the new edek
   * after re-encryption.
   * <p>
   * Assumptions are the object initialization happens when dir lock is held,
   * and inode is valid and is encrypted during initialization.
   * <p>
   * Namespace changes may happen during re-encryption, and if inode is changed
   * the re-encryption is skipped.
   */
ReencryptionUpdater (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java)/**
 * Class for finalizing re-encrypt EDEK operations, by updating file xattrs with
 * edeks returned from reencryption.
 * <p>
 * The tasks are submitted by ReencryptionHandler.
 * <p>
 * It is assumed only 1 Updater will be running, since updating file xattrs
 * requires namespace write lock, and performance gain from multi-threading
 * is limited.
 */
SafeMode (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SafeMode.java)/** SafeMode related operations. */
SaveNamespaceContext (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SaveNamespaceContext.java)/**
 * Context for an ongoing SaveNamespace operation. This class
 * allows cancellation, and also is responsible for accumulating
 * failed storage directories.
 */
CommandLineOpts (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java)/**
   * Container for parsed command-line options.
   */
SecondaryNameNode (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java)/**********************************************************
 * The Secondary NameNode is a helper to the primary NameNode.
 * The Secondary is responsible for supporting periodic checkpoints 
 * of the HDFS metadata. The current design allows only one Secondary
 * NameNode per HDFs cluster.
 *
 * The Secondary NameNode is a daemon that periodically wakes
 * up (determined by the schedule specified in the configuration),
 * triggers a periodic checkpoint and then goes back to sleep.
 * The Secondary NameNode uses the NamenodeProtocol to talk to the
 * primary NameNode.
 *
 **********************************************************/
SecondaryNameNodeInfoMXBean (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNodeInfoMXBean.java)/**
 * JMX information of the secondary NameNode
 */
SerialNumberMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SerialNumberMap.java)/**
 * Map object to serial number.
 * 
 * <p>It allows to get the serial number of an object, if the object doesn't
 * exist in the map, a new serial number increased by 1 is generated to
 * map to the object. The mapped object can also be got through the serial
 * number.
 * 
 * <p>The map is thread-safe.
 */
AbstractINodeDiff (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/AbstractINodeDiff.java)/**
 * The difference of an inode between in two snapshots.
 * {@link AbstractINodeDiffList} maintains a list of snapshot diffs,
 * <pre>
 *   d_1 -> d_2 -> ... -> d_n -> null,
 * </pre>
 * where -> denotes the {@link AbstractINodeDiff#posteriorDiff} reference. The
 * current directory state is stored in the field of {@link INode}.
 * The snapshot state can be obtained by applying the diffs one-by-one in
 * reversed chronological order.  Let s_1, s_2, ..., s_n be the corresponding
 * snapshots.  Then,
 * <pre>
 *   s_n                     = (current state) - d_n;
 *   s_{n-1} = s_n - d_{n-1} = (current state) - d_n - d_{n-1};
 *   ...
 *   s_k     = s_{k+1} - d_k = (current state) - d_n - d_{n-1} - ... - d_k.
 * </pre>
 */
AbstractINodeDiffList (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/AbstractINodeDiffList.java)/**
 * A list of snapshot diffs for storing snapshot data.
 *
 * @param <N> The {@link INode} type.
 * @param <D> The diff type, which must extend {@link AbstractINodeDiff}.
 */
DiffList (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DiffList.java)/**
 * This interface defines the methods used to store and manage InodeDiffs.
 * @param <T> Type of the object in this list.
 */
DiffListByArrayList (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DiffListByArrayList.java)/**
 * Resizable-array implementation of the DiffList interface.
 * @param <T> Type of the object in the list
 */
SkipListNode (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DiffListBySkipList.java)/**
   * SkipListNode is an implementation of a DirectoryDiff List node,
   * which stores a Directory Diff and references to subsequent nodes.
   */
DiffListBySkipList (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DiffListBySkipList.java)/**
 * SkipList is an implementation of a data structure for storing a sorted list
 * of Directory Diff elements, using a hierarchy of linked lists that connect
 * increasingly sparse subsequences(defined by skip interval here) of the diffs.
 * The elements contained in the tree must be mutually comparable.
 * <p>
 * Consider  a case where we have 10 snapshots for a directory starting from s0
 * to s9 each associated with certain change records in terms of inodes deleted
 * and created after a particular snapshot and before the next snapshot. The
 * sequence will look like this:
 * <p>
 * {@literal s0->s1->s2->s3->s4->s5->s6->s7->s8->s9}.
 * <p>
 * Assuming a skip interval of 3, which means a new diff will be added at a
 * level higher than the current level after we have  ore than 3 snapshots.
 * Next level promotion happens after 9 snapshots and so on.
 * <p>
 * level 2:   {@literal s08------------------------------->s9}
 * level 1:   {@literal S02------->s35-------->s68-------->s9}
 * level 0:  {@literal s0->s1->s2->s3->s4->s5->s6->s7->s8->s9}
 * <p>
 * s02 will be created by combining diffs for s0, s1, s2 once s3 gets created.
 * Similarly, s08 will be created by combining s02, s35 and s68 once s9 gets
 * created.So, for constructing the children list fot s0, we have  to combine
 * s08, s9 and reverse apply to the live fs.
 * <p>
 * Similarly, for constructing the children list for s2, s2, s35, s68 and s9
 * need to get combined(or added) and reverse applied to current fs.
 * <p>
 * This approach will improve the snapshot deletion and snapshot diff
 * calculation.
 * <p>
 * Once a snapshot gets deleted, the list needs to be balanced.
 */
DirectoryDiffListFactory (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryDiffListFactory.java)/** For creating {@link DiffList} for {@link DirectoryDiff}. */
DirectorySnapshottableFeature (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java)/**
 * A directory with this feature is a snapshottable directory, where snapshots
 * can be taken. This feature extends {@link DirectoryWithSnapshotFeature}, and
 * maintains extra information about all the snapshots taken on this directory.
 */
ChildrenDiff (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature.java)/**
   * The difference between the current state and a previous snapshot
   * of the children list of an INodeDirectory.
   */
DirectoryDiff (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature.java)/**
   * The difference of an {@link INodeDirectory} between two snapshots.
   */
DirectoryDiffList (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature.java)/** A list of directory diffs. */
DirectoryWithSnapshotFeature (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature.java)/**
 * Feature used to store and process the snapshot diff information for a
 * directory. In particular, it contains a directory diff list recording changes
 * made to the directory and its children for each snapshot.
 */
FileDiff (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiff.java)/**
 * The difference of an {@link INodeFile} between two snapshots.
 */
FileDiffList (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java)/** A list of FileDiffs for storing snapshot data. */
FileWithSnapshotFeature (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileWithSnapshotFeature.java)/**
 * Feature for file with snapshot-related information.
 */
Loader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java)/**
   * Loading snapshot related information from protobuf based FSImage
   */
Saver (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java)/**
   * Saving snapshot related information to protobuf based FSImage
   */
Root (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/Snapshot.java)/** The root directory of the snapshot. */
Snapshot (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/Snapshot.java)/** Snapshot of a sub-tree in the namesystem. */
SnapshotDiffInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotDiffInfo.java)/**
 * A class describing the difference between snapshots of a snapshottable
 * directory.
 */
ReferenceMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotFSImageFormat.java)/** A reference map for fsimage serialization. */
SnapshotFSImageFormat (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotFSImageFormat.java)/**
 * A helper class defining static methods for reading/writing snapshot related
 * information from/to FSImage.
 */
SnapshotManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java)/**
 * Manage snapshottable directories and their snapshots.
 * 
 * This class includes operations that create, access, modify snapshots and/or
 * snapshot-related data. In general, the locking structure of snapshot
 * operations is: <br>
 * 
 * 1. Lock the {@link FSNamesystem} lock in {@link FSNamesystem} before calling
 * into {@link SnapshotManager} methods.<br>
 * 2. Lock the {@link FSDirectory} lock for the {@link SnapshotManager} methods
 * if necessary.
 */
SnapshotStatsMXBean (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotStatsMXBean.java)/**
 * This is an interface used to retrieve statistic information related to
 * snapshots
 */
BlockMovementListener (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockMovementListener.java)/**
 * Interface for notifying about block movement attempt completion.
 */
BlockMoveTaskHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockMoveTaskHandler.java)/**
 * Interface for implementing different ways of block moving approaches. One can
 * connect directly to DN and request block move, and other can talk NN to
 * schedule via heart-beats.
 */
BlocksStorageMovementAttemptMonitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementAttemptedItems.java)/**
   * A monitor class for checking block storage movement attempt status and long
   * waiting items periodically.
   */
BlockStorageMovementAttemptedItems (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementAttemptedItems.java)/**
 * A monitor class for checking whether block storage movements attempt
 * completed or not. If this receives block storage movement attempt
 * status(either success or failure) from DN then it will just remove the
 * entries from tracking. If there is no DN reports about movement attempt
 * finished for a longer time period, then such items will retries automatically
 * after timeout. The default timeout would be 5 minutes.
 */
SPSPathIdProcessor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java)/**
   * Take dir tack ID from the spsDirsToBeTraveresed queue and collect child
   * ID's to process for satisfy the policy.
   */
DirPendingWorkInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java)/**
   * Info for directory recursive scan.
   */
BlockStorageMovementNeeded (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java)/**
 * A Class to track the block collection IDs (Inode's ID) for which physical
 * storage movement needed as per the Namespace and StorageReports from DN.
 * It scan the pending directories for which storage movement is required and
 * schedule the block collection IDs for movement. It track the info of
 * scheduled items and remove the SPS xAttr from the file/Directory once
 * movement is success.
 */
Context (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/Context.java)/**
 * An interface for the communication between SPS and Namenode module.
 */
DatanodeCacheManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/DatanodeCacheManager.java)/**
 * The Datanode cache Manager handles caching of {@link DatanodeStorageReport}.
 *
 * This class is instantiated by StoragePolicySatisifer. It maintains the array
 * of datanode storage reports. It has a configurable refresh interval and
 * periodically refresh the datanode cache by fetching latest
 * {@link Context#getLiveDatanodeStorageReport()} once it reaches refresh
 * interval.
 */
FileCollector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/FileCollector.java)/**
 * An interface for scanning the directory recursively and collect files
 * under the given directory.
 */
ItemInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/ItemInfo.java)/**
 * ItemInfo is a file info object for which need to satisfy the policy.
 */
SPSService (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/SPSService.java)/**
 * An interface for SPSService, which exposes life cycle and processing APIs.
 */
BlocksMovingAnalysis (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java)/**
   * Represents the collective analysis status for all blocks.
   */
StorageTypeNodePair (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java)/**
   * Keeps datanode with its respective storage type.
   */
DatanodeMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java)/**
   * Maintains storage type map with the available datanodes in the cluster.
   */
StorageDetails (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java)/** Storage details in a datanode storage type. */
DatanodeWithStorage (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java)/**
   * Keeps datanode with its respective set of supported storage types. It holds
   * the available space in each volumes and will be used while pairing the
   * target datanodes.
   */
AttemptedItemInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java)/**
   * This class contains information of an attempted blocks and its last
   * attempted or reported time stamp. This is used by
   * {@link BlockStorageMovementAttemptedItems#storageMovementAttemptedItems}.
   */
StoragePolicySatisfier (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java)/**
 * Setting storagePolicy on a file after the file write will only update the new
 * storage policy type in Namespace, but physical block storage movement will
 * not happen until user runs "Mover Tool" explicitly for such files. The
 * StoragePolicySatisfier Daemon thread implemented for addressing the case
 * where users may want to physically move the blocks by a dedicated daemon (can
 * run inside Namenode or stand alone) instead of running mover tool explicitly.
 * Just calling client API to satisfyStoragePolicy on a file/dir will
 * automatically trigger to move its physical storage locations as expected in
 * asynchronous manner. Here SPS will pick the file blocks which are expecting
 * to change its storages, then it will build the mapping of source block
 * location and expected storage type and location to move. After that this
 * class will also prepare requests to send to Datanode for processing the
 * physical block movements.
 */
StoragePolicySatisfyManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java)/**
 * This manages satisfy storage policy invoked path ids and expose methods to
 * process these path ids. It maintains sps mode(EXTERNAL/NONE)
 * configured by the administrator.
 *
 * <p>
 * If the configured mode is {@link StoragePolicySatisfierMode#EXTERNAL}, then
 * it won't do anything, just maintains the sps invoked path ids. Administrator
 * requires to start external sps service explicitly, to fetch the sps invoked
 * path ids from namenode, then do necessary computations and block movement in
 * order to satisfy the storage policy. Please refer
 * {@link ExternalStoragePolicySatisfier} class to understand more about the
 * external sps service functionality.
 *
 * <p>
 * If the configured mode is {@link StoragePolicySatisfierMode#NONE}, then it
 * will disable the sps feature completely by clearing all queued up sps path's
 * hint.
 *
 * This class is instantiated by the BlockManager.
 */
AbstractTracking (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/AbstractTracking.java)/**
 * Abstract base of internal data structures used for tracking progress.  For
 * primitive long properties, {@link Long#MIN_VALUE} is used as a sentinel value
 * to indicate that the property is undefined.
 */
PhaseTracking (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/PhaseTracking.java)/**
 * Internal data structure used to track progress of a {@link Phase}.
 */
Counter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java)/**
   * Allows a caller to increment a counter for tracking progress.
   */
StartupProgress (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java)/**
 * StartupProgress is used in various parts of the namenode codebase to indicate
 * startup progress.  Its methods provide ways to indicate begin and end of a
 * {@link Phase} or {@link Step} within a phase.  Additional methods provide ways
 * to associate a step or phase with optional information, such as a file name or
 * file size.  It also provides counters, which can be incremented by the  caller
 * to indicate progress through a long-running task.
 * 
 * This class is thread-safe.  Any number of threads may call any methods, even
 * for the same phase or step, without risk of corrupting internal state.  For
 * all begin/end methods and set methods, the last one in wins, overwriting any
 * prior writes.  Instances of {@link Counter} provide an atomic increment
 * operation to prevent lost updates.
 * 
 * After startup completes, the tracked data is frozen.  Any subsequent updates
 * or counter increments are no-ops.
 * 
 * For read access, call {@link #createView()} to create a consistent view with
 * a clone of the data.
 */
StartupProgressMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgressMetrics.java)/**
 * Links {@link StartupProgress} to a {@link MetricsSource} to expose its
 * information via JMX.
 */
StartupProgressView (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgressView.java)/**
 * StartupProgressView is an immutable, consistent, read-only view of namenode
 * startup progress.  Callers obtain an instance by calling
 * {@link StartupProgress#createView()} to clone current startup progress state.
 * Subsequent updates to startup progress will not alter the view.  This isolates
 * the reader from ongoing updates and establishes a guarantee that the values
 * returned by the view are consistent and unchanging across multiple related
 * read operations.  Calculations that require aggregation, such as overall
 * percent complete, will not be impacted by mutations performed in other threads
 * mid-way through the calculation.
 * 
 * Methods that return primitive long may return {@link Long#MIN_VALUE} as a
 * sentinel value to indicate that the property is undefined.
 */
Step (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/Step.java)/**
 * A step performed by the namenode during a {@link Phase} of startup.
 */
StepTracking (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/StepTracking.java)/**
 * Internal data structure used to track progress of a {@link Step}.
 */
StartupProgressServlet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/StartupProgressServlet.java)/**
 * Servlet that provides a JSON representation of the namenode's current startup
 * progress.
 */
StorageTypeAllocation (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/StoragePolicySummary.java)/**
   * Internal class which represents a unique Storage type combination
   *
   */
StoragePolicySummary (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/StoragePolicySummary.java)/**
 * Aggregate the storage type information for a set of blocks
 *
 */
StreamLimiter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/StreamLimiter.java)/**
 * An object that allows you to set a limit on a stream.  This limit
 * represents the number of bytes that can be read without getting an
 * exception.
 */
TopMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/metrics/TopMetrics.java)/**
 * The interface to the top metrics.
 * <p>
 * Metrics are collected by a custom audit logger, {@link org.apache.hadoop
 * .hdfs.server.namenode.top.TopAuditLogger}, which calls TopMetrics to
 * increment per-operation, per-user counts on every audit log call. These
 * counts are used to show the top users by NameNode operation as well as
 * across all operations.
 * <p>
 * TopMetrics maintains these counts for a configurable number of time
 * intervals, e.g. 1min, 5min, 25min. Each interval is tracked by a
 * RollingWindowManager.
 * <p>
 * These metrics are published as a JSON string via {@link org.apache.hadoop
 * .hdfs.server .namenode.metrics.FSNamesystemMBean#getTopWindows}. This is
 * done by calling {@link org.apache.hadoop.hdfs.server.namenode.top.window
 * .RollingWindowManager#snapshot} on each RollingWindowManager.
 * <p>
 * Thread-safe: relies on thread-safety of RollingWindowManager
 */
TopAuditLogger (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/TopAuditLogger.java)/**
 * An {@link AuditLogger} that sends logged data directly to the metrics
 * systems. It is used when the top service is used directly by the name node
 */
TopConf (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/TopConf.java)/**
 * This class is a common place for NNTop configuration.
 */
Bucket (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindow.java)/**
   * Thread-safety is provided by synchronization when resetting the update time
   * as well as atomic fields.
   */
RollingWindow (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindow.java)/**
 * A class for exposing a rolling window view on the event that occur over time.
 * Events are reported based on occurrence time. The total number of events in
 * the last period covered by the rolling window can be retrieved by the
 * {@link #getSum(long)} method.
 * <p>
 *
 * Assumptions:
 * <p>
 *
 * (1) Concurrent invocation of {@link #incAt} method are possible
 * <p>
 *
 * (2) The time parameter of two consecutive invocation of {@link #incAt} could
 * be in any given order
 * <p>
 *
 * (3) The buffering delays are not more than the window length, i.e., after two
 * consecutive invocation {@link #incAt(long time1, long)} and
 * {@link #incAt(long time2, long)}, time1 &lt; time2 || time1 - time2 &lt;
 * windowLenMs.
 * This assumption helps avoiding unnecessary synchronizations.
 * <p>
 *
 * Thread-safety is built in the {@link RollingWindow.Bucket}
 */
TopWindow (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.java)/**
   * Represents a snapshot of the rolling window. It contains one Op per 
   * operation in the window, with ranked users for each Op.
   */
Op (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.java)/**
   * Represents an operation within a TopWindow. It contains a ranked 
   * set of the top users for the operation.
   */
User (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.java)/**
   * Represents a user who called an Op within a TopWindow. Specifies the 
   * user and the number of times the user called the operation.
   */
RollingWindowManager (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.java)/**
 * A class to manage the set of {@link RollingWindow}s. This class is the
 * interface of metrics system to the {@link RollingWindow}s to retrieve the
 * current top metrics.
 * <p>
 * Thread-safety is provided by each {@link RollingWindow} being thread-safe as
 * well as {@link ConcurrentHashMap} for the collection of them.
 */
TransferFsImage (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java)/**
 * This class provides fetching a specified file from the NameNode.
 */
UnsupportedActionException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/UnsupportedActionException.java)/**
 * This exception is thrown when an operation is not supported.
 */
NamenodeWebHdfsMethods (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java)/** Web-hdfs NameNode implementation. */
XAttrFeature (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/XAttrFeature.java)/**
 * Feature for extended attributes.
 */
XAttrPermissionFilter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/XAttrPermissionFilter.java)/**
 * There are four types of extended attributes &lt;XAttr&gt; defined by the
 * following namespaces:
 * <br>
 * USER - extended user attributes: these can be assigned to files and
 * directories to store arbitrary additional information. The access
 * permissions for user attributes are defined by the file permission
 * bits. For sticky directories, only the owner and privileged user can 
 * write attributes.
 * <br>
 * TRUSTED - trusted extended attributes: these are visible/accessible
 * only to/by the super user.
 * <br>
 * SECURITY - extended security attributes: these are used by the HDFS
 * core for security purposes and are not available through admin/user
 * API.
 * <br>
 * SYSTEM - extended system attributes: these are used by the HDFS
 * core and are not available through admin/user API.
 * <br>
 * RAW - extended system attributes: these are used for internal system
 *   attributes that sometimes need to be exposed. Like SYSTEM namespace
 *   attributes they are not visible to the user except when getXAttr/getXAttrs
 *   is called on a file or directory in the /.reserved/raw HDFS directory
 *   hierarchy. These attributes can only be accessed by the user who have
 *   read access.
 * <br>
 */
XAttrStorage (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/XAttrStorage.java)/**
 * XAttrStorage is used to read and set xattrs for an inode.
 */
BalancerBandwidthCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BalancerBandwidthCommand.java)/**
 * Balancer bandwidth command instructs each datanode to change its value for
 * the max amount of network bandwidth it may use during the block balancing
 * operation.
 * 
 * The Balancer Bandwidth Command contains the new bandwidth value as its
 * payload. The bandwidth value is in bytes per second.
 */
BalancerProtocols (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BalancerProtocols.java)/** The full set of protocols used by the Balancer. */
BlockCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlockCommand.java)/****************************************************
 * A BlockCommand is an instruction to a datanode 
 * regarding some blocks under its control.  It tells
 * the DataNode to either invalidate a set of indicated
 * blocks, or to copy a set of indicated blocks to 
 * another DataNode.
 * 
 ****************************************************/
BlockECReconstructionInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java)/** Block and targets pair */
BlockECReconstructionCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlockECReconstructionCommand.java)/**
 * A BlockECReconstructionCommand is an instruction to a DataNode to
 * reconstruct a striped block group with missing blocks.
 *
 * Upon receiving this command, the DataNode pulls data from other DataNodes
 * hosting blocks in this group and reconstructs the lost blocks through codec
 * calculation.
 *
 * After the reconstruction, the DataNode pushes the reconstructed blocks to
 * their final destinations if necessary (e.g., the destination is different
 * from the reconstruction node, or multiple blocks in a group are to be
 * reconstructed).
 */
BlockIdCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlockIdCommand.java)/****************************************************
 * A BlockIdCommand is an instruction to a datanode 
 * regarding some blocks under its control.
 ****************************************************/
RecoveringBlock (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlockRecoveryCommand.java)/**
   * This is a block with locations from which it should be recovered
   * and the new generation stamp, which the block will have after 
   * successful recovery.
   * 
   * The new generation stamp of the block, also plays role of the recovery id.
   */
BlockRecoveryCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlockRecoveryCommand.java)/**
 * BlockRecoveryCommand is an instruction to a data-node to recover
 * the specified blocks.
 *
 * The data-node that receives this command treats itself as a primary
 * data-node in the recover process.
 *
 * Block recovery is identified by a recoveryId, which is also the new
 * generation stamp, which the block will have after the recovery succeeds.
 */
BlockReportContext (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlockReportContext.java)/**
 * The context of the block report.
 *
 * This is a set of fields that the Datanode sends to provide context about a
 * block report RPC.  The context includes a unique 64-bit ID which
 * identifies the block report as a whole.  It also includes the total number
 * of RPCs which this block report is split into, and the index into that
 * total for the current RPC.
 */
BlocksStorageMoveAttemptFinished (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlocksStorageMoveAttemptFinished.java)/**
 * This class represents, the blocks for which storage movements has done by
 * datanodes. The movementFinishedBlocks array contains all the blocks that are
 * attempted to do the movement and it could be finished with either success or
 * failure.
 */
BlockMovingInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlockStorageMovementCommand.java)/**
   * Stores block to storage info that can be used for block movement.
   */
BlockStorageMovementCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlockStorageMovementCommand.java)/**
 * A BlockStorageMovementCommand is an instruction to a DataNode to move the
 * given set of blocks to specified target DataNodes to fulfill the block
 * storage policy.
 *
 * Upon receiving this command, this DataNode pass the array of block movement
 * details to
 * {@link org.apache.hadoop.hdfs.server.sps.ExternalSPSBlockMoveTaskHandler}
 * service. Later, ExternalSPSBlockMoveTaskHandler will schedule block movement
 * tasks for these blocks and monitors the completion of each task. After the
 * block movement attempt is finished(with success or failure) this DataNode
 * will send response back to NameNode about the block movement attempt
 * finished details.
 */
BlockWithLocations (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java)/**
   * A class to keep track of a block and its locations
   */
BlocksWithLocations (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlocksWithLocations.java)/**
 * Maintains an array of blocks and their corresponding storage IDs.
 */
CheckpointCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/CheckpointCommand.java)/**
 * Checkpoint command.
 * <p>
 * Returned to the backup node by the name-node as a reply to the
 * {@link NamenodeProtocol#startCheckpoint(NamenodeRegistration)}
 * request.<br>
 * Contains:
 * <ul>
 * <li>{@link CheckpointSignature} identifying the particular checkpoint</li>
 * <li>indicator whether the backup image should be discarded before starting 
 * the checkpoint</li>
 * <li>indicator whether the image should be transfered back to the name-node
 * upon completion of the checkpoint.</li>
 * </ul>
 */
DatanodeCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/DatanodeCommand.java)/**
 * Base class for data-node command.
 * Issued by the name-node to notify data-nodes what should be done.
 */
DatanodeLifelineProtocol (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/DatanodeLifelineProtocol.java)/**
 * Protocol used by a DataNode to send lifeline messages to a NameNode.
 */
DatanodeProtocol (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java)/**********************************************************************
 * Protocol that a DFS datanode uses to communicate with the NameNode.
 * It's used to upload current load information and block reports.
 *
 * The only way a NameNode can communicate with a DataNode is by
 * returning values from these functions.
 *
 **********************************************************************/
DatanodeRegistration (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/DatanodeRegistration.java)/** 
 * DatanodeRegistration class contains all information the name-node needs
 * to identify and verify a data-node when it contacts the name-node.
 * This information is sent by data-node with each communication request.
 */
DisallowedDatanodeException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/DisallowedDatanodeException.java)/**
 * This exception is thrown when a datanode tries to register or communicate
 * with the namenode when it does not appear on the list of included nodes, 
 * or has been specifically excluded.
 * 
 */
DropSPSWorkCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/DropSPSWorkCommand.java)/**
 * A DropSPSWorkCommand is an instruction to a datanode to drop the SPSWorker's
 * pending block storage movement queues.
 */
FencedException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/FencedException.java)/**
 * If a previous user of a resource tries to use a shared resource, after
 * fenced by another user, this exception is thrown.
 */
FenceResponse (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/FenceResponse.java)/**
 * Response to a journal fence request. See {@link JournalProtocol#fence}
 */
FinalizeCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/FinalizeCommand.java)/**
 * A BlockCommand is an instruction to a datanode to register with the namenode.
 */
InterDatanodeProtocol (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/InterDatanodeProtocol.java)/** An inter-datanode protocol for updating generation stamp
 */
JournalInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/JournalInfo.java)/**
 * Information that describes a journal
 */
JournalProtocol (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/JournalProtocol.java)/**
 * Protocol used to journal edits to a remote node. Currently,
 * this is used to publish edits from the NameNode to a BackupNode.
 */
NamenodeCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/NamenodeCommand.java)/**
 * Base class for name-node command.
 * Issued by the name-node to notify other name-nodes what should be done.
 */
NamenodeProtocol (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/NamenodeProtocol.java)/*****************************************************************************
 * Protocol that a secondary NameNode uses to communicate with the NameNode.
 * Also used by external storage policy satisfier. It's used to get part of the
 * name node state
 *****************************************************************************/
NamenodeProtocols (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/NamenodeProtocols.java)/** The full set of RPC methods implemented by the Namenode.  */
NamenodeRegistration (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/NamenodeRegistration.java)/**
 * Information sent by a subordinate name-node to the active name-node
 * during the registration process. 
 */
NamespaceInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/NamespaceInfo.java)/**
 * NamespaceInfo is returned by the name-node in reply 
 * to a data-node handshake.
 * 
 */
NodeRegistration (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/NodeRegistration.java)/**
 * Generic class specifying information, which need to be sent to the name-node
 * during the registration process. 
 */
ReceivedDeletedBlockInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ReceivedDeletedBlockInfo.java)/**
 * A data structure to store the blocks in an incremental block report. 
 */
RegisterCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/RegisterCommand.java)/**
 * A RegisterCommand is an instruction to a datanode to register with the namenode.
 * This command can't be combined with other commands in the same response.
 * This is because after the datanode processes RegisterCommand, it will skip
 * the rest of the DatanodeCommands in the same HeartbeatResponse.
 */
RemoteEditLogManifest (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/RemoteEditLogManifest.java)/**
 * An enumeration of logs available on a remote NameNode.
 */
ReplicaRecoveryInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ReplicaRecoveryInfo.java)/**
 * Replica recovery information.
 */
ServerCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ServerCommand.java)/**
 * Base class for a server command.
 * Issued by the name-node to notify other servers what should be done.
 * Commands are defined by actions defined in respective protocols.
 * 
 * @see DatanodeProtocol
 * @see NamenodeProtocol
 */
StorageBlockReport (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/StorageBlockReport.java)/**
 * Block report for a Datanode storage
 */
StorageReceivedDeletedBlocks (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/StorageReceivedDeletedBlocks.java)/**
 * Report of block received and deleted per Datanode
 * storage.
 */
VolumeFailureSummary (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/VolumeFailureSummary.java)/**
 * Summarizes information about data volume failures on a DataNode.
 */
BlockMovingTask (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSBlockMoveTaskHandler.java)/**
   * This class encapsulates the process of moving the block replica to the
   * given target.
   */
ExternalSPSBlockMoveTaskHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSBlockMoveTaskHandler.java)/**
 * This class handles the external SPS block movements. This will move the
 * given block to a target datanode by directly establishing socket connection
 * to it and invokes function
 * {@link Sender#replaceBlock(ExtendedBlock, StorageType, Token, String,
 * DatanodeInfo, String)}.
 */
ExternalBlockMovementListener (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSContext.java)/**
   * Its an implementation of BlockMovementListener.
   */
ExternalSPSContext (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSContext.java)/**
 * This class used to connect to Namenode and gets the required information to
 * SPS from Namenode state.
 */
ExternalSPSFilePathCollector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java)/**
 * This class is to scan the paths recursively. If file is directory, then it
 * will scan for files recursively. If the file is non directory, then it will
 * just submit the same file to process. This will use file string path
 * representation.
 */
ExternalStoragePolicySatisfier (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalStoragePolicySatisfier.java)/**
 * This class starts and runs external SPS service.
 */
SWebHdfsDtFetcher (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SWebHdfsDtFetcher.java)/**
 *  DtFetcher for SWebHdfsFileSystem using the base class HdfsDtFetcher impl.
 */
AdminHelper (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/AdminHelper.java)/**
 * Helper methods for CacheAdmin/CryptoAdmin/StoragePolicyAdmin
 */
CacheAdmin (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CacheAdmin.java)/**
 * This class implements command-line operations on the HDFS Cache.
 */
CryptoAdmin (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CryptoAdmin.java)/**
 * This class implements crypto command-line operations.
 */
DebugCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DebugAdmin.java)/**
   * The base class for debug commands.
   */
VerifyMetaCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DebugAdmin.java)/**
   * The command for verifying a block metadata file and possibly block file.
   */
ComputeMetaCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DebugAdmin.java)/**
   * The command for verifying a block metadata file and possibly block file.
   */
RecoverLeaseCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DebugAdmin.java)/**
   * The command for recovering a file lease.
   */
HelpCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DebugAdmin.java)/**
   * The command for getting help about other commands.
   */
DebugAdmin (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DebugAdmin.java)/**
 * This class implements debug operations on the HDFS command-line.
 *
 * These operations are only for debugging, and may change or disappear
 * between HDFS versions.
 */
DelegationTokenFetcher (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java)/**
 * Fetch a DelegationToken from the current Namenode and store it in the
 * specified file.
 */
DFSAdminCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java)/**
   * An abstract class for the execution of a file system command
   */
ClearQuotaCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java)/** A class that supports command clearQuota */
SetQuotaCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java)/** A class that supports command setQuota */
ClearSpaceQuotaCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java)/** A class that supports command clearSpaceQuota */
SetSpaceQuotaCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java)/** A class that supports command setQuota */
DFSAdmin (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java)/**
 * This class provides some DFS administrative access shell commands.
 */
DFSck (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSck.java)/**
 * This class provides rudimentary checking of DFS volumes for errors and
 * sub-optimal conditions.
 * <p>The tool scans all files and directories, starting from an indicated
 *  root path. The following abnormal conditions are detected and handled:</p>
 * <ul>
 * <li>files with blocks that are completely missing from all datanodes.<br>
 * In this case the tool can perform one of the following actions:
 *  <ul>
 *      <li>move corrupted files to /lost+found directory on DFS
 *      ({@link org.apache.hadoop.hdfs.server.namenode.NamenodeFsck#doMove}).
 *      Remaining data blocks are saved as a
 *      block chains, representing longest consecutive series of valid blocks.
 *      </li>
 *      <li>delete corrupted files
 *      ({@link org.apache.hadoop.hdfs.server.namenode.NamenodeFsck#doDelete})
 *      </li>
 *  </ul>
 *  </li>
 *  <li>detect files with under-replicated or over-replicated blocks</li>
 *  </ul>
 *  Additionally, the tool collects a detailed overall DFS statistics, and
 *  optionally can print detailed statistics on block locations and replication
 *  factors of each file.
 *  The tool also provides and option to filter open files during the scan.
 *  
 */
DFSHAAdmin (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java)/**
 * Class to extend HAAdmin to do a little bit of HDFS-specific configuration.
 */
DiskBalancerCLI (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DiskBalancerCLI.java)/**
 * DiskBalancer is a tool that can be used to ensure that data is spread evenly
 * across volumes of same storage type.
 * <p>
 * For example, if you have 3 disks, with 100 GB , 600 GB and 200 GB on each
 * disk, this tool will ensure that each disk will have 300 GB.
 * <p>
 * This tool can be run while data nodes are fully functional.
 * <p>
 * At very high level diskbalancer computes a set of moves that will make disk
 * utilization equal and then those moves are executed by the datanode.
 */
ListECPoliciesCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/ECAdmin.java)/** Command to list the set of enabled erasure coding policies. */
AddECPoliciesCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/ECAdmin.java)/** Command to add a set of erasure coding policies. */
GetECPolicyCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/ECAdmin.java)/** Command to get the erasure coding policy for a file or directory. */
RemoveECPolicyCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/ECAdmin.java)/** Command to remove an erasure coding policy. */
SetECPolicyCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/ECAdmin.java)/** Command to set the erasure coding policy to a file/directory. */
UnsetECPolicyCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/ECAdmin.java)/** Command to unset the erasure coding policy set for a file/directory. */
ListECCodecsCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/ECAdmin.java)/** Command to list the set of supported erasure coding codecs and coders. */
EnableECPolicyCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/ECAdmin.java)/** Command to enable an existing erasure coding policy. */
DisableECPolicyCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/ECAdmin.java)/** Command to disable an existing erasure coding policy. */
VerifyClusterSetupCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/ECAdmin.java)/**
   * Command to verify the cluster setup can support all enabled EC policies.
   */
ECAdmin (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/ECAdmin.java)/**
 * CLI for the erasure code encoding operations.
 */
CommandHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/GetConf.java)/** 
   * Handler to return value for key corresponding to the {@link Command}
   */
NameNodesCommandHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/GetConf.java)/**
   * Handler for {@link Command#NAMENODE}
   */
BackupNodesCommandHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/GetConf.java)/**
   * Handler for {@link Command#BACKUP}
   */
JournalNodeCommandHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/GetConf.java)/**
   * Handler for {@linke Command#JOURNALNODE}.
   */
SecondaryNameNodesCommandHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/GetConf.java)/**
   * Handler for {@link Command#SECONDARY}
   */
NNRpcAddressesCommandHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/GetConf.java)/**
   * Handler for {@link Command#NNRPCADDRESSES}
   * If rpc addresses are defined in configuration, we return them. Otherwise, 
   * return empty string.
   */
GetConf (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/GetConf.java)/**
 * Tool for getting configuration information from a configuration file.
 * 
 * Adding more options:
 * <ul>
 * <li>
 * If adding a simple option to get a value corresponding to a key in the 
 * configuration, use regular {@link GetConf.CommandHandler}. 
 * See {@link GetConf.Command#EXCLUDE_FILE} example.
 * </li>
 * <li>
 * If adding an option that is does not return a value for a key, add
 * a subclass of {@link GetConf.CommandHandler} and set it up in 
 * {@link GetConf.Command}.
 * 
 * See {@link GetConf.Command#NAMENODE} for example.
 * 
 * Add for the new option added, a map entry with the corresponding
 * {@link GetConf.CommandHandler}.
 * </ul>
 */
GetGroups (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/GetGroups.java)/**
 * HDFS implementation of a tool for getting the groups which a given user
 * belongs to.
 */
JMXGet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/JMXGet.java)/**
 * tool to get data from NameNode or DataNode using MBeans currently the
 * following MBeans are available (under hadoop domain):
 * hadoop:service=NameNode,name=FSNamesystemState (static)
 * hadoop:service=NameNode,name=NameNodeActivity (dynamic)
 * hadoop:service=NameNode,name=RpcActivityForPort9000 (dynamic)
 * hadoop:service=DataNode,name=RpcActivityForPort9867 (dynamic)
 * hadoop:name=service=DataNode,FSDatasetState-UndefinedStorageId663800459
 * (static)
 * hadoop:service=DataNode,name=DataNodeActivity-UndefinedStorageId-520845215
 * (dynamic)
 * 
 * 
 * implementation note: all logging is sent to System.err (since it is a command
 * line tool)
 */
NNHAServiceTarget (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java)/**
 * One of the NN NameNodes acting as the target of an administrative command
 * (e.g. failover).
 */
BinaryEditsVisitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/BinaryEditsVisitor.java)/**
 * BinaryEditsVisitor implements a binary EditsVisitor
 */
OfflineEditsBinaryLoader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsBinaryLoader.java)/**
 * OfflineEditsBinaryLoader loads edits from a binary edits file
 */
OfflineEditsLoader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsLoader.java)/**
 * OfflineEditsLoader walks an EditsVisitor over an EditLogInputStream
 */
OfflineEditsViewer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsViewer.java)/**
 * This class implements an offline edits viewer, tool that
 * can be used to view edit logs.
 */
OfflineEditsVisitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsVisitor.java)/**
 * An implementation of OfflineEditsVisitor can traverse the structure of an
 * Hadoop edits log and respond to each of the structures within the file.
 */
OfflineEditsVisitorFactory (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsVisitorFactory.java)/**
 * EditsVisitorFactory for different implementations of EditsVisitor
 *
 */
OfflineEditsXmlLoader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsXmlLoader.java)/**
 * OfflineEditsXmlLoader walks an EditsVisitor over an OEV XML file
 */
StatisticsEditsVisitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/StatisticsEditsVisitor.java)/**
 * StatisticsEditsVisitor implements text version of EditsVisitor
 * that aggregates counts of op codes processed
 *
 */
TeeOutputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TeeOutputStream.java)/**
 * A TeeOutputStream writes its output to multiple output streams.
 */
XmlEditsVisitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java)/**
 * An XmlEditsVisitor walks over an EditLog structure and writes out
 * an equivalent XML document that contains the EditLog's components.
 */
DelimitedImageVisitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/DelimitedImageVisitor.java)/**
 * A DelimitedImageVisitor generates a text representation of the fsimage,
 * with each element separated by a delimiter string.  All of the elements
 * common to both inodes and inodes-under-construction are included. When 
 * processing an fsimage with a layout version that did not include an 
 * element, such as AccessTime, the output file will include a column
 * for the value, but no value will be included.
 * 
 * Individual block information for each file is not currently included.
 * 
 * The default delimiter is tab, as this is an unlikely value to be included
 * an inode path or other text metadata.  The delimiter value can be via the
 * constructor.
 */
DepthCounter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/DepthCounter.java)/**
 * Utility class for tracking descent into the structure of the
 * Visitor class (ImageVisitor, EditsVisitor etc.)
 */
FileDistributionCalculator (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FileDistributionCalculator.java)/**
 * This is the tool for analyzing file sizes in the namespace image. In order to
 * run the tool one should define a range of integers <tt>[0, maxSize]</tt> by
 * specifying <tt>maxSize</tt> and a <tt>step</tt>. The range of integers is
 * divided into segments of size <tt>step</tt>:
 * <tt>[0, s<sub>1</sub>, ..., s<sub>n-1</sub>, maxSize]</tt>, and the visitor
 * calculates how many files in the system fall into each segment
 * <tt>[s<sub>i-1</sub>, s<sub>i</sub>)</tt>. Note that files larger than
 * <tt>maxSize</tt> always fall into the very last segment.
 *
 * <h3>Input.</h3>
 * <ul>
 * <li><tt>filename</tt> specifies the location of the image file;</li>
 * <li><tt>maxSize</tt> determines the range <tt>[0, maxSize]</tt> of files
 * sizes considered by the visitor;</li>
 * <li><tt>step</tt> the range is divided into segments of size step.</li>
 * </ul>
 *
 * <h3>Output.</h3> The output file is formatted as a tab separated two column
 * table: Size and NumFiles. Where Size represents the start of the segment, and
 * numFiles is the number of files form the image which size falls in this
 * segment.
 *
 */
FileContext (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FileDistributionVisitor.java)/**
   * File or directory information.
   */
FileDistributionVisitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FileDistributionVisitor.java)/**
 * File size distribution visitor.
 * 
 * <h3>Description.</h3>
 * This is the tool for analyzing file sizes in the namespace image.
 * In order to run the tool one should define a range of integers
 * <tt>[0, maxSize]</tt> by specifying <tt>maxSize</tt> and a <tt>step</tt>.
 * The range of integers is divided into segments of size <tt>step</tt>: 
 * <tt>[0, s<sub>1</sub>, ..., s<sub>n-1</sub>, maxSize]</tt>,
 * and the visitor calculates how many files in the system fall into 
 * each segment <tt>[s<sub>i-1</sub>, s<sub>i</sub>)</tt>. 
 * Note that files larger than <tt>maxSize</tt> always fall into 
 * the very last segment.
 * 
 * <h3>Input.</h3>
 * <ul>
 * <li><tt>filename</tt> specifies the location of the image file;</li>
 * <li><tt>maxSize</tt> determines the range <tt>[0, maxSize]</tt> of files
 * sizes considered by the visitor;</li>
 * <li><tt>step</tt> the range is divided into segments of size step.</li>
 * </ul>
 *
 * <h3>Output.</h3>
 * The output file is formatted as a tab separated two column table:
 * Size and NumFiles. Where Size represents the start of the segment,
 * and numFiles is the number of files form the image which size falls in 
 * this segment.
 */
FSImageHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageHandler.java)/**
 * Implement the read-only WebHDFS API for fsimage.
 */
FSImageLoader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java)/**
 * FSImageLoader loads fsimage and provide methods to return JSON formatted
 * file status of the namespace of the fsimage.
 */
IgnoreSnapshotException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/IgnoreSnapshotException.java)/**
 * Signals that a snapshot is ignored.
 */
LoaderFactory (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/ImageLoader.java)/**
   * Factory for obtaining version of image loader that can read
   * a particular image format.
   */
ImageLoader (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/ImageLoader.java)/**
 * An ImageLoader can accept a DataInputStream to an Hadoop FSImage file
 * and walk over its structure using the supplied ImageVisitor.
 *
 * Each implementation of ImageLoader is designed to rapidly process an
 * image file.  As long as minor changes are made from one layout version
 * to another, it is acceptable to tweak one implementation to read the next.
 * However, if the layout version changes enough that it would make a
 * processor slow or difficult to read, another processor should be created.
 * This allows each processor to quickly read an image without getting
 * bogged down in dealing with significant differences between layout versions.
 */
ImageLoaderCurrent (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/ImageLoaderCurrent.java)/**
 * ImageLoaderCurrent processes Hadoop FSImage files and walks over
 * them using a provided ImageVisitor, calling the visitor at each element
 * enumerated below.
 *
 * The only difference between v18 and v19 was the utilization of the
 * stickybit.  Therefore, the same viewer can reader either format.
 *
 * Versions -19 fsimage layout (with changes from -16 up):
 * Image version (int)
 * Namepsace ID (int)
 * NumFiles (long)
 * Generation stamp (long)
 * INodes (count = NumFiles)
 *  INode
 *    Path (String)
 *    Replication (short)
 *    Modification Time (long as date)
 *    Access Time (long) // added in -16
 *    Block size (long)
 *    Num blocks (int)
 *    Blocks (count = Num blocks)
 *      Block
 *        Block ID (long)
 *        Num bytes (long)
 *        Generation stamp (long)
 *    Namespace Quota (long)
 *    Diskspace Quota (long) // added in -18
 *    Permissions
 *      Username (String)
 *      Groupname (String)
 *      OctalPerms (short -> String)  // Modified in -19
 *    Symlink (String) // added in -23
 * NumINodesUnderConstruction (int)
 * INodesUnderConstruction (count = NumINodesUnderConstruction)
 *  INodeUnderConstruction
 *    Path (bytes as string)
 *    Replication (short)
 *    Modification time (long as date)
 *    Preferred block size (long)
 *    Num blocks (int)
 *    Blocks
 *      Block
 *        Block ID (long)
 *        Num bytes (long)
 *        Generation stamp (long)
 *    Permissions
 *      Username (String)
 *      Groupname (String)
 *      OctalPerms (short -> String)
 *    Client Name (String)
 *    Client Machine (String)
 *    NumLocations (int)
 *    DatanodeDescriptors (count = numLocations) // not loaded into memory
 *      short                                    // but still in file
 *      long
 *      string
 *      long
 *      int
 *      string
 *      string
 *      enum
 *    CurrentDelegationKeyId (int)
 *    NumDelegationKeys (int)
 *      DelegationKeys (count = NumDelegationKeys)
 *        DelegationKeyLength (vint)
 *        DelegationKey (bytes)
 *    DelegationTokenSequenceNumber (int)
 *    NumDelegationTokens (int)
 *    DelegationTokens (count = NumDelegationTokens)
 *      DelegationTokenIdentifier
 *        owner (String)
 *        renewer (String)
 *        realUser (String)
 *        issueDate (vlong)
 *        maxDate (vlong)
 *        sequenceNumber (vint)
 *        masterKeyId (vint)
 *      expiryTime (long)     
 *
 */
ImageVisitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/ImageVisitor.java)/**
 * An implementation of ImageVisitor can traverse the structure of an
 * Hadoop fsimage and respond to each of the structures within the file.
 */
IndentedImageVisitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/IndentedImageVisitor.java)/**
 * IndentedImageVisitor walks over an FSImage and displays its structure 
 * using indenting to organize sections within the image file.
 */
LsImageVisitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/LsImageVisitor.java)/**
 * LsImageVisitor displays the blocks of the namespace in a format very similar
 * to the output of ls/lsr.  Entries are marked as directories or not,
 * permissions listed, replication, username and groupname, along with size,
 * modification date and full path.
 *
 * Note: A significant difference between the output of the lsr command
 * and this image visitor is that this class cannot sort the file entries;
 * they are listed in the order they are stored within the fsimage file. 
 * Therefore, the output of this class cannot be directly compared to the
 * output of the lsr command.
 */
NameDistributionVisitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/NameDistributionVisitor.java)/**
 * File name distribution visitor. 
 * <p>
 * It analyzes file names in fsimage and prints the following information:
 * <ul>
 * <li>Number of unique file names</li> 
 * <li>Number file names and the corresponding number range of files that use 
 * these same names</li>
 * <li>Heap saved if the file name objects are reused</li>
 * </ul>
 */
SectionProcessor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java)/**
   * A processor for an FSImage XML section.
   */
NameSectionProcessor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java)/**
   * Processes the NameSection containing last allocated block ID, etc.
   */
OfflineImageViewer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java)/**
 * OfflineImageViewer to dump the contents of an Hadoop image file to XML
 * or the console.  Main entry point into utility, either via the
 * command line or programmatically.
 */
OfflineImageViewerPB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewerPB.java)/**
 * OfflineImageViewerPB to dump the contents of an Hadoop image file to XML or
 * the console. Main entry point into utility, either via the command line or
 * programmatically.
 */
PBImageCorruption (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageCorruption.java)/**
 * Class representing a corruption in the PBImageCorruptionDetector processor.
 */
OutputEntryBuilder (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageCorruptionDetector.java)/**
   * Builder object for producing entries (lines) for
   * PBImageCorruptionDetector. The isSnapshot field is mandatory.
   */
PBImageCorruptionDetector (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageCorruptionDetector.java)/**
 * The PBImageCorruptionDetector detects corruptions in the image.
 * It produces a file with the found issues similar to the Delimited
 * processor. The default delimiter is tab, as this is an unlikely value
 * to be included in an inode path. The delimiter value can be changed
 * via the constructor.
 *
 * It looks for the following kinds of corruptions:
 *  - an INode id is mentioned in the INodeDirectorySection, but not present
 *    in the INodeSection (corrupt INode case)
 *  - an INode has children, but at least one of them is corrupted
 *    (missing children case)
 * If multiple layers of directory structure are damaged then it is possible
 * that an INode is corrupted and also having corrupted children.
 *
 * Note that the OIV DetectCorruption processor check is not exhaustive,
 * and only catches the corruptions like above. This processor may be up to
 * extension in the future when new aspects of corruption are found.
 */
PBImageDelimitedTextWriter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageDelimitedTextWriter.java)/**
 * A PBImageDelimitedTextWriter generates a text representation of the PB fsimage,
 * with each element separated by a delimiter string.  All of the elements
 * common to both inodes and inodes-under-construction are included. When
 * processing an fsimage with a layout version that did not include an
 * element, such as AccessTime, the output file will include a column
 * for the value, but no value will be included.
 *
 * Individual block information for each file is not currently included.
 *
 * The default delimiter is tab, as this is an unlikely value to be included in
 * an inode path or other text metadata. The delimiter value can be via the
 * constructor.
 */
MetadataMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java)/**
   * This metadata map is used to construct the namespace before generating
   * text outputs.
   *
   * It contains two mapping relationships:
   * <p>
   *   <li>It maps each inode (inode Id) to its parent directory (inode Id).</li>
   *   <li>It maps each directory from its inode Id.</li>
   * </p>
   */
Dir (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java)/**
     * Represent a directory in memory.
     */
CorruptedDir (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java)/**
     * If the Dir entry does not exist (i.e. the inode was not contained in
     * INodeSection) we still create a Dir entry which throws exceptions
     * for calls other than getId().
     * We can make sure this way, the getId and getParentId calls will
     * always succeed if we have the information.
     */
InMemoryMetadataDB (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java)/**
   * Maintain all the metadata in memory.
   */
LevelDBStore (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java)/**
     * Store metadata in LevelDB.
     */
DirPathCache (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java)/**
     * A LRU cache for directory path strings.
     *
     * The key of this LRU cache is the inode of a directory.
     */
LevelDBMetadataMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java)/**
   * A MetadataMap that stores metadata in LevelDB.
   */
PBImageTextWriter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java)/**
 * This class reads the protobuf-based fsimage and generates text output
 * for each inode to {@link PBImageTextWriter#out}. The sub-class can override
 * {@link getEntry()} to generate formatted string for each inode.
 *
 * Since protobuf-based fsimage does not guarantee the order of inodes and
 * directories, PBImageTextWriter runs two-phase scans:
 *
 * <ol>
 *   <li>The first phase, PBImageTextWriter scans the INode sections to reads the
 *   filename of each directory. It also scans the INode_Dir sections to loads
 *   the relationships between a directory and its children. It uses these metadata
 *   to build FS namespace and stored in {@link MetadataMap}</li>
 *   <li>The second phase, PBImageTextWriter re-scans the INode sections. For each
 *   inode, it looks up the path of the parent directory in the {@link MetadataMap},
 *   and generate output.</li>
 * </ol>
 *
 * Two various of {@link MetadataMap} are provided. {@link InMemoryMetadataDB}
 * stores all metadata in memory (O(n) memory) while
 * {@link LevelDBMetadataMap} stores metadata in LevelDB on disk (O(1) memory).
 * User can choose between them based on the time/space tradeoffs.
 */
PBImageXmlWriter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java)/**
 * PBImageXmlWriter walks over an fsimage structure and writes out
 * an equivalent XML document that contains the fsimage's components.
 */
TextWriterImageVisitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/TextWriterImageVisitor.java)/**
 * TextWriterImageProcessor mixes in the ability for ImageVisitor
 * implementations to easily write their output to a text file.
 *
 * Implementing classes should be sure to call the super methods for the
 * constructors, finish and finishAbnormally methods, in order that the
 * underlying file may be opened and closed correctly.
 *
 * Note, this class does not add newlines to text written to file or (if
 * enabled) screen.  This is the implementing class' responsibility.
 */
WebImageViewer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/WebImageViewer.java)/**
 * WebImageViewer loads a fsimage and exposes read-only WebHDFS API for its
 * namespace.
 */
XmlImageVisitor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/XmlImageVisitor.java)/**
 * An XmlImageVisitor walks over an fsimage structure and writes out
 * an equivalent XML document that contains the fsimage's components.
 */
LsSnapshottableDir (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/LsSnapshottableDir.java)/**
 * A tool used to list all snapshottable directories that are owned by the 
 * current user. The tool returns all the snapshottable directories if the user
 * is a super user.
 */
SnapshotDiff (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/snapshot/SnapshotDiff.java)/**
 * A tool used to get the difference report between two snapshots, or between
 * a snapshot and the current status of a directory. 
 * <pre>
 * Usage: SnapshotDiff snapshotDir from to
 * For from/to, users can use "." to present the current status, and use 
 * ".snapshot/snapshot_name" to present a snapshot, where ".snapshot/" can be 
 * omitted.
 * </pre>
 */
ListStoragePoliciesCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/StoragePolicyAdmin.java)/** Command to list all the existing storage policies */
GetStoragePolicyCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/StoragePolicyAdmin.java)/** Command to get the storage policy of a file/directory */
SetStoragePolicyCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/StoragePolicyAdmin.java)/** Command to set the storage policy to a file/directory */
SatisfyStoragePolicyCommand (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/StoragePolicyAdmin.java)/** Command to schedule blocks to move based on specified policy. */
StoragePolicyAdmin (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/StoragePolicyAdmin.java)/**
 * This class implements block storage policy operations.
 */
AtomicFileOutputStream (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/AtomicFileOutputStream.java)/**
 * A FileOutputStream that has the property that it will only show
 * up at its destination once it has been entirely written and flushed
 * to disk. While being written, it will use a .tmp suffix.
 * 
 * When the output stream is closed, it is flushed, fsynced, and
 * will be moved into place, overwriting any file that already
 * exists at that location.
 * 
 * <b>NOTE</b>: on Windows platforms, it will not atomically
 * replace the target file - instead the target file is deleted
 * before this one is moved into place.
 */
BestEffortLongFile (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/BestEffortLongFile.java)/**
 * Class that represents a file on disk which stores a single <code>long</code>
 * value, but does not make any effort to make it truly durable. This is in
 * contrast to {@link PersistentLongFile} which fsync()s the value on every
 * change.
 * 
 * This should be used for values which are updated frequently (such that
 * performance is important) and not required to be up-to-date for correctness.
 * 
 * This class also differs in that it stores the value as binary data instead
 * of a textual string.
 */
ByteArray (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ByteArray.java)/** 
 * Wrapper for byte[] to use byte[] as key in HashMap
 */
Canceler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/Canceler.java)/**
 * Provides a simple interface where one thread can mark an operation
 * for cancellation, and another thread can poll for whether the
 * cancellation has occurred.
 */
ConstEnumException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ConstEnumCounters.java)/**
   * An exception class for modification on ConstEnumCounters.
   */
ConstEnumCounters (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ConstEnumCounters.java)/**
 * Const Counters for an enum type.
 *
 * It's the const version of EnumCounters. Any modification ends with a
 * ConstEnumException.
 *
 * @see org.apache.hadoop.hdfs.util.EnumCounters
 */
CyclicIterator (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/CyclicIteration.java)/** An {@link Iterator} for {@link CyclicIteration}. */
CyclicIteration (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/CyclicIteration.java)/** Provide an cyclic {@link Iterator} for a {@link NavigableMap}.
 * The {@link Iterator} navigates the entries of the map
 * according to the map's ordering.
 * If the {@link Iterator} hits the last entry of the map,
 * it will then continue from the first entry.
 */
DataTransferThrottler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/DataTransferThrottler.java)/** 
 * a class to throttle the data transfers.
 * This class is thread safe. It can be shared by multiple threads.
 * The parameter bandwidthPerSec specifies the total bandwidth shared by
 * threads.
 */
Element (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/Diff.java)/** An interface for the elements in a {@link Diff}. */
Processor (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/Diff.java)/** An interface for passing a method in order to process elements. */
Container (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/Diff.java)/** Containing exactly one element. */
UndoInfo (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/Diff.java)/** 
   * Undo information for some operations such as delete(E)
   * and {@link Diff#modify(Element, Element)}.
   */
Diff (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/Diff.java)/**
 * The difference between the current state and a previous state of a list.
 * 
 * Given a previous state of a set and a sequence of create, delete and modify
 * operations such that the current state of the set can be obtained by applying
 * the operations on the previous state, the following algorithm construct the
 * difference between the current state and the previous state of the set.
 * 
 * <pre>
 * Two lists are maintained in the algorithm:
 * - c-list for newly created elements
 * - d-list for the deleted elements
 *
 * Denote the state of an element by the following
 *   (0, 0): neither in c-list nor d-list
 *   (c, 0): in c-list but not in d-list
 *   (0, d): in d-list but not in c-list
 *   (c, d): in both c-list and d-list
 *
 * For each case below, ( , ) at the end shows the result state of the element.
 *
 * Case 1. Suppose the element i is NOT in the previous state.           (0, 0)
 *   1.1. create i in current: add it to c-list                          (c, 0)
 *   1.1.1. create i in current and then create: impossible
 *   1.1.2. create i in current and then delete: remove it from c-list   (0, 0)
 *   1.1.3. create i in current and then modify: replace it in c-list    (c', 0)
 *
 *   1.2. delete i from current: impossible
 *
 *   1.3. modify i in current: impossible
 *
 * Case 2. Suppose the element i is ALREADY in the previous state.       (0, 0)
 *   2.1. create i in current: impossible
 *
 *   2.2. delete i from current: add it to d-list                        (0, d)
 *   2.2.1. delete i from current and then create: add it to c-list      (c, d)
 *   2.2.2. delete i from current and then delete: impossible
 *   2.2.2. delete i from current and then modify: impossible
 *
 *   2.3. modify i in current: put it in both c-list and d-list          (c, d)
 *   2.3.1. modify i in current and then create: impossible
 *   2.3.2. modify i in current and then delete: remove it from c-list   (0, d)
 *   2.3.3. modify i in current and then modify: replace it in c-list    (c', d)
 * </pre>
 *
 * @param <K> The key type.
 * @param <E> The element type, which must implement {@link Element} interface.
 */
EnumCounters (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/EnumCounters.java)/**
 * Counters for an enum type.
 * 
 * For example, suppose there is an enum type
 * <pre>
 * enum Fruit { APPLE, ORANGE, GRAPE }
 * </pre>
 * An {@link EnumCounters} object can be created for counting the numbers of
 * APPLE, ORANGE and GRAPE.
 *
 * @param <E> the enum type
 */
EnumDoubles (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/EnumDoubles.java)/**
 * Similar to {@link EnumCounters} except that the value type is double.
 *
 * @param <E> the enum type
 */
Node (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/FoldedTreeSet.java)/**
   * Internal tree node that holds a sorted array of entries.
   *
   * @param <E> type of the elements
   */
FoldedTreeSet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/FoldedTreeSet.java)/**
 * A memory efficient implementation of RBTree. Instead of having a Node for
 * each entry each node contains an array holding 64 entries.
 *
 * Based on the Apache Harmony folded TreeMap.
 *
 * @param <E> Entry type
 */
Holder (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/Holder.java)/**
 * A Holder is simply a wrapper around some other object. This is useful
 * in particular for storing immutable values like boxed Integers in a
 * collection without having to do the &quot;lookup&quot; of the value twice.
 */
LinkedElement (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/LightWeightHashSet.java)/**
   * Elements of {@link LightWeightLinkedSet}.
   */
LightWeightHashSet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/LightWeightHashSet.java)/**
 * A low memory linked hash set implementation, which uses an array for storing
 * the elements and linked lists for collision resolution. This class does not
 * support null element.
 *
 * This class is not thread safe.
 *
 */
DoubleLinkedElement (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/LightWeightLinkedSet.java)/**
   * Elements of {@link LightWeightLinkedSet}.
   */
LightWeightLinkedSet (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/LightWeightLinkedSet.java)/**
 * A low memory linked hash set implementation, which uses an array for storing
 * the elements and linked lists for collision resolution. In addition it stores
 * elements in a linked list to ensure ordered traversal. This class does not
 * support null element.
 *
 * This class is not thread safe.
 *
 */
MD5FileUtils (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java)/**
 * Static functions for dealing with files of the same format
 * that the Unix "md5sum" utility writes.
 */
PersistentLongFile (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/PersistentLongFile.java)/**
 * Class that represents a file on disk which persistently stores
 * a single <code>long</code> value. The file is updated atomically
 * and durably (i.e fsynced). 
 */
Util (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ReadOnlyList.java)/**
   * Utilities for {@link ReadOnlyList}
   */
ReadOnlyList (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ReadOnlyList.java)/**
 * A {@link ReadOnlyList} is a unmodifiable list,
 * which supports read-only operations.
 * 
 * @param <E> The type of the list elements.
 */
ReferenceCounter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ReferenceCountMap.java)/**
   * Interface for the reference count holder
   */
ReferenceCountMap (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ReferenceCountMap.java)/**
 * Class for de-duplication of instances. <br>
 * Hold the references count to a single instance. If there are no references
 * then the entry will be removed.<br>
 * Type E should implement {@link ReferenceCounter}<br>
 * Note: This class is NOT thread-safe.
 */
RwLock (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/RwLock.java)/** Read-write lock interface. */
InvalidXmlException (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/XMLUtils.java)/**
   * Exception that reflects an invalid XML document.
   */
UnmanglingError (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/XMLUtils.java)/**
   * Exception that reflects a string that cannot be unmangled.
   */
Stanza (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/XMLUtils.java)/**
   * Represents a bag of key-value pairs encountered during parsing an XML
   * file.
   */
XMLUtils (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/XMLUtils.java)/**
 * General xml utilities.
 *   
 */
AuthFilter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/AuthFilter.java)/**
 * Subclass of {@link AuthenticationFilter} that
 * obtains Hadoop-Auth configuration for webhdfs.
 */
AuthFilterInitializer (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/AuthFilterInitializer.java)/**
 * Filter initializer to initialize {@link AuthFilter}.
 */
JsonUtil (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java)/** JSON Utilities */
ParamFilter (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/ParamFilter.java)/**
 * A filter to change parameter names to lower cases
 * so that parameter names are considered as case insensitive.
 */
ExceptionHandler (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/ExceptionHandler.java)/** Handle exceptions. */
NamenodeAddressParam (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/NamenodeAddressParam.java)/** Namenode RPC address parameter. */
UriFsPathParam (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/UriFsPathParam.java)/** The FileSystem path parameter. */
UserProvider (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/UserProvider.java)/** Inject user information to http operations. */
WebHdfsDtFetcher (/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/WebHdfsDtFetcher.java)/**
 *  DtFetcher for WebHdfsFileSystem using the base class HdfsDtFetcher impl.
 */
TestAclCLIWithPosixAclInheritance (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestAclCLIWithPosixAclInheritance.java)/**
 * Test ACL CLI with POSIX ACL inheritance enabled.
 */
HDFSContract (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/contract/hdfs/HDFSContract.java)/**
 * The contract of HDFS
 * This changes its feature set from platform for platform -the default
 * set is updated during initialization.
 */
TestHDFSContractConcat (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/contract/hdfs/TestHDFSContractConcat.java)/**
 * Test dir operations on a the local FS.
 */
TestHDFSContractDelete (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/contract/hdfs/TestHDFSContractDelete.java)/**
 * Test dir operations on a the local FS.
 */
TestHDFSContractMkdir (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/contract/hdfs/TestHDFSContractMkdir.java)/**
 * Test dir operations on a the local FS.
 */
TestHDFSContractMultipartUploader (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/contract/hdfs/TestHDFSContractMultipartUploader.java)/**
 * Test MultipartUploader tests on HDFS.
 */
TestHDFSContractOpen (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/contract/hdfs/TestHDFSContractOpen.java)/**
 * Test Open operations on HDFS.
 */
TestHDFSContractPathHandle (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/contract/hdfs/TestHDFSContractPathHandle.java)/**
 * Verify HDFS compliance with {@link org.apache.hadoop.fs.PathHandle}
 * semantics.
 */
TestHDFSContractRootDirectory (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/contract/hdfs/TestHDFSContractRootDirectory.java)/**
 * Test dir operations on a the local FS.
 */
TestHDFSContractSeek (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/contract/hdfs/TestHDFSContractSeek.java)/**
 * Test dir operations on a the local FS.
 */
TestLoadGenerator (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/loadGenerator/TestLoadGenerator.java)/**
 * This class tests if a balancer schedules tasks correctly.
 */
TestHdfsTextCommand (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/shell/TestHdfsTextCommand.java)/**
 * This class tests the logic for displaying the binary formats supported
 * by the Text command.
 */
TestEnhancedByteBufferAccess (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestEnhancedByteBufferAccess.java)/**
 * This class tests if EnhancedByteBufferAccess works correctly.
 */
FSTestWrapperGlobTest (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java)/**
   * A glob test that can be run on either FileContext or FileSystem.
   */
AcceptAllPathFilter (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java)/**
   * Accept all paths.
   */
AcceptPathsEndingInZ (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java)/**
   * Accept only paths ending in Z.
   */
TestGlobWithSymlinks (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java)/**
   * Test globbing through symlinks.
   */
TestGlobWithSymlinksToSymlinks (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java)/**
   * Test globbing symlinks to symlinks.
   *
   * Also test globbing dangling symlinks.  It should NOT throw any exceptions!
   */
TestGlobSymlinksWithCustomPathFilter (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java)/**
   * Test globbing symlinks with a custom PathFilter
   */
TestGlobFillsInScheme (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java)/**
   * Test that globStatus fills in the scheme even when it is not provided.
   */
TestRelativePath (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java)/**
   * Test that globStatus works with relative paths.
   **/
TestGlobAccessDenied (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java)/**
   * Test that trying to glob through a directory we don't have permission
   * to list fails with AccessControlException rather than succeeding or
   * throwing any other exception.
   **/
TestReservedHdfsPaths (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java)/**
   * Test that trying to list a reserved path on HDFS via the globber works.
   **/
TestGlobRoot (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java)/**
   * Test trying to glob the root.  Regression test for HDFS-5888.
   **/
TestNonTerminalGlobs (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java)/**
   * Test glob expressions that don't appear at the end of the path.  Regression
   * test for HADOOP-10957.
   **/
TestResolveHdfsSymlink (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestResolveHdfsSymlink.java)/**
 * Tests whether FileContext can resolve an hdfs path that has a symlink to
 * local file system. Also tests getDelegationTokens API in file context with
 * underlying file system as Hdfs.
 */
TestSWebHdfsFileContextMainOperations (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestSWebHdfsFileContextMainOperations.java)/**
 * Test of FileContext apis on SWebhdfs.
 */
TestSymlinkHdfs (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestSymlinkHdfs.java)/**
 * Test symbolic links in Hdfs.
 */
TestUrlStreamHandler (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestUrlStreamHandler.java)/**
 * Test of the URL stream handler.
 */
TestWebHdfsFileContextMainOperations (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestWebHdfsFileContextMainOperations.java)/**
 * Test of FileContext apis on Webhdfs.
 */
TestViewFileSystemAtHdfsRoot (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemAtHdfsRoot.java)/**
 * Make sure that ViewFileSystem works when the root of an FS is mounted to a
 * ViewFileSystem mount point.
 */
TestViewFileSystemLinkFallback (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkFallback.java)/**
 * Test for viewfs with LinkFallback mount table entries.
 */
TestViewFileSystemLinkMergeSlash (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemLinkMergeSlash.java)/**
 * Test for viewfs with LinkMergeSlash mount table entries.
 */
TestViewFileSystemWithAcls (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemWithAcls.java)/**
 * Verify ACL through ViewFileSystem functionality.
 */
TestViewFileSystemWithTruncate (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemWithTruncate.java)/**
 * Verify truncate through ViewFileSystem functionality.
 *
 */
TestViewFileSystemWithXAttrs (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemWithXAttrs.java)/**
 * Verify XAttrs through ViewFileSystem functionality.
 */
TestViewFsAtHdfsRoot (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsAtHdfsRoot.java)/**
 * Make sure that ViewFs works when the root of an FS is mounted to a ViewFs
 * mount point.
 */
TestViewFsDefaultValue (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsDefaultValue.java)/**
 * Tests for viewfs implementation of default fs level values.
 * This tests for both passing in a path (based on mount point)
 * to obtain the default value of the fs that the path is mounted on
 * or just passing in no arguments.
 */
TestViewFsWithAcls (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsWithAcls.java)/**
 * Verify ACL through ViewFs functionality.
 */
TestViewFsWithXAttrs (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsWithXAttrs.java)/**
 * Verify XAttrs through ViewFs functionality.
 */
AdminStatesBaseTest (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/AdminStatesBaseTest.java)/**
 * This class provide utilities for testing of the admin operations of nodes.
 */
AppendTestUtil (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/AppendTestUtil.java)/** Utilities for append-related tests */
BenchmarkThroughput (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/BenchmarkThroughput.java)/**
 * This class benchmarks the performance of the local file system, raw local
 * file system and HDFS at reading and writing files. The user should invoke
 * the main of this class and optionally include a repetition count.
 */
BlockReaderTestUtil (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/BlockReaderTestUtil.java)/**
 * A helper class to setup the cluster, and get to BlockReader and DataNode for a block.
 */
TestBlockReaderIoProvider (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/TestBlockReaderIoProvider.java)/**
 * Tests {@link BlockReaderIoProvider}'s profiling of short circuit read
 * latencies.
 */
TestBlockReaderLocalByteBufferFastLaneReads (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/TestBlockReaderLocal.java)/**
   * Test reads that bypass the bounce buffer (because they are aligned
   * and bigger than the readahead).
   */
TestBlockReaderLocalMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/TestBlockReaderLocalMetrics.java)/**
 * Tests {@link BlockReaderLocalMetrics}'s statistics.
 */
TestBlockReaderRemote (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/TestBlockReaderRemote.java)/**
 * This tests BlockReaderRemote.
 */
MyFile (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java)/** class MyFile contains enough information to recreate the contents of
   * a single file.
   */
MockUnixGroupsMapping (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java)/**
   * mock class to get group mapping for fake users
   * 
   */
ShortCircuitTestContext (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java)/**
   * A short-circuit test context which makes it easier to get a short-circuit
   * configuration and set everything up.
   */
DFSTestUtil (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java)/** Utilities for HDFS tests */
CallableBase (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/ErasureCodeBenchmarkThroughput.java)/**
   * A Callable that returns the number of bytes read/written
   */
ErasureCodeBenchmarkThroughput (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/ErasureCodeBenchmarkThroughput.java)/**
 * This class benchmarks the throughput of client read/write for both replica
 * and Erasure Coding.
 * <p/>
 * Currently 4 operations are supported: read, write, generate and cleanup data.
 * Users should specify an operation, the amount of data in MB for a single
 * client, and which storage policy to use, i.e. EC or replication.
 * Optionally, users can specify the number of clients to launch concurrently.
 * The tool launches 1 thread for each client. Number of client is 1 by default.
 * For reading, users can also specify whether stateful or positional read
 * should be used. Stateful read is chosen by default.
 * <p/>
 * Each client reads and writes different files.
 * For writing, client writes a temporary file at the desired amount, and the
 * file will be cleaned up when the test finishes.
 * For reading, each client tries to read the file specific to itself. And the
 * client simply returns if such file does not exist. Therefore, users should
 * generate the files before testing read. Generating data is essentially the
 * same as writing, except that the files won't be cleared at the end.
 * For example, if the user wants to test reading 1024MB data with 10 clients,
 * he/she should firstly generate 1024MB data with 10 (or more) clients.
 */
FileAppendTest4 (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/FileAppendTest4.java)/** This is a comprehensive append test that tries
 * all combinations of file length and number of appended bytes
 * In each iteration, it creates a file of len1. Then reopen
 * the file for append. It first append len2 bytes, calls hflush,
 * append len3 bytes and close the file. Afterwards, the content of
 * the file is validated.
 * Len1 ranges from [0, 2*BLOCK_SIZE+1], len2 ranges from [0, BLOCK_SIZE+1],
 * and len3 ranges from [0, BLOCK_SIZE+1].
 *
 */
LogVerificationAppender (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/LogVerificationAppender.java)/**
 * Used to verify that certain exceptions or messages are present in log output.
 */
Builder (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java)/**
   * Class to construct instances of MiniDFSClusters with specific options.
   */
NameNodeInfo (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java)/**
   * Stores the information related to a namenode in the cluster
   */
MiniDFSCluster (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java)/**
 * This class creates a single-process DFS cluster for junit testing.
 * The data directories for non-simulated DFS are under the testing directory.
 * For simulated data nodes, no underlying fs storage is used.
 */
MiniDFSNNTopology (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSNNTopology.java)/**
 * This class is used to specify the setup of namenodes when instantiating
 * a MiniDFSCluster. It consists of a set of nameservices, each of which
 * may have one or more namenodes (in the case of HA)
 */
TestDFSNetworkTopology (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java)/**
 * This class tests the correctness of storage type info stored in
 * DFSNetworkTopology.
 */
TestDFSNetworkTopologyPerformance (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopologyPerformance.java)/**
 * Performance test of the new DFSNetworkTopology chooseRandom.
 *
 * NOTE that the tests are not for correctness but for performance comparison,
 * so the tests are printing and writing down values rather than doing assertion
 * checks or timeout checks. Therefore, it is pointless to run these
 * tests without something reading the value. So disabled the tests to for now,
 * anyone interested in looking at the numbers can enable them.
 */
ParameterizedTestDFSStripedOutputStreamWithFailure (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/ParameterizedTestDFSStripedOutputStreamWithFailure.java)/**
 * Test striped file write operation with data node failures with parameterized
 * test cases.
 */
ParameterizedTestDFSStripedOutputStreamWithFailureWithRandomECPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/ParameterizedTestDFSStripedOutputStreamWithFailureWithRandomECPolicy.java)/**
 * This tests write operation of DFS striped file with a random erasure code
 * policy except for the default policy under Datanode failure conditions.
 */
TestBlackListBasedTrustedChannelResolver (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/TestBlackListBasedTrustedChannelResolver.java)/**
 * Test class for  {@link BlackListBasedTrustedChannelResolver}.
 */
TestAnnotations (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestAnnotations.java)/**
 * Tests to make sure all the protocol class public methods have
 * either {@link Idempotent} or {@link AtMostOnce} once annotations.
 */
TestLayoutVersion (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestLayoutVersion.java)/**
 * Test for {@link LayoutVersion}
 */
TestPBHelper (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocolPB/TestPBHelper.java)/**
 * Tests for {@link PBHelper}
 */
DirectExecutorService (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/DirectExecutorService.java)/**
 * A very basic ExecutorService for running submitted Callables serially.
 * Many bits of functionality are not implemented.
 */
TestQuorumJournalManager (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQuorumJournalManager.java)/**
 * Functional tests for QuorumJournalManager.
 * For true unit tests, see {@link TestQuorumJournalManagerUnit}.
 */
TestQuorumJournalManagerUnit (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQuorumJournalManagerUnit.java)/**
 * True unit tests for QuorumJournalManager
 */
JournalTestUtil (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/JournalTestUtil.java)/**
 * Utilities for testing {@link Journal} instances.
 */
TestJournaledEditsCache (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournaledEditsCache.java)/**
 * Test the {@link JournaledEditsCache} used for caching edits in-memory on the
 * {@link Journal}.
 */
TestJournalNodeMXBean (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournalNodeMXBean.java)/**
 * Test {@link JournalNodeMXBean}
 */
TestJournalNodeRespectsBindHostKeys (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournalNodeRespectsBindHostKeys.java)/**
 * This test checks that the JournalNode respects the following keys.
 *
 *  - DFS_JOURNALNODE_RPC_BIND_HOST_KEY
 *  - DFS_JOURNALNODE_HTTP_BIND_HOST_KEY
 *  - DFS_JOURNALNODE_HTTPS_BIND_HOST_KEY
 */
TestJournalNodeSync (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournalNodeSync.java)/**
 * Unit test for Journal Node formatting upon re-installation and syncing.
 */
ReadStripedFileWithDecodingHelper (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/ReadStripedFileWithDecodingHelper.java)/**
 * Utility class for testing online recovery of striped files.
 */
SecurityTestUtil (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/security/token/block/SecurityTestUtil.java)/** Utilities for security tests */
TestBlockToken (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/security/token/block/TestBlockToken.java)/** Unit tests for block tokens */
ITestInMemoryAliasMap (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/aliasmap/ITestInMemoryAliasMap.java)/**
 * ITestInMemoryAliasMap is an integration test that writes and reads to
 * an AliasMap. This is an integration test because it can't be run in parallel
 * like normal unit tests since there is conflict over the port being in use.
 */
TestSecureAliasMap (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/aliasmap/TestSecureAliasMap.java)/**
 * Test DN & NN communication in secured hdfs with alias map.
 */
NewNodeInfo (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java)/**
   * Class which contains information about the
   * new nodes to be added to the cluster for balancing.
   */
HostNameBasedNodes (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java)/**
   * The host names of new nodes are specified
   */
PortNumberBasedNodes (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java)/**
   * The number of data nodes to be started are specified.
   * The data nodes will have same host name, but different port numbers.
   *
   */
TestBalancer (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java)/**
 * This class tests if a balancer schedules tasks correctly.
 */
TestBalancerRPCDelay (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerRPCDelay.java)/**
 * The Balancer ensures that it disperses RPCs to the NameNode
 * in order to avoid NN's RPC queue saturation.
 */
TestBalancerService (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerService.java)/**
 * Test balancer run as a service.
 */
TestBalancerWithHANameNodes (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java)/**
 * Test balancer with HA NameNodes
 */
Suite (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java)/** Common objects used in various methods. */
TestBalancerWithMultipleNameNodes (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithMultipleNameNodes.java)/**
 * Test balancer with multiple NameNodes
 */
TestBalancerWithNodeGroup (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java)/**
 * This class tests if a balancer schedules tasks correctly.
 */
TestKeyManager (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestKeyManager.java)/**
 * Test KeyManager class.
 */
TestBlockInfoStriped (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfoStriped.java)/**
 * Test {@link BlockInfoStriped}.
 */
TestBlockManagerSafeMode (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManagerSafeMode.java)/**
 * This test is for testing {@link BlockManagerSafeMode} package local APIs.
 *
 * They use heavily mocked objects, treating the {@link BlockManagerSafeMode}
 * as white-box. Tests are light-weight thus no multi-thread scenario or real
 * mini-cluster is tested.
 *
 * @see org.apache.hadoop.hdfs.TestSafeMode
 * @see org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode
 * @see org.apache.hadoop.hdfs.TestSafeModeWithStripedFile
 */
TestBlockPlacementStatusDefault (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockPlacementStatusDefault.java)/**
 * Unit tests to validate the BlockPlacementStatusDefault policy, focusing on
 * the getAdditionAlReplicasRequired method.
 */
TestBlockPlacementStatusWithUpgradeDomain (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockPlacementStatusWithUpgradeDomain.java)/**
 * Unit tests for BlockPlacementStatusWithUpgradeDomain class.
 */
TestBlockReportLease (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockReportLease.java)/**
 * Tests that BlockReportLease in BlockManager.
 */
TestBlockStatsMXBean (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java)/**
 * Class for testing {@link BlockStatsMXBean} implementation
 */
TestBlockUnderConstructionFeature (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockUnderConstructionFeature.java)/**
 * This class provides tests for {@link BlockUnderConstructionFeature} class
 */
TestComputeInvalidateWork (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java)/**
 * Test if FSNamesystem handles heartbeat right
 */
TestCorruptReplicaInfo (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestCorruptReplicaInfo.java)/**
 * This test makes sure that 
 *   CorruptReplicasMap::numBlocksWithCorruptReplicas and
 *   CorruptReplicasMap::getCorruptReplicaBlockIds
 *   return the correct values
 */
TestDatanodeDescriptor (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestDatanodeDescriptor.java)/**
 * This class tests that methods in DatanodeDescriptor
 */
MyResolver (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestDatanodeManager.java)/**
   * MyResolver class provides resolve method which always returns null 
   * in order to simulate unresolved topology mapping.
   */
MockDfsNetworkTopology (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestDatanodeManager.java)/**
   * A NetworkTopology implementation for test.
   *
   */
TestHeartbeatHandling (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHeartbeatHandling.java)/**
 * Test if FSNamesystem handles heartbeat right
 */
TestLowRedundancyBlockQueues (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestLowRedundancyBlockQueues.java)/**
 * Test {@link LowRedundancyBlocks}.
 */
TestNodeCount (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestNodeCount.java)/**
 * Test if live nodes count per node is correct 
 * so NN makes right decision for under/over-replicated blocks
 */
TestPendingInvalidateBlock (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingInvalidateBlock.java)/**
 * Test if we can correctly delay the deletion of blocks.
 */
TestPendingReconstruction (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingReconstruction.java)/**
 * This class tests the internals of PendingReconstructionBlocks.java, as well
 * as how PendingReconstructionBlocks acts in BlockManager
 */
TestPendingRecoveryBlocks (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingRecoveryBlocks.java)/**
 * This class contains unit tests for PendingRecoveryBlocks.java functionality.
 */
TestProvidedStorageMap (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestProvidedStorageMap.java)/**
 * This class tests the {@link ProvidedStorageMap}.
 */
TestRBWBlockInvalidation (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestRBWBlockInvalidation.java)/**
 * Test when RBW block is removed. Invalidation of the corrupted block happens
 * and then the under replicated block gets replicated to the datanode.
 */
TestRedundancyMonitor (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestRedundancyMonitor.java)/**
 * This class tests RedundancyMonitor in BlockManager.
 */
TestSequentialBlockGroupId (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestSequentialBlockGroupId.java)/**
 * Tests the sequential blockGroup ID generation mechanism and blockGroup ID
 * collision handling.
 */
TestSequentialBlockId (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestSequentialBlockId.java)/**
 * Tests the sequential block ID generation mechanism and block ID
 * collision handling.
 */
TestSlowDiskTracker (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestSlowDiskTracker.java)/**
 * Tests for {@link SlowDiskTracker}.
 */
TestSlowPeerTracker (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestSlowPeerTracker.java)/**
 * Tests for {@link SlowPeerTracker}.
 */
TestSortLocatedStripedBlock (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestSortLocatedStripedBlock.java)/**
 * This class tests the sorting of located striped blocks based on
 * decommissioned states.
 */
TestInMemoryLevelDBAliasMapClient (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TestInMemoryLevelDBAliasMapClient.java)/**
 * Tests the {@link InMemoryLevelDBAliasMapClient}.
 */
TestLevelDBFileRegionAliasMap (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TestLevelDBFileRegionAliasMap.java)/**
 * Tests for the {@link LevelDBFileRegionAliasMap}.
 */
TestLevelDbMockAliasMapClient (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TestLevelDbMockAliasMapClient.java)/**
 * Tests the in-memory alias map with a mock level-db implementation.
 */
TestTextBlockAliasMap (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TestTextBlockAliasMap.java)/**
 * Test for the text based block format for provided block maps.
 */
StorageAdapter (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/StorageAdapter.java)/**
 * Test methods that need to access package-private parts of
 * Storage
 */
TestGetUriFromString (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestGetUriFromString.java)/**
 * This is a unit test, which tests {@link Util#stringAsURI(String)}
 * for Windows and Unix style file paths.
 */
TestHostRestrictingAuthorizationFilter (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestHostRestrictingAuthorizationFilter.java)/**
 * Test Host Restriction Filter.
 */
BlockReportTestBase (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java)/**
 * This is the base class for simulating a variety of situations
 * when blocks are being intentionally corrupted, unexpectedly modified,
 * and so on before a block report is happening.
 *
 * By overriding {@link #sendBlockReports}, derived classes can test
 * different variations of how block reports are split across storages
 * and messages.
 */
DummyChecker (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/checker/TestDatasetVolumeChecker.java)/**
   * A checker to wraps the result of {@link FsVolumeSpi#check} in
   * an ImmediateFuture.
   */
TestDatasetVolumeChecker (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/checker/TestDatasetVolumeChecker.java)/**
 * Tests for {@link DatasetVolumeChecker} when the {@link FsVolumeSpi#check}
 * method returns different values of {@link VolumeCheckResult}.
 */
TestDatasetVolumeCheckerFailures (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/checker/TestDatasetVolumeCheckerFailures.java)/**
 * Test a few more conditions not covered by TestDatasetVolumeChecker.
 */
TestDatasetVolumeCheckerTimeout (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/checker/TestDatasetVolumeCheckerTimeout.java)/**
 * Test that timeout is triggered during Disk Volume Checker.
 */
TestStorageLocationChecker (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/checker/TestStorageLocationChecker.java)/**
 * Unit tests for the {@link StorageLocationChecker} class.
 */
NoOpCheckable (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/checker/TestThrottledAsyncChecker.java)/**
   * A Checkable that just returns its input.
   */
ThrowingCheckable (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/checker/TestThrottledAsyncChecker.java)/**
   * A Checkable that throws an exception when checked.
   */
StalledCheckable (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/checker/TestThrottledAsyncChecker.java)/**
   * A checkable that hangs forever when checked.
   */
TestThrottledAsyncChecker (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/checker/TestThrottledAsyncChecker.java)/**
 * Verify functionality of {@link ThrottledAsyncChecker}.
 */
DummyCheckable (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/checker/TestThrottledAsyncCheckerTimeout.java)/**
   * A dummy Checkable that just returns true after acquiring lock.
   */
DataNodeTestUtils (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/DataNodeTestUtils.java)/**
 * Utility class for accessing package-private DataNode information during tests.
 * Must not contain usage of classes that are not explicitly listed as
 * dependencies to {@link MiniDFSCluster}.
 */
TestExternalDataset (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/extdataset/TestExternalDataset.java)/**
 * Tests the ability to create external FsDatasetSpi implementations.
 *
 * The purpose of this suite of tests is to ensure that it is possible to
 * construct subclasses of FsDatasetSpi outside the Hadoop tree
 * (specifically, outside of the org.apache.hadoop.hdfs.server.datanode
 * package).  This consists of creating subclasses of the two key classes
 * (FsDatasetSpi and FsVolumeSpi) *and* instances or subclasses of any
 * classes/interfaces their methods need to produce.  If methods are added
 * to or changed in any superclasses, or if constructors of other classes
 * are changed, this package will fail to compile.  In fixing this
 * compilation error, any new class dependencies should receive the same
 * treatment.
 *
 * It is worth noting what these tests do *not* accomplish.  Just as
 * important as being able to produce instances of the appropriate classes
 * is being able to access all necessary methods on those classes as well
 * as on any additional classes accepted as inputs to FsDatasetSpi's
 * methods.  It wouldn't be correct to mandate all methods be public, as
 * that would defeat encapsulation.  Moreover, there is no natural
 * mechanism that would prevent a manually-constructed list of methods
 * from becoming stale.  Rather than creating tests with no clear means of
 * maintaining them, this problem is left unsolved for now.
 *
 * Lastly, though merely compiling this package should signal success,
 * explicit testInstantiate* unit tests are included below so as to have a
 * tangible means of referring to each case.
 */
FsDatasetImplMaterializedReplica (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImplTestUtils.java)/**
   * A reference to the replica that is used to corrupt block / meta later.
   */
FsDatasetImplTestUtils (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImplTestUtils.java)/**
 * Test-related utilities to access blocks in {@link FsDatasetImpl}.
 */
ClusterWithRamDiskBuilder (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/LazyPersistTestCase.java)/**
   * Builder class that allows controlling RAM disk-specific properties for a
   * MiniDFSCluster.
   */
TestAddBlockPoolException (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestAddBlockPoolException.java)/**
 * Tests to ensure AddBlockPoolException behaves correctly when additional
 * exceptions are merged, as this exception is a wrapper for multiple
 * exceptions and hence carries some additional logic.
 */
TestCacheByPmemMappableBlockLoader (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestCacheByPmemMappableBlockLoader.java)/**
 * Tests HDFS persistent memory cache by PmemMappableBlockLoader.
 *
 * Bogus persistent memory volume is used to cache blocks.
 */
TestDatanodeRestart (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestDatanodeRestart.java)/** Test if a datanode can correctly upgrade itself */
TestInterDatanodeProtocol (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestInterDatanodeProtocol.java)/**
 * This tests InterDataNodeProtocol for block handling. 
 */
TestLazyPersistLockedMemory (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestLazyPersistLockedMemory.java)/**
 * Verify that locked memory is used correctly when writing to replicas in
 * memory
 */
TestFileRegionIterator (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestProvidedImpl.java)/**
   * A simple FileRegion iterator for tests.
   */
TestFileRegionBlockAliasMap (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestProvidedImpl.java)/**
   * A simple FileRegion BlockAliasMap for tests.
   */
TestProvidedImpl (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestProvidedImpl.java)/**
 * Basic test cases for provided implementation.
 */
TestReplicaCachingGetSpaceUsed (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestReplicaCachingGetSpaceUsed.java)/**
 * Unit test for ReplicaCachingGetSpaceUsed class.
 */
TestReplicaMap (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestReplicaMap.java)/**
 * Unit test for ReplicasMap class
 */
TestReservedSpaceCalculator (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestReservedSpaceCalculator.java)/**
 * Unit testing for different types of ReservedSpace calculators.
 */
TestScrLazyPersistFiles (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestScrLazyPersistFiles.java)/**
 * Test Lazy persist behavior with short-circuit reads. These tests
 * will be run on Linux only with Native IO enabled. The tests fake
 * RAM_DISK storage using local disk.
 */
TestSpaceReservation (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestSpaceReservation.java)/**
 * Ensure that the DN reserves disk space equivalent to a full block for
 * replica being written (RBW) & Replica being copied from another DN.
 */
TestWriteToReplica (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestWriteToReplica.java)/** Test if FSDataset#append, writeToRbw, and writeToTmp */
FsDatasetImplTestUtilsFactory (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/FsDatasetImplTestUtilsFactory.java)/**
 * A factory for creating {@link FsDatasetImplTestUtils} objects.
 */
MaterializedReplica (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/FsDatasetTestUtils.java)/**
   * A replica to be corrupted.
   *
   * It is safe to corrupt this replica even if the MiniDFSCluster is shutdown.
   */
FsDatasetTestUtils (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/FsDatasetTestUtils.java)/**
 * Provide block access for FsDataset white box tests.
 */
InternalDataNodeTestUtils (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/InternalDataNodeTestUtils.java)/**
 * An internal-facing only collection of test utilities for the DataNode. This
 * is to ensure that test-scope dependencies aren't inadvertently leaked
 * to clients, e.g. Mockito.
 */
TestDataNodeOutlierDetectionViaMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/metrics/TestDataNodeOutlierDetectionViaMetrics.java)/**
 * Test that the {@link DataNodePeerMetrics} class is able to detect
 * outliers i.e. slow nodes via the metrics it maintains.
 */
TestSlowNodeDetector (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/metrics/TestSlowNodeDetector.java)/**
 * Unit tests for {@link OutlierDetector}.
 */
SimpleBlocksMovementsStatusHandler (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimpleBlocksMovementsStatusHandler.java)/**
 * Blocks movements status handler, which is used to collect details of the
 * completed block movements and later these attempted finished(with success or
 * failure) blocks can be accessed to notify respective listeners, if any.
 */
TestUtilsFactory (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java)/**
   * Used to change the default number of data storages and to mark the
   * FSDataset as simulated.
   */
SimulatedBPStorage (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java)/**
   * Class is used for tracking block pool storage utilization similar
   * to {@link BlockPoolSlice}
   */
SimulatedStorage (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java)/**
   * Class used for tracking datanode level storage utilization similar
   * to {@link FSVolumeSet}
   */
SimulatedInputStream (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java)/**
   * Simulated input and output streams.
   */
SimulatedOutputStream (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java)/**
   * This class implements an output stream that merely throws its data away, but records its
   * length.
   */
SimulatedFSDataset (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java)/**
 * This class implements a simulated FSDataset.
 *
 * Blocks that are created are recorded but their data (plus their CRCs) are
 *  discarded.
 * Fixed data is returned when blocks are read; a null CRC meta file is
 * created for such data.
 *
 * This FSDataset does not remember any block information across its
 * restarts; it does however offer an operation to inject blocks
 *  (See the TestInectionForSImulatedStorage()
 * for a usage example of injection.
 *
 * Note the synchronization is coarse grained - it is at each method.
 */
TestBatchIbr (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBatchIbr.java)/**
 * This test verifies that incremental block reports are sent in batch mode
 * and the namenode allows closing a file with COMMITTED blocks.
 */
TestBlockCountersInPendingIBR (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockCountersInPendingIBR.java)/**
 * Test counters for number of blocks in pending IBR.
 */
TestBlockHasMultipleReplicasOnSameDN (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockHasMultipleReplicasOnSameDN.java)/**
 * This test verifies NameNode behavior when it gets unexpected block reports
 * from DataNodes. The same block is reported by two different storages on
 * the same DataNode. Excess replicas on the same DN should be ignored by the NN.
 */
StubBlockPoolSliceStorage (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockPoolSliceStorage.java)/**
   * BlockPoolSliceStorage with a dummy storage directory. The directory
   * need not exist. We need to extend BlockPoolSliceStorage so we can
   * call {@link Storage#addStorageDir}.
   */
TestBlockPoolSliceStorage (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockPoolSliceStorage.java)/**
 * Test that BlockPoolSliceStorage can correctly generate trash and
 * restore directories for a given block file path.
*/
TestBlockRecovery (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java)/**
 * This tests if sync all replicas in block recovery works correctly.
 */
TestBlockReplacement (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReplacement.java)/**
 * This class tests if block replacement request to data nodes work correctly.
 */
HeartbeatAnswer (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java)/**
   * Mock answer for heartbeats which returns an empty set of commands
   * and the HA status for the chosen NN from the
   * {@link TestBPOfferService#mockHaStatuses} array.
   */
TestBpServiceActorScheduler (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBpServiceActorScheduler.java)/**
 * Verify the block report and heartbeat scheduling logic of BPServiceActor
 * using a few different values .
 */
TestCorruptMetadataFile (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestCorruptMetadataFile.java)/**
 * Tests to ensure that a block is not read successfully from a datanode
 * when it has a corrupt metadata file.
 */
TestDataNodeErasureCodingMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeErasureCodingMetrics.java)/**
 * This file tests the erasure coding metrics in DataNode.
 */
TestDataNodeExit (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeExit.java)/** 
 * Tests if DataNode process exits if all Block Pool services exit. 
 */
TestDataNodeFaultInjector (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeFaultInjector.java)/**
 * This class tests various cases where faults are injected to DataNode.
 */
TestDataNodeInitStorage (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeInitStorage.java)/**
 * Test to verify that the DataNode Uuid is correctly initialized before
 * FsDataSet initialization.
 */
LatchAwaitingAnswer (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeLifeline.java)/**
   * Waits on a {@link CountDownLatch} before calling through to the method.
   */
LatchCountingAnswer (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeLifeline.java)/**
   * Counts on a {@link CountDownLatch} after each call through to the method.
   */
TestDataNodeLifeline (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeLifeline.java)/**
 * Test suite covering lifeline protocol handling in the DataNode.
 */
TestFakeMetric (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetricsLogger.java)/**
   * MBean for testing
   */
PatternMatchingAppender (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetricsLogger.java)/**
   * An appender that matches logged messages against the given regular
   * expression.
   */
TestDataNodeMetricsLogger (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetricsLogger.java)/**
 * Test periodic logging of DataNode metrics.
 */
TestDataNodeMXBean (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMXBean.java)/**
 * Class for testing {@link DataNodeMXBean} implementation
 */
TestDataNodePeerMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodePeerMetrics.java)/**
 * This class tests various cases of DataNode peer metrics.
 */
TestDatanodeProtocolRetryPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDatanodeProtocolRetryPolicy.java)/**
 * This tests DatanodeProtocol retry policy
 */
TestDataNodeReconfiguration (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeReconfiguration.java)/**
 * Test to reconfigure some parameters for DataNode without restart
 */
TestDataNodeRollingUpgrade (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeRollingUpgrade.java)/**
 * Ensure that the DataNode correctly handles rolling upgrade
 * finalize and rollback.
 */
TestDatanodeStartupOptions (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDatanodeStartupOptions.java)/**
 * This test verifies DataNode command line processing.
 */
TestDataNodeTcpNoDelay (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeTcpNoDelay.java)/**
 * Checks that used sockets have TCP_NODELAY set when configured.
 */
TestDataNodeVolumeFailure (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailure.java)/**
 * Fine-grain testing of block files and locations after volume failure.
 */
TestDataNodeVolumeFailureReporting (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java)/**
 * Test reporting of DN volume failure counts and metrics.
 */
TestDataNodeVolumeFailureToleration (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java)/**
 * Test the ability of a DN to tolerate volume failures.
 */
TestDataNodeVolumeMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeMetrics.java)/**
 * Test class for DataNodeVolumeMetrics.
 */
NullServer (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataXceiverBackwardsCompat.java)/**
     * Class for accepting incoming an incoming connection. Does not read
     * data or repeat in any way: simply allows a single client to connect to
     * a local URL.
     */
NullDataNode (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataXceiverBackwardsCompat.java)/**
   * Used for mocking DataNode. Mockito does not provide a way to mock
   * properties (like data or saslClient) so we have to manually set up mocks
   * of those properties inside our own class.
   */
TestDataXceiverBackwardsCompat (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataXceiverBackwardsCompat.java)/**
 * Mock-based unit test to verify that DataXceiver does not fail when no
 * storageId or targetStorageTypes are passed - as is the case in Hadoop 2.x.
 */
TestDataXceiverLazyPersistHint (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataXceiverLazyPersistHint.java)/**
 * Mock-based unit test to verify that the DataXceiver correctly handles the
 * LazyPersist hint from clients.
 */
TestDeleteBlockPool (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDeleteBlockPool.java)/**
 * Tests deleteBlockPool functionality.
 */
TestDirectoryScanner (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java)/**
 * Tests {@link DirectoryScanner} handling of differences between blocks on the
 * disk and block in memory.
 */
TestDiskError (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java)/**
 * Test that datanodes can correctly handle errors during block read/write.
 */
TestDnRespectsBlockReportSplitThreshold (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDnRespectsBlockReportSplitThreshold.java)/**
 * Tests that the DataNode respects
 * {@link DFSConfigKeys#DFS_BLOCKREPORT_SPLIT_THRESHOLD_KEY}
 */
TestDNUsageReport (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDNUsageReport.java)/**
 * Test class for {@link DataNodeUsageReport}.
 */
TestFsDatasetCacheRevocation (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCacheRevocation.java)/**
 * Tests FsDatasetCache behaviors.
 */
TestHdfsServerConstants (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestHdfsServerConstants.java)/**
 * Test enumerations in TestHdfsServerConstants.
 */
TestIncrementalBlockReports (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBlockReports.java)/**
 * Verify that incremental block reports are generated in response to
 * block additions/deletions.
 */
TestIncrementalBrVariations (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBrVariations.java)/**
 * This test verifies that incremental block reports from a single DataNode are
 * correctly handled by NN. Tests the following variations:
 *  #1 - Incremental BRs from all storages combined in a single call.
 *  #2 - Incremental BRs from separate storages sent in separate calls.
 *  #3 - Incremental BR from an unknown storage should be rejected.
 *
 *  We also verify that the DataNode is not splitting the reports (it may do so
 *  in the future).
 */
TestLargeBlockReport (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestLargeBlockReport.java)/**
 * Tests that very large block reports can pass through the RPC server and
 * deserialization layers successfully if configured.
 */
TestNNHandlesBlockReportPerStorage (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestNNHandlesBlockReportPerStorage.java)/**
 * Runs all tests in BlockReportTestBase, sending one block per storage.
 * This is the default DataNode behavior post HDFS-2832.
 */
TestNNHandlesCombinedBlockReport (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestNNHandlesCombinedBlockReport.java)/**
 * Runs all tests in BlockReportTestBase, sending one block report
 * per DataNode. This tests that the NN can handle the legacy DN
 * behavior where it presents itself as a single logical storage.
 */
TestProvidedReplicaImpl (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestProvidedReplicaImpl.java)/**
 * Tests the implementation of {@link ProvidedReplica}.
 */
TestReadOnlySharedStorage (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestReadOnlySharedStorage.java)/**
 * Test proper {@link BlockManager} replication counting for {@link DatanodeStorage}s
 * with {@link DatanodeStorage.State#READ_ONLY_SHARED READ_ONLY} state.
 * 
 * Uses {@link SimulatedFSDataset} to inject read-only replicas into a DataNode.
 */
TestRefreshNamenodes (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestRefreshNamenodes.java)/**
 * Tests datanode refresh namenode list functionality.
 */
TestSimulatedFSDataset (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestSimulatedFSDataset.java)/**
 * this class tests the methods of the  SimulatedFSDataset.
 */
TestSimulatedFSDatasetWithMultipleStorages (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestSimulatedFSDatasetWithMultipleStorages.java)/**
 * Test that the {@link SimulatedFSDataset} works correctly when configured
 * with multiple storages.
 */
TestStartSecureDataNode (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestStartSecureDataNode.java)/**
 * This test starts a 1 NameNode 1 DataNode MiniDFSCluster with
 * kerberos authentication enabled using user-specified KDC,
 * principals, and keytabs.
 *
 * A secure DataNode has to be started by root, so this test needs to
 * be run by root.
 *
 * To run, users must specify the following system properties:
 *   externalKdc=true
 *   java.security.krb5.conf
 *   dfs.namenode.kerberos.principal
 *   dfs.namenode.kerberos.internal.spnego.principal
 *   dfs.namenode.keytab.file
 *   dfs.datanode.kerberos.principal
 *   dfs.datanode.keytab.file
 */
TestTransferRbw (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestTransferRbw.java)/** Test transferring RBW between datanodes */
TestTriggerBlockReport (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestTriggerBlockReport.java)/**
 * Test manually requesting that the DataNode send a block report.
 */
TestDatanodeHttpXFrame (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/TestDatanodeHttpXFrame.java)/**
 * Test that X-Frame-Options works correctly with DatanodeHTTPServer.
 */
TestDiskBalancerCommand (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java)/**
 * Tests various CLI commands of DiskBalancer.
 */
NullConnector (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/NullConnector.java)/**
 * This connector allows user to create an in-memory cluster
 * and is useful in testing.
 */
DiskBalancerResultVerifier (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/DiskBalancerResultVerifier.java)/**
 * Helps in verifying test results.
 */
DiskBalancerTestUtil (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/DiskBalancerTestUtil.java)/**
 * Helper class to create various cluster configurations at run time.
 */
TestConnectors (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/TestConnectors.java)/**
 * Test Class that tests connectors.
 */
TestDataModels (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/TestDataModels.java)/**
 * Tests DiskBalancer Data models.
 */
ClusterBuilder (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/TestDiskBalancer.java)/**
   * Helper class that allows us to create different kinds of MiniDFSClusters
   * and populate data.
   */
TestDiskBalancer (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/TestDiskBalancer.java)/**
 * Test Disk Balancer.
 */
TestDiskBalancerRPC (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/TestDiskBalancerRPC.java)/**
 * Test DiskBalancer RPC.
 */
TestMover (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/TestDiskBalancerWithMockMover.java)/**
   * Allows us to control mover class for test purposes.
   */
TestDiskBalancerWithMockMover (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/TestDiskBalancerWithMockMover.java)/**
 * Tests diskbalancer with a mock mover.
 */
TestPlanner (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/TestPlanner.java)/**
 * Test Planner.
 */
NamespaceScheme (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java)/**
   * This scheme defines files/directories and their block storage policies. It
   * also defines snapshots.
   */
ClusterScheme (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java)/**
   * This scheme defines DataNodes and their storage, including storage types
   * and remaining capacities.
   */
TestStorageMover (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java)/**
 * Test the data migration tool (for Archival Storage)
 */
AclTestHelpers (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/AclTestHelpers.java)/**
 * Helper methods useful for writing ACL tests.
 */
FileNameGenerator (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FileNameGenerator.java)/**
 * File name generator.
 * 
 * Each directory contains not more than a fixed number (filesPerDir) 
 * of files and directories.
 * When the number of files in one directory reaches the maximum,
 * the generator creates a new directory and proceeds generating files in it.
 * The generated namespace tree is balanced that is any path to a leaf
 * file is not less than the height of the tree minus one.
 */
FSAclBaseTest (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java)/**
 * Tests NameNode interaction for all ACL modification APIs.  This test suite
 * also covers interaction of setPermission with inodes that have ACLs.
 */
FSImageTestUtil (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java)/**
 * Utility functions for testing fsimage storage.
 */
FSXAttrBaseTest (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSXAttrBaseTest.java)/**
 * Tests NameNode interaction for all XAttr APIs.
 * This test suite covers restarting the NN, saving a new checkpoint. 
 */
HAStressTestHarness (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HAStressTestHarness.java)/**
 * Utility class to start an HA cluster, and then start threads
 * to periodically fail back and forth, accelerate block deletion
 * processing, etc.
 */
HATestUtil (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java)/**
 * Static utility functions useful for testing HA.
 */
TestBootstrapAliasmap (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapAliasmap.java)/**
 * Test for aliasmap bootstrap.
 */
TestBootstrapStandbyWithQJM (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandbyWithQJM.java)/**
 * Test BootstrapStandby when QJM is used for shared edits. 
 */
TestRpcScheduler (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestConsistentReadsObserver.java)/**
   * A dummy test scheduler that starts backoff after a fixed number
   * of requests.
   */
TestConsistentReadsObserver (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestConsistentReadsObserver.java)/**
 * Test consistency of reads while accessing an ObserverNode.
 * The tests are based on traditional (non fast path) edits tailing.
 */
TestDelegationTokensWithHA (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDelegationTokensWithHA.java)/**
 * Test case for client support of delegation tokens in an HA cluster.
 * See HDFS-2904 for more info.
 **/
TestDFSUpgradeWithHA (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDFSUpgradeWithHA.java)/**
 * Tests for upgrading with HA enabled.
 */
RandomDeleterPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDNFencing.java)/**
   * A BlockPlacementPolicy which, rather than using space available, makes
   * random decisions about which excess replica to delete. This is because,
   * in the test cases, the two NNs will usually (but not quite always)
   * make the same decision of which replica to delete. The fencing issues
   * are exacerbated when the two NNs make different decisions, which can
   * happen in "real life" when they have slightly out-of-sync heartbeat
   * information regarding disk usage.
   */
TestDNFencingWithReplication (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDNFencingWithReplication.java)/**
 * Stress-test for potential bugs when replication is changing
 * on blocks during a failover.
 */
TestEditLogsDuringFailover (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestEditLogsDuringFailover.java)/**
 * Test cases for the handling of edit logs during failover
 * and startup of the standby node.
 */
TestHAConfiguration (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAConfiguration.java)/**
 * Test cases that the HA configuration is reasonably validated and
 * interpreted in various places. These should be proper unit tests
 * which don't start daemons.
 */
TestHAMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAMetrics.java)/**
 * Make sure HA-related metrics are updated and reported appropriately.
 */
TestHASafeMode (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java)/**
 * Tests that exercise safemode in an HA cluster.
 */
TestHAStateTransitions (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAStateTransitions.java)/**
 * Tests state transition from active->standby, and manual failover
 * and failback between two namenodes.
 */
TestLossyRetryInvocationHandler (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestLossyRetryInvocationHandler.java)/**
 * This test makes sure that when
 * {@link HdfsClientConfigKeys#DFS_CLIENT_TEST_DROP_NAMENODE_RESPONSE_NUM_KEY} is set,
 * DFSClient instances can still be created within NN/DN (e.g., the fs instance
 * used by the trash emptier thread in NN)
 */
TestMultiObserverNode (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestMultiObserverNode.java)/**
 * Tests multiple ObserverNodes.
 */
TestObserverNode (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestObserverNode.java)/**
 * Test main functionality of ObserverNode.
 */
NameNodeAnswer (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestObserverReadProxyProvider.java)/**
   * An {@link Answer} used for mocking of {@link ClientProtocol}.
   * Setting the state or unreachability of this
   * Answer will make the linked ClientProtocol respond as if it was
   * communicating with a NameNode of the corresponding state. It is in Standby
   * state by default.
   */
TestObserverReadProxyProvider (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestObserverReadProxyProvider.java)/**
 * Tests for {@link ObserverReadProxyProvider} under various configurations of
 * NameNode states. Mainly testing that the proxy provider picks the correct
 * NameNode to communicate with.
 */
PipelineTestThread (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestPipelinesFailover.java)/**
   * Test thread which creates a file, has another fake user recover
   * the lease on the file, and then ensures that the file's contents
   * are properly readable. If any of these steps fails, propagates
   * an exception back to the test context, causing the test case
   * to fail.
   */
TestPipelinesFailover (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestPipelinesFailover.java)/**
 * Test cases regarding pipeline recovery during NN failover.
 */
TestRemoteNameNodeInfo (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRemoteNameNodeInfo.java)/**
 * Test that we correctly obtain remote namenode information
 */
DummyRetryInvocationHandler (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** 
   * A dummy invocation handler extending RetryInvocationHandler. We can use
   * a boolean flag to control whether the method invocation succeeds or not. 
   */
CreateSnapshotOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** createSnapshot operaiton */
DeleteSnapshotOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** deleteSnapshot */
RenameSnapshotOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** renameSnapshot */
CreateOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** create file operation (without OverWrite) */
AppendOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** append operation */
RenameOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** rename */
Rename2Op (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** rename2 */
ConcatOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** concat */
DeleteOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** delete */
CreateSymlinkOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** createSymlink */
UpdatePipelineOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** updatePipeline */
AddCacheDirectiveInfoOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** addCacheDirective */
ModifyCacheDirectiveInfoOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** modifyCacheDirective */
RemoveCacheDirectiveInfoOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** removeCacheDirective */
AddCachePoolOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** addCachePool */
ModifyCachePoolOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** modifyCachePool */
RemoveCachePoolOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** removeCachePool */
SetXAttrOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** setXAttr */
RemoveXAttrOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java)/** removeXAttr */
TestSeveralNameNodes (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestSeveralNameNodes.java)/**
 * Test that we can start several and run with namenodes on the same minicluster
 */
TestStandbyBlockManagement (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestStandbyBlockManagement.java)/**
 * Makes sure that standby doesn't do the unnecessary block management such as
 * invalidate block, etc.
 */
SlowCodec (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestStandbyCheckpoints.java)/**
   * A codec which just slows down the saving of the image significantly
   * by sleeping a few milliseconds on every write. This makes it easy to
   * catch the standby in the middle of saving a checkpoint.
   */
TestStandbyInProgressTail (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestStandbyInProgressTail.java)/**
 * Test cases for in progress tailing edit logs by
 * the standby node.
 */
TestStandbyIsHot (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestStandbyIsHot.java)/**
 * The hotornot.com of unit tests: makes sure that the standby not only
 * has namespace information, but also has the correct block reports, etc.
 */
TestStateTransitionFailure (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestStateTransitionFailure.java)/**
 * Tests to verify the behavior of failing to fully start transition HA states.
 */
TestXAttrsWithHA (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestXAttrsWithHA.java)/**
 * Tests interaction of XAttrs with HA failover.
 */
TestNameNodeMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java)/**
 * Test for metrics published by the Namenode
 */
TestNNMetricFilesInGetListingOps (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNNMetricFilesInGetListingOps.java)/**
 * Test case for FilesInGetListingOps metric in Namenode
 */
TestTopMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestTopMetrics.java)/**
 * Test for MetricsSource part of the {@link TopMetrics} impl.
 */
MockNameNodeResourceChecker (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/MockNameNodeResourceChecker.java)/**
 * Mock NameNodeResourceChecker with resource availability flag which will be
 * used to simulate the Namenode resource status.
 */
NameNodeAdapter (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NameNodeAdapter.java)/**
 * This is a utility class to expose NameNode functionality for unit tests.
 */
OperationStatsBase (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java)/**
   * Base class for collecting operation statistics.
   * 
   * Overload this class in order to run statistics for a 
   * specific name-node operation.
   */
StatsDaemon (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java)/**
   * One of the threads that perform stats operations.
   */
CleanAllStats (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java)/**
   * Clean all benchmark result directories.
   */
CreateFileStats (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java)/**
   * File creation statistics.
   * 
   * Each thread creates the same (+ or -1) number of files.
   * File names are pre-generated during initialization.
   * The created files do not have blocks.
   */
MkdirsStats (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java)/**
   * Directory creation statistics.
   *
   * Each thread creates the same (+ or -1) number of directories.
   * Directory names are pre-generated during initialization.
   */
OpenFileStats (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java)/**
   * Open file statistics.
   * 
   * Measure how many open calls (getBlockLocations()) 
   * the name-node can handle per second.
   */
DeleteFileStats (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java)/**
   * Delete file statistics.
   * 
   * Measure how many delete calls the name-node can handle per second.
   */
FileStatusStats (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java)/**
   * List file status statistics.
   * 
   * Measure how many get-file-status calls the name-node can handle per second.
   */
RenameFileStats (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java)/**
   * Rename file statistics.
   * 
   * Measure how many rename calls the name-node can handle per second.
   */
TinyDatanode (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java)/**
   * Minimal data-node simulator.
   */
BlockReportStats (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java)/**
   * Block report statistics.
   * 
   * Each thread here represents its own data-node.
   * Data-nodes send the same block report each time.
   * The block report may contain missing or non-existing blocks.
   */
ReplicationStats (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java)/**
   * Measures how fast redundancy monitor can compute data-node work.
   *
   * It runs only one thread until no more work can be scheduled.
   */
NNThroughputBenchmark (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java)/**
 * Main class for a series of name-node benchmarks.
 * 
 * Each benchmark measures throughput and average execution time 
 * of a specific name-node operation, e.g. file creation or block reports.
 * 
 * The benchmark does not involve any other hadoop components
 * except for the name-node. Each operation is executed
 * by calling directly the respective name-node method.
 * The name-node here is real all other components are simulated.
 *
 * For usage, please see <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Benchmarking.html#NNThroughputBenchmark">the documentation</a>.
 * Meanwhile, if you change the usage of this program, please also update the
 * documentation accordingly.
 */
OfflineEditsViewerHelper (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java)/**
 * OfflineEditsViewerHelper is a helper class for TestOfflineEditsViewer,
 * it performs NN operations that generate all op codes
 */
Node (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotTestHelper.java)/**
     * The class representing a node in {@link TestDirectoryTree}.
     * <br>
     * This contains:
     * <ul>
     * <li>Two children representing the two snapshottable directories</li>
     * <li>A list of files for testing, so that we can check snapshots
     * after file creation/deletion/modification.</li>
     * <li>A list of non-snapshottable directories, to test snapshots with
     * directory creation/deletion. Note that this is needed because the
     * deletion of a snapshottale directory with snapshots is not allowed.</li>
     * </ul>
     */
TestDirectoryTree (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotTestHelper.java)/**
   * A class creating directories trees for snapshot testing. For simplicity,
   * the directory tree is a binary tree, i.e., each directory has two children
   * as snapshottable directories.
   */
SnapshotTestHelper (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotTestHelper.java)/**
 * Helper for writing snapshot related tests
 */
TestAclWithSnapshot (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestAclWithSnapshot.java)/**
 * Tests interaction of ACLs with snapshots.
 */
TestDiffListBySkipList (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestDiffListBySkipList.java)/**
 * This class tests the DirectoryDiffList API's.
 */
TestDisallowModifyROSnapshot (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestDisallowModifyROSnapshot.java)/**
 * This class tests snapshot functionality. One or multiple snapshots are
 * created. The snapshotted directory is changed and verification is done to
 * ensure snapshots remain unchanges.
 */
TestGetContentSummaryWithSnapshot (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestGetContentSummaryWithSnapshot.java)/**
 * Verify content summary is computed correctly when
 * 1. There are snapshots taken under the directory
 * 2. The given path is a snapshot path
 */
TestINodeFileUnderConstructionWithSnapshot (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestINodeFileUnderConstructionWithSnapshot.java)/**
 * Test snapshot functionalities while file appending.
 */
TestNestedSnapshots (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestNestedSnapshots.java)/** Testing nested snapshots. */
TestRandomOpsWithSnapshots (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRandomOpsWithSnapshots.java)/**
 * Testing random FileSystem operations with random Snapshot operations.
 */
TestRenameWithSnapshots (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java)/** Testing rename with snapshots. */
TestSnapRootDescendantDiff (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapRootDescendantDiff.java)/**
 * Test snapshot diff report for the snapshot root descendant directory.
 */
Modification (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java)/**
   * Base class to present changes applied to current file/dir. A modification
   * can be file creation, deletion, or other modifications such as appending on
   * an existing file. Three abstract methods need to be implemented by
   * subclasses: loadSnapshots() captures the states of snapshots before the
   * modification, modify() applies the modification to the current directory,
   * and checkSnapshots() verifies the snapshots do not change after the
   * modification.
   */
FileStatusChange (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java)/**
   * Modifications that change the file status. We check the FileStatus of
   * snapshot files before/after the modification.
   */
FileChangePermission (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java)/**
   * Change the file permission
   */
FileChangeReplication (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java)/**
   * Change the replication factor of file
   */
FileChown (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java)/**
   * Change the owner:group of a file
   */
FileAppend (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java)/**
   * Appending a specified length to an existing file
   */
FileAppendNotClose (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java)/**
   * Appending a specified length to an existing file but not close the file
   */
FileAppendClose (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java)/**
   * Appending a specified length to an existing file
   */
FileCreation (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java)/**
   * New file creation
   */
FileDeletion (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java)/**
   * File deletion
   */
DirCreationOrDeletion (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java)/**
   * Directory creation or deletion.
   */
DirRename (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java)/**
   * Directory creation or deletion.
   */
TestSnapshot (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java)/**
 * This class tests snapshot functionality. One or multiple snapshots are
 * created. The snapshotted directory is changed and verification is done to
 * ensure snapshots remain unchanges.
 */
TestSnapshotBlocksMap (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotBlocksMap.java)/**
 * Test cases for snapshot-related information in blocksMap.
 */
TestSnapshotDeletion (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDeletion.java)/**
 * Tests snapshot deletion.
 */
TestSnapshotDiffReport (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java)/**
 * Tests snapshot deletion.
 */
TestSnapshotManager (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotManager.java)/**
 * Testing snapshot manager functionality.
 */
TestSnapshotMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotMetrics.java)/**
 * Test the snapshot-related metrics
 */
TestSnapshotRename (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotRename.java)/**
 * Test for renaming snapshot
 */
TestSnapshotReplication (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotReplication.java)/**
 * This class tests the replication handling/calculation of snapshots to make
 * sure the number of replication is calculated correctly with/without
 * snapshots.
 */
TestXAttrWithSnapshot (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestXAttrWithSnapshot.java)/**
 * Tests interaction of XAttrs with snapshots.
 */
TestBlockStorageMovementAttemptedItems (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/sps/TestBlockStorageMovementAttemptedItems.java)/**
 * Tests that block storage movement attempt failures are reported from DN and
 * processed them correctly or not.
 */
TestStoragePolicySatisfierWithStripedFile (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/sps/TestStoragePolicySatisfierWithStripedFile.java)/**
 * Tests that StoragePolicySatisfier daemon is able to check the striped blocks
 * to be moved and finding its expected target locations in order to satisfy the
 * storage policy.
 */
StartupProgressTestHelper (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgressTestHelper.java)/**
 * Utility methods that help with writing tests covering startup progress.
 */
TestAclConfigFlag (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAclConfigFlag.java)/**
 * Tests that the configuration flag that controls support for ACLs is off by
 * default and causes all attempted operations related to ACLs to fail.  The
 * NameNode can still load ACLs from fsimage or edits.
 */
TestAclTransformation (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAclTransformation.java)/**
 * Tests operations that modify ACLs.  All tests in this suite have been
 * cross-validated against Linux setfacl/getfacl to check for consistency of the
 * HDFS implementation.
 */
TestAddBlock (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlock.java)/**
 * Test AddBlockOp is written and read correctly
 */
TestAddBlockRetry (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlockRetry.java)/**
 * Race between two threads simultaneously calling
 * FSNamesystem.getAdditionalBlock().
 */
TestAllowFormat (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAllowFormat.java)/**
 * Startup and format tests
 * 
 */
TestAuditLogAtDebug (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogAtDebug.java)/**
 * Test that the HDFS Audit logger respects DFS_NAMENODE_AUDIT_LOG_DEBUG_CMDLIST. 
 */
TestAuditLogger (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogger.java)/**
 * Tests for the {@link AuditLogger} custom audit logging interface.
 */
TestAuditLogs (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogs.java)/**
 * A JUnit test that audit logs are generated
 */
DoCheckpointThread (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java)/**
   * A utility class to perform a checkpoint in a different thread.
   */
TestCheckpoint (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java)/**
 * This class tests the creation and validation of a checkpoint.
 */
TestCheckPointForSecurityTokens (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCheckPointForSecurityTokens.java)/**
 * This class tests the creation and validation of a checkpoint.
 */
TestClientNameNodeAddress (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestClientNameNodeAddress.java)/**
 * Test that {@link NameNodeUtils#getClientNamenodeAddress}  correctly
 * computes the client address for WebHDFS redirects for different
 * combinations of HA, federated and single NN setups.
 */
TestCommitBlockSynchronization (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java)/**
 * Verify that TestCommitBlockSynchronization is idempotent.
 */
TestCreateEditsLog (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCreateEditsLog.java)/**
 * Tests the CreateEditsLog utility.
 */
TestDeadDatanode (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeadDatanode.java)/**
 * Test to ensure requests from dead datnodes are rejected by namenode with
 * appropriate exceptions/failure response
 */
TestDecommissioningStatus (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDecommissioningStatus.java)/**
 * This class tests the decommissioning of nodes.
 */
TestDeleteRace (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeleteRace.java)/**
 * Test race between delete and other operations.  For now only addBlock()
 * is tested since all others are acquiring FSNamesystem lock for the 
 * whole duration.
 */
GarbageMkdirOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLog.java)/**
   * A garbage mkdir op which is used for testing
   * {@link EditLogFileInputStream#scanEditLog(File, long, boolean)}
   */
AbortSpec (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLog.java)/** 
   * Specification for a failure during #setupEdits
   */
TestEditLog (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLog.java)/**
 * This class tests the creation and validation of a checkpoint.
 */
TestEditLogFileOutputStream (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogFileOutputStream.java)/**
 * Test the EditLogFileOutputStream
 */
TestEditLogRace (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogRace.java)/**
 * This class tests various synchronization bugs in FSEditLog rolling
 * and namespace saving.
 */
TestEnabledECPolicies (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEnabledECPolicies.java)/**
 * Test that ErasureCodingPolicyManager correctly parses the set of enabled
 * erasure coding policies from configuration and exposes this information.
 */
TestEncryptionZoneManager (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEncryptionZoneManager.java)/**
 * Test class for EncryptionZoneManager methods. Added tests for
 * listEncryptionZones method, for cases where inode can and cannot have a
 * parent inode.
 */
TestFileContextAcl (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileContextAcl.java)/**
 * Tests for ACL operation through FileContext APIs
 */
FileContextFS (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileContextXAttr.java)/**
   * This reuses FSXAttrBaseTest's testcases by creating a filesystem
   * implementation which uses FileContext by only overriding the xattr related
   * methods. Other operations will use the normal filesystem.
   */
TestFileContextXAttr (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileContextXAttr.java)/**
 * Tests of XAttr operations using FileContext APIs.
 */
TestFileLimit (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileLimit.java)/**
 * This class tests that a file system adheres to the limit of
 * maximum number of files that is configured.
 */
TestFsck (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsck.java)/**
 * A JUnit test for doing fsck.
 */
Suite (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsckWithMultipleNameNodes.java)/** Common objects used in various methods. */
TestFsckWithMultipleNameNodes (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsckWithMultipleNameNodes.java)/**
 * Test fsck with multiple NameNodes
 */
TestFSDirAttrOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSDirAttrOp.java)/**
 * Test {@link FSDirAttrOp}.
 */
TestFSDirectory (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSDirectory.java)/**
 * Test {@link FSDirectory}, the in-memory namespace tree.
 */
TestFSImageWithSnapshot (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithSnapshot.java)/**
 * Test FSImage save/load when Snapshot is supported
 */
TestFSImageWithXAttr (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithXAttr.java)/**
 * 1) save xattrs, restart NN, assert xattrs reloaded from edit log, 
 * 2) save xattrs, create new checkpoint, restart NN, assert xattrs 
 * reloaded from fsimage
 */
TestFSNamesystemLock (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSNamesystemLock.java)/**
 * Tests the FSNamesystemLock, looking at lock compatibilities and
 * proper logging of lock hold times.
 */
MBeanClient (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSNamesystemMBean.java)/**
   * MBeanClient tries to access FSNamesystem/FSNamesystemState/NameNodeInfo
   * JMX properties. If it can access all the properties, the test is
   * considered successful.
   */
TestFSNamesystemMBean (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSNamesystemMBean.java)/**
 * Class for testing {@link NameNodeMXBean} implementation
 */
TestFSPermissionChecker (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSPermissionChecker.java)/**
 * Unit tests covering FSPermissionChecker.  All tests in this suite have been
 * cross-validated against Linux setfacl/getfacl to check for consistency of the
 * HDFS implementation.
 */
TestGetContentSummaryWithPermission (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestGetContentSummaryWithPermission.java)/**
 * This class tests get content summary with permission settings.
 */
TestHostsFiles (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHostsFiles.java)/**
 * DFS_HOSTS and DFS_HOSTS_EXCLUDE tests
 * 
 */
TestThread (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestLargeDirectoryDelete.java)/**
   * An abstract class for tests that catches exceptions and can 
   * rethrow them on a different thread, and has an {@link #endThread()} 
   * operation that flips a volatile boolean before interrupting the thread.
   * Also: after running the implementation of {@link #execute()} in the 
   * implementation class, the thread is notified: other threads can wait
   * for it to terminate
   */
TestLargeDirectoryDelete (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestLargeDirectoryDelete.java)/**
 * Ensure during large directory delete, namenode does not block until the 
 * deletion completes and handles new requests from other clients
 */
TestListCorruptFileBlocks (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestListCorruptFileBlocks.java)/**
 * This class tests the listCorruptFileBlocks API.
 * We create 3 files; intentionally delete their blocks
 * Use listCorruptFileBlocks to validate that we get the list of corrupt
 * files/blocks; also test the "paging" support by calling the API
 * with a block # from a previous call and validate that the subsequent
 * blocks/files are also returned.
 */
TestListOpenFiles (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestListOpenFiles.java)/**
 * Verify open files listing.
 */
TestMetaSave (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java)/**
 * This class tests the creation and validation of metasave
 */
TestNameCache (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameCache.java)/**
 * Test for {@link NameCache} class
 */
TestNameEditsConfigs (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameEditsConfigs.java)/**
 * This class tests various combinations of dfs.namenode.name.dir 
 * and dfs.namenode.edits.dir configurations.
 */
TestNameNodeAcl (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeAcl.java)/**
 * Tests NameNode interaction for all ACL modification APIs.  This test suite
 * also covers interaction of setPermission with inodes that have ACLs.
 */
TestNamenodeCapacityReport (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeCapacityReport.java)/**
 * This tests InterDataNodeProtocol for block handling. 
 */
TestNameNodeHttpServerXFrame (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeHttpServerXFrame.java)/**
 * A class to test the XFrameoptions of Namenode HTTP Server. We are not reusing
 * the TestNameNodeHTTPServer since it is a parameterized class and these
 * following tests will run multiple times doing the same thing, if we had the
 * code in that classs.
 */
TestNameNode (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeMetricsLogger.java)/**
   * A NameNode that stubs out the NameSystem for testing.
   */
TestFakeMetric (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeMetricsLogger.java)/**
   * MBean for testing
   */
PatternMatchingAppender (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeMetricsLogger.java)/**
   * An appender that matches logged messages against the given
   * regular expression.
   */
TestNameNodeMetricsLogger (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeMetricsLogger.java)/**
 * Test periodic logging of NameNode metrics.
 */
TestNameNodeMXBean (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeMXBean.java)/**
 * Class for testing {@link NameNodeMXBean} implementation
 */
EditLogTestSetup (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRecovery.java)/**
   * A test scenario for the edit log
   */
EltsTestEmptyLog (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRecovery.java)/**
   * Test the scenario where we have an empty edit log.
   *
   * This class is also useful in testing whether we can correctly handle
   * various amounts of padding bytes at the end of the log.  We should be
   * able to handle any amount of padding (including no padding) without
   * throwing an exception.
   */
EltsTestNonDefaultMaxOpSize (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRecovery.java)/**
   * Test using a non-default maximum opcode length.
   */
EltsTestOpcodesAfterPadding (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRecovery.java)/**
   * Test the scenario where an edit log contains some padding (0xff) bytes
   * followed by valid opcode data.
   *
   * These edit logs are corrupt, but all the opcodes should be recoverable
   * with recovery mode.
   */
Corruptor (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRecovery.java)/**
   * An algorithm for corrupting an edit log.
   */
TestNameNodeRespectsBindHostKeys (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRespectsBindHostKeys.java)/**
 * This test checks that the NameNode respects the following keys:
 *
 *  - DFS_NAMENODE_RPC_BIND_HOST_KEY
 *  - DFS_NAMENODE_SERVICE_RPC_BIND_HOST_KEY
 *  - DFS_NAMENODE_LIFELINE_RPC_BIND_HOST_KEY
 *  - DFS_NAMENODE_HTTP_BIND_HOST_KEY
 *  - DFS_NAMENODE_HTTPS_BIND_HOST_KEY
 */
TestNamenodeRetryCache (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeRetryCache.java)/**
 * Tests for ensuring the namenode retry cache works correctly for
 * non-idempotent requests.
 * 
 * Retry cache works based on tracking previously received request based on the
 * ClientId and CallId received in RPC requests and storing the response. The
 * response is replayed on retry when the same request is received again.
 * 
 * The test works by manipulating the Rpc {@link Server} current RPC call. For
 * testing retried requests, an Rpc callId is generated only once using
 * {@link #newCall()} and reused for many method calls. For testing non-retried
 * request, a new callId is generated using {@link #newCall()}.
 */
TestNameNodeRetryCacheMetrics (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRetryCacheMetrics.java)/**
 * Tests for ensuring the namenode retry cache metrics works correctly for
 * non-idempotent requests.
 *
 * Retry cache works based on tracking previously received request based on the
 * ClientId and CallId received in RPC requests and storing the response. The
 * response is replayed on retry when the same request is received again.
 *
 */
TestNameNodeStatusMXBean (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeStatusMXBean.java)/**
 * Class for testing {@link NameNodeStatusMXBean} implementation.
 */
TestVolumeChoosingPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeStorageDirectives.java)/**
   * A VolumeChoosingPolicy test stub used to verify that the storageId passed
   * in is indeed in the list of volumes.
   * @param <V>
   */
TestNamenodeStorageDirectives (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeStorageDirectives.java)/**
 * Test to ensure that the StorageType and StorageID sent from Namenode
 * to DFSClient are respected.
 */
TestNameNodeXAttr (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeXAttr.java)/**
 * Tests NameNode interaction for all XAttr APIs.
 * This test suite covers restarting NN, saving new checkpoint, 
 * and also includes test of xattrs for symlinks. 
 */
TestNestedEncryptionZones (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNestedEncryptionZones.java)/**
 * Test the behavior of nested encryption zones.
 */
TestNNStorageRetentionFunctional (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNNStorageRetentionFunctional.java)/**
 * Functional tests for NNStorageRetentionManager. This differs from
 * {@link TestNNStorageRetentionManager} in that the other test suite
 * is only unit/mock-based tests whereas this suite starts miniclusters,
 * etc.
 */
TestParallelImageWrite (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestParallelImageWrite.java)/**
 * A JUnit test for checking if restarting DFS preserves integrity.
 * Specifically with FSImage being written in parallel
 */
TestPathComponents (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestPathComponents.java)/**
 * 
 */
TestPersistentStoragePolicySatisfier (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestPersistentStoragePolicySatisfier.java)/**
 * Test persistence of satisfying files/directories.
 */
TestProtectedDirectories (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestProtectedDirectories.java)/**
 * Verify that the dfs.namenode.protected.directories setting is respected.
 */
TestQuotaCounts (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestQuotaCounts.java)/**
 * Test QuotaCounts.
 */
TestQuotaWithStripedBlocks (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestQuotaWithStripedBlocks.java)/**
 * Make sure we correctly update the quota usage with the striped blocks.
 */
TestQuotaWithStripedBlocksWithRandomECPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestQuotaWithStripedBlocksWithRandomECPolicy.java)/**
 * This test extends TestQuotaWithStripedBlocks to use a random
 * (non-default) EC policy.
 */
TestRedudantBlocks (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestRedudantBlocks.java)/**
 * Test RedudantBlocks.
 */
TestReencryption (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestReencryption.java)/**
 * Test class for re-encryption.
 */
TestReencryptionHandler (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestReencryptionHandler.java)/**
 * Test class for ReencryptionHandler.
 */
TestReencryptionWithKMS (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestReencryptionWithKMS.java)/**
 * Test class for re-encryption with minikms.
 */
TestRefreshNamenodeReplicationConfig (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestRefreshNamenodeReplicationConfig.java)/**
 * This class tests the replication related parameters in the namenode can
 * be refreshed dynamically, without a namenode restart.
 */
TestSaveNamespace (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSaveNamespace.java)/**
 * Test various failure scenarios during saveNamespace() operation.
 * Cases covered:
 * <ol>
 * <li>Recover from failure while saving into the second storage directory</li>
 * <li>Recover from failure while moving current into lastcheckpoint.tmp</li>
 * <li>Recover from failure while moving lastcheckpoint.tmp into
 * previous.checkpoint</li>
 * <li>Recover from failure while rolling edits file</li>
 * </ol>
 */
TestSecondaryNameNodeUpgrade (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSecondaryNameNodeUpgrade.java)/**
 * Regression test for HDFS-3597, SecondaryNameNode upgrade -- when a 2NN
 * starts up with an existing directory structure with an old VERSION file, it
 * should delete the snapshot and download a new one from the NN.
 */
TestSecureNameNodeWithExternalKdc (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSecureNameNodeWithExternalKdc.java)/**
 * This test brings up a MiniDFSCluster with 1 NameNode and 0
 * DataNodes with kerberos authentication enabled using user-specified
 * KDC, principals, and keytabs.
 *
 * To run, users must specify the following system properties:
 *   externalKdc=true
 *   java.security.krb5.conf
 *   dfs.namenode.kerberos.principal
 *   dfs.namenode.kerberos.internal.spnego.principal
 *   dfs.namenode.keytab.file
 *   user.principal (do not specify superuser!)
 *   user.keytab
 */
TestSecurityTokenEditLog (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSecurityTokenEditLog.java)/**
 * This class tests the creation and validation of a checkpoint.
 */
TestSnapshotPathINodes (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSnapshotPathINodes.java)/** Test snapshot related operations. */
TestStartup (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartup.java)/**
 * Startup and checkpoint tests
 * 
 */
TestStartupOptionUpgrade (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartupOptionUpgrade.java)/**
 * This class tests various upgrade cases from earlier versions to current
 * version with and without clusterid.
 */
TestStoragePolicySatisfierWithHA (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStoragePolicySatisfierWithHA.java)/**
 * Tests that StoragePolicySatisfier is able to work with HA enabled.
 */
TestStorageRestore (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStorageRestore.java)/**
 * Startup and checkpoint tests
 * 
 */
TestStripedINodeFile (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStripedINodeFile.java)/**
 * This class tests INodeFile with striped feature.
 */
TestTruncateQuotaUpdate (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestTruncateQuotaUpdate.java)/**
 * Make sure we correctly update the quota usage for truncate.
 * We need to cover the following cases:
 * 1. No snapshot, truncate to 0
 * 2. No snapshot, truncate at block boundary
 * 3. No snapshot, not on block boundary
 * 4~6. With snapshot, all the current blocks are included in latest
 *      snapshots, repeat 1~3
 * 7~9. With snapshot, blocks in the latest snapshot and blocks in the current
 *      file diverged, repeat 1~3
 */
TestUpgradeDomainBlockPlacementPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestUpgradeDomainBlockPlacementPolicy.java)/**
 * End-to-end test case for upgrade domain
 * The test configs upgrade domain for nodes via admin json
 * config file and put some nodes to decommission state.
 * The test then verifies replicas are placed on the nodes that
 * satisfy the upgrade domain policy.
 *
 */
TestValidateConfigurationSettings (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestValidateConfigurationSettings.java)/**
 * This class tests the validation of the configuration object when passed 
 * to the NameNode
 */
TestXAttrConfigFlag (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestXAttrConfigFlag.java)/**
 * Tests that the configuration flag that controls support for XAttrs is off
 * and causes all attempted operations related to XAttrs to fail.  The
 * NameNode can still load XAttrs from fsimage or edits.
 */
TestWebHdfsCreatePermissions (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/web/resources/TestWebHdfsCreatePermissions.java)/**
 * Test WebHDFS files/directories creation to make sure it follows same rules
 * from dfs CLI for specifying files/directories permissions.
 */
TestWebHdfsDataLocality (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/web/resources/TestWebHdfsDataLocality.java)/**
 * Test WebHDFS which provides data locality using HTTP redirection.
 */
ExternalBlockMovementListener (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/sps/TestExternalStoragePolicySatisfier.java)/**
   * Implementation of listener callback, where it collects all the sps move
   * attempted blocks for assertion.
   */
TestExternalStoragePolicySatisfier (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/sps/TestExternalStoragePolicySatisfier.java)/**
 * Tests the external sps service plugins.
 */
TestShortCircuitLocalRead (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/shortcircuit/TestShortCircuitLocalRead.java)/**
 * Test for short circuit read functionality using {@link BlockReaderLocal}.
 * When a block is being read by a client is on the local datanode, instead of
 * using {@link DataTransferProtocol} and connect to datanode,
 * the short circuit read allows reading the file directly
 * from the files on the local file system.
 */
TestAbandonBlock (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAbandonBlock.java)/**
 * Test abandoning blocks, which clients do on pipeline creation failure.
 */
UserOp (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAclsEndToEnd.java)/**
   * Simple interface that defines an operation to perform.
   */
TestAclsEndToEnd (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAclsEndToEnd.java)/**
 * This class tests the ACLs system through the full code path.  It overlaps
 * slightly with the ACL tests in common, but the approach is more holistic.
 *
 * <b>NOTE:</b> Because of the mechanics of JAXP, when the KMS config files are
 * written to disk, a config param with a blank value ("") will be written in a
 * way that the KMS will read as unset, which is different from blank. For this
 * reason, when testing the effects of blank config params, this test class
 * sets the values of those config params to a space (" ").  A whitespace value
 * will be preserved by JAXP when writing out the config files and will be
 * interpreted by KMS as a blank value. (The KMS strips whitespace from ACL
 * values before interpreting them.)
 */
TestAppendDifferentChecksum (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAppendDifferentChecksum.java)/**
 * Test cases for trying to append to a file with a different
 * checksum than the file was originally written with.
 */
TestAppendSnapshotTruncate (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAppendSnapshotTruncate.java)/**
 * Test randomly mixing append, snapshot and truncate operations.
 * Use local file system to simulate the each operation and verify
 * the correctness.
 */
TestBalancerBandwidth (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBalancerBandwidth.java)/**
 * This test ensures that the balancer bandwidth is dynamically adjusted
 * correctly.
 */
TestBlocksScheduledCounter (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBlocksScheduledCounter.java)/**
 * This class tests DatanodeDescriptor.getBlocksScheduled() at the
 * NameNode. This counter is supposed to keep track of blocks currently
 * scheduled to a datanode.
 */
TestBlockStoragePolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBlockStoragePolicy.java)/** Test {@link BlockStoragePolicy} */
TestBlockTokenWrappingQOP (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBlockTokenWrappingQOP.java)/**
 * This tests enabling NN sending the established QOP back to client,
 * in encrypted message, using block access token key.
 */
TestByteBufferPread (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestByteBufferPread.java)/**
 * This class tests the DFS positional read functionality on a single node
 * mini-cluster. These tests are inspired from {@link TestPread}. The tests
 * are much less comprehensive than other pread tests because pread already
 * internally uses {@link ByteBuffer}s.
 */
TestClientProtocolForPipelineRecovery (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClientProtocolForPipelineRecovery.java)/**
 * This tests pipeline recovery related client protocol works correct or not.
 */
TestClientReportBadBlock (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClientReportBadBlock.java)/**
 * Class is used to test client reporting corrupted block replica to name node.
 * The reporting policy is if block replica is more than one, if all replicas
 * are corrupted, client does not report (since the client can handicapped). If
 * some of the replicas are corrupted, client reports the corrupted block
 * replicas. In case of only one block replica, client always reports corrupted
 * replica.
 */
TestConnCache (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestConnCache.java)/**
 * This class tests the client connection caching in a single node
 * mini-cluster.
 */
TestCrcCorruption (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestCrcCorruption.java)/**
 * A JUnit test for corrupted file handling.
 * This test creates a bunch of files/directories with replication 
 * factor of 2. Then verifies that a client can automatically 
 * access the remaining valid replica inspite of the following 
 * types of simulated errors:
 *
 *  1. Delete meta file on one replica
 *  2. Truncates meta file on one replica
 *  3. Corrupts the meta file header on one replica
 *  4. Corrupts any random offset and portion of the meta file
 *  5. Swaps two meta files, i.e the format of the meta files 
 *     are valid but their CRCs do not match with their corresponding 
 *     data blocks
 * The above tests are run for varied values of dfs.bytes-per-checksum 
 * and dfs.blocksize. It tests for the case when the meta file is 
 * multiple blocks.
 *
 * Another portion of the test is commented out till HADOOP-1557 
 * is addressed:
 *  1. Create file with 2 replica, corrupt the meta file of replica, 
 *     decrease replication factor from 2 to 1. Validate that the 
 *     remaining replica is the good one.
 *  2. Create file with 2 replica, corrupt the meta file of one replica, 
 *     increase replication factor of file to 3. verify that the new 
 *     replica was created from the non-corrupted replica.
 */
TestDatanodeConfig (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDatanodeConfig.java)/**
 * Tests if a data-node can startup depending on configuration parameters.
 */
Modify (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDatanodeDeath.java)/**
   * A class that kills one datanode and recreates a new one. It waits to
   * ensure that that all workers have finished at least one file since the 
   * last kill of a datanode. This guarantees that all three replicas of
   * a block do not get killed (otherwise the file will be corrupt and the
   * test will fail).
   */
TestDatanodeDeath (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDatanodeDeath.java)/**
 * This class tests that pipelines survive data node death and recovery.
 */
TestDatanodeRegistration (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDatanodeRegistration.java)/**
 * This class tests data node registration.
 */
TestDatanodeReport (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDatanodeReport.java)/**
 * This test ensures the all types of data node report work correctly.
 */
TestDatanodeStartupFixesLegacyStorageIDs (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDatanodeStartupFixesLegacyStorageIDs.java)/**
 * The test verifies that legacy storage IDs in older DataNode
 * images are replaced with UUID-based storage IDs. The startup may
 * or may not involve a Datanode Layout upgrade. Each test case uses
 * the following resource files.
 *
 *    1. testCaseName.tgz - NN and DN directories corresponding
 *                          to a specific layout version.
 *    2. testCaseName.txt - Text file listing the checksum of each file
 *                          in the cluster and overall checksum. See
 *                          TestUpgradeFromImage for the file format.
 *
 * If any test case is renamed then the corresponding resource files must
 * also be renamed.
 */
TestDataTransferProtocol (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferProtocol.java)/**
 * This tests data transfer protocol handling in the Datanode. It sends
 * various forms of wrong data and verifies that Datanode handles it well.
 */
TestDecommission (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java)/**
 * This class tests the decommissioning of nodes.
 */
TestDecommissionWithStriped (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommissionWithStriped.java)/**
 * This class tests the decommissioning of datanode with striped blocks.
 */
TestDFSClientExcludedNodes (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientExcludedNodes.java)/**
 * These tests make sure that DFSClient excludes writing data to
 * a DN properly in case of errors.
 */
DummyLegacyFailoverProxyProvider (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java)/** Dummy implementation of plain FailoverProxyProvider */
FailNTimesAnswer (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java)/**
   * Mock Answer implementation of NN.getBlockLocations that will return
   * a poisoned block list a certain number of times before returning
   * a proper one.
   */
TestDFSClientRetries (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java)/**
 * These tests make sure that DFSClient retries fetching data from DFS
 * properly in case of errors.
 */
TestDFSFinalize (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSFinalize.java)/**
 * This test ensures the appropriate response from the system when 
 * the system is finalized.
 */
TestDFSInotifyEventInputStreamKerberized (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSInotifyEventInputStreamKerberized.java)/**
 * Class for Kerberized test cases for {@link DFSInotifyEventInputStream}.
 */
TestDFSMkdirs (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSMkdirs.java)/**
 * This class tests that the DFS command mkdirs only creates valid
 * directories, and generally behaves as expected.
 */
TestDFSPermission (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSPermission.java)/** Unit tests for permission */
TestDFSRollback (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSRollback.java)/**
* This test ensures the appropriate response (successful or failure) from
* the system when the system is rolled back under various storage state and
* version conditions.
*/
TestDFSShell (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java)/**
 * This class tests commands from DFSShell.
 */
StorageData (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStartupVersions.java)/**
   * Class used for initializing version information for tests
   */
TestDFSStartupVersions (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStartupVersions.java)/**
 * This test ensures the appropriate response (successful or failure) from 
 * a Datanode when the system is started with differing version combinations. 
 */
TestDFSStorageStateRecovery (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStorageStateRecovery.java)/**
* This test ensures the appropriate response (successful or failure) from
* the system when the system is started under various storage state and
* version conditions.
*/
TestDFSStripedInputStreamWithRandomECPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedInputStreamWithRandomECPolicy.java)/**
 * This tests read operation of DFS striped file with a random erasure code
 * policy except for the default policy.
 */
TestDFSStripedOutputStreamWithFailure (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedOutputStreamWithFailure.java)/**
 * Test striped file write operation with data node failures with fixed
 * parameter test cases.
 */
TestDFSStripedOutputStreamWithFailureBase (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedOutputStreamWithFailureBase.java)/**
 * Base class for test striped file write operation.
 */
TestDFSStripedOutputStreamWithFailureWithRandomECPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedOutputStreamWithFailureWithRandomECPolicy.java)/**
 * This tests write operation of DFS striped file with a random erasure code
 * policy except for the default policy under Datanode failure conditions.
 */
TestDFSStripedOutputStreamWithRandomECPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedOutputStreamWithRandomECPolicy.java)/**
 * This tests write operation of DFS striped file with a random erasure code
 * policy except for the default policy.
 */
TestDFSUpgrade (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUpgrade.java)/**
* This test ensures the appropriate response (successful or failure) from
* the system when the system is upgraded under various storage state and
* version conditions.
*/
TestDFSUpgradeFromImage (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUpgradeFromImage.java)/**
 * This tests data transfer protocol handling in the Datanode. It sends
 * various forms of wrong data and verifies that Datanode handles it well.
 * 
 * This test uses the following items from src/test/.../dfs directory :
 *   1) hadoop-22-dfs-dir.tgz and other tarred pre-upgrade NN / DN 
 *      directory images
 *   2) hadoop-dfs-dir.txt : checksums that are compared in this test.
 * Please read hadoop-dfs-dir.txt for more information.  
 */
TestDisableConnCache (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDisableConnCache.java)/**
 * This class tests disabling client connection caching in a single node
 * mini-cluster.
 */
TestDistributedFileSystemWithECFile (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystemWithECFile.java)/**
 * Testing correctness of FileSystem.getFileBlockLocations and
 * FileSystem.listFiles for erasure coded files.
 */
TestDistributedFileSystemWithECFileWithRandomECPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystemWithECFileWithRandomECPolicy.java)/**
 * This test extends TestDistributedFileSystemWithECFile to use a random
 * (non-default) EC policy.
 */
TestEncryptionZonesWithHA (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZonesWithHA.java)/**
 * Tests interaction of encryption zones with HA failover.
 */
TestErasureCodeBenchmarkThroughput (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodeBenchmarkThroughput.java)/**
 * To test {@link org.apache.hadoop.hdfs.ErasureCodeBenchmarkThroughput}.
 */
TestErasureCodingAddConfig (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodingAddConfig.java)/**
 * Test that ensures addition of user defined EC policies is allowed only when
 * dfs.namenode.ec.userdefined.policy.allowed is set to true.
 */
TestErasureCodingExerciseAPIs (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodingExerciseAPIs.java)/**
 * Test after enable Erasure Coding on cluster, exercise Java API make sure they
 * are working as expected.
 *
 */
TestErasureCodingMultipleRacks (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodingMultipleRacks.java)/**
 * Test erasure coding block placement with skewed # nodes per rack.
 */
TestErasureCodingPoliciesWithRandomECPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodingPoliciesWithRandomECPolicy.java)/**
 * This test extends TestErasureCodingPolicies to use a random (non-default) EC
 * policy.
 */
TestErasureCodingPolicyWithSnapshotWithRandomECPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodingPolicyWithSnapshotWithRandomECPolicy.java)/**
 * This test extends TestErasureCodingPolicyWithSnapshot to use a random
 * (non-default) EC policy.
 */
TestExtendedAcls (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestExtendedAcls.java)/**
 * A class for testing the behavior of HDFS directory and file ACL.
 */
TestFileAppend (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppend.java)/**
 * This class tests the building blocks that are needed to
 * support HDFS appends.
 */
TestFileAppend2 (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppend2.java)/**
 * This class tests the building blocks that are needed to
 * support HDFS appends.
 */
TestFileAppend3 (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppend3.java)/** This class implements some of tests posted in HADOOP-2658. */
TestFileAppendRestart (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppendRestart.java)/**
 * Unit test to make sure that Append properly logs the right
 * things to the edit log, such that files aren't lost or truncated
 * on restart.
 */
TestFileChecksum (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileChecksum.java)/**
 * This test serves a prototype to demo the idea proposed so far. It creates two
 * files using the same data, one is in replica mode, the other is in stripped
 * layout. For simple, it assumes 6 data blocks in both files and the block size
 * are the same.
 */
TestFileChecksumCompositeCrc (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileChecksumCompositeCrc.java)/**
 * End-to-end tests for COMPOSITE_CRC combine mode.
 */
TestFileConcurrentReader (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileConcurrentReader.java)/**
 * This class tests the cases of a concurrent reads/writes to a file;
 * ie, one writer and one or more readers can see unfinsihed blocks
 */
TestFileCorruption (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCorruption.java)/**
 * A JUnit test for corrupted file handling.
 */
TestFileCreation (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreation.java)/**
 * This class tests various cases during file creation.
 */
TestFileCreationClient (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreationClient.java)/**
 * This class tests client lease recovery.
 */
TestFileCreationEmpty (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreationEmpty.java)/**
 * Test empty file creation.
 */
TestFileLengthOnClusterRestart (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileLengthOnClusterRestart.java)/** Test the fileLength on cluster restarts */
TestFileStatus (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileStatus.java)/**
 * This class tests the FileStatus API.
 */
TestFileStatusSerialization (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileStatusSerialization.java)/**
 * Verify compatible FileStatus/HdfsFileStatus serialization.
 */
TestFileStatusWithDefaultECPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileStatusWithDefaultECPolicy.java)/**
 * This test ensures the statuses of EC files with the default policy.
 */
TestFileStatusWithRandomECPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileStatusWithRandomECPolicy.java)/**
 * This test extends TestFileStatusWithDefaultECPolicy to use a random
 * (non-default) EC policy.
 */
TestFSInputChecker (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFSInputChecker.java)/**
 * This class tests if FSInputChecker works correctly.
 */
TestFSOutputSummer (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFSOutputSummer.java)/**
 * This class tests if FSOutputSummer works correctly.
 */
TestFsShellPermission (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFsShellPermission.java)/**
 * This test covers privilege related aspects of FsShell
 *
 */
TestGetBlocks (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestGetBlocks.java)/**
 * This class tests if getblocks request works correctly.
 */
TestHAAuxiliaryPort (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHAAuxiliaryPort.java)/**
 * Test NN auxiliary port with HA.
 */
TestHDFSPolicyProvider (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHDFSPolicyProvider.java)/**
 * Test suite covering HDFSPolicyProvider.  We expect that it contains a
 * security policy definition for every RPC protocol used in HDFS.  The test
 * suite works by scanning an RPC server's class to find the protocol interfaces
 * it implements, and then comparing that to the protocol interfaces covered in
 * HDFSPolicyProvider.  This is a parameterized test repeated for multiple HDFS
 * RPC server classes.
 */
TestHDFSServerPorts (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHDFSServerPorts.java)/**
 * This test checks correctness of port usage by hdfs components:
 * NameNode, DataNode, SecondaryNamenode and BackupNode.
 * 
 * The correct behavior is:<br> 
 * - when a specific port is provided the server must either start on that port 
 * or fail by throwing {@link java.net.BindException}.<br>
 * - if the port = 0 (ephemeral) then the server should choose 
 * a free port and start on it.
 */
TestHDFSTrash (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHDFSTrash.java)/**
 * Test trash using HDFS
 */
TestHFlush (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHFlush.java)/** Class contains a set of tests to verify the correctness of 
 * newly introduced {@link FSDataOutputStream#hflush()} method */
TestInjectionForSimulatedStorage (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestInjectionForSimulatedStorage.java)/**
 * This class tests the replication and injection of blocks of a DFS file for simulated storage.
 */
TestIsMethodSupported (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestIsMethodSupported.java)/**
 * Test cases to verify that client side translators correctly implement the
 * isMethodSupported method in ProtocolMetaInterface.
 */
TestLargeBlock (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLargeBlock.java)/**
 * This class tests that blocks can be larger than 2GB
 */
TestListFilesInDFS (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestListFilesInDFS.java)/**
 * This class tests the FileStatus API.
 */
TestListFilesInFileContext (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestListFilesInFileContext.java)/**
 * This class tests the FileStatus API.
 */
TestLocalDFS (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLocalDFS.java)/**
 * This class tests the DFS class via the FileSystem interface in a single node
 * mini-cluster.
 */
TestMaintenanceState (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMaintenanceState.java)/**
 * This class tests node maintenance.
 */
TestMiniDFSCluster (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java)/**
 * Tests MiniDFS cluster setup/teardown and isolation.
 * Every instance is brought up with a new data dir, to ensure that
 * shutdown work in background threads don't interfere with bringing up
 * the new cluster.
 */
TestMissingBlocksAlert (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMissingBlocksAlert.java)/**
 * The test makes sure that NameNode detects presense blocks that do not have
 * any valid replicas. In addition, it verifies that HDFS front page displays
 * a warning in such a case.
 */
TestModTime (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestModTime.java)/**
 * This class tests the decommissioning of nodes.
 */
TestMultipleNNPortQOP (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMultipleNNPortQOP.java)/**
 * This test tests access NameNode on different port with different
 * configured QOP.
 */
TestMultiThreadedHflush (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMultiThreadedHflush.java)/**
 * This class tests hflushing concurrently from many threads.
 */
ReadWorkerHelper (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelReadUtil.java)/**
   * Providers of this interface implement two different read APIs. Instances of
   * this interface are shared across all ReadWorkerThreads, so should be stateless.
   */
DirectReadWorkerHelper (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelReadUtil.java)/**
   * Uses read(ByteBuffer...) style APIs
   */
CopyingReadWorkerHelper (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelReadUtil.java)/**
   * Uses the read(byte[]...) style APIs
   */
MixedWorkloadHelper (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelReadUtil.java)/**
   * Uses a mix of both copying
   */
ReadWorker (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelReadUtil.java)/**
   * A worker to do one "unit" of read.
   */
TestParallelReadUtil (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelReadUtil.java)/**
 * Driver class for testing the use of DFSInputStream by multiple concurrent
 * readers, using the different read APIs.
 *
 * This class is marked as @Ignore so that junit doesn't try to execute the
 * tests in here directly.  They are executed from subclasses.
 */
TestParallelShortCircuitReadUnCached (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelShortCircuitReadUnCached.java)/**
 * This class tests short-circuit local reads without any FileInputStream or
 * Socket caching.  This is a regression test for HDFS-4417.
 */
TestPersistBlocks (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPersistBlocks.java)/**
 * A JUnit test for checking if restarting DFS preserves the
 * blocks that are part of an unclosed file.
 */
TestPread (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPread.java)/**
 * This class tests the DFS positional read functionality in a single node
 * mini-cluster.
 */
TestQuota (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestQuota.java)/** A class for testing quota-related commands */
TestReadStripedFileWithDecodingCorruptData (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReadStripedFileWithDecodingCorruptData.java)/**
 * Test online recovery with corrupt files. This test is parameterized.
 */
TestReadStripedFileWithDecodingDeletedData (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReadStripedFileWithDecodingDeletedData.java)/**
 * Test online recovery with files with deleted blocks. This test is
 * parameterized.
 */
TestReadStripedFileWithDNFailure (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReadStripedFileWithDNFailure.java)/**
 * Test online recovery with failed DNs. This test is parameterized.
 */
TestReadStripedFileWithMissingBlocks (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReadStripedFileWithMissingBlocks.java)/**
 * Test reading a striped file when some of its blocks are missing (not included
 * in the block locations returned by the NameNode).
 */
TestReadWhileWriting (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReadWhileWriting.java)/** Test reading from hdfs while a file is being written. */
TestReconstructStripedFileWithRandomECPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReconstructStripedFileWithRandomECPolicy.java)/**
 * This test extends TestReconstructStripedFile to use a random
 * (non-default) EC policy.
 */
TestReplaceDatanodeFailureReplication (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReplaceDatanodeFailureReplication.java)/**
 * Verify the behaviours of HdfsClientConfigKeys.BlockWrite.
 * ReplaceDatanodeOnFailure.MIN_REPLICATION.if live block location datanodes is
 * greater than or equal to
 * 'dfs.client.block.write.replace-datanode-on-failure.min.replication'
 * threshold value, if yes continue writing to the two remaining nodes.
 * Otherwise it will throw exception.
 * <p>
 * If this HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.
 * MIN_REPLICATION is set to 0 or less than zero, an exception will be thrown
 * if a replacement could not be found.
 */
TestReplaceDatanodeOnFailure (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReplaceDatanodeOnFailure.java)/**
 * This class tests that data nodes are correctly replaced on failure.
 */
CorruptFileSimulatedInputStream (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReplication.java)/**
     * Simulated input and output streams.
     *
     */
TestReplication (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReplication.java)/**
 * This class tests the replication of a DFS file.
 */
TestRestartDFS (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRestartDFS.java)/**
 * A JUnit test for checking if restarting DFS preserves integrity.
 */
TestRollingUpgrade (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java)/**
 * This class tests rolling upgrade.
 */
TestRollingUpgradeRollback (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgradeRollback.java)/**
 * This class tests rollback for rolling upgrade.
 */
TestSafeMode (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSafeMode.java)/**
 * Tests to verify safe mode correctness.
 */
TestSafeModeWithStripedFileWithRandomECPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSafeModeWithStripedFileWithRandomECPolicy.java)/**
 * This test extends TestSafeModeWithStripedFile to use a random
 * (non-default) EC policy.
 */
TestSecureEncryptionZoneWithKMS (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSecureEncryptionZoneWithKMS.java)/**
 * Test for HDFS encryption zone without external Kerberos KDC by leveraging
 * Kerby-based MiniKDC, MiniKMS and MiniDFSCluster. This provides additional
 * unit test coverage on Secure(Kerberos) KMS + HDFS.
 */
TestSeekBug (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSeekBug.java)/**
 * This class tests the presence of seek bug as described
 * in HADOOP-508 
 */
TestSetTimes (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSetTimes.java)/**
 * This class tests the access time on files.
 *
 */
TestSmallBlock (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSmallBlock.java)/**
 * This class tests the creation of files with block-size
 * smaller than the default buffer size of 4K.
 */
TestSnapshotCommands (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSnapshotCommands.java)/**
 * This class includes end-to-end tests for snapshot related FsShell and
 * DFSAdmin commands.
 */
TestStateAlignmentContextWithHA (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestStateAlignmentContextWithHA.java)/**
 * Class is used to test server sending state alignment information to clients
 * via RPC and likewise clients receiving and updating their last known
 * state alignment info.
 * These tests check that after a single RPC call a client will have caught up
 * to the most recent alignment state of the server.
 */
TestStripedFileAppend (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestStripedFileAppend.java)/**
 * Tests append on erasure coded file.
 */
TestTrashWithEncryptionZones (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestTrashWithEncryptionZones.java)/**
 * This class tests Trash functionality in Encryption Zones.
 */
TestTrashWithSecureEncryptionZones (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestTrashWithSecureEncryptionZones.java)/**
 * This class tests Trash functionality in Encryption Zones with Kerberos
 * enabled.
 */
TestUnsetAndChangeDirectoryEcPolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestUnsetAndChangeDirectoryEcPolicy.java)/**
 * Test unset and change directory's erasure coding policy.
 */
TestWriteBlockGetsBlockLengthHint (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestWriteBlockGetsBlockLengthHint.java)/**
 * Test to verify that the DFSClient passes the expected block length to
 * the DataNode via DataTransferProtocol.
 */
TestWriteConfigurationToDFS (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestWriteConfigurationToDFS.java)/**
 * Regression test for HDFS-1542, a deadlock between the main thread
 * and the DFSOutputStream.DataStreamer thread caused because
 * Configuration.writeXML holds a lock on itself while writing to DFS.
 */
ECXMLHandler (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/TestOfflineImageViewer.java)/**
   *  SAX handler to verify EC Files and their policies.
   */
TestOfflineImageViewerForAcl (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/TestOfflineImageViewerForAcl.java)/**
 * Tests OfflineImageViewer if the input fsimage has HDFS ACLs
 */
TestOfflineImageViewerForContentSummary (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/TestOfflineImageViewerForContentSummary.java)/**
 * Tests GETCONTENTSUMMARY operation for WebImageViewer
 */
TestOfflineImageViewerForStoragePolicy (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/TestOfflineImageViewerForStoragePolicy.java)/**
 * Tests OfflineImageViewer if the input fsimage has HDFS StoragePolicy entries.
 */
TestOfflineImageViewerForXAttr (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/TestOfflineImageViewerForXAttr.java)/**
 * Tests OfflineImageViewer if the input fsimage has XAttributes
 */
TestPBImageCorruption (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/TestPBImageCorruption.java)/**
 * Unit tests for PBImageCorruptionType, CorruptionEntryBuilder and
 * PBImageCorruption classes.
 */
TestAdminHelper (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestAdminHelper.java)/**
 * Test class to test Admin Helper.
 */
TestDFSAdmin (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdmin.java)/**
 * set/clrSpaceQuote are tested in {@link org.apache.hadoop.hdfs.TestQuota}.
 */
TestDFSHAAdminMiniCluster (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdminMiniCluster.java)/**
 * Tests for HAAdmin command with {@link MiniDFSCluster} set up in HA mode.
 */
ZKFCThread (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSZKFailoverController.java)/**
   * Test-thread which runs a ZK Failover Controller corresponding
   * to a given NameNode in the minicluster.
   */
TestECAdmin (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestECAdmin.java)/**
 * Tests some ECAdmin scenarios that are hard to test from
 * {@link org.apache.hadoop.cli.TestErasureCodingCLI}.
 */
TestGetConf (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestGetConf.java)/**
 * Test for {@link GetConf}
 */
TestGetGroups (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestGetGroups.java)/**
 * Tests for the HDFS implementation of {@link GetGroups}
 */
TestStoragePolicyCommands (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestStoragePolicyCommands.java)/**
 * Test StoragePolicyAdmin commands
 */
TestStoragePolicySatisfyAdminCommands (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestStoragePolicySatisfyAdminCommands.java)/**
 * Test StoragePolicySatisfy admin commands.
 */
TestViewFSStoragePolicyCommands (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestViewFSStoragePolicyCommands.java)/**
 * Test StoragePolicyAdmin commands with ViewFileSystem.
 */
TestWebHDFSStoragePolicyCommands (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestWebHDFSStoragePolicyCommands.java)/**
 * Test StoragePolicyAdmin commands with WebHDFS.
 */
UpgradeUtilities (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/UpgradeUtilities.java)/**
 * This class defines a number of static helper methods used by the
 * DFS Upgrade unit tests.  By default, a singleton master populated storage
 * directory is created for a Namenode (contains edits, fsimage,
 * version, and time files) and a Datanode (contains version and
 * block files).  The master directories are lazily created.  They are then
 * copied by the createStorageDirs() method to create new storage
 * directories of the appropriate type (Namenode or Datanode).
 */
FoldedTreeSetTest (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/FoldedTreeSetTest.java)/**
 * Test of TreeSet
 */
TestCombinedHostsFileReader (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestCombinedHostsFileReader.java)/**
 * Test for JSON based HostsFileReader.
 */
TestDiff (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestDiff.java)/**
 * Test {@link Diff} with {@link INode}.
 */
TestObject (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestLightWeightHashSet.java)/**
   * Wrapper class which is used in
   * {@link TestLightWeightHashSet#testGetElement()}
   */
TestStripedBlockUtil (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestStripedBlockUtil.java)/**
 * Need to cover the following combinations:
 * 1. Block group size:
 *  1.1 One byte
 *  1.2 Smaller than cell
 *  1.3 One full cell
 *  1.4 x full cells, where x is smaller than number of data blocks
 *  1.5 x full cells plus a partial cell
 *  1.6 One full stripe
 *  1.7 One full stripe plus a partial cell
 *  1.8 One full stripe plus x full cells
 *  1.9 One full stripe plus x full cells plus a partial cell
 *  1.10 y full stripes, but smaller than full block group size
 *  1.11 Full block group size
 *
 * 2. Byte range start
 *  2.1 Zero
 *  2.2 Within first cell
 *  2.3 End of first cell
 *  2.4 Start of a middle* cell in the first stripe (* neither first or last)
 *  2.5 End of middle cell in the first stripe
 *  2.6 Within a middle cell in the first stripe
 *  2.7 Start of the last cell in the first stripe
 *  2.8 Within the last cell in the first stripe
 *  2.9 End of the last cell in the first stripe
 *  2.10 Start of a middle stripe
 *  2.11 Within a middle stripe
 *  2.12 End of a middle stripe
 *  2.13 Start of the last stripe
 *  2.14 Within the last stripe
 *  2.15 End of the last stripe (last byte)
 *
 * 3. Byte range length: same settings as block group size
 *
 * We should test in total 11 x 15 x 11 = 1815 combinations
 * TODO: test parity block logic
 */
Ticker (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java)/** A timer for measuring performance. */
TestWebHDFS (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java)/** Test WebHDFS */
TestWebHDFSAcl (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFSAcl.java)/**
 * Tests ACL APIs via WebHDFS.
 */
TestWebHdfsTimeouts (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsTimeouts.java)/**
 * This test suite checks that WebHdfsFileSystem sets connection timeouts and
 * read timeouts on its sockets, thus preventing threads from hanging
 * indefinitely on an undefined/infinite timeout.  The tests work by starting a
 * bogus server on the namenode HTTP port, which is rigged to not accept new
 * connections or to accept connections but not send responses.
 */
Initializer (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsWithAuthenticationFilter.java)/** Initializer for Custom Filter. */
TestWebHdfsWithMultipleNameNodes (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsWithMultipleNameNodes.java)/**
 * Test WebHDFS with multiple NameNodes
 */
TestWebHdfsWithRestCsrfPreventionFilter (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsWithRestCsrfPreventionFilter.java)/**
 * Tests use of the cross-site-request forgery (CSRF) prevention filter with
 * WebHDFS.  This is a parameterized test that covers various combinations of
 * CSRF protection enabled or disabled at the NameNode, the DataNode and the
 * WebHDFS client.  If the server is configured with CSRF prevention, but the
 * client is not, then protected operations are expected to fail.
 */
TestWebHDFSXAttr (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFSXAttr.java)/**
 * Tests XAttr APIs via WebHDFS.
 */
TestRollingFileSystemSinkWithHdfs (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithHdfs.java)/**
 * Test the {@link RollingFileSystemSink} class in the context of HDFS.
 */
TestRollingFileSystemSinkWithSecureHdfs (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithSecureHdfs.java)/**
 * Test the {@link RollingFileSystemSink} class in the context of HDFS with
 * Kerberos enabled.
 */
TestPermission (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestPermission.java)/** Unit tests for permission */
HdfsTestDriver (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/test/HdfsTestDriver.java)/**
 * Driver for HDFS tests. The tests should NOT depend on map-reduce APIs.
 */
MiniDFSClusterManager (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/test/MiniDFSClusterManager.java)/**
 * This class drives the creation of a mini-cluster on the local machine. By
 * default, a MiniDFSCluster is spawned on the first available ports that are
 * found.
 * 
 * A series of command line flags controls the startup cluster options.
 * 
 * This class can dump a Hadoop configuration and some basic metadata (in JSON)
 * into a textfile.
 * 
 * To shutdown the cluster, kill the process.
 * 
 * To run this from the command line, do the following (replacing the jar
 * version as appropriate):
 * 
 * $HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/hdfs/hadoop-hdfs-0.24.0-SNAPSHOT-tests.jar org.apache.hadoop.test.MiniDFSClusterManager -options...
 */
TestGenericRefresh (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestGenericRefresh.java)/**
 * Before all tests, a MiniDFSCluster is spun up.
 * Before each test, mock refresh handlers are created and registered.
 * After each test, the mock handlers are unregistered.
 * After all tests, the cluster is spun down.
 */
TestHdfsConfigFields (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/tools/TestHdfsConfigFields.java)/**
 * Unit test class to compare the following MR Configuration classes:
 * <p></p>
 * {@link org.apache.hadoop.hdfs.DFSConfigKeys}
 * <p></p>
 * against hdfs-default.xml for missing properties.  Currently only
 * throws an error if the class is missing a property.
 * <p></p>
 * Refer to {@link org.apache.hadoop.conf.TestConfigurationFieldsBase}
 * for how this class works.
 */
TestJMXGet (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/tools/TestJMXGet.java)/**
 * Startup and checkpoint tests
 * 
 */
TestTraceAdmin (/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/tracing/TestTraceAdmin.java)/**
 * Test cases for TraceAdmin.
 */
DirListingIterator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/fs/Hdfs.java)/**
   * This class defines an iterator that returns
   * the file status of each file/subdirectory of a directory
   * 
   * if needLocation, status contains block location if it is a file
   * throws a RuntimeException with the error as its cause.
   * 
   * @param <T> the type of the file status
   */
HdfsBlockLocation (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/fs/HdfsBlockLocation.java)/**
 * Wrapper for {@link BlockLocation} that also includes a {@link LocatedBlock},
 * allowing more detailed queries to the datanode about a block.
 *
 */
SWebHdfs (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/fs/SWebHdfs.java)/**
 * AbstractFileSystem implementation for HDFS over the web (secure).
 */
WebHdfs (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/fs/WebHdfs.java)/**
 * AbstractFileSystem implementation for HDFS over the web.
 */
XAttr (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/fs/XAttr.java)/**
 * XAttr is the POSIX Extended Attribute model similar to that found in
 * traditional Operating Systems.  Extended Attributes consist of one
 * or more name/value pairs associated with a file or directory. Five
 * namespaces are defined: user, trusted, security, system and raw.
 *   1) USER namespace attributes may be used by any user to store
 *   arbitrary information. Access permissions in this namespace are
 *   defined by a file directory's permission bits. For sticky directories,
 *   only the owner and privileged user can write attributes.
 * <br>
 *   2) TRUSTED namespace attributes are only visible and accessible to
 *   privileged users. This namespace is available from both user space
 *   (filesystem API) and fs kernel.
 * <br>
 *   3) SYSTEM namespace attributes are used by the fs kernel to store
 *   system objects.  This namespace is only available in the fs
 *   kernel. It is not visible to users.
 * <br>
 *   4) SECURITY namespace attributes are used by the fs kernel for
 *   security features. It is not visible to users.
 * <br>
 *   5) RAW namespace attributes are used for internal system attributes that
 *   sometimes need to be exposed. Like SYSTEM namespace attributes they are
 *   not visible to the user except when getXAttr/getXAttrs is called on a file
 *   or directory in the /.reserved/raw HDFS directory hierarchy.  These
 *   attributes can only be accessed by the superuser.
 * <p>
 * @see <a href="http://en.wikipedia.org/wiki/Extended_file_attributes">
 * http://en.wikipedia.org/wiki/Extended_file_attributes</a>
 *
 */
BlockMissingException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockMissingException.java)/**
  * This exception is thrown when a read encounters a block that has no
  * locations associated with it.
  */
BlockReader (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReader.java)/**
 * A BlockReader is responsible for reading a single block
 * from a single datanode.
 */
CannotObtainBlockLengthException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/CannotObtainBlockLengthException.java)/**
 * This exception is thrown when the length of a LocatedBlock instance
 * can not be obtained.
 */
BlockReportOptions (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/BlockReportOptions.java)/**
 * Options that can be specified when manually triggering a block report.
 */
HdfsAdmin (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsAdmin.java)/**
 * The public API for performing administrative functions on HDFS. Those writing
 * applications against HDFS should prefer this interface to directly accessing
 * functionality in DistributedFileSystem or DFSClient.
 *
 * Note that this is distinct from the similarly-named DFSAdmin, which
 * is a class that provides the functionality for the CLI `hdfs dfsadmin ...'
 * commands.
 */
DeprecatedKeys (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java)/**
   * These are deprecated config keys to client code.
   */
Retry (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java)/** dfs.client.retry configuration properties */
Failover (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java)/** dfs.client.failover configuration properties */
Write (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java)/** dfs.client.write configuration properties */
BlockWrite (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java)/** dfs.client.block.write configuration properties */
Read (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java)/** dfs.client.read configuration properties */
ShortCircuit (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java)/** dfs.client.short.circuit configuration properties */
Mmap (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java)/** dfs.client.mmap configuration properties */
HedgedRead (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java)/** dfs.client.hedged.read configuration properties */
StripedRead (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java)/** dfs.client.read.striped configuration properties */
HttpClient (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java)/** dfs.http.client configuration properties */
HdfsClientConfigKeys (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java)/** Client configuration properties */
HdfsDataInputStream (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsDataInputStream.java)/**
 * The Hdfs implementation of {@link FSDataInputStream}.
 */
HdfsDataOutputStream (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsDataOutputStream.java)/**
 * The Hdfs implementation of {@link FSDataOutputStream}.
 */
HdfsUtils (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsUtils.java)/**
 * The public utility API for HDFS.
 */
BlockReaderFactory (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java)/**
 * Utility class to create BlockReader implementations.
 */
BlockReaderLocal (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java)/**
 * BlockReaderLocal enables local short circuited reads. If the DFS client is on
 * the same machine as the datanode, then the client can read files directly
 * from the local file system rather than going through the datanode for better
 * performance. <br>
 * {@link BlockReaderLocal} works as follows:
 * <ul>
 * <li>The client performing short circuit reads must be configured at the
 * datanode.</li>
 * <li>The client gets the file descriptors for the metadata file and the data
 * file for the block using
 * {@link org.apache.hadoop.hdfs.server.datanode.DataXceiver#requestShortCircuitFds}.
 * </li>
 * <li>The client reads the file descriptors.</li>
 * </ul>
 */
BlockReaderLocalLegacy (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java)/**
 * BlockReaderLocalLegacy enables local short circuited reads. If the DFS client
 * is on the same machine as the datanode, then the client can read files
 * directly from the local file system rather than going through the datanode
 * for better performance. <br>
 *
 * This is the legacy implementation based on HDFS-2246, which requires
 * permissions on the datanode to be set so that clients can directly access the
 * blocks. The new implementation based on HDFS-347 should be preferred on UNIX
 * systems where the required native code has been implemented.<br>
 *
 * {@link BlockReaderLocalLegacy} works as follows:
 * <ul>
 * <li>The client performing short circuit reads must be configured at the
 * datanode.</li>
 * <li>The client gets the path to the file where block is stored using
 * {@link org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol#getBlockLocalPathInfo(ExtendedBlock, Token)}
 * RPC call</li>
 * <li>Client uses kerberos authentication to connect to the datanode over RPC,
 * if security is enabled.</li>
 * </ul>
 */
BlockReaderRemote (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java)/**
 * This is a wrapper around connection to datanode
 * and understands checksum, offset etc.
 *
 * Terminology:
 * <dl>
 * <dt>block</dt>
 *   <dd>The hdfs block, typically large (~64MB).
 *   </dd>
 * <dt>chunk</dt>
 *   <dd>A block is divided into chunks, each comes with a checksum.
 *       We want transfers to be chunk-aligned, to be able to
 *       verify checksums.
 *   </dd>
 * <dt>packet</dt>
 *   <dd>A grouping of chunks used for transport. It contains a
 *       header, followed by checksum data, followed by real data.
 *   </dd>
 * </dl>
 * Please see DataNode for the RPC specification.
 *
 * This is a new implementation introduced in Hadoop 0.23 which
 * is more efficient and simpler than the older BlockReader
 * implementation. It is renamed to BlockReaderRemote from BlockReaderRemote2.
 *
 */
BlockReaderUtil (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderUtil.java)/**
 * For sharing between the local and remote block reader implementations.
 */
CorruptFileBlockIterator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/CorruptFileBlockIterator.java)/**
 * Provides an iterator interface for listCorruptFileBlocks.
 * This class is used by DistributedFileSystem and Hdfs.
 */
ShortCircuitConf (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java)/**
   * Configuration for short-circuit reads.
   */
DfsClientConf (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java)/**
 * DFSClient configuration.
 */
ExternalBlockReader (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/ExternalBlockReader.java)/**
 * An ExternalBlockReader uses pluggable ReplicaAccessor objects to read from
 * replicas.
 */
Factory (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/LeaseRenewer.java)/**
   * A factory for sharing {@link LeaseRenewer} objects
   * among {@link DFSClient} instances
   * so that there is only one renewer per authority per user.
   */
LeaseRenewer (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/LeaseRenewer.java)/**
 * <p>
 * Used by {@link org.apache.hadoop.hdfs.DFSClient} for renewing
 * file-being-written leases on the namenode.
 * When a file is opened for write (create or append),
 * namenode stores a file lease for recording the identity of the writer.
 * The writer (i.e. the DFSClient) is required to renew the lease periodically.
 * When the lease is not renewed before it expires,
 * the namenode considers the writer as failed and then it may either let
 * another writer to obtain the lease or close the file.
 * </p>
 * <p>
 * This class also provides the following functionality:
 * <ul>
 * <li>
 * It maintains a map from (namenode, user) pairs to lease renewers.
 * The same {@link LeaseRenewer} instance is used for renewing lease
 * for all the {@link org.apache.hadoop.hdfs.DFSClient} to the same namenode and
 * the same user.
 * </li>
 * <li>
 * Each renewer maintains a list of {@link org.apache.hadoop.hdfs.DFSClient}.
 * Periodically the leases for all the clients are renewed.
 * A client is removed from the list when the client is closed.
 * </li>
 * <li>
 * A thread per namenode per user is used by the {@link LeaseRenewer}
 * to renew the leases.
 * </li>
 * </ul>
 * <p>
 */
BlockReaderIoProvider (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/metrics/BlockReaderIoProvider.java)/**
 * Profiles {@link org.apache.hadoop.hdfs.client.impl.BlockReaderLocal} short
 * circuit read latencies when ShortCircuit read metrics is enabled through
 * {@link ShortCircuitConf#scrMetricsEnabled}.
 */
BlockReaderLocalMetrics (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/metrics/BlockReaderLocalMetrics.java)/**
 * This class maintains a metric of rolling average latency for short circuit
 * reads.
 */
SnapshotDiffReportGenerator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/SnapshotDiffReportGenerator.java)/**
 * This class represents to end users the difference between two snapshots of
 * the same directory, or the difference between a snapshot of the directory and
 * its current state. Instead of capturing all the details of the diff, this
 * class only lists where the changes happened and their types.
 */
ClientContext (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ClientContext.java)/**
 * ClientContext contains context information for a client.
 *
 * This allows us to share caches such as the socket cache across
 * DFSClient instances.
 */
ClientGSIContext (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ClientGSIContext.java)/**
 * Global State Id context for the client.
 * <p/>
 * This is the client side implementation responsible for receiving
 * state alignment info from server(s).
 */
DFSDataInputStream (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java)/**
   * @deprecated use {@link HdfsDataInputStream} instead.
   */
DFSClient (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java)/********************************************************
 * DFSClient can connect to a Hadoop Filesystem and
 * perform basic file tasks.  It uses the ClientProtocol
 * to communicate with a NameNode daemon, and connects
 * directly to DataNodes to read/write block data.
 *
 * Hadoop DFS users should obtain an instance of
 * DistributedFileSystem, which uses DFSClient to handle
 * filesystem tasks.
 *
 ********************************************************/
DFSClientFaultInjector (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClientFaultInjector.java)/**
 * Used for injecting faults in DFSClient and DFSOutputStream tests.
 * Calls into this are a no-op in production code.
 */
DFSHedgedReadMetrics (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSHedgedReadMetrics.java)/**
 * The client-side metrics for hedged read feature.
 * This class has a number of metrics variables that are publicly accessible,
 * we can grab them from client side, like HBase.
 */
DFSInotifyEventInputStream (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInotifyEventInputStream.java)/**
 * Stream for reading inotify events. DFSInotifyEventInputStreams should not
 * be shared among multiple threads.
 */
DNAddrPair (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java)/** Utility class to encapsulate data node info and its address. */
DFSInputStream (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java)/****************************************************************
 * DFSInputStream provides bytes from a named file.  It handles
 * negotiation of the namenode and various datanodes as necessary.
 ****************************************************************/
DFSMultipartUploaderFactory (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSMultipartUploaderFactory.java)/**
 * Support for HDFS multipart uploads, built on
 * {@link FileSystem#concat(Path, Path[])}.
 */
DFSOpsCountStatistics (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOpsCountStatistics.java)/**
 * This storage statistics tracks how many times each DFS operation was issued.
 *
 * For each tracked DFS operation, there is a respective entry in the enum
 * {@link OpType}. To use, increment the value the {@link DistributedFileSystem}
 * and {@link org.apache.hadoop.hdfs.web.WebHdfsFileSystem}.
 *
 * This class is thread safe, and is generally shared by multiple threads.
 */
DFSOutputStream (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java)/****************************************************************
 * DFSOutputStream creates files from a stream of bytes.
 *
 * The client application writes data that is cached internally by
 * this stream. Data is broken up into packets, each packet is
 * typically 64K in size. A packet comprises of chunks. Each chunk
 * is typically 512 bytes and has an associated checksum with it.
 *
 * When a client application fills up the currentPacket, it is
 * enqueued into the dataQueue of DataStreamer. DataStreamer is a
 * thread that picks up packets from the dataQueue and sends it to
 * the first datanode in the pipeline.
 *
 ****************************************************************/
DFSStripedInputStream (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java)/**
 * DFSStripedInputStream reads from striped block groups.
 */
Coordinator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java)/** Coordinate the communication between the streamers. */
CellBuffers (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java)/** Buffers for writing the data and parity cells of a stripe. */
DFSStripedOutputStream (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java)/**
 * This class supports writing files in striped layout and erasure coded format.
 * Each stripe contains a sequence of cells.
 */
CorruptedBlocks (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java)/**
   * A utility class as a container to put corrupted blocks, shared by client
   * and datanode.
   */
DirListingIterator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java)/**
   * This class defines an iterator that returns
   * the file status of each file/subdirectory of a directory
   *
   * if needLocation, status contains block location if it is a file
   * throws a RuntimeException with the error as its cause.
   *
   * @param <T> the type of the file status
   */
SnapshotDiffReportListingIterator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java)/**
   * This class defines an iterator that returns
   * the SnapshotDiffReportListing for a snapshottable directory
   * between two given snapshots.
   */
HdfsDataOutputStreamBuilder (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java)/**
   * HdfsDataOutputStreamBuilder provides the HDFS-specific capabilities to
   * write file on HDFS.
   */
DistributedFileSystem (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java)/****************************************************************
 * Implementation of the abstract FileSystem for the DFS system.
 * This object is the way end-user code interacts with a Hadoop
 * DistributedFileSystem.
 *
 *****************************************************************/
ExceptionLastSeen (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ExceptionLastSeen.java)/**
 * The exception last seen by the {@link DataStreamer} or
 * {@link DFSOutputStream}.
 */
ExtendedBlockId (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ExtendedBlockId.java)/**
 * An immutable key which identifies a block.
 */
FileChecksumComputer (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java)/**
   * A common abstract class to compute file checksum.
   */
ReplicatedFileChecksumComputer (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java)/**
   * Replicated file checksum computer.
   */
StripedFileNonStripedChecksumComputer (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java)/**
   * Non-striped checksum computing for striped files.
   */
FileChecksumHelper (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java)/**
 * Utility classes to compute file checksum for both replicated and striped
 * files.
 */
HdfsConfiguration (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/HdfsConfiguration.java)/**
 * Adds deprecated keys into the configuration.
 */
HdfsKMSUtil (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/HdfsKMSUtil.java)/**
 * Utility class for key provider related methods in hdfs client package.
 *
 */
CloseEvent (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/inotify/Event.java)/**
   * Sent when a file is closed after append or create.
   */
CreateEvent (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/inotify/Event.java)/**
   * Sent when a new file is created (including overwrite).
   */
MetadataUpdateEvent (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/inotify/Event.java)/**
   * Sent when there is an update to directory or file (none of the metadata
   * tracked here applies to symlinks) that is not associated with another
   * inotify event. The tracked metadata includes atime/mtime, replication,
   * owner/group, permissions, ACLs, and XAttributes. Fields not relevant to the
   * metadataType of the MetadataUpdateEvent will be null or will have their default
   * values.
   */
RenameEvent (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/inotify/Event.java)/**
   * Sent when a file, directory, or symlink is renamed.
   */
AppendEvent (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/inotify/Event.java)/**
   * Sent when an existing file is opened for append.
   */
UnlinkEvent (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/inotify/Event.java)/**
   * Sent when a file, directory, or symlink is deleted.
   */
TruncateEvent (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/inotify/Event.java)/**
   * Sent when a file is truncated.
   */
Event (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/inotify/Event.java)/**
 * Events sent by the inotify system. Note that no events are necessarily sent
 * when a file is opened for read (although a MetadataUpdateEvent will be sent
 * if the atime is updated).
 */
EventBatch (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/inotify/EventBatch.java)/**
 * A batch of events that all happened on the same transaction ID.
 */
EventBatchList (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/inotify/EventBatchList.java)/**
 * Contains a list of event batches, the transaction ID in the edit log up to
 * which we read to produce these events, and the first txid we observed when
 * producing these events (the last of which is for the purpose of determining
 * whether we have missed events due to edit deletion). Also contains the most
 * recent txid that the NameNode has sync'ed, so the client can determine how
 * far behind in the edit log it is.
 */
ProxyAndInfo (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/NameNodeProxiesClient.java)/**
   * Wrapper for a client proxy as well as its associated service ID.
   * This is simply used as a tuple-like return type for created NN proxy.
   */
NameNodeProxiesClient (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/NameNodeProxiesClient.java)/**
 * Create proxy objects with {@link ClientProtocol} and
 * {@link HAServiceProtocol} to communicate with a remote NN. For the former,
 * generally use {@link NameNodeProxiesClient#createProxyWithClientProtocol(
 * Configuration, URI, AtomicBoolean)}, which will create either an HA- or
 * non-HA-enabled client proxy as appropriate.
 *
 * For creating proxy objects with other protocols, please see
 * NameNodeProxies#createProxy(Configuration, URI, Class).
 */
BasicInetPeer (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/net/BasicInetPeer.java)/**
 * Represents a peer that we communicate with by using a basic Socket
 * that has no associated Channel.
 *
 */
DomainPeer (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/net/DomainPeer.java)/**
 * Represents a peer that we communicate with by using blocking I/O
 * on a UNIX domain socket.
 */
EncryptedPeer (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/net/EncryptedPeer.java)/**
 * Represents a peer that we communicate with by using an encrypted
 * communications medium.
 */
NioInetPeer (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/net/NioInetPeer.java)/**
 * Represents a peer that we communicate with by using non-blocking I/O
 * on a Socket.
 */
Peer (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/net/Peer.java)/**
 * Represents a connection to a peer.
 */
PeerCache (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PeerCache.java)/**
 * A cache of input stream sockets to Data Node.
 */
PositionStripeReader (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PositionStripeReader.java)/**
 * The reader for reading a complete {@link StripedBlockUtil.AlignedStripe}
 * which may cross multiple stripes with cellSize width.
 */
AclException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/AclException.java)/**
 * Indicates a failure manipulating an ACL.
 */
AddErasureCodingPolicyResponse (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/AddErasureCodingPolicyResponse.java)/**
 * A response of add an ErasureCoding policy.
 */
AlreadyBeingCreatedException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/AlreadyBeingCreatedException.java)/**
 * The exception that happens when you ask to create a file that already
 * is being created, but is not closed yet.
 */
Block (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/Block.java)/**
 * A Block is a Hadoop FS primitive, identified by its block ID (a long). A
 * block also has an accompanying generation stamp. A generation stamp is a
 * monotonically increasing 8-byte number for each block that is maintained
 * persistently by the NameNode. However, for the purposes of this class, two
 * Blocks are considered equal iff they have the same block ID.
 *
 * @see Block#equals(Object)
 * @see Block#hashCode()
 * @see Block#compareTo(Block)
 */
BlockChecksumOptions (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/BlockChecksumOptions.java)/**
 * Encapsulates various options related to how fine-grained data checksums are
 * combined into block-level checksums.
 */
BlockLocalPathInfo (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/BlockLocalPathInfo.java)/**
 * A block and the full path information to the block data file and
 * the metadata file stored on the local file system.
 */
BlockStoragePolicy (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/BlockStoragePolicy.java)/**
 * A block storage policy describes how to select the storage types
 * for the replicas of a block.
 */
CacheDirectiveEntry (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveEntry.java)/**
 * Describes a path-based cache directive entry.
 */
Builder (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveInfo.java)/**
   * A builder for creating new CacheDirectiveInfo instances.
   */
Expiration (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveInfo.java)/**
   * Denotes a relative or absolute expiration time for a CacheDirective. Use
   * factory methods {@link CacheDirectiveInfo.Expiration#newAbsolute(Date)} and
   * {@link CacheDirectiveInfo.Expiration#newRelative(long)} to create an
   * Expiration.
   * <p>
   * In either case, the server-side clock is used to determine when a
   * CacheDirective expires.
   */
CacheDirectiveInfo (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveInfo.java)/**
 * Describes a path-based cache directive.
 */
SingleEntry (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java)/**
   * Used for compatibility when communicating with a server version that
   * does not support filtering directives by ID.
   */
CacheDirectiveIterator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java)/**
 * CacheDirectiveIterator is a remote iterator that iterates cache directives.
 * It supports retrying in case of namenode failover.
 */
CacheDirectiveStats (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveStats.java)/**
 * Describes a path-based cache directive.
 */
CachePoolEntry (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolEntry.java)/**
 * Describes a Cache Pool entry.
 */
CachePoolInfo (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolInfo.java)/**
 * CachePoolInfo describes a cache pool.
 *
 * This class is used in RPCs to create and modify cache pools.
 * It is serializable and can be stored in the edit log.
 */
CachePoolIterator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolIterator.java)/**
 * CachePoolIterator is a remote iterator that iterates cache pools.
 * It supports retrying in case of namenode failover.
 */
CachePoolStats (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CachePoolStats.java)/**
 * CachePoolStats describes cache pool statistics.
 */
ClientDatanodeProtocol (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ClientDatanodeProtocol.java)/** An client-datanode protocol for block recovery
 */
ClientProtocol (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ClientProtocol.java)/**********************************************************************
 * ClientProtocol is used by user code via the DistributedFileSystem class to
 * communicate with the NameNode.  User code can manipulate the directory
 * namespace, as well as open/close file streams, etc.
 *
 **********************************************************************/
CorruptFileBlocks (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CorruptFileBlocks.java)/**
 * Contains a list of paths corresponding to corrupt files and a cookie
 * used for iterative calls to NameNode.listCorruptFileBlocks.
 *
 */
DatanodeAdminProperties (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeAdminProperties.java)/**
 * The class describes the configured admin properties for a datanode.
 *
 * It is the static configuration specified by administrators via dfsadmin
 * command; different from the runtime state. CombinedHostFileManager uses
 * the class to deserialize the configurations from json-based file format.
 *
 * To decommission a node, use AdminStates.DECOMMISSIONED.
 */
DatanodeID (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeID.java)/**
 * This class represents the primary identifier for a Datanode.
 * Datanodes are identified by how they can be contacted (hostname
 * and ports) and their storage ID, a unique number that associates
 * the Datanodes blocks with a particular Datanode.
 *
 * {@link DatanodeInfo#getName()} should be used to get the network
 * location (for topology) of a datanode, instead of using
 * {@link DatanodeID#getXferAddr()} here. Helpers are defined below
 * for each context in which a DatanodeID is used.
 */
DatanodeInfoBuilder (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java)/**
   * Building the DataNodeInfo.
   */
DatanodeInfo (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeInfo.java)/**
 * This class extends the primary identifier of a Datanode with ephemeral
 * state, eg usage information, current administrative state, and the
 * network location that is communicated to clients.
 */
DatanodeLocalInfo (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeLocalInfo.java)/**
 * Locally available datanode information
 */
DatanodeVolumeInfo (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DatanodeVolumeInfo.java)/**
 * Locally available datanode volume information.
 */
BlockPinningException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/BlockPinningException.java)/**
 * Indicates a failure due to block pinning.
 */
DataTransferProtocol (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/DataTransferProtocol.java)/**
 * Transfer data to/from datanode using a streaming protocol.
 */
DataTransferProtoUtil (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/DataTransferProtoUtil.java)/**
 * Static utilities for dealing with the protocol buffers used by the
 * Data Transfer Protocol.
 */
InvalidEncryptionKeyException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/InvalidEncryptionKeyException.java)/**
 * Encryption key verification failed.
 */
IOStreamPair (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/IOStreamPair.java)/**
 * A little struct class to wrap an InputStream and an OutputStream.
 */
PacketHeader (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/PacketHeader.java)/**
 * Header data for each packet that goes through the read/write pipelines.
 * Includes all of the information about the packet, excluding checksums and
 * actual data.
 *
 * This data includes:
 *  - the offset in bytes into the HDFS block of the data in this packet
 *  - the sequence number of this packet in the pipeline
 *  - whether or not this is the last packet in the pipeline
 *  - the length of the data in this packet
 *  - whether or not this packet should be synced by the DNs.
 *
 * When serialized, this header is written out as a protocol buffer, preceded
 * by a 4-byte integer representing the full packet length, and a 2-byte short
 * representing the header length.
 */
PacketReceiver (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/PacketReceiver.java)/**
 * Class to handle reading packets one-at-a-time from the wire.
 * These packets are used both for reading and writing data to/from
 * DataNodes.
 */
PipelineAck (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/PipelineAck.java)/** Pipeline Acknowledgment **/
Condition (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/ReplaceDatanodeOnFailure.java)/** Datanode replacement condition */
ReplaceDatanodeOnFailure (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/ReplaceDatanodeOnFailure.java)/**
 * The setting of replace-datanode-on-failure feature.
 */
DataEncryptionKeyFactory (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/DataEncryptionKeyFactory.java)/**
 * Creates a new {@link DataEncryptionKey} on demand.
 */
DataTransferSaslUtil (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/DataTransferSaslUtil.java)/**
 * Utility methods implementing SASL negotiation for DataTransferProtocol.
 */
SaslClientCallbackHandler (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java)/**
   * Sets user name and password when asked by the client-side SASL object.
   */
SaslDataTransferClient (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java)/**
 * Negotiates SASL for DataTransferProtocol on behalf of a client.  There are
 * two possible supported variants of SASL negotiation: either a general-purpose
 * negotiation supporting any quality of protection, or a specialized
 * negotiation that enforces privacy as the quality of protection using a
 * cryptographically strong encryption key.
 *
 * This class is used in both the HDFS client and the DataNode.  The DataNode
 * needs it, because it acts as a client to other DataNodes during write
 * pipelines and block transfers.
 */
SaslParticipant (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslParticipant.java)/**
 * Strongly inspired by Thrift's TSaslTransport class.
 *
 * Used to abstract over the <code>SaslServer</code> and
 * <code>SaslClient</code> classes, which share a lot of their interface, but
 * unfortunately don't share a common superclass.
 */
Sender (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Sender.java)/** Sender */
TrustedChannelResolver (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/TrustedChannelResolver.java)/**
 * Class used to indicate whether a channel is trusted or not.
 * The default implementation is to return false indicating that
 * the channel is not trusted.
 * This class can be overridden to provide custom logic to determine
 * whether a channel is trusted or not.
 * The custom class can be specified via configuration.
 *
 */
DirectoryListing (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/DirectoryListing.java)/**
 * This class defines a partial listing of a directory to support
 * iterative directory listing.
 */
ECBlockGroupStats (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ECBlockGroupStats.java)/**
 * Get statistics pertaining to blocks of type {@link BlockType#STRIPED}
 * in the filesystem.
 * <p>
 * @see ClientProtocol#getECBlockGroupStats()
 */
EncryptionZone (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/EncryptionZone.java)/**
 * A simple class for representing an encryption zone. Presently an encryption
 * zone only has a path (the root of the encryption zone), a key name, and a
 * unique id. The id is used to implement batched listing of encryption zones.
 */
EncryptionZoneIterator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/EncryptionZoneIterator.java)/**
 * EncryptionZoneIterator is a remote iterator that iterates over encryption
 * zones. It supports retrying in case of namenode failover.
 */
ErasureCodingPolicy (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ErasureCodingPolicy.java)/**
 * A policy about how to write/read/code an erasure coding file.
 * <p>
 * Note this class should be lightweight and immutable, because it's cached
 * by {@link SystemErasureCodingPolicies}, to be returned as a part of
 * {@link HdfsFileStatus}.
 */
ErasureCodingPolicyInfo (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ErasureCodingPolicyInfo.java)/**
 * HDFS internal presentation of a {@link ErasureCodingPolicy}. Also contains
 * additional information such as {@link ErasureCodingPolicyState}.
 */
ExtendedBlock (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ExtendedBlock.java)/**
 * Identifies a Block uniquely across the block pools
 */
FsPermissionExtension (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/FsPermissionExtension.java)/**
 * @deprecated ACLs, encryption, and erasure coding are managed on FileStatus.
 */
Builder (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsFileStatus.java)/**
   * Builder class for HdfsFileStatus instances. Note default values for
   * parameters.
   */
HdfsFileStatus (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsFileStatus.java)/**
 * HDFS metadata for an entity in the filesystem.
 */
HdfsLocatedFileStatus (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsLocatedFileStatus.java)/**
 * HDFS metadata for an entity in the filesystem with locations. Note that
 * symlinks and directories are returned as {@link HdfsLocatedFileStatus} for
 * backwards compatibility.
 */
HdfsNamedFileStatus (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsNamedFileStatus.java)/**
 * HDFS metadata for an entity in the filesystem without locations. Note that
 * symlinks and directories are returned as {@link HdfsLocatedFileStatus} for
 * backwards compatibility.
 */
HdfsPathHandle (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsPathHandle.java)/**
 * Opaque handle to an entity in HDFS.
 */
LastBlockWithStatus (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/LastBlockWithStatus.java)/**
 * Class to contain Lastblock and HdfsFileStatus for the Append operation
 */
ProvidedLastComparator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/LocatedBlock.java)/**
   * Comparator that ensures that a PROVIDED storage type is greater than any
   * other storage type. Any other storage types are considered equal.
   */
LocatedBlock (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/LocatedBlock.java)/**
 * Associates a block with the Datanodes that contain its replicas
 * and other block metadata (E.g. the file offset associated with this
 * block, whether it is corrupt, a location is cached in memory,
 * security token, etc).
 */
LocatedBlocks (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/LocatedBlocks.java)/**
 * Collection of blocks with their locations and the file length.
 */
LocatedStripedBlock (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/LocatedStripedBlock.java)/**
 * {@link LocatedBlock} with striped block support. For a striped block, each
 * datanode storage is associated with a block in the block group. We need to
 * record the index (in the striped block group) for each of them.
 */
NoECPolicySetException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/NoECPolicySetException.java)/**
 *Thrown when no EC policy is set explicitly on the directory.
 */
OpenFileEntry (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/OpenFileEntry.java)/**
 * An open file entry for use by DFSAdmin commands.
 */
OpenFilesIterator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/OpenFilesIterator.java)/**
 * OpenFilesIterator is a remote iterator that iterates over the open files list
 * managed by the NameNode. Since the list is retrieved in batches, it does not
 * represent a consistent view of all open files.
 */
ProvidedStorageLocation (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ProvidedStorageLocation.java)/**
 * ProvidedStorageLocation is a location in an external storage system
 * containing the data for a block (~Replica).
 */
QuotaExceededException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/QuotaExceededException.java)/**
 * This exception is thrown when modification to HDFS results in violation
 * of a directory quota. A directory quota might be namespace quota (limit
 * on number of files and directories) or a diskspace quota (limit on space
 * taken by all the file under the directory tree). <br> <br>
 *
 * The message for the exception specifies the directory where the quota
 * was violated and actual quotas. Specific message is generated in the
 * corresponding Exception class:
 *  DSQuotaExceededException or
 *  NSQuotaExceededException
 */
ReconfigurationProtocol (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ReconfigurationProtocol.java)/**********************************************************************
 * ReconfigurationProtocol is used by HDFS admin to reload configuration
 * for NN/DN without restarting them.
 **********************************************************************/
ReencryptionStatus (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ReencryptionStatus.java)/**
 * A class representing information about re-encrypting encryption zones. It
 * contains a collection of @{code ZoneReencryptionStatus} for each EZ.
 * <p>
 * FSDirectory lock is used for synchronization (except test-only methods, which
 * are not protected).
 */
ReencryptionStatusIterator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ReencryptionStatusIterator.java)/**
 * ReencryptionStatusIterator is a remote iterator that iterates over the
 * reencryption status of encryption zones.
 * It supports retrying in case of namenode failover.
 */
ReplicatedBlockStats (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ReplicatedBlockStats.java)/**
 * Get statistics pertaining to blocks of type {@link BlockType#CONTIGUOUS}
 * in the filesystem.
 * <p>
 * @see ClientProtocol#getReplicatedBlockStats()
 */
RollingUpgradeInfo (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/RollingUpgradeInfo.java)/**
 * Rolling upgrade information
 */
RollingUpgradeStatus (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/RollingUpgradeStatus.java)/**
 * Rolling upgrade status
 */
SnapshotAccessControlException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotAccessControlException.java)/** Snapshot access related exception. */
DiffReportEntry (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotDiffReport.java)/**
   * Representing the full path and diff type of a file/directory where changes
   * have happened.
   */
DiffStats (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotDiffReport.java)/**
   * Records the stats related to Snapshot diff operation.
   */
SnapshotDiffReport (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotDiffReport.java)/**
 * This class represents to end users the difference between two snapshots of
 * the same directory, or the difference between a snapshot of the directory and
 * its current state. Instead of capturing all the details of the diff, this
 * class only lists where the changes happened and their types.
 */
DiffReportListingEntry (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotDiffReportListing.java)/**
   * Representing the full path and diff type of a file/directory where changes
   * have happened.
   */
SnapshotDiffReportListing (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotDiffReportListing.java)/**
 * This class represents to  the difference between two snapshots of
 * the same directory, or the difference between a snapshot of the directory and
 * its current state. This Class serves the purpose of collecting diff entries
 * in 3 lists : created, deleted and modified list combined size of which is set
 * by dfs.snapshotdiff-report.limit over one rpc call to the namenode.
 */
SnapshottableDirectoryStatus (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshottableDirectoryStatus.java)/**
 * Metadata about a snapshottable directory
 */
StripedBlockInfo (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/StripedBlockInfo.java)/**
 * Striped block info that can be sent elsewhere to do block group level things,
 * like checksum, and etc.
 */
SystemErasureCodingPolicies (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SystemErasureCodingPolicies.java)/**
 * <p>The set of built-in erasure coding policies.</p>
 * <p>Although this is a private class, EC policy IDs need to be treated like a
 * stable interface. Adding, modifying, or removing built-in policies can cause
 * inconsistencies with older clients.</p>
 */
UnresolvedPathException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/UnresolvedPathException.java)/**
 * Thrown when a symbolic link is encountered in a path.
 */
Builder (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ZoneReencryptionStatus.java)/**
   * Builder of {@link ZoneReencryptionStatus}.
   */
ZoneReencryptionStatus (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ZoneReencryptionStatus.java)/**
 * A class representing information about re-encryption of an encryption zone.
 * <p>
 * FSDirectory lock is used for synchronization (except test-only methods, which
 * are not protected).
 */
ClientDatanodeProtocolTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientDatanodeProtocolTranslatorPB.java)/**
 * This class is the client side translator to translate the requests made on
 * {@link ClientDatanodeProtocol} interfaces to the RPC server implementing
 * {@link ClientDatanodeProtocolPB}.
 */
ClientNamenodeProtocolTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java)/**
 * This class forwards NN's ClientProtocol calls as RPC calls to the NN server
 * while translating from the parameter types used in ClientProtocol to the
 * new PB types.
 */
PBHelperClient (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java)/**
 * Utilities for converting protobuf classes to and from hdfs-client side
 * implementation classes and other helper utilities to help in dealing with
 * protobuf.
 *
 * Note that when converting from an internal type to protobuf type, the
 * converter never return null for protobuf type. The check for internal type
 * being null must be done before calling the convert() method.
 */
ReconfigurationProtocolPB (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolPB.java)/**
 * Protocol that clients use to communicate with the NN/DN to do
 * reconfiguration on the fly.
 *
 * Note: This extends the protocolbuffer service based interface to
 * add annotations required for security.
 */
ReconfigurationProtocolTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolTranslatorPB.java)/**
 * This class is the client side translator to translate the requests made on
 * {@link ReconfigurationProtocol} interfaces to the RPC server implementing
 * {@link ReconfigurationProtocolPB}.
 */
ReconfigurationProtocolUtils (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolUtils.java)/**
 * This is a client side utility class that handles
 * common logic to to parameter reconfiguration.
 */
ReaderStrategy (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ReaderStrategy.java)/**
 * Wraps different possible read implementations so that callers can be
 * strategy-agnostic.
 */
ByteArrayStrategy (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ReaderStrategy.java)/**
 * Used to read bytes into a byte array buffer. Note it's not thread-safe
 * and the behavior is not defined if concurrently operated.
 */
ByteBufferStrategy (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ReaderStrategy.java)/**
 * Used to read bytes into a user-supplied ByteBuffer. Note it's not thread-safe
 * and the behavior is not defined if concurrently operated. When read operation
 * is performed, the position of the underlying byte buffer will move forward as
 * stated in ByteBufferReadable#read(ByteBuffer buf) method.
 */
ReadStatistics (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ReadStatistics.java)/**
 * A utility class that maintains statistics for reading.
 */
ReplicaAccessor (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ReplicaAccessor.java)/**
 * The public API for ReplicaAccessor objects.
 */
ReplicaAccessorBuilder (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ReplicaAccessorBuilder.java)/**
 * The public API for creating a new ReplicaAccessor.
 */
BlockTokenSelector (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSelector.java)/**
 * A block token selector for HDFS
 */
DataEncryptionKey (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/security/token/block/DataEncryptionKey.java)/**
 * A little struct class to contain all fields required to perform encryption of
 * the DataTransferProtocol.
 */
InvalidBlockTokenException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/security/token/block/InvalidBlockTokenException.java)/**
 * Access token verification failed.
 */
DelegationTokenIdentifier (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenIdentifier.java)/**
 * A delegation token identifier that is specific to HDFS.
 */
DelegationTokenSelector (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSelector.java)/**
 * A delegation token that is specialized for HDFS
 */
BlockMetadataHeader (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java)/**
 * BlockMetadataHeader manages metadata for data blocks on Datanodes.
 * This is not related to the Block related functionality in Namenode.
 * The biggest part of data block metadata is CRC for the block.
 */
CachingStrategy (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/datanode/CachingStrategy.java)/**
 * The caching strategy we should use for an HDFS read or write operation.
 */
CorruptMetaHeaderException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/datanode/CorruptMetaHeaderException.java)/**
 * Exception object that is thrown when the block metadata file is corrupt.
 */
DiskBalancerWorkItem (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancerWorkItem.java)/**
 * Keeps track of how much work has finished.
 */
DiskBalancerWorkEntry (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancerWorkStatus.java)/**
   * A class that is used to report each work item that we are working on. This
   * class describes the Source, Destination and how much data has been already
   * moved, errors encountered etc. This is useful for the disk balancer stats
   * as well as the queryStatus RPC.
   */
DiskBalancerWorkStatus (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancerWorkStatus.java)/**
 * Helper class that reports how much work has has been done by the node.
 */
ReplicaNotFoundException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaNotFoundException.java)/**
 * Exception indicating that DataNode does not have a replica
 * that matches the target block.
 */
NNProxyInfo (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/AbstractNNFailoverProxyProvider.java)/**
   * ProxyInfo to a NameNode. Includes its address.
   */
ConfiguredFailoverProxyProvider (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ConfiguredFailoverProxyProvider.java)/**
 * A FailoverProxyProvider implementation which allows one to configure
 * multiple URIs to connect to during fail-over. A random configured address is
 * tried first, and on a fail-over event the other addresses are tried
 * sequentially in a random order.
 */
HAProxyFactory (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/HAProxyFactory.java)/**
 * This interface aims to decouple the proxy creation implementation that used
 * in {@link AbstractNNFailoverProxyProvider}. Client side can use
 * {@link org.apache.hadoop.hdfs.protocol.ClientProtocol} to initialize the
 * proxy while the server side can use NamenodeProtocols
 */
InMemoryAliasMapFailoverProxyProvider (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/InMemoryAliasMapFailoverProxyProvider.java)/**
 * A {@link ConfiguredFailoverProxyProvider} implementation used to connect
 * to an InMemoryAliasMap.
 */
IPFailoverProxyProvider (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/IPFailoverProxyProvider.java)/**
 * A NNFailoverProxyProvider implementation which works on IP failover setup.
 * Only one proxy is used to connect to both servers and switching between
 * the servers is done by the environment/infrastructure, which guarantees
 * clients can consistently reach only one node at a time.
 *
 * Clients with a live connection will likely get connection reset after an
 * IP failover. This case will be handled by the
 * FailoverOnNetworkExceptionRetry retry policy. I.e. if the call is
 * not idempotent, it won't get retried.
 *
 * A connection reset while setting up a connection (i.e. before sending a
 * request) will be handled in ipc client.
 *
 * The namenode URI must contain a resolvable host name.
 */
ObserverReadInvocationHandler (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java)/**
   * An InvocationHandler to handle incoming requests. This class's invoke
   * method contains the primary logic for redirecting to observers.
   *
   * If observer reads are enabled, attempt to send read operations to the
   * current proxy. If it is not an observer, or the observer fails, adjust
   * the current proxy and retry on the next one. If all proxies are tried
   * without success, the request is forwarded to the active.
   *
   * Write requests are always forwarded to the active.
   */
ObserverReadProxyProvider (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java)/**
 * A {@link org.apache.hadoop.io.retry.FailoverProxyProvider} implementation
 * that supports reading from observer namenode(s).
 *
 * This constructs a wrapper proxy that sends the request to observer
 * namenode(s), if observer read is enabled. In case there are multiple
 * observer namenodes, it will try them one by one in case the RPC failed. It
 * will fail back to the active namenode after it has exhausted all the
 * observer namenodes.
 *
 * Read and write requests will still be sent to active NN if reading from
 * observer is turned off.
 */
ObserverReadProxyProviderWithIPFailover (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProviderWithIPFailover.java)/**
 * Extends {@link ObserverReadProxyProvider} to support NameNode IP failover.
 *
 * For Observer reads a client needs to know physical addresses of all
 * NameNodes, so that it could switch between active and observer nodes
 * for write and read requests.
 *
 * Traditional {@link IPFailoverProxyProvider} works with a virtual
 * address of the NameNode. If active NameNode fails the virtual address
 * is assigned to the standby NameNode, and IPFailoverProxyProvider, which
 * keeps talking to the same virtual address is in fact now connects to
 * the new physical server.
 *
 * To combine these behaviors ObserverReadProxyProviderWithIPFailover
 * should both
 * <ol>
 * <li> Maintain all physical addresses of NameNodes in order to allow
 * observer reads, and
 * <li> Should rely on the virtual address of the NameNode in order to
 * perform failover by assuming that the virtual address always points
 * to the active NameNode.
 * </ol>
 *
 * An example of a configuration to leverage
 * ObserverReadProxyProviderWithIPFailover
 * should include the following values:
 * <pre>{@code
 * fs.defaultFS = hdfs://mycluster
 * dfs.nameservices = mycluster
 * dfs.ha.namenodes.mycluster = ha1,ha2
 * dfs.namenode.rpc-address.mycluster.ha1 = nn01-ha1.com:8020
 * dfs.namenode.rpc-address.mycluster.ha2 = nn01-ha2.com:8020
 * dfs.client.failover.ipfailover.virtual-address.mycluster =
 *     hdfs://nn01.com:8020
 * dfs.client.failover.proxy.provider.mycluster =
 *     org.apache...ObserverReadProxyProviderWithIPFailover
 * }</pre>
 * Here {@code nn01.com:8020} is the virtual address of the active NameNode,
 * while {@code nn01-ha1.com:8020} and {@code nn01-ha2.com:8020}
 * are the physically addresses the two NameNodes.
 *
 * With this configuration, client will use
 * ObserverReadProxyProviderWithIPFailover, which creates proxies for both
 * nn01-ha1 and nn01-ha2, used for read/write RPC calls, but for the failover,
 * it relies on the virtual address nn01.com
 */
RequestHedgingProxyProvider (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.java)/**
 * A FailoverProxyProvider implementation that technically does not "failover"
 * per-se. It constructs a wrapper proxy that sends the request to ALL
 * underlying proxies simultaneously. It assumes the in an HA setup, there will
 * be only one Active, and the active should respond faster than any configured
 * standbys. Once it receive a response from any one of the configured proxies,
 * outstanding requests to other proxies are immediately cancelled.
 */
WrappedFailoverProxyProvider (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/WrappedFailoverProxyProvider.java)/**
 * A NNFailoverProxyProvider implementation which wrapps old implementations
 * directly implementing the {@link FailoverProxyProvider} interface.
 *
 * It is assumed that the old impelmentation is using logical URI.
 */
NotReplicatedYetException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/NotReplicatedYetException.java)/**
 * The file has not finished being written to enough datanodes yet.
 */
SafeModeException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/SafeModeException.java)/**
 * This exception is thrown when the name node is in safe mode.
 * Client cannot modified namespace until the safe mode is off.
 *
 */
DatanodeStorage (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/protocol/DatanodeStorage.java)/**
 * Class captures information of a storage in Datanode.
 */
DatanodeStorageReport (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/protocol/DatanodeStorageReport.java)/**
 * Class captures information of a datanode and its storages.
 */
Builder (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/protocol/DataNodeUsageReport.java)/**
   * Builder class for {@link DataNodeUsageReport}.
   */
DataNodeUsageReport (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/protocol/DataNodeUsageReport.java)/**
 * A class that allows DataNode to communicate information about
 * usage statistics/metrics to NameNode.
 */
DataNodeUsageReportUtil (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/protocol/DataNodeUsageReportUtil.java)/**
 * This class is helper class to generate a live usage report by calculating
 * the delta between current DataNode usage metrics and the usage metrics
 * captured at the time of the last report.
 */
SlowDiskReports (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/protocol/SlowDiskReports.java)/**
 * A class that allows a DataNode to communicate information about all
 * its disks that appear to be slow.
 *
 * The wire representation of this structure is a list of
 * SlowDiskReportProto messages.
 */
SlowPeerReports (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/protocol/SlowPeerReports.java)/**
 * A class that allows a DataNode to communicate information about all
 * its peer DataNodes that appear to be slow.
 *
 * The wire representation of this structure is a list of
 * SlowPeerReportProto messages.
 */
StorageReport (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/protocol/StorageReport.java)/**
 * Utilization report for a Datanode storage
 */
ClientMmap (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ClientMmap.java)/**
 * A reference to a memory-mapped region used by an HDFS client.
 */
DfsClientShm (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DfsClientShm.java)/**
 * DfsClientShm is a subclass of ShortCircuitShm which is used by the
 * DfsClient.
 * When the UNIX domain socket associated with this shared memory segment
 * closes unexpectedly, we mark the slots inside this segment as disconnected.
 * ShortCircuitReplica objects that contain disconnected slots are stale,
 * and will not be used to service new reads or mmap operations.
 * However, in-progress read or mmap operations will continue to proceed.
 * Once the last slot is deallocated, the segment can be safely munmapped.
 *
 * Slots may also become stale because the associated replica has been deleted
 * on the DataNode.  In this case, the DataNode will clear the 'valid' bit.
 * The client will then see these slots as stale (see
 * #{ShortCircuitReplica#isStale}).
 */
EndpointShmManager (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.java)/**
   * Manages short-circuit memory segments that pertain to a given DataNode.
   */
DfsClientShmManager (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.java)/**
 * Manages short-circuit memory segments for an HDFS client.
 *
 * Clients are responsible for requesting and releasing shared memory segments
 * used for communicating with the DataNode. The client will try to allocate new
 * slots in the set of existing segments, falling back to getting a new segment
 * from the DataNode via {@link DataTransferProtocol#requestShortCircuitFds}.
 *
 * The counterpart to this class on the DataNode is ShortCircuitRegistry.
 * See ShortCircuitRegistry for more information on the communication protocol.
 */
CacheCleaner (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java)/**
   * Expiry thread which makes sure that the file descriptors get closed
   * after a while.
   */
SlotReleaser (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java)/**
   * A task which asks the DataNode to release a short-circuit shared memory
   * slot.  If successful, this will tell the DataNode to stop monitoring
   * changes to the mlock status of the replica associated with the slot.
   * It will also allow us (the client) to re-use this slot for another
   * replica.  If we can't communicate with the DataNode for some reason,
   * we tear down the shared memory segment to avoid being in an inconsistent
   * state.
   */
ShortCircuitCache (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java)/**
 * The ShortCircuitCache tracks things which the client needs to access
 * HDFS block files via short-circuit.
 *
 * These things include: memory-mapped regions, file descriptors, and shared
 * memory areas for communicating with the DataNode.
 */
ShortCircuitReplica (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java)/**
 * A ShortCircuitReplica object contains file descriptors for a block that
 * we are reading via short-circuit local reads.
 *
 * The file descriptors can be shared between multiple threads because
 * all the operations we perform are stateless-- i.e., we use pread
 * instead of read, to avoid using the shared position state.
 */
ShmId (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.java)/**
   * Identifies a DfsClientShm.
   */
SlotId (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.java)/**
   * Uniquely identifies a slot.
   */
Slot (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.java)/**
   * A slot containing information about a replica.
   *
   * The format is:
   * word 0
   *   bit 0:32   Slot flags (see below).
   *   bit 33:63  Anchor count.
   * word 1:7
   *   Reserved for future use, such as statistics.
   *   Padding is also useful for avoiding false sharing.
   *
   * Little-endian versus big-endian is not relevant here since both the client
   * and the server reside on the same computer and use the same orientation.
   */
ShortCircuitShm (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.java)/**
 * A shared memory segment used to implement short-circuit reads.
 */
StatefulStripeReader (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StatefulStripeReader.java)/**
 * The reader for reading a complete {@link StripedBlockUtil.AlignedStripe}
 * which belongs to a single stripe.
 * Reading cross multiple strips is not supported in this reader.
 */
StripedDataStreamer (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripedDataStreamer.java)/**
 * This class extends {@link DataStreamer} to support writing striped blocks
 * to datanodes.
 * A {@link DFSStripedOutputStream} has multiple {@link StripedDataStreamer}s.
 * Whenever the streamers need to talk the namenode, only the fastest streamer
 * sends an rpc call to the namenode and then populates the result for the
 * other streamers.
 */
StripeReader (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java)/**
 * The reader for reading a complete {@link StripedBlockUtil.AlignedStripe}.
 * Note that an {@link StripedBlockUtil.AlignedStripe} may cross multiple
 * stripes with cellSize width.
 */
UnknownCipherSuiteException (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/UnknownCipherSuiteException.java)/**
 * Thrown when an unknown cipher suite is encountered.
 */
Counter (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java)/**
   * A counter with a time stamp so that it is reset automatically
   * if there is no increment for the time period.
   */
CounterMap (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java)/** A map from integers to counters. */
FixedLengthManager (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java)/** Manage byte arrays with the same fixed length. */
ManagerMap (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java)/** A map from array lengths to byte array managers. */
Conf (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java)/**
   * Configuration for ByteArrayManager.
   */
NewByteArrayWithoutLimit (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java)/**
   * A dummy implementation which simply calls new byte[].
   */
Impl (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java)/**
   * Manage byte array allocation and provide a mechanism for recycling the byte
   * array objects.
   */
ByteArrayManager (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java)/**
 * Manage byte array creation and release.
 */
ByteBufferOutputStream (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ByteBufferOutputStream.java)/**
 * OutputStream that writes into a {@link ByteBuffer}.
 */
CombinedHostsFileReader (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/CombinedHostsFileReader.java)/**
 * Reader support for JSON-based datanode configuration, an alternative format
 * to the exclude/include files configuration.
 * The JSON file format defines the array of elements where each element
 * in the array describes the properties of a datanode. The properties of
 * a datanode is defined by {@link DatanodeAdminProperties}. For example,
 *
 * [
 *   {"hostName": "host1"},
 *   {"hostName": "host2", "port": 50, "upgradeDomain": "ud0"},
 *   {"hostName": "host3", "port": 0, "adminState": "DECOMMISSIONED"}
 * ]
 */
CombinedHostsFileWriter (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/CombinedHostsFileWriter.java)/**
 * Writer support for JSON-based datanode configuration, an alternative format
 * to the exclude/include files configuration.
 * The JSON file format defines the array of elements where each element
 * in the array describes the properties of a datanode. The properties of
 * a datanode is defined by {@link DatanodeAdminProperties}. For example,
 *
 * [
 *   {"hostName": "host1"},
 *   {"hostName": "host2", "port": 50, "upgradeDomain": "ud0"},
 *   {"hostName": "host3", "port": 0, "adminState": "DECOMMISSIONED"}
 * ]
 */
ECPolicyLoader (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ECPolicyLoader.java)/**
 * A EC policy loading tool that loads user defined EC policies from XML file.
 */
LongBitFormat (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/LongBitFormat.java)/**
 * Bit format in a long.
 */
BlockReadStats (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java)/**
   * Struct holding the read statistics. This is used when reads are done
   * asynchronously, to allow the async threads return the read stats and let
   * the main reading thread to update the stats. This is important for the
   * ThreadLocal stats for the main reading thread to be correct.
   */
ChunkByteBuffer (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java)/**
   * A utility to manage ByteBuffer slices for a reader.
   */
StripingChunkReadResult (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java)/**
   * This class represents result from a striped read request.
   * If the task was successful or the internal computation failed,
   * an index is also returned.
   */
StripeRange (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java)/** Used to indicate the buffered data's range in the block group. */
URLOpener (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/ByteRangeInputStream.java)/**
   * This class wraps a URL and provides method to open connection.
   * It can be overridden to change how a connection is opened.
   */
ByteRangeInputStream (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/ByteRangeInputStream.java)/**
 * To support HTTP byte streams, a new connection to an HTTP server needs to be
 * created each time. This class hides the complexity of those multiple
 * connections from the client. Whenever seek() is called, a new connection
 * is made on the successive read(). The normal input stream functions are
 * connected to the currently active input stream.
 */
JsonUtilClient (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java)/**
 * Utility methods used in WebHDFS/HttpFS JSON conversion.
 */
KerberosUgiAuthenticator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/KerberosUgiAuthenticator.java)/**
 * Use UserGroupInformation as a fallback authenticator
 * if the server does not use Kerberos SPNEGO HTTP authentication.
 */
AccessTokenProvider (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/oauth2/AccessTokenProvider.java)/**
 * Provide an OAuth2 access token to be used to authenticate http calls in
 * WebHDFS.
 */
AccessTokenTimer (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/oauth2/AccessTokenTimer.java)/**
 * Access tokens generally expire.  This timer helps keep track of that.
 */
ConfCredentialBasedAccessTokenProvider (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/oauth2/ConfCredentialBasedAccessTokenProvider.java)/**
 * Obtain an access token via a a credential (provided through the
 * Configuration) using the
 * <a href="https://tools.ietf.org/html/rfc6749#section-4.4">
 *   Client Credentials Grant workflow</a>.
 */
ConfRefreshTokenBasedAccessTokenProvider (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/oauth2/ConfRefreshTokenBasedAccessTokenProvider.java)/**
 * Supply a access token obtained via a refresh token (provided through the
 * Configuration using the second half of the
 * <a href="https://tools.ietf.org/html/rfc6749#section-4.1">
 *   Authorization Code Grant workflow</a>.
 */
CredentialBasedAccessTokenProvider (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/oauth2/CredentialBasedAccessTokenProvider.java)/**
 * Obtain an access token via the credential-based OAuth2 workflow.  This
 * abstract class requires only that implementations provide the credential,
 * which the class then uses to obtain a refresh token.
 */
OAuth2ConnectionConfigurator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/oauth2/OAuth2ConnectionConfigurator.java)/**
 * Configure a connection to use OAuth2 authentication.
 */
OAuth2Constants (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/oauth2/OAuth2Constants.java)/**
 * Sundry constants relating to OAuth2 within WebHDFS.
 */
AccessTimeParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/AccessTimeParam.java)/** Access time parameter. */
AclPermissionParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/AclPermissionParam.java)/** AclPermission parameter. */
BlockSizeParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/BlockSizeParam.java)/** Block size parameter. */
Domain (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/BooleanParam.java)/** The domain of the parameter. */
BooleanParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/BooleanParam.java)/** Boolean parameter. */
BufferSizeParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/BufferSizeParam.java)/** Buffer size parameter. */
ConcatSourcesParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/ConcatSourcesParam.java)/** The concat source paths parameter. */
CreateFlagParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/CreateFlagParam.java)/**
 * CreateFlag enum.
 */
CreateParentParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/CreateParentParam.java)/** Create Parent parameter. */
DelegationParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/DelegationParam.java)/** Represents delegation token used for authentication. */
DeleteOpParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/DeleteOpParam.java)/** Http DELETE operation parameter. */
DestinationParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/DestinationParam.java)/** Destination path parameter. */
DoAsParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/DoAsParam.java)/** DoAs parameter for proxy user. */
ECPolicyParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/ECPolicyParam.java)/** policy parameter. */
Domain (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/EnumParam.java)/** The domain of the parameter. */
Domain (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/EnumSetParam.java)/** The domain of the parameter. */
ExcludeDatanodesParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/ExcludeDatanodesParam.java)/** Exclude datanodes param */
FsActionParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/FsActionParam.java)/** {@link FsAction} Parameter */
GetOpParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/GetOpParam.java)/** Http GET operation parameter. */
GroupParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/GroupParam.java)/** Group parameter. */
Op (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/HttpOpParam.java)/** Http operation interface. */
TemporaryRedirectOp (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/HttpOpParam.java)/** Expects HTTP response 307 "Temporary Redirect". */
HttpOpParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/HttpOpParam.java)/** Http operation parameter. */
Domain (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/IntegerParam.java)/** The domain of the parameter. */
IntegerParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/IntegerParam.java)/** Integer parameter. */
LengthParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/LengthParam.java)/** Length parameter. */
Domain (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/LongParam.java)/** The domain of the parameter. */
LongParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/LongParam.java)/** Long parameter. */
ModificationTimeParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/ModificationTimeParam.java)/** Modification time parameter. */
NameSpaceQuotaParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/NameSpaceQuotaParam.java)/** The name space quota parameter for directory. */
NewLengthParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/NewLengthParam.java)/** NewLength parameter. */
NoRedirectParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/NoRedirectParam.java)/** Overwrite parameter. */
OffsetParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/OffsetParam.java)/** Offset parameter. */
OldSnapshotNameParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/OldSnapshotNameParam.java)/**
 * The old snapshot name parameter for renameSnapshot operation.
 */
OverwriteParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/OverwriteParam.java)/** Overwrite parameter. */
OwnerParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/OwnerParam.java)/** Owner parameter. */
Domain (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/Param.java)/** Base class of parameter domains. */
Param (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/Param.java)/** Base class of parameters. */
PermissionParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/PermissionParam.java)/** Permission parameter, use a Short to represent a FsPermission. */
PostOpParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/PostOpParam.java)/** Http POST operation parameter. */
PutOpParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/PutOpParam.java)/** Http POST operation parameter. */
RecursiveParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/RecursiveParam.java)/** Recursive parameter. */
RenameOptionSetParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/RenameOptionSetParam.java)/** Rename option set parameter. */
RenewerParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/RenewerParam.java)/** Renewer parameter. */
ReplicationParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/ReplicationParam.java)/** Replication parameter. */
Domain (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/ShortParam.java)/** The domain of the parameter. */
ShortParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/ShortParam.java)/** Short parameter. */
SnapshotNameParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/SnapshotNameParam.java)/**
 * The snapshot name parameter for createSnapshot and deleteSnapshot operation.
 * Also used to indicate the new snapshot name for renameSnapshot operation.
 */
StartAfterParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/StartAfterParam.java)/**
 * Used during batched ListStatus operations.
 */
StoragePolicyParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/StoragePolicyParam.java)/** policy parameter. */
StorageSpaceQuotaParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/StorageSpaceQuotaParam.java)/** The storage space quota parameter for directory. */
StorageTypeParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/StorageTypeParam.java)/** storage type parameter. */
Domain (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/StringParam.java)/** The domain of the parameter. */
StringParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/StringParam.java)/** String parameter. */
TokenArgumentParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/TokenArgumentParam.java)/**
 * Represents delegation token parameter as method arguments. This is
 * different from {@link DelegationParam}.
 */
UnmaskedPermissionParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/UnmaskedPermissionParam.java)/**
 * Unmasked permission parameter, use a Short to represent a FsPermission.
 */
UserParam (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/resources/UserParam.java)/** User parameter. */
SSLConnectionConfigurator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/SSLConnectionConfigurator.java)/**
 * Configure a connection to use SSL authentication.
 */
TokenManagementDelegator (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/TokenAspect.java)/**
   * Callbacks for token management
   */
TokenAspect (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/TokenAspect.java)/**
 * This class implements the aspects that relate to delegation tokens for all
 * HTTP-based file system.
 */
URLConnectionFactory (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/URLConnectionFactory.java)/**
 * Utilities for handling URLs
 */
AbstractRunner (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java)/**
   * This class is for initialing a HTTP connection, connecting to server,
   * obtaining a response, and also handling retry on failures.
   */
AbstractFsPathRunner (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java)/**
   * Abstract base class to handle path-based operations with params
   */
FsPathRunner (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java)/**
   * Default path-based implementation expects no json response
   */
FsPathResponseRunner (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java)/**
   * Handle path-based operations with a json response
   */
FsPathBooleanRunner (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java)/**
   * Handle path-based operations with json boolean response
   */
FsPathOutputStreamRunner (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java)/**
   * Handle create/append output streams
   */
URLRunner (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java)/**
   * Used by open() which tracks the resolved url itself
   */
WebHdfsInputStream (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java)/**
   * This class is used for opening, reading, and seeking files while using the
   * WebHdfsFileSystem. This class will invoke the retry policy when performing
   * any of these actions.
   */
ReadRunner (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java)/**
   * This class will allow retries to occur for both open and read operations.
   * The first WebHdfsFileSystem#open creates a new WebHdfsInputStream object,
   * which creates a new ReadRunner object that will be used to open a
   * connection and read or seek into the input stream.
   *
   * ReadRunner is a subclass of the AbstractRunner class, which will run the
   * ReadRunner#getUrl(), ReadRunner#connect(URL), and ReadRunner#getResponse
   * methods within a retry loop, based on the configured retry policy.
   * ReadRunner#connect will create a connection if one has not already been
   * created. Otherwise, it will return the previously created connection
   * object. This is necessary because a new connection should not be created
   * for every read.
   * Likewise, ReadRunner#getUrl will construct a new URL object only if the
   * connection has not previously been established. Otherwise, it will return
   * the previously created URL object.
   * ReadRunner#getResponse will initialize the input stream if it has not
   * already been initialized and read the requested data from the specified
   * input stream.
   */
WebHdfsFileSystem (/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java)/** A FileSystem for HDFS over the web. */
TestUrlStreamHandlerFactory (/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/fs/TestUrlStreamHandlerFactory.java)/**
 * Test of the URL stream handler factory.
 */
TestXAttr (/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/fs/TestXAttr.java)/**
 * Tests for <code>XAttr</code> objects.
 */
TestBlockType (/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/protocol/TestBlockType.java)/**
 * Test the BlockType class.
 */
TestErasureCodingPolicy (/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/protocol/TestErasureCodingPolicy.java)/**
 * Test ErasureCodingPolicy.
 */
TestErasureCodingPolicyInfo (/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/protocol/TestErasureCodingPolicyInfo.java)/**
 * Test {@link ErasureCodingPolicyInfo}.
 */
TestHdfsFileStatusMethods (/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/protocol/TestHdfsFileStatusMethods.java)/**
 * Unit test verifying that {@link HdfsFileStatus} is a superset of
 * {@link FileStatus}.
 */
TestReadOnly (/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/protocol/TestReadOnly.java)/**
 * Testing class for {@link ReadOnly} annotation on {@link ClientProtocol}.
 */
TestConfiguredFailoverProxyProvider (/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestConfiguredFailoverProxyProvider.java)/**
 * Test {@link ConfiguredFailoverProxyProvider}.
 * This manages failover logic for a given set of nameservices/namenodes
 * (aka proxies).
 */
TestDefaultNameNodePort (/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/TestDefaultNameNodePort.java)/** Test NameNode port defaulting code. */
TestDFSOpsCountStatistics (/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/TestDFSOpsCountStatistics.java)/**
 * This tests basic operations of {@link DFSOpsCountStatistics} class.
 */
TestByteArrayManager (/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/util/TestByteArrayManager.java)/**
 * Test {@link ByteArrayManager}.
 */
TestECPolicyLoader (/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/util/TestECPolicyLoader.java)/**
 * Test load EC policy file.
 */
HttpFSDataInputStream (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/client/HttpFSFileSystem.java)/**
   * HttpFSServer subclass of the <code>FSDataInputStream</code>.
   * <p>
   * This implementation does not support the
   * <code>PositionReadable</code> and <code>Seekable</code> methods.
   */
HttpFSDataOutputStream (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/client/HttpFSFileSystem.java)/**
   * HttpFSServer subclass of the <code>FSDataOutputStream</code>.
   * <p>
   * This implementation closes the underlying HTTP connection validating the Http connection status
   * at closing time.
   */
HttpFSFileSystem (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/client/HttpFSFileSystem.java)/**
 * HttpFSServer implementation of the FileSystemAccess FileSystem.
 * <p>
 * This implementation allows a user to access HDFS over HTTP via a HttpFSServer server.
 */
HttpFSUtils (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/client/HttpFSUtils.java)/**
 * Utility methods used by HttpFS classes.
 */
HttpsFSFileSystem (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/client/HttpsFSFileSystem.java)/**
 * <p>HttpFSServer implementation of the FileSystemAccess FileSystem for SSL.
 * </p>
 * <p>This implementation allows a user to access HDFS over HTTPS via a
 * HttpFSServer server.</p>
 */
CheckUploadContentTypeFilter (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/CheckUploadContentTypeFilter.java)/**
 * Filter that Enforces the content-type to be application/octet-stream for
 * POST and PUT requests.
 */
FSAppend (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs an append FileSystemAccess files system operation.
   */
FSConcat (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a concat FileSystemAccess files system operation.
   */
FSTruncate (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a truncate FileSystemAccess files system operation.
   */
FSContentSummary (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a content-summary FileSystemAccess files system operation.
   */
FSQuotaUsage (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a quota-usage FileSystemAccess files system
   * operation.
   */
FSCreate (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a create FileSystemAccess files system operation.
   */
FSDelete (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a delete FileSystemAccess files system operation.
   */
FSFileChecksum (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a file-checksum FileSystemAccess files system operation.
   */
FSFileStatus (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a file-status FileSystemAccess files system operation.
   */
FSHomeDir (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a home-dir FileSystemAccess files system operation.
   */
FSListStatus (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a list-status FileSystemAccess files system operation.
   */
WrappedFileSystem (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
     * Simple wrapper filesystem that exposes the protected batched
     * listStatus API so we can use it.
     */
FSListStatusBatch (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a batched directory listing.
   */
FSMkdirs (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a mkdirs FileSystemAccess files system operation.
   */
FSOpen (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a open FileSystemAccess files system operation.
   */
FSRename (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a rename FileSystemAccess files system operation.
   */
FSSetOwner (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a set-owner FileSystemAccess files system operation.
   */
FSSetPermission (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a set-permission FileSystemAccess files system operation.
   */
FSSetAcl (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that sets the acl for a file in a FileSystem
   */
FSRemoveAcl (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that removes all acls from a file in a FileSystem
   */
FSModifyAclEntries (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that modifies acl entries for a file in a FileSystem
   */
FSRemoveAclEntries (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that removes acl entries from a file in a FileSystem
   */
FSRemoveDefaultAcl (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that removes the default acl from a directory in a FileSystem
   */
FSTrashRoot (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs getting trash root FileSystemAccess
   * files system operation.
   */
FSAclStatus (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that gets the ACL information for a given file.
   */
FSSetReplication (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a set-replication FileSystemAccess files system operation.
   */
FSSetTimes (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a set-times FileSystemAccess files system operation.
   */
FSSetXAttr (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a setxattr FileSystemAccess files system operation.
   */
FSRemoveXAttr (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a removexattr FileSystemAccess files system 
   * operation.
   */
FSListXAttrs (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs listing xattrs FileSystemAccess files system 
   * operation.
   */
FSGetXAttrs (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs getting xattrs FileSystemAccess files system 
   * operation.
   */
FSGetAllStoragePolicies (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a getAllStoragePolicies FileSystemAccess files
   * system operation.
   */
FSGetStoragePolicy (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a getStoragePolicy FileSystemAccess files system
   * operation.
   */
FSSetStoragePolicy (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a setStoragePolicy FileSystemAccess files system
   * operation.
   */
FSUnsetStoragePolicy (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   * Executor that performs a unsetStoragePolicy FileSystemAccess files system
   * operation.
   */
FSAllowSnapshot (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   *  Executor that performs an allowSnapshot operation.
   */
FSDisallowSnapshot (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   *  Executor that performs an disallowSnapshot operation.
   */
FSCreateSnapshot (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   *  Executor that performs a createSnapshot FileSystemAccess operation.
   */
FSDeleteSnapshot (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   *  Executor that performs a deleteSnapshot FileSystemAccess operation.
   */
FSRenameSnapshot (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   *  Executor that performs a renameSnapshot FileSystemAccess operation.
   */
FSGetSnapshotDiff (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   *  Executor that performs a getSnapshotDiff operation.
   */
FSGetSnapshottableDirListing (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
   *  Executor that performs a getSnapshottableDirListing operation.
   */
FSOperations (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java)/**
 * FileSystem operation executors used by {@link HttpFSServer}.
 */
HttpFSAuthenticationFilter (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSAuthenticationFilter.java)/**
 * Subclass of hadoop-auth <code>AuthenticationFilter</code> that obtains its
 * configuration from HttpFSServer's server configuration.
 */
HttpFSExceptionProvider (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSExceptionProvider.java)/**
 * JAX-RS <code>ExceptionMapper</code> implementation that maps HttpFSServer's
 * exceptions to HTTP status codes.
 */
AccessTimeParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for access-time parameter.
   */
BlockSizeParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for block-size parameter.
   */
DataParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for data parameter.
   */
NoRedirectParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for noredirect parameter.
   */
OperationParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for operation parameter.
   */
RecursiveParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for delete's recursive parameter.
   */
FilterParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for filter parameter.
   */
GroupParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for group parameter.
   */
LenParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for len parameter.
   */
ModifiedTimeParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for modified-time parameter.
   */
OffsetParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for offset parameter.
   */
NewLengthParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for newlength parameter.
   */
OverwriteParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for overwrite parameter.
   */
OwnerParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for owner parameter.
   */
PermissionParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for permission parameter.
   */
UnmaskedPermissionParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for unmaskedpermission parameter.
   */
AclPermissionParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for AclPermission parameter.
   */
ReplicationParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for replication parameter.
   */
SourcesParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for concat sources parameter.
   */
DestinationParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for to-path parameter.
   */
XAttrNameParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for xattr parameter.
   */
XAttrValueParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for xattr parameter.
   */
XAttrSetFlagParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for xattr parameter.
   */
XAttrEncodingParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for xattr parameter.
   */
StartAfterParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for startafter parameter.
   */
PolicyNameParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for policyName parameter.
   */
SnapshotNameParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for SnapshotName parameter.
   */
OldSnapshotNameParam (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
   * Class for OldSnapshotName parameter.
   */
HttpFSParametersProvider (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.java)/**
 * HttpFS ParametersProvider.
 */
HttpFSReleaseFilter (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSReleaseFilter.java)/**
 * Filter that releases FileSystemAccess filesystem instances upon HTTP request
 * completion.
 */
HttpFSServer (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java)/**
 * Main class of HttpFSServer server.
 * <p>
 * The <code>HttpFSServer</code> class uses Jersey JAX-RS to binds HTTP requests to the
 * different operations.
 */
HttpFSServerWebApp (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServerWebApp.java)/**
 * Bootstrap class that manages the initialization and destruction of the
 * HttpFSServer server, it is a <code>javax.servlet.ServletContextListener
 * </code> implementation that is wired in HttpFSServer's WAR
 * <code>WEB-INF/web.xml</code>.
 * <p>
 * It provides acces to the server context via the singleton {@link #get}.
 * <p>
 * All the configuration is loaded from configuration properties prefixed
 * with <code>httpfs.</code>.
 */
HttpFSServerWebServer (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServerWebServer.java)/**
 * The HttpFS web server.
 */
RunnableCallable (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/lang/RunnableCallable.java)/**
 * Adapter class that allows <code>Runnable</code>s and <code>Callable</code>s to
 * be treated as the other.
 */
ERROR (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/lang/XException.java)/**
   * Interface to define error codes.
   */
XException (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/lang/XException.java)/**
 * Generic exception that requires error codes and uses the a message
 * template from the error code.
 */
BaseService (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/server/BaseService.java)/**
 * Convenience class implementing the {@link Service} interface.
 */
Server (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/server/Server.java)/**
 * A Server class provides standard configuration, logging and {@link Service}
 * lifecyle management.
 * <p>
 * A Server normally has a home directory, a configuration directory, a temp
 * directory and logs directory.
 * <p>
 * The Server configuration is loaded from 2 overlapped files,
 * <code>#SERVER#-default.xml</code> and <code>#SERVER#-site.xml</code>. The
 * default file is loaded from the classpath, the site file is laoded from the
 * configuration directory.
 * <p>
 * The Server collects all configuration properties prefixed with
 * <code>#SERVER#</code>. The property names are then trimmed from the
 * <code>#SERVER#</code> prefix.
 * <p>
 * The Server log configuration is loaded from the
 * <code>#SERVICE#-log4j.properties</code> file in the configuration directory.
 * <p>
 * The lifecycle of server is defined in by {@link Server.Status} enum.
 * When a server is create, its status is UNDEF, when being initialized it is
 * BOOTING, once initialization is complete by default transitions to NORMAL.
 * The <code>#SERVER#.startup.status</code> configuration property can be used
 * to specify a different startup status (NORMAL, ADMIN or HALTED).
 * <p>
 * Services classes are defined in the <code>#SERVER#.services</code> and
 * <code>#SERVER#.services.ext</code> properties. They are loaded in order
 * (services first, then services.ext).
 * <p>
 * Before initializing the services, they are traversed and duplicate service
 * interface are removed from the service list. The last service using a given
 * interface wins (this enables a simple override mechanism).
 * <p>
 * After the services have been resoloved by interface de-duplication they are
 * initialized in order. Once all services are initialized they are
 * post-initialized (this enables late/conditional service bindings).
 */
ServerException (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/server/ServerException.java)/**
 * Exception thrown by the {@link Server} class.
 */
Service (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/server/Service.java)/**
 * Service interface for components to be managed by the {@link Server} class.
 */
ServiceException (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/server/ServiceException.java)/**
 * Exception thrown by {@link Service} implementations.
 */
FileSystemReleaseFilter (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/servlet/FileSystemReleaseFilter.java)/**
 * The <code>FileSystemReleaseFilter</code> releases back to the
 * {@link FileSystemAccess} service a <code>FileSystem</code> instance.
 * <p>
 * This filter is useful in situations where a servlet request
 * is streaming out HDFS data and the corresponding filesystem
 * instance have to be closed after the streaming completes.
 */
HostnameFilter (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/servlet/HostnameFilter.java)/**
 * Filter that resolves the requester hostname.
 */
MDCFilter (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/servlet/MDCFilter.java)/**
 * Filter that sets request contextual information for the slf4j MDC.
 * <p>
 * It sets the following values:
 * <ul>
 * <li>hostname: if the {@link HostnameFilter} is present and configured
 * before this filter</li>
 * <li>user: the <code>HttpServletRequest.getUserPrincipal().getName()</code></li>
 * <li>method: the HTTP method fo the request (GET, POST, ...)</li>
 * <li>path: the path of the request URL</li>
 * </ul>
 */
ServerWebApp (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/servlet/ServerWebApp.java)/**
 * {@link Server} subclass that implements <code>ServletContextListener</code>
 * and uses its lifecycle to start and stop the server.
 */
Check (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/util/Check.java)/**
 * Utility methods to check preconditions.
 * <p>
 * Commonly used for method arguments preconditions.
 */
ConfigurationUtils (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/util/ConfigurationUtils.java)/**
 * Configuration utilities.
 */
Parameters (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/wsrs/Parameters.java)/**
 * Class that contains all parsed JAX-RS parameters.
 * <p>
 * Instances are created by the {@link ParametersProvider} class.
 */
ParametersProvider (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/wsrs/ParametersProvider.java)/**
 * Jersey provider that parses the request parameters based on the
 * given parameter definition. 
 */
MockGroups (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServer.java)/**
   * Mock groups.
   */
TestHttpFSServer (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServer.java)/**
 * Main test class for HttpFSServer.
 */
TestHttpFSServerNoACLs (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServerNoACLs.java)/**
 * This test class ensures that everything works as expected when ACL
 * support is turned off HDFS.  This is the default configuration.  The other
 * tests operate with ACL support turned on.
 */
TestHttpFSServerNoXAttrs (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServerNoXAttrs.java)/**
 * This test class ensures that everything works as expected when XAttr
 * support is turned off HDFS.  This is the default configuration.  The other
 * tests operate with XAttr support turned on.
 */
TestHttpFSServerWebServer (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServerWebServer.java)/**
 * Test {@link HttpFSServerWebServer}.
 */
TestHttpFSServerWebServerWithRandomSecret (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServerWebServerWithRandomSecret.java)/**
 * Unlike {@link TestHttpFSServerWebServer}, httpfs-signature.secret doesn't
 * exist. In this case, a random secret is used.
 */
HadoopUsersConfTestHelper (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/test/HadoopUsersConfTestHelper.java)/**
 * Helper to configure FileSystemAccess user/group and proxyuser
 * configuration for testing using Java System properties.
 * <p/>
 * It uses the {@link SysPropsForTestsLoader} to load JavaSystem
 * properties for testing.
 */
Predicate (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/test/HTestCase.java)/**
   * A predicate 'closure' used by the {@link #waitFor(int, Predicate)} and
   * {@link #waitFor(int, boolean, Predicate)} methods.
   */
KerberosTestUtils (/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/test/KerberosTestUtils.java)/**
 * Test helper class for Java Kerberos setup.
 */
TestFuseDFS (/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/fuse-dfs/test/TestFuseDFS.java)/**
 * Basic functional tests on a fuse-dfs mount.
 */
NfsConfiguration (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/conf/NfsConfiguration.java)/**
 * Adds deprecated keys into the configuration.
 */
Mountd (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/mount/Mountd.java)/**
 * Main class for starting mountd daemon. This daemon implements the NFS
 * mount protocol. When receiving a MOUNT request from an NFS client, it checks
 * the request against the list of currently exported file systems. If the
 * client is permitted to mount the file system, rpc.mountd obtains a file
 * handle for requested directory and returns it to the client.
 */
RpcProgramMountd (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/mount/RpcProgramMountd.java)/**
 * RPC program corresponding to mountd daemon. See {@link Mountd}.
 */
WriteBackTask (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/AsyncDataService.java)/**
   * A task to write data back to HDFS for a file. Since only one thread can
   * write to a file, there should only be one task at any time for a file
   * (in queue or executing), and this should be guaranteed by the caller.
   */
AsyncDataService (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/AsyncDataService.java)/**
 * This class is a thread pool to easily schedule async data operations. Current
 * async data operation is write back operation. In the future, we could use it
 * for readahead operations too.
 */
DFSClientCache (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/DFSClientCache.java)/**
 * A cache saves DFSClient objects for different users.
 */
Nfs3 (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/Nfs3.java)/**
 * Nfs server. Supports NFS v3 using {@link RpcProgramNfs3}.
 * Currently Mountd program is also started inside this class.
 * Only TCP server is supported and UDP is not supported.
 */
Nfs3HttpServer (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/Nfs3HttpServer.java)/**
 * Encapsulates the HTTP server started by the NFS3 gateway.
 */
Nfs3Metrics (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/Nfs3Metrics.java)/**
 * This class is for maintaining the various NFS gateway activity statistics and
 * publishing them through the metrics interfaces.
 */
Nfs3Utils (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/Nfs3Utils.java)/**
 * Utility/helper methods related to NFS
 */
OffsetRange (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OffsetRange.java)/**
 * OffsetRange is the range of read/write request. A single point (e.g.,[5,5])
 * is not a valid range.
 */
OpenFileCtx (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java)/**
 * OpenFileCtx saves the context of one HDFS file output stream. Access to it is
 * synchronized by its member lock.
 */
StreamMonitor (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java)/**
   * StreamMonitor wakes up periodically to find and closes idle streams.
   */
OpenFileCtxCache (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java)/**
 * A cache saves OpenFileCtx objects for different users. Each cache entry is
 * used to maintain the writing context for a single file.
 */
PrivilegedNfsGatewayStarter (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/PrivilegedNfsGatewayStarter.java)/**
 * This class is used to allow the initial registration of the NFS gateway with
 * the system portmap daemon to come from a privileged (&lt; 1024) port. This is
 * necessary on certain operating systems to work around this bug in rpcbind:
 * 
 * Red Hat: https://bugzilla.redhat.com/show_bug.cgi?id=731542
 * SLES: https://bugzilla.novell.com/show_bug.cgi?id=823364
 * Debian: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=594880
 */
RpcProgramNfs3 (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java)/**
 * RPC program corresponding to nfs daemon. See {@link Nfs3}.
 */
WriteCtx (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java)/**
 * WriteCtx saves the context of one write request, such as request, channel,
 * xid and reply status.
 */
WriteManager (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java)/**
 * Manage the writes and responds asynchronously.
 */
TestReaddir (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestReaddir.java)/**
 * Test READDIR and READDIRPLUS request with zero, nonzero cookies
 */
TestRpcProgramNfs3 (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestRpcProgramNfs3.java)/**
 * Tests for {@link RpcProgramNfs3}
 */
TestViewfsWithNfs3 (/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestViewfsWithNfs3.java)/**
 * Tests for {@link RpcProgramNfs3} with
 * {@link org.apache.hadoop.fs.viewfs.ViewFileSystem}.
 */
RouterAdminProtocol (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/protocolPB/RouterAdminProtocol.java)/**
 * Protocol used by routeradmin to communicate with statestore.
 */
RouterAdminProtocolPB (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/protocolPB/RouterAdminProtocolPB.java)/**
 * Protocol that a clients use to communicate with the NameNode.
 * Note: This extends the protocolbuffer service based interface to
 * add annotations required for security.
 */
RouterAdminProtocolServerSideTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/protocolPB/RouterAdminProtocolServerSideTranslatorPB.java)/**
 * This class is used on the server side. Calls come across the wire for the for
 * protocol {@link RouterAdminProtocolPB}. This class translates the PB data
 * types to the native data types used inside the HDFS Router as specified in
 * the generic RouterAdminProtocol.
 */
RouterAdminProtocolTranslatorPB (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/protocolPB/RouterAdminProtocolTranslatorPB.java)/**
 * This class forwards NN's ClientProtocol calls as RPC calls to the NN server
 * while translating from the parameter types used in ClientProtocol to the
 * new PB types.
 */
RouterPolicyProvider (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/protocolPB/RouterPolicyProvider.java)/**
 * {@link HDFSPolicyProvider} for RBF protocols.
 */
FederationMBean (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/FederationMBean.java)/**
 * JMX interface for the federation statistics.
 */
FederationRPCMBean (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/FederationRPCMBean.java)/**
 * JMX interface for the RPC server.
 * TODO use the default RPC MBean.
 */
FederationRPCMetrics (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/FederationRPCMetrics.java)/**
 * Implementation of the RPC metrics collector.
 */
FederationRPCPerformanceMonitor (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/FederationRPCPerformanceMonitor.java)/**
 * Customizable RPC performance monitor. Receives events from the RPC server
 * and aggregates them via JMX.
 */
NamenodeBeanMetrics (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java)/**
 * Expose the Namenode metrics as the Router was one.
 */
NullStateStoreMetrics (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NullStateStoreMetrics.java)/**
 * Implementation of the State Store metrics which does not do anything.
 * This is used when the metrics are disabled (e.g., tests).
 */
RBFMetrics (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java)/**
 * Implementation of the Router metrics collector.
 */
RouterMBean (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RouterMBean.java)/**
 * JMX interface for the router specific metrics.
 */
StateStoreMBean (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/StateStoreMBean.java)/**
 * JMX interface for the State Store metrics.
 */
StateStoreMetrics (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/StateStoreMetrics.java)/**
 * Implementations of the JMX interface for the State Store metrics.
 */
ActiveNamenodeResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/ActiveNamenodeResolver.java)/**
 * Locates the most active NN for a given nameservice ID or blockpool ID. This
 * interface is used by the {@link
 * org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer
 * RouterRpcServer} to:
 * <ul>
 * <li>Determine the target NN for a given subcluster.
 * <li>List of all namespaces discovered/active in the federation.
 * <li>Update the currently active NN empirically.
 * </ul>
 * The interface is also used by the {@link
 * org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService
 * NamenodeHeartbeatService} to register a discovered NN.
 */
FederationNamenodeContext (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/FederationNamenodeContext.java)/**
 * Interface for a discovered NN and its current server endpoints.
 */
FederationNamespaceInfo (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/FederationNamespaceInfo.java)/**
 * Represents information about a single nameservice/namespace in a federated
 * HDFS cluster.
 */
FileSubclusterResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/FileSubclusterResolver.java)/**
 * Interface to map a file path in the global name space to a specific
 * subcluster and path in an HDFS name space.
 * <p>
 * Each path in the global/federated namespace may map to 1-N different HDFS
 * locations.  Each location specifies a single nameservice and a single HDFS
 * path.  The behavior is similar to MergeFS and Nfly and allows the merger
 * of multiple HDFS locations into a single path.  See HADOOP-8298 and
 * HADOOP-12077
 * <p>
 * For example, a directory listing will fetch listings for each destination
 * path and combine them into a single set of results.
 * <p>
 * When multiple destinations are available for a path, the destinations are
 * prioritized in a consistent manner.  This allows the proxy server to
 * guess the best/most likely destination and attempt it first.
 */
MembershipNamenodeResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MembershipNamenodeResolver.java)/**
 * Implements a cached lookup of the most recently active namenode for a
 * particular nameservice. Relies on the {@link StateStoreService} to
 * discover available nameservices and namenodes.
 */
MountTableManager (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableManager.java)/**
 * Manage a mount table.
 */
MountTableResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java)/**
 * Mount table to map between global paths and remote locations. This allows the
 * {@link org.apache.hadoop.hdfs.server.federation.router.Router Router} to map
 * the global HDFS view to the remote namespaces. This is similar to
 * {@link org.apache.hadoop.fs.viewfs.ViewFs ViewFs}.
 * This is implemented as a tree.
 */
MultipleDestinationMountTableResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MultipleDestinationMountTableResolver.java)/**
 * Mount table resolver that supports multiple locations for each mount entry.
 * The returned location contains prioritized remote paths from highest priority
 * to the lowest priority. Multiple locations for a mount point are optional.
 * When multiple locations are specified, both will be checked for the presence
 * of a file and the nameservice for a new file/dir is chosen based on the
 * results of a consistent hashing algorithm.
 * <p>
 * Does the Mount table entry for this path have multiple destinations?
 * <ul>
 * <li>No: Return the location
 * <li>Yes: Return all locations, prioritizing the best guess from the
 * consistent hashing algorithm.
 * </ul>
 * <p>
 * It has multiple options to order the locations: HASH (default), LOCAL,
 * RANDOM, and HASH_ALL.
 * <p>
 * The consistent hashing result is dependent on the number and combination of
 * nameservices that are registered for particular mount point. The order of
 * nameservices/locations in the mount table is not prioritized. Each consistent
 * hash calculation considers only the set of unique nameservices present for
 * the mount table location.
 */
NamenodePriorityComparator (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/NamenodePriorityComparator.java)/**
 * Compares NNs in the same namespace and prioritizes by their status. The
 * priorities are:
 * <ul>
 * <li>ACTIVE
 * <li>STANDBY
 * <li>UNAVAILABLE
 * </ul>
 * When two NNs have the same state, the last modification date is the tie
 * breaker, newest has priority. Expired NNs are excluded.
 */
NamenodeStatusReport (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/NamenodeStatusReport.java)/**
 * Status of the namenode.
 */
SubclusterAvailableSpace (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/AvailableSpaceResolver.java)/**
   * Inner class that stores cluster available space info.
   */
SubclusterSpaceComparator (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/AvailableSpaceResolver.java)/**
   * Customized comparator for SubclusterAvailableSpace. If more available
   * space the one cluster has, the higher priority it will have. But this
   * is not absolute, there is a balanced preference to make this use a higher
   * probability (instead of "always") to compare by this way.
   */
AvailableSpaceResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/AvailableSpaceResolver.java)/**
 * Order the destinations based on available space. This resolver uses a
 * higher probability (instead of "always") to choose the cluster with higher
 * available space.
 */
HashFirstResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/HashFirstResolver.java)/**
 * Variation of HashResolver that only uses the first level of the path.
 */
HashResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/HashResolver.java)/**
 * Order the destinations based on consistent hashing.
 */
LocalResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java)/**
 * The local subcluster (where the writer is) should be tried first. The writer
 * is defined from the RPC query received in the RPC server.
 */
OrderedResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/OrderedResolver.java)/**
 * Policy that decides which should be the first location accessed given
 * multiple destinations.
 */
RandomResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/RandomResolver.java)/**
 * Order the destinations randomly.
 */
RouterResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/RouterResolver.java)/**
 * The order resolver that depends upon the Router service.
 *
 * @param <K> The key type of subcluster mapping info queried from Router.
 * @param <V> The value type of subcluster mapping info queried from Router.
 */
PathLocation (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/PathLocation.java)/**
 * A map of the properties and target destinations (name space + path) for
 * a path in the global/federated name space.
 * This data is generated from the @see MountTable records.
 */
RemoteLocation (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/RemoteLocation.java)/**
 * A location in a remote namespace consisting of a nameservice ID and a HDFS
 * path (destination). It also contains the federated location (source).
 */
RouterResolveException (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/RouterResolveException.java)/**
 * Thrown by FileSubclusterResolver when a path can't be resolved.
 */
ConnectionContext (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionContext.java)/**
 * Context to track a connection in a {@link ConnectionPool}. When a client uses
 * a connection, it increments a counter to mark it as active. Once the client
 * is done with the connection, it decreases the counter. It also takes care of
 * closing the connection once is not active.
 *
 * The protocols currently used are:
 * <ul>
 * <li>{@link org.apache.hadoop.hdfs.protocol.ClientProtocol}
 * <li>{@link org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol}
 * </ul>
 */
CleanupTask (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java)/**
   * Removes stale connections not accessed recently from the pool. This is
   * invoked periodically.
   */
ConnectionCreator (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java)/**
   * Thread that creates connections asynchronously.
   */
ConnectionManager (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java)/**
 * Implements a pool of connections for the {@link Router} to be able to open
 * many connections to many Namenodes.
 */
ConnectionNullException (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionNullException.java)/**
 * Exception when can not get a non-null connection.
 */
ProtoImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionPool.java)/** Class to store the protocol implementation. */
ConnectionPool (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionPool.java)/**
 * Maintains a pool of connections for each User (including tokens) + NN. The
 * RPC client maintains a single socket, to achieve throughput similar to a NN,
 * each request is multiplexed across multiple sockets/connections from a
 * pool.
 */
ConnectionPoolId (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionPoolId.java)/**
 * Identifier for a connection for a user to a namenode.
 */
DFSRouter (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/DFSRouter.java)/**
 * Tool to start the {@link Router} for Router-based federation.
 */
ErasureCoding (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ErasureCoding.java)/**
 * Module that implements all the RPC calls in
 * {@link org.apache.hadoop.hdfs.protocol.ClientProtocol} related to
 * Erasure Coding in the {@link RouterRpcServer}.
 */
FederationUtil (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/FederationUtil.java)/**
 * Utilities for managing HDFS federation.
 */
IsRouterActiveServlet (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/IsRouterActiveServlet.java)/**
 * Detect if the Router is active and ready to serve requests.
 */
MountTableRefresherService (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/MountTableRefresherService.java)/**
 * This service is invoked from {@link MountTableStore} when there is change in
 * mount table entries and it updates mount table entry cache on local router as
 * well as on all remote routers. Refresh on local router is done by calling
 * {@link MountTableStore#loadCache(boolean)}} API directly, no RPC call
 * involved, but on remote routers refresh is done through RouterClient(RPC
 * call). To improve performance, all routers are refreshed in separate thread
 * and all connection are cached. Cached connections are removed from
 * cache and closed when their max live time is elapsed.
 */
MountTableRefresherThread (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/MountTableRefresherThread.java)/**
 * Base class for updating mount table cache on all the router.
 */
NamenodeHeartbeatService (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java)/**
 * The {@link Router} periodically checks the state of a Namenode (usually on
 * the same server) and reports their high availability (HA) state and
 * load/space status to the
 * {@link org.apache.hadoop.hdfs.server.federation.store.StateStoreService}
 * . Note that this is an optional role as a Router can be independent of any
 * subcluster.
 * <p>
 * For performance with Namenode HA, the Router uses the high availability state
 * information in the State Store to forward the request to the Namenode that is
 * most likely to be active.
 * <p>
 * Note that this service can be embedded into the Namenode itself to simplify
 * the operation.
 */
NameserviceManager (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NameserviceManager.java)/**
 * Interface for enable/disable name service.
 */
NoNamenodesAvailableException (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NoNamenodesAvailableException.java)/**
 * Exception when no namenodes are available.
 */
PeriodicService (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/PeriodicService.java)/**
 * Service to periodically execute a runnable.
 */
Quota (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Quota.java)/**
 * Module that implements the quota relevant RPC calls
 * {@link ClientProtocol#setQuota(String, long, long, StorageType)}
 * and
 * {@link ClientProtocol#getQuotaUsage(String)}
 * in the {@link RouterRpcServer}.
 */
RBFConfigKeys (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RBFConfigKeys.java)/**
 * Config fields for router-based hdfs federation.
 */
RemoteLocationContext (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RemoteLocationContext.java)/**
 * Base class for objects that are unique to a namespace.
 */
RemoteMethod (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RemoteMethod.java)/**
 * Determines the remote client protocol method and the parameter list for a
 * specific location.
 */
RemoteParam (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RemoteParam.java)/**
 * A dynamically assignable parameter that is location-specific.
 * <p>
 * There are 2 ways this mapping is determined:
 * <ul>
 * <li>Default: Uses the RemoteLocationContext's destination
 * <li>Map: Uses the value of the RemoteLocationContext key provided in the
 * parameter map.
 * </ul>
 */
RemoteResult (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RemoteResult.java)/**
 * Result from a remote location.
 * It includes the exception if there was any error.
 * @param <T> Type of the remote location.
 * @param <R> Type of the result.
 */
Router (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java)/**
 * Router that provides a unified view of multiple federated HDFS clusters. It
 * has two main roles: (1) federated interface and (2) NameNode heartbeat.
 * <p>
 * For the federated interface, the Router receives a client request, checks the
 * State Store for the correct subcluster, and forwards the request to the
 * active Namenode of that subcluster. The reply from the Namenode then flows in
 * the opposite direction. The Routers are stateless and can be behind a load
 * balancer. HDFS clients connect to the router using the same interfaces as are
 * used to communicate with a namenode, namely the ClientProtocol RPC interface
 * and the WebHdfs HTTP interface exposed by the router. {@link RouterRpcServer}
 * {@link RouterHttpServer}
 * <p>
 * For NameNode heartbeat, the Router periodically checks the state of a
 * NameNode (usually on the same server) and reports their high availability
 * (HA) state and load/space status to the State Store. Note that this is an
 * optional role as a Router can be independent of any subcluster.
 * {@link StateStoreService} {@link NamenodeHeartbeatService}
 */
RouterAdminServer (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java)/**
 * This class is responsible for handling all of the Admin calls to the HDFS
 * router. It is created, started, and stopped by {@link Router}.
 */
RouterCacheAdmin (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterCacheAdmin.java)/**
 * Module that implements all the RPC calls in
 * {@link org.apache.hadoop.hdfs.protocol.ClientProtocol} related to Cache Admin
 * in the {@link RouterRpcServer}.
 */
RouterClient (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClient.java)/**
 * Client to connect to the {@link Router} via the admin protocol.
 */
RouterClientProtocol (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java)/**
 * Module that implements all the RPC calls in {@link ClientProtocol} in the
 * {@link RouterRpcServer}.
 */
RouterHeartbeatService (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterHeartbeatService.java)/**
 * Service to periodically update the Router current state in the State Store.
 */
RouterHttpServer (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterHttpServer.java)/**
 * Web interface for the {@link Router}. It exposes the Web UI and the WebHDFS
 * methods from {@link RouterWebHdfsMethods}.
 */
RouterMetrics (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterMetrics.java)/**
 * This class is for maintaining the various Router activity statistics
 * and publishing them through the metrics interfaces.
 */
RouterMetricsService (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterMetricsService.java)/**
 * Service to manage the metrics of the Router.
 */
RouterNamenodeProtocol (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterNamenodeProtocol.java)/**
 * Module that implements all the RPC calls in {@link NamenodeProtocol} in the
 * {@link RouterRpcServer}.
 */
RouterPermissionChecker (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterPermissionChecker.java)/**
 * Class that helps in checking permissions in Router-based federation.
 */
RouterQuotaManager (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterQuotaManager.java)/**
 * Router quota manager in Router. The manager maintains
 * {@link RouterQuotaUsage} cache of mount tables and do management
 * for the quota caches.
 */
RouterQuotaUpdateService (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterQuotaUpdateService.java)/**
 * Service to periodically update the {@link RouterQuotaUsage}
 * cached information in the {@link Router} and update corresponding
 * mount table in State Store.
 */
Builder (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterQuotaUsage.java)/** Build the instance based on the builder. */
RouterQuotaUsage (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterQuotaUsage.java)/**
 * The subclass of {@link QuotaUsage} used in Router-based federation.
 */
RouterRpcClient (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java)/**
 * A client proxy for Router to NN communication using the NN ClientProtocol.
 * <p>
 * Provides routers to invoke remote ClientProtocol methods and handle
 * retries/failover.
 * <ul>
 * <li>invokeSingle Make a single request to a single namespace
 * <li>invokeSequential Make a sequential series of requests to multiple
 * ordered namespaces until a condition is met.
 * <li>invokeConcurrent Make concurrent requests to multiple namespaces and
 * return all of the results.
 * </ul>
 * Also maintains a cached pool of connections to NNs. Connections are managed
 * by the ConnectionManager and are unique to each user + NN. The size of the
 * connection pool can be configured. Larger pools allow for more simultaneous
 * requests to a single NN from a single user.
 */
RouterRpcMonitor (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcMonitor.java)/**
 * Metrics and monitoring interface for the router RPC server. Allows pluggable
 * diagnostics and monitoring services to be attached.
 */
RouterRpcServer (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java)/**
 * This class is responsible for handling all of the RPC calls to the It is
 * created, started, and stopped by {@link Router}. It implements the
 * {@link ClientProtocol} to mimic a
 * {@link org.apache.hadoop.hdfs.server.namenode.NameNode NameNode} and proxies
 * the requests to the active
 * {@link org.apache.hadoop.hdfs.server.namenode.NameNode NameNode}.
 */
RouterSafemodeService (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSafemodeService.java)/**
 * Service to periodically check if the {@link
 * org.apache.hadoop.hdfs.server.federation.store.StateStoreService
 * StateStoreService} cached information in the {@link Router} is up to date.
 * This is for performance and removes the {@link
 * org.apache.hadoop.hdfs.server.federation.store.StateStoreService
 * StateStoreService} from the critical path in common operations.
 */
RouterSnapshot (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSnapshot.java)/**
 * Module that implements all the RPC calls related to snapshots in
 * {@link ClientProtocol} in the {@link RouterRpcServer}.
 */
RouterStateManager (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterStateManager.java)/**
 * Interface of managing the Router state.
 */
RouterStoragePolicy (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterStoragePolicy.java)/**
 * Module that implements all the RPC calls in
 * {@link org.apache.hadoop.hdfs.protocol.ClientProtocol} related to
 * Storage Policy in the {@link RouterRpcServer}.
 */
RouterUserProtocol (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterUserProtocol.java)/**
 * Module that implements all the RPC calls in
 * {@link RefreshUserMappingsProtocol} {@link GetUserMappingsProtocol} in the
 * {@link RouterRpcServer}.
 */
RouterWebHdfsMethods (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java)/**
 * WebHDFS Router implementation. This is an extension of
 * {@link NamenodeWebHdfsMethods}, and tries to reuse as much as possible.
 */
RouterSecurityManager (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/RouterSecurityManager.java)/**
 * Manager to hold underlying delegation token secret manager implementations.
 */
ZKDelegationTokenSecretManagerImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/token/ZKDelegationTokenSecretManagerImpl.java)/**
 * Zookeeper based router delegation token store implementation.
 */
SubClusterTimeoutException (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/SubClusterTimeoutException.java)/**
 * Exception when timing out waiting for the reply of a subcluster.
 */
CachedRecordStore (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/CachedRecordStore.java)/**
 * Record store that takes care of caching the records in memory.
 *
 * @param <R> Record to store by this interface.
 */
DisabledNameserviceStore (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/DisabledNameserviceStore.java)/**
 * State store record to track disabled name services.
 */
StateStoreBaseImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreBaseImpl.java)/**
 * Base implementation of a State Store driver. It contains default
 * implementations for the optional functions. These implementations use an
 * uncached read/write all algorithm for all changes. In most cases it is
 * recommended to override the optional functions.
 * <p>
 * Drivers may optionally override additional routines for performance
 * optimization, such as custom get/put/remove queries, depending on the
 * capabilities of the data store.
 */
StateStoreFileBaseImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java)/**
 * {@link StateStoreDriver} implementation based on files. In this approach, we
 * use temporary files for the writes and renaming "atomically" to the final
 * value. Instead of writing to the final location, it will go to a temporary
 * one and then rename to the final destination.
 */
StateStoreFileImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileImpl.java)/**
 * StateStoreDriver implementation based on a local file.
 */
StateStoreFileSystemImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileSystemImpl.java)/**
 * {@link StateStoreDriver} implementation based on a filesystem. The common
 * implementation uses HDFS as a backend. The path can be specified setting
 * dfs.federation.router.driver.fs.path=hdfs://host:port/path/to/store.
 */
StateStoreSerializableImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreSerializableImpl.java)/**
 * State Store driver that stores a serialization of the records. The serializer
 * is pluggable.
 */
StateStoreSerializerPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreSerializerPBImpl.java)/**
 * Protobuf implementation of the State Store serializer.
 */
StateStoreZooKeeperImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java)/**
 * {@link StateStoreDriver} driver implementation that uses ZooKeeper as a
 * backend.
 * <p>
 * The structure of the znodes in the ensemble is:
 * PARENT_PATH
 * |--- MOUNT
 * |--- MEMBERSHIP
 * |--- REBALANCER
 * |--- ROUTERS
 */
StateStoreDriver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/StateStoreDriver.java)/**
 * Driver class for an implementation of a {@link StateStoreService}
 * provider. Driver implementations will extend this class and implement some of
 * the default methods.
 */
StateStoreRecordOperations (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/StateStoreRecordOperations.java)/**
 * Operations for a driver to manage records in the State Store.
 */
StateStoreSerializer (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/StateStoreSerializer.java)/**
 * Serializer to store and retrieve data in the State Store.
 */
DisabledNameserviceStoreImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/impl/DisabledNameserviceStoreImpl.java)/**
 * Implementation of {@link DisabledNameserviceStore}.
 */
MembershipStoreImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/impl/MembershipStoreImpl.java)/**
 * Implementation of the {@link MembershipStore} State Store API.
 */
MountTableStoreImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/impl/MountTableStoreImpl.java)/**
 * Implementation of the {@link MountTableStore} state store API.
 */
RouterStoreImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/impl/RouterStoreImpl.java)/**
 * Implementation of the {@link RouterStore} state store API.
 */
MembershipStore (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/MembershipStore.java)/**
 * Management API for NameNode registrations stored in {@link
 * org.apache.hadoop.hdfs.server.federation.store.records.MembershipState
 * MembershipState} records. The {@link
 * org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService
 * RouterHeartbeatService} periodically polls each NN to update the NameNode
 * metadata(addresses, operational) and HA state(active, standby). Each
 * NameNode may be polled by multiple
 * {@link org.apache.hadoop.hdfs.server.federation.router.Router Router}
 * instances.
 * <p>
 * Once fetched from the
 * {@link org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver
 * StateStoreDriver}, NameNode registrations are cached until the next query.
 * The fetched registration data is aggregated using a quorum to determine the
 * best/most accurate state for each NameNode. The cache is periodically updated
 * by the @{link StateStoreCacheUpdateService}.
 */
MountTableStore (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/MountTableStore.java)/**
 * Management API for the HDFS mount table information stored in
 * {@link org.apache.hadoop.hdfs.server.federation.store.records.MountTable
 * MountTable} records. The mount table contains entries that map a particular
 * global namespace path one or more HDFS nameservices (NN) + target path. It is
 * possible to map mount locations for root folders, directories or individual
 * files.
 * <p>
 * Once fetched from the
 * {@link org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver
 * StateStoreDriver}, MountTable records are cached in a tree for faster access.
 * Each path in the global namespace is mapped to a nameserivce ID and local
 * path upon request. The cache is periodically updated by the @{link
 * StateStoreCacheUpdateService}.
 */
AddMountTableEntryRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/AddMountTableEntryRequest.java)/**
 * API request for adding a mount table entry to the state store.
 */
AddMountTableEntryResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/AddMountTableEntryResponse.java)/**
 * API response for adding a mount table entry to the state store.
 */
DisableNameserviceRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/DisableNameserviceRequest.java)/**
 * API request for disabling a name service and updating its state in the
 * State Store.
 */
DisableNameserviceResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/DisableNameserviceResponse.java)/**
 * API response for disabling a name service and updating its state in the
 * State Store.
 */
EnableNameserviceRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/EnableNameserviceRequest.java)/**
 * API request for enabling a name service and updating its state in the
 * State Store.
 */
EnableNameserviceResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/EnableNameserviceResponse.java)/**
 * API response for enabling a name service and updating its state in the
 * State Store.
 */
EnterSafeModeRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/EnterSafeModeRequest.java)/**
 * API request for the Router entering safe mode state and updating
 * its state in State Store.
 */
EnterSafeModeResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/EnterSafeModeResponse.java)/**
 * API response for the Router entering safe mode state and updating
 * its state in State Store.
 */
GetDestinationRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetDestinationRequest.java)/**
 * API request for getting the destination subcluster of a file.
 */
GetDestinationResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetDestinationResponse.java)/**
 * API response for getting the destination subcluster of a file.
 */
GetDisabledNameservicesRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetDisabledNameservicesRequest.java)/**
 * API request for getting the disabled name services.
 */
GetDisabledNameservicesResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetDisabledNameservicesResponse.java)/**
 * API response for getting the disabled nameservices in the state store.
 */
GetMountTableEntriesRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetMountTableEntriesRequest.java)/**
 * API request for listing mount table entries present in the state store.
 */
GetMountTableEntriesResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetMountTableEntriesResponse.java)/**
 * API response for listing mount table entries present in the state store.
 */
GetNamenodeRegistrationsRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetNamenodeRegistrationsRequest.java)/**
 * API request for listing namenode registrations present in the state store.
 */
GetNamenodeRegistrationsResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetNamenodeRegistrationsResponse.java)/**
 * API response for listing namenode registrations present in the state store.
 */
GetNamespaceInfoRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetNamespaceInfoRequest.java)/**
 * API response for listing HDFS namespaces present in the state store.
 */
GetNamespaceInfoResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetNamespaceInfoResponse.java)/**
 * API response for listing HDFS namespaces present in the state store.
 */
GetRouterRegistrationRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetRouterRegistrationRequest.java)/**
 * API request for retrieving a single router registration present in the state
 * store.
 */
GetRouterRegistrationResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetRouterRegistrationResponse.java)/**
 * API response for retrieving a single router registration present in the state
 * store.
 */
GetRouterRegistrationsRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetRouterRegistrationsRequest.java)/**
 * API request for retrieving a all non-expired router registrations present in
 * the state store.
 */
GetRouterRegistrationsResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetRouterRegistrationsResponse.java)/**
 * API response for retrieving a all non-expired router registrations present in
 * the state store.
 */
GetSafeModeRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetSafeModeRequest.java)/**
 * API request for verifying if current Router state is safe mode.
 */
GetSafeModeResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/GetSafeModeResponse.java)/**
 * API response for verifying if current Router state is safe mode.
 */
AddMountTableEntryRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/AddMountTableEntryRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * AddMountTableEntryRequest.
 */
AddMountTableEntryResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/AddMountTableEntryResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * AddMountTableEntryResponse.
 */
DisableNameserviceRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/DisableNameserviceRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * DisableNameserviceRequest.
 */
DisableNameserviceResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/DisableNameserviceResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * {@link DisableNameserviceResponse}.
 */
EnableNameserviceRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/EnableNameserviceRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * EnableNameserviceRequest.
 */
EnableNameserviceResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/EnableNameserviceResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * EnableNameserviceResponse.
 */
EnterSafeModeRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/EnterSafeModeRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * EnterSafeModeRequest.
 */
EnterSafeModeResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/EnterSafeModeResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * EnterSafeModeResponse.
 */
FederationProtocolPBTranslator (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/FederationProtocolPBTranslator.java)/**
 * Helper class for setting/getting data elements in an object backed by a
 * protobuf implementation.
 */
GetDestinationRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetDestinationRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetDestinationRequest.
 */
GetDestinationResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetDestinationResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetDestinationResponse.
 */
GetDisabledNameservicesRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetDisabledNameservicesRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetDisabledNameservicesRequest.
 */
GetDisabledNameservicesResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetDisabledNameservicesResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetDisabledNameservicesResponse.
 */
GetMountTableEntriesRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetMountTableEntriesRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetMountTableEntriesRequest.
 */
GetMountTableEntriesResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetMountTableEntriesResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetMountTableEntriesResponse.
 */
GetNamenodeRegistrationsRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetNamenodeRegistrationsRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetNamenodeRegistrationsRequest.
 */
GetNamenodeRegistrationsResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetNamenodeRegistrationsResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetNamenodeRegistrationsResponse.
 */
GetNamespaceInfoRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetNamespaceInfoRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetNamespaceInfoRequest.
 */
GetNamespaceInfoResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetNamespaceInfoResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetNamespaceInfoResponse.
 */
GetRouterRegistrationRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetRouterRegistrationRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetRouterRegistrationRequest.
 */
GetRouterRegistrationResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetRouterRegistrationResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetRouterRegistrationResponse.
 */
GetRouterRegistrationsRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetRouterRegistrationsRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetRouterRegistrationsRequest.
 */
GetRouterRegistrationsResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetRouterRegistrationsResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetRouterRegistrationsResponse.
 */
GetSafeModeRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetSafeModeRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetSafeModeRequest.
 */
GetSafeModeResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/GetSafeModeResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * GetSafeModeResponse.
 */
LeaveSafeModeRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/LeaveSafeModeRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * LeaveSafeModeRequest.
 */
LeaveSafeModeResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/LeaveSafeModeResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * LeaveSafeModeResponse.
 */
NamenodeHeartbeatRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/NamenodeHeartbeatRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * NamenodeHeartbeatRequest.
 */
NamenodeHeartbeatResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/NamenodeHeartbeatResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * NamenodeHeartbeatResponse.
 */
RefreshMountTableEntriesRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/RefreshMountTableEntriesRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * RefreshMountTableEntriesRequest.
 */
RefreshMountTableEntriesResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/RefreshMountTableEntriesResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * RefreshMountTableEntriesResponse.
 */
RemoveMountTableEntryRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/RemoveMountTableEntryRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * RemoveMountTableEntryRequest.
 */
RemoveMountTableEntryResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/RemoveMountTableEntryResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * RemoveMountTableEntryResponse.
 */
RouterHeartbeatRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/RouterHeartbeatRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * RouterHeartbeatRequest.
 */
RouterHeartbeatResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/RouterHeartbeatResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * RouterHeartbeatResponse.
 */
UpdateMountTableEntryRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/UpdateMountTableEntryRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * UpdateMountTableEntryRequest.
 */
UpdateMountTableEntryResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/UpdateMountTableEntryResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * UpdateMountTableEntryResponse.
 */
UpdateNamenodeRegistrationRequestPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/UpdateNamenodeRegistrationRequestPBImpl.java)/**
 * Protobuf implementation of the state store API object
 * OverrideNamenodeRegistrationRequest.
 */
UpdateNamenodeRegistrationResponsePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/UpdateNamenodeRegistrationResponsePBImpl.java)/**
 * Protobuf implementation of the state store API object
 * OverrideNamenodeRegistrationResponse.
 */
LeaveSafeModeRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/LeaveSafeModeRequest.java)/**
 * API request for the Router leaving safe mode state and updating
 * its state in State Store.
 */
LeaveSafeModeResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/LeaveSafeModeResponse.java)/**
 * API response for the Router leaving safe mode state and updating
 * its state in State Store.
 */
NamenodeHeartbeatRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/NamenodeHeartbeatRequest.java)/**
 * API request for registering a namenode with the state store.
 */
NamenodeHeartbeatResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/NamenodeHeartbeatResponse.java)/**
 * API response for registering a namenode with the state store.
 */
RefreshMountTableEntriesRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/RefreshMountTableEntriesRequest.java)/**
 * API request for refreshing mount table cached entries from state store.
 */
RefreshMountTableEntriesResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/RefreshMountTableEntriesResponse.java)/**
 * API response for refreshing mount table entries cache from state store.
 */
RemoveMountTableEntryRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/RemoveMountTableEntryRequest.java)/**
 * API request for removing a mount table path present in the state store.
 */
RemoveMountTableEntryResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/RemoveMountTableEntryResponse.java)/**
 * API response for removing a mount table path present in the state store.
 */
RouterHeartbeatRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/RouterHeartbeatRequest.java)/**
 * API request for registering a router with the state store.
 */
RouterHeartbeatResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/RouterHeartbeatResponse.java)/**
 * API response for registering a router with the state store.
 */
UpdateMountTableEntryRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/UpdateMountTableEntryRequest.java)/**
 * API request for updating the destination of an existing mount point in the
 * state store.
 */
UpdateMountTableEntryResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/UpdateMountTableEntryResponse.java)/**
 * API response for updating the destination of an existing mount point in the
 * state store.
 */
UpdateNamenodeRegistrationRequest (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/UpdateNamenodeRegistrationRequest.java)/**
 * API request for overriding an existing namenode registration in the state
 * store.
 */
UpdateNamenodeRegistrationResponse (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/protocol/UpdateNamenodeRegistrationResponse.java)/**
 * API response for overriding an existing namenode registration in the state
 * store.
 */
BaseRecord (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/BaseRecord.java)/**
 * Abstract base of a data record in the StateStore. All StateStore records are
 * derived from this class. Data records are persisted in the data store and
 * are identified by their primary key. Each data record contains:
 * <ul>
 * <li>A primary key consisting of a combination of record data fields.
 * <li>A modification date.
 * <li>A creation date.
 * </ul>
 */
DisabledNameservice (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/DisabledNameservice.java)/**
 * Data record indicating a specific name service ID has been disabled and
 * is no longer valid. Allows quick disabling of name services.
 */
DisabledNameservicePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/DisabledNameservicePBImpl.java)/**
 * Protobuf implementation of the {@link DisabledNameservice} record.
 */
MembershipStatePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/MembershipStatePBImpl.java)/**
 * Protobuf implementation of the MembershipState record.
 */
MembershipStatsPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/MembershipStatsPBImpl.java)/**
 * Protobuf implementation of the MembershipStats record.
 */
MountTablePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/MountTablePBImpl.java)/**
 * Protobuf implementation of the MountTable record.
 */
PBRecord (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/PBRecord.java)/**
 * A record implementation using Protobuf.
 */
RouterStatePBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/RouterStatePBImpl.java)/**
 * Protobuf implementation of the RouterState record.
 */
StateStoreVersionPBImpl (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/StateStoreVersionPBImpl.java)/**
 * Protobuf implementation of the StateStoreVersion record.
 */
MembershipState (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/MembershipState.java)/**
 * Data schema for storing NN registration information in the
 * {@link org.apache.hadoop.hdfs.server.federation.store.StateStoreService
 * FederationStateStoreService}.
 */
MembershipStats (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/MembershipStats.java)/**
 * Data schema for storing NN stats in the
 * {@link org.apache.hadoop.hdfs.server.federation.store.StateStoreService
 * StateStoreService}.
 */
MountTable (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/MountTable.java)/**
 * Data schema for {@link
 * org.apache.hadoop.hdfs.server.federation.store.MountTableStore
 * FederationMountTableStore} data stored in the {@link
 * org.apache.hadoop.hdfs.server.federation.store.StateStoreService
 * FederationStateStoreService}. Supports string serialization.
 */
Query (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/Query.java)/**
 * Check if a record matches a query. The query is usually a partial record.
 *
 * @param <T> Type of the record to query.
 */
QueryResult (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/QueryResult.java)/**
 * Encapsulates a state store query result that includes a set of records and a
 * time stamp for the result.
 */
RouterState (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/RouterState.java)/**
 * Entry to log the state of a
 * {@link org.apache.hadoop.hdfs.server.federation.router.Router Router} in the
 * {@link org.apache.hadoop.hdfs.server.federation.store.StateStoreService
 * FederationStateStoreService}.
 */
StateStoreVersion (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/records/StateStoreVersion.java)/**
 * Entry to track the version of the State Store data stored in the State Store
 * by a Router.
 */
RecordStore (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/RecordStore.java)/**
 * Store records in the State Store. Subclasses provide interfaces to operate on
 * those records.
 *
 * @param <R> Record to store by this interface.
 */
RouterStore (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/RouterStore.java)/**
 * Management API for {@link
 * org.apache.hadoop.hdfs.server.federation.store.records.RouterState
 * RouterState} records in the state store. Accesses the data store via the
 * {@link org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver
 * StateStoreDriver} interface. No data is cached.
 */
StateStoreCache (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreCache.java)/**
 * Interface for a cached copy of the State Store.
 */
StateStoreCacheUpdateService (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreCacheUpdateService.java)/**
 * Service to periodically update the {@link StateStoreService}
 * cached information in the
 * {@link org.apache.hadoop.hdfs.server.federation.router.Router Router}.
 * This is for performance and removes the State Store from the critical path
 * in common operations.
 */
StateStoreConnectionMonitorService (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreConnectionMonitorService.java)/**
 * Service to periodically monitor the connection of the StateStore
 * {@link StateStoreService} data store and to re-open the connection
 * to the data store if required.
 */
StateStoreService (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreService.java)/**
 * A service to initialize a
 * {@link org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver
 * StateStoreDriver} and maintain the connection to the data store. There are
 * multiple state store driver connections supported:
 * <ul>
 * <li>File {@link
 * org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl
 * StateStoreFileImpl}
 * <li>ZooKeeper {@link
 * org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl
 * StateStoreZooKeeperImpl}
 * </ul>
 * <p>
 * The service also supports the dynamic registration of record stores like:
 * <ul>
 * <li>{@link MembershipStore}: state of the Namenodes in the
 * federation.
 * <li>{@link MountTableStore}: Mount table between to subclusters.
 * See {@link org.apache.hadoop.fs.viewfs.ViewFs ViewFs}.
 * <li>{@link RouterStore}: Router state in the federation.
 * <li>{@link DisabledNameserviceStore}: Disabled name services.
 * </ul>
 */
StateStoreUnavailableException (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreUnavailableException.java)/**
 * Thrown when the state store is not reachable or available. Cached APIs and
 * queries may succeed. Client should retry again later.
 */
StateStoreUtils (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreUtils.java)/**
 * Set of utility functions used to work with the State Store.
 */
ConsistentHashRing (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/utils/ConsistentHashRing.java)/**
 * Consistent hash ring to distribute items across nodes (locations). If we add
 * or remove nodes, it minimizes the item migration.
 */
ACLEntity (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/tools/federation/RouterAdmin.java)/**
   * Inner class that stores ACL info of mount table.
   */
RouterAdmin (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/tools/federation/RouterAdmin.java)/**
 * This class provides some Federation administrative access shell commands.
 */
RouterHDFSContract (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/RouterHDFSContract.java)/**
 * The contract of Router-based Federated HDFS.
 */
SecurityConfUtil (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/SecurityConfUtil.java)/**
 * Test utility to provide a standard routine to initialize the configuration
 * for secure RBF HDFS cluster.
 */
TestRouterHDFSContractAppend (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractAppend.java)/**
 * Test append operations on the Router-based FS.
 */
TestRouterHDFSContractAppendSecure (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractAppendSecure.java)/**
 * Test secure append operations on the Router-based FS.
 */
TestRouterHDFSContractConcat (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractConcat.java)/**
 * Test concat operations on the Router-based FS.
 */
TestRouterHDFSContractConcatSecure (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractConcatSecure.java)/**
 * Test secure concat operations on the Router-based FS.
 */
TestRouterHDFSContractCreate (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractCreate.java)/**
 * Test create operations on the Router-based FS.
 */
TestRouterHDFSContractCreateSecure (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractCreateSecure.java)/**
 * Test secure create operations on the Router-based FS.
 */
TestRouterHDFSContractDelegationToken (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractDelegationToken.java)/**
 * Test to verify router contracts for delegation token operations.
 */
TestRouterHDFSContractDelete (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractDelete.java)/**
 * Test delete operations on the Router-based FS.
 */
TestRouterHDFSContractDeleteSecure (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractDeleteSecure.java)/**
 * Test secure delete operations on the Router-based FS.
 */
TestRouterHDFSContractGetFileStatus (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractGetFileStatus.java)/**
 * Test get file status operations on the Router-based FS.
 */
TestRouterHDFSContractGetFileStatusSecure (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractGetFileStatusSecure.java)/**
 * Test secure get file status operations on the Router-based FS.
 */
TestRouterHDFSContractMkdir (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractMkdir.java)/**
 * Test dir operations on the Router-based FS.
 */
TestRouterHDFSContractMkdirSecure (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractMkdirSecure.java)/**
 * Test secure dir operations on the Router-based FS.
 */
TestRouterHDFSContractOpen (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractOpen.java)/**
 * Test open operations on the Router-based FS.
 */
TestRouterHDFSContractOpenSecure (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractOpenSecure.java)/**
 * Test secure open operations on the Router-based FS.
 */
TestRouterHDFSContractRename (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractRename.java)/**
 * Test rename operations on the Router-based FS.
 */
TestRouterHDFSContractRenameSecure (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractRenameSecure.java)/**
 * Test secure rename operations on the Router-based FS.
 */
TestRouterHDFSContractRootDirectory (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractRootDirectory.java)/**
 * Test root dir operations on the Router-based FS.
 */
TestRouterHDFSContractRootDirectorySecure (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractRootDirectorySecure.java)/**
 * Test secure root dir operations on the Router-based FS.
 */
TestRouterHDFSContractSeek (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractSeek.java)/**
 * Test seek operations on the Router-based FS.
 */
TestRouterHDFSContractSeekSecure (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractSeekSecure.java)/**
 * Test secure seek operations on the Router-based FS.
 */
TestRouterHDFSContractSetTimes (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractSetTimes.java)/**
 * Test set times operations on the Router-based FS.
 */
TestRouterHDFSContractSetTimesSecure (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/TestRouterHDFSContractSetTimesSecure.java)/**
 * Test secure set times operations on the Router-based FS.
 */
RouterWebHDFSContract (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/web/RouterWebHDFSContract.java)/**
 * The contract of Router-based Federated HDFS
 * This changes its feature set from platform for platform -the default
 * set is updated during initialization.
 */
TestRouterWebHDFSContractAppend (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/web/TestRouterWebHDFSContractAppend.java)/**
 * Test append operations on a Router WebHDFS FS.
 */
TestRouterWebHDFSContractConcat (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/web/TestRouterWebHDFSContractConcat.java)/**
 * Test concat operations on a Router WebHDFS FS.
 */
TestRouterWebHDFSContractCreate (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/web/TestRouterWebHDFSContractCreate.java)/**
 * Test create operations on a Router WebHDFS FS.
 */
TestRouterWebHDFSContractDelete (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/web/TestRouterWebHDFSContractDelete.java)/**
 * Test delete operations on a Router WebHDFS FS.
 */
TestRouterWebHDFSContractMkdir (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/web/TestRouterWebHDFSContractMkdir.java)/**
 * Test dir operations on a Router WebHDFS FS.
 */
TestRouterWebHDFSContractOpen (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/web/TestRouterWebHDFSContractOpen.java)/**
 * Test open operations on a Router WebHDFS FS.
 */
TestRouterWebHDFSContractRename (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/web/TestRouterWebHDFSContractRename.java)/**
 * Test rename operations on a Router WebHDFS FS.
 */
TestRouterWebHDFSContractRootDirectory (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/web/TestRouterWebHDFSContractRootDirectory.java)/**
 * Test dir operations on a Router WebHDFS FS.
 */
TestRouterWebHDFSContractSeek (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/fs/contract/router/web/TestRouterWebHDFSContractSeek.java)/**
 * Test seek operations on a Router WebHDFS FS.
 */
FederationTestUtils (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/FederationTestUtils.java)/**
 * Helper utilities for testing HDFS Federation.
 */
TestMetricsBase (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/metrics/TestMetricsBase.java)/**
 * Test the basic metrics functionality.
 */
TestRBFMetrics (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/metrics/TestRBFMetrics.java)/**
 * Test the JMX interface for the {@link Router}.
 */
RouterContext (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/MiniRouterDFSCluster.java)/**
   * Router context.
   */
NamenodeContext (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/MiniRouterDFSCluster.java)/**
   * Namenode context in the federated cluster.
   */
MiniRouterDFSCluster (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/MiniRouterDFSCluster.java)/**
 * Test utility to mimic a federated HDFS cluster with multiple routers.
 */
MockNamenode (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/MockNamenode.java)/**
 * Mock for the network interfaces (e.g., RPC and HTTP) of a Namenode. This is
 * used by the Routers in a mock cluster.
 */
MockResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/MockResolver.java)/**
 * In-memory cache/mock of a namenode and file resolver. Stores the most
 * recently updated NN information for each nameservice and block pool. It also
 * stores a virtual mount table for resolving global namespace paths to local NN
 * paths.
 */
TestAvailableSpaceResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/resolver/order/TestAvailableSpaceResolver.java)/**
 * Test the {@link AvailableSpaceResolver}.
 */
TestLocalResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/resolver/order/TestLocalResolver.java)/**
 * Test the {@link LocalResolver}.
 */
TestInitializeMountTableResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/resolver/TestInitializeMountTableResolver.java)/**
 * Test {@link MountTableResolver} initialization.
 */
TestMountTableResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/resolver/TestMountTableResolver.java)/**
 * Test the {@link MountTableStore} from the {@link Router}.
 */
TestMultipleDestinationResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/resolver/TestMultipleDestinationResolver.java)/**
 * Test the multiple destination resolver.
 */
TestNamenodeResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/resolver/TestNamenodeResolver.java)/**
 * Test the basic {@link ActiveNamenodeResolver} functionality.
 */
TestConnectionManager (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestConnectionManager.java)/**
 * Test functionalities of {@link ConnectionManager}, which manages a pool
 * of connections to NameNodes.
 */
TestDisableNameservices (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestDisableNameservices.java)/**
 * Test the behavior when disabling name services.
 */
TestDisableRouterQuota (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestDisableRouterQuota.java)/**
 * Test the behavior when disabling the Router quota.
 */
TestFederationUtil (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestFederationUtil.java)/**
 * Tests Router federation utility methods.
 */
TestRBFConfigFields (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRBFConfigFields.java)/**
 * Unit test class to compare the following RBF configuration class:
 * <p></p>
 * {@link RBFConfigKeys}
 * <p></p>
 * against hdfs-rbf-default.xml for missing properties.
 * <p></p>
 * Refer to {@link org.apache.hadoop.conf.TestConfigurationFieldsBase}
 * for how this class works.
 */
TestRouter (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouter.java)/**
 * The the safe mode for the {@link Router} controlled by
 * {@link SafeModeTimer}.
 */
TestRouterAdmin (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdmin.java)/**
 * The administrator interface of the {@link Router} implemented by
 * {@link RouterAdminServer}.
 */
TestRouterAdminCLI (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdminCLI.java)/**
 * Tests Router admin commands.
 */
TestRouterAdminGenericRefresh (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAdminGenericRefresh.java)/**
 * Before all tests, a router is spun up.
 * Before each test, mock refresh handlers are created and registered.
 * After each test, the mock handlers are unregistered.
 * After all tests, the router is spun down.
 */
TestRouterAllResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterAllResolver.java)/**
 * Tests the use of the resolvers that write in all subclusters from the
 * Router. It supports:
 * <li>HashResolver
 * <li>RandomResolver.
 */
TestRouterClientRejectOverload (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterClientRejectOverload.java)/**
 * Test the Router overload control which rejects requests when the RPC client
 * is overloaded. This feature is managed by
 * {@link RBFConfigKeys#DFS_ROUTER_CLIENT_REJECT_OVERLOAD}.
 */
TaskResults (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterFaultTolerant.java)/**
   * Class to summarize the results of running a task.
   */
TestRouterFaultTolerant (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterFaultTolerant.java)/**
 * Test the handling of fault tolerant mount points in the Router.
 */
TestRouterHeartbeatService (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterHeartbeatService.java)/**
 * Test cases for router heartbeat service.
 */
TestRouterMissingFolderMulti (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterMissingFolderMulti.java)/**
 * Test the behavior when listing a mount point mapped to multiple subclusters
 * and one of the subclusters is missing it.
 */
TestRouterMountTable (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterMountTable.java)/**
 * Test a router end-to-end including the MountTable.
 */
TestRouterMountTableCacheRefresh (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterMountTableCacheRefresh.java)/**
 * This test class verifies that mount table cache is updated on all the routers
 * when MountTableRefreshService is enabled and there is a change in mount table
 * entries.
 */
TestRouterNamenodeHeartbeat (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterNamenodeHeartbeat.java)/**
 * Test the service that heartbeats the state of the namenodes to the State
 * Store.
 */
TestRouterNamenodeMonitoring (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterNamenodeMonitoring.java)/**
 * Test namenodes monitor behavior in the Router.
 */
TestRouterPolicyProvider (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterPolicyProvider.java)/**
 * Test suite covering RouterPolicyProvider. We expect that it contains a
 * security policy definition for every RPC protocol used in HDFS. The test
 * suite works by scanning an RPC server's class to find the protocol interfaces
 * it implements, and then comparing that to the protocol interfaces covered in
 * RouterPolicyProvider. This is a parameterized test repeated for multiple HDFS
 * RPC server classes.
 */
TestRouterQuota (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterQuota.java)/**
 * Tests quota behaviors in Router-based Federation.
 */
TestRouterQuotaManager (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterQuotaManager.java)/**
 * Tests for class {@link RouterQuotaManager}.
 */
TestRouterRpc (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java)/**
 * The the RPC interface of the {@link Router} implemented by
 * {@link RouterRpcServer}.
 * Tests covering the functionality of RouterRPCServer with
 * multi nameServices.
 */
TestRouterRPCClientRetries (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRPCClientRetries.java)/**
 * Test retry behavior of the Router RPC Client.
 */
TestRouterRpcMultiDestination (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpcMultiDestination.java)/**
 * The the RPC interface of the {@link getRouter()} implemented by
 * {@link RouterRpcServer}.
 */
TestRouterRPCMultipleDestinationMountTableResolver (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRPCMultipleDestinationMountTableResolver.java)/**
 * Tests router rpc with multiple destination mount table resolver.
 */
TestRouterRpcSingleNS (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpcSingleNS.java)/**
 * The the RPC interface of the {@link Router} implemented by
 * {@link RouterRpcServer}.
 * Tests covering the functionality of RouterRPCServer with
 * single nameService.
 */
TestRouterRpcStoragePolicySatisfier (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpcStoragePolicySatisfier.java)/**
 * Test StoragePolicySatisfy through router rpc calls.
 */
TestRouterSafemode (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterSafemode.java)/**
 * Test the safe mode for the {@link Router} controlled by
 * {@link RouterSafemodeService}.
 */
MockUnixGroupsMapping (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterUserMappings.java)/**
   * Mock class to get group mapping for fake users.
   */
TestRouterUserMappings (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterUserMappings.java)/**
 * Test RefreshUserMappingsProtocol and GetUserMappingsProtocol with Routers.
 */
TestRouterWithSecureStartup (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterWithSecureStartup.java)/**
 * Test secure router start up scenarios.
 */
TestSafeMode (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestSafeMode.java)/**
 * Test the SafeMode.
 */
RouterConfigBuilder (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/RouterConfigBuilder.java)/**
 * Constructs a router configuration with individual features enabled/disabled.
 */
MockDelegationTokenSecretManager (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/security/MockDelegationTokenSecretManager.java)/**
 * Mock functionality of AbstractDelegationTokenSecretManager.
 * for testing
 */
NoAuthFilterInitializer (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/security/TestRouterHttpDelegationToken.java)/**
   * The initializer of custom filter.
   */
NoAuthFilter (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/security/TestRouterHttpDelegationToken.java)/**
   * Custom filter to be able to test auth methods and let the other ones go.
   */
TestRouterHttpDelegationToken (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/security/TestRouterHttpDelegationToken.java)/**
 * Test Delegation Tokens from the Router HTTP interface.
 */
TestRouterSecurityManager (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/security/TestRouterSecurityManager.java)/**
 * Test functionality of {@link RouterSecurityManager}, which manages
 * delegation tokens for router.
 */
StateStoreDFSCluster (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/StateStoreDFSCluster.java)/**
 * Test utility to mimic a federated HDFS cluster with a router and a state
 * store.
 */
TestStateStoreDriverBase (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/driver/TestStateStoreDriverBase.java)/**
 * Base tests for the driver. The particular implementations will use this to
 * test their functionality.
 */
TestStateStoreFile (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/driver/TestStateStoreFile.java)/**
 * Test the FileSystem (e.g., HDFS) implementation of the State Store driver.
 */
TestStateStoreFileBase (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/driver/TestStateStoreFileBase.java)/**
 * Tests for the State Store file based implementation.
 */
TestStateStoreFileSystem (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/driver/TestStateStoreFileSystem.java)/**
 * Test the FileSystem (e.g., HDFS) implementation of the State Store driver.
 */
TestStateStoreZK (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/driver/TestStateStoreZK.java)/**
 * Test the ZooKeeper implementation of the State Store driver.
 */
FederationStateStoreTestUtils (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/FederationStateStoreTestUtils.java)/**
 * Utilities to test the State Store.
 */
TestMembershipState (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/records/TestMembershipState.java)/**
 * Test the Membership State records.
 */
TestMountTable (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/records/TestMountTable.java)/**
 * Test the Mount Table entry in the State Store.
 */
TestRouterState (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/records/TestRouterState.java)/**
 * Test the Router State records.
 */
TestStateStoreBase (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/TestStateStoreBase.java)/**
 * Test the basic {@link StateStoreService} {@link MountTableStore}
 * functionality.
 */
TestStateStoreDisabledNameservice (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/TestStateStoreDisabledNameservice.java)/**
 * Test the basic {@link StateStoreService}
 * {@link DisabledNameserviceStore} functionality.
 */
TestStateStoreMembershipState (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/TestStateStoreMembershipState.java)/**
 * Test the basic {@link MembershipStore} membership functionality.
 */
TestStateStoreMountTable (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/TestStateStoreMountTable.java)/**
 * Test the basic {@link StateStoreService}
 * {@link MountTableStore} functionality.
 */
TestStateStoreRouterState (/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/store/TestStateStoreRouterState.java)/**
 * Test the basic {@link StateStoreService} {@link RouterStore} functionality.
 */
Null (/hadoop-mapreduce-project/dev-support/jdiff/Null.java)/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
LocalContainerLauncher (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java)/**
 * Runs the container task locally in a thread.
 * Since all (sub)tasks share the same local directory, they must be executed
 * sequentially in order to avoid creating/deleting the same files/dirs.
 */
TaskAttemptListenerImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java)/**
 * This class is responsible for talking to the task umblical.
 * It also converts all the old data structures
 * to yarn data structures.
 * 
 * This class HAS to be in this package to access package private 
 * methods/classes.
 */
WrappedJvmID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/WrappedJvmID.java)/**
 * A simple wrapper for increasing the visibility.
 */
YarnChild (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/YarnChild.java)/**
 * The main() for MapReduce task processes.
 */
YarnOutputFiles (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/YarnOutputFiles.java)/**
 * Manipulate the working area for the transient store for maps and reduces.
 *
 * This class is used by map and reduce tasks to identify the directories that
 * they need to write to/read from for intermediate files. The callers of
 * these methods are from child space.
 */
JobHistoryCopyService (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryCopyService.java)/**
 * Reads in history events from the JobHistoryFile and sends them out again
 * to be recorded.
 */
JobHistoryEventHandler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java)/**
 * The job history events get routed to this class. This class writes the Job
 * history events to the DFS directly into a staging dir and then moved to a
 * done-dir. JobHistory implementation is in this package to access package
 * private classes.
 */
AppContext (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/AppContext.java)/**
 * Context interface for sharing information across components in YARN App.
 */
MRClientService (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java)/**
 * This module is responsible for talking to the 
 * jobclient (user facing).
 *
 */
JobEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/event/JobEvent.java)/**
 * This class encapsulates job related events.
 *
 */
TaskAttemptEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/event/TaskAttemptEvent.java)/**
 * This class encapsulates task attempt related events.
 *
 */
TaskAttemptStatus (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/event/TaskAttemptStatusUpdateEvent.java)/**
   * The internal TaskAttemptStatus object corresponding to remote Task status.
   * 
   */
TaskAttemptTooManyFetchFailureEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/event/TaskAttemptTooManyFetchFailureEvent.java)/**
 * TaskAttemptTooManyFetchFailureEvent is used for TA_TOO_MANY_FETCH_FAILURE.
 */
TaskEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/event/TaskEvent.java)/**
 * this class encapsulates task related events.
 *
 */
TaskTAttemptKilledEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/event/TaskTAttemptKilledEvent.java)/**
 * Task Attempt killed event.
 */
JobImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java)/** Implementation of Job interface. Maintains the state machines of Job.
 * The read and write calls use ReadWriteLock for concurrency.
 */
ExitFinishingOnContainerCompletedTransition (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java)/**
   * Transition from SUCCESS_FINISHING_CONTAINER or FAIL_FINISHING_CONTAINER
   * state upon receiving TA_CONTAINER_COMPLETED event
   */
ExitFinishingOnTimeoutTransition (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java)/**
   * Transition from SUCCESS_FINISHING_CONTAINER or FAIL_FINISHING_CONTAINER
   * state upon receiving TA_TIMED_OUT event
   */
CleanupContainerTransition (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java)/**
   * Finish and clean up the container
   */
MoveContainerToSucceededFinishingTransition (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java)/**
   * Transition to SUCCESS_FINISHING_CONTAINER upon receiving TA_DONE event
   */
MoveContainerToFailedFinishingTransition (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java)/**
   * Transition to FAIL_FINISHING_CONTAINER upon receiving TA_FAILMSG event
   */
TaskAttemptImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java)/**
 * Implementation of TaskAttempt interface.
 */
TaskImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskImpl.java)/**
 * Implementation of Task interface.
 */
Job (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/Job.java)/**
 * Main interface to interact with the job.
 */
Task (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/Task.java)/**
 * Read only view of Task.
 */
TaskAttempt (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/TaskAttempt.java)/**
 * Read only view of TaskAttempt.
 */
JobEndNotifier (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/JobEndNotifier.java)/**
 * <p>This class handles job end notification. Submitters of jobs can choose to
 * be notified of the end of a job by supplying a URL to which a connection
 * will be established.
 * <ul><li> The URL connection is fire and forget by default.</li> <li>
 * User can specify number of retry attempts and a time interval at which to
 * attempt retries</li><li>
 * Cluster administrators can set final parameters to set maximum number of
 * tries (0 would disable job end notification) and max time interval and a
 * proxy if needed</li><li>
 * The URL may contain sentinels which will be replaced by jobId and jobStatus 
 * (eg. SUCCEEDED/KILLED/FAILED) </li> </ul>
 */
EventProcessor (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/launcher/ContainerLauncherImpl.java)/**
   * Setup and start the container on remote nodemanager.
   */
ContainerLauncherImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/launcher/ContainerLauncherImpl.java)/**
 * This class is responsible for launching of containers.
 */
LocalContainerAllocator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/local/LocalContainerAllocator.java)/**
 * Allocates containers locally. Doesn't allocate a real container;
 * instead sends an allocated event for all requests.
 */
ContainerAllocatorRouter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java)/**
   * By the time life-cycle of this router starts, job-init would have already
   * happened.
   */
ContainerLauncherRouter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java)/**
   * By the time life-cycle of this router starts, job-init would have already
   * happened.
   */
NoopEventHandler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java)/**
   * Eats events that are not needed in some error cases.
   */
Action (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java)/**
   * Action to be wrapped with setting and unsetting the job classloader
   */
AMPreemptionPolicy (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/AMPreemptionPolicy.java)/**
 * Policy encoding the {@link org.apache.hadoop.mapreduce.v2.app.MRAppMaster}
 * response to preemption requests from the ResourceManager.
 * @see org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator
 */
CheckpointAMPreemptionPolicy (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/CheckpointAMPreemptionPolicy.java)/**
 * This policy works in combination with an implementation of task
 * checkpointing. It computes the tasks to be preempted in response to the RM
 * request for preemption. For strict requests, it maps containers to
 * corresponding tasks; for fungible requests, it attempts to pick the best
 * containers to preempt (reducers in reverse allocation order). The
 * TaskAttemptListener will interrogate this policy when handling a task
 * heartbeat to check whether the task should be preempted or not. When handling
 * fungible requests, the policy discount the RM ask by the amount of currently
 * in-flight preemptions (i.e., tasks that are checkpointing).
 *
 * This class it is also used to maintain the list of checkpoints for existing
 * tasks. Centralizing this functionality here, allows us to have visibility on
 * preemption and checkpoints in a single location, thus coordinating preemption
 * and checkpoint management decisions in a single policy.
 */
KillAMPreemptionPolicy (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java)/**
 * Sample policy that aggressively kills tasks when requested.
 */
NoopAMPreemptionPolicy (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/NoopAMPreemptionPolicy.java)/**
 * NoOp policy that ignores all the requests for preemption.
 */
RMCommunicator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMCommunicator.java)/**
 * Registers/unregisters to RM and sends heartbeats to RM.
 */
RMContainerAllocationException (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocationException.java)/**
 * Exception to denote fatal failure in allocating containers from RM.
 */
RMContainerAllocator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java)/**
 * Allocates the container from the ResourceManager scheduler.
 */
RMContainerRequestor (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java)/**
 * Keeps the data structures to send container requests to RM.
 */
ClientHSPolicyProvider (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/security/authorize/ClientHSPolicyProvider.java)/**
 * {@link PolicyProvider} for MapReduce history server protocols.
 */
MRAMPolicyProvider (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/security/authorize/MRAMPolicyProvider.java)/**
 * {@link PolicyProvider} for YARN MapReduce protocols.
 */
ExponentiallySmoothedTaskRuntimeEstimator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/speculate/ExponentiallySmoothedTaskRuntimeEstimator.java)/**
 * This estimator exponentially smooths the rate of progress versus wallclock
 * time.  Conceivably we could write an estimator that smooths time per
 * unit progress, and get different results.
 */
Speculator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/speculate/Speculator.java)/**
 * Speculator component. Task Attempts' status updates are sent to this
 * component. Concrete implementation runs the speculative algorithm and
 * sends the TaskEventType.T_ADD_ATTEMPT.
 *
 * An implementation also has to arrange for the jobs to be scanned from
 * time to time, to launch the speculations.
 */
TaskAttemptFinishingMonitor (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/TaskAttemptFinishingMonitor.java)/**
 * This class generates TA_TIMED_OUT if the task attempt stays in FINISHING
 * state for too long.
 */
TaskAttemptListener (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/TaskAttemptListener.java)/**
 * This class listens for changes to the state of a Task.
 */
TaskHeartbeatHandler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/TaskHeartbeatHandler.java)/**
 * This class keeps track of tasks that have already been launched. It
 * determines if a task is alive and running or marks a task as dead if it does
 * not hear from it for a long time.
 * 
 */
AMParams (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AMParams.java)/**
 * Params constants for the AM webapp and the history webapp.
 */
AMWebApp (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AMWebApp.java)/**
 * Application master webapp
 */
AppController (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppController.java)/**
 * This class renders the various pages that the web app supports.
 */
ConfBlock (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/ConfBlock.java)/**
 * Render the configuration for this job.
 */
JobTaskAttemptState (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/dao/JobTaskAttemptState.java)/**
 * Job task attempt state.
 */
JobConfPage (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/JobConfPage.java)/**
 * Render a page with the configuration for a given job in it.
 */
SingleCounterPage (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/SingleCounterPage.java)/**
 * Render the counters page
 */
TestTaskAttemptListenerImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapred/TestTaskAttemptListenerImpl.java)/**
 * Tests the behavior of TaskAttemptListenerImpl.
 */
TestYarnChild (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapred/TestYarnChild.java)/**
 * Tests the behavior of YarnChild.
 */
JHEventHandlerForSigtermTest (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestJobHistoryEventHandler.java)/**
 * Class to help with testSigTermedFunctionality
 */
TestMapreduceConfigFields (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/TestMapreduceConfigFields.java)/**
 * Unit test class to compare the following MR Configuration classes:
 * <p></p>
 * {@link org.apache.hadoop.mapreduce.MRJobConfig}
 * {@link org.apache.hadoop.mapreduce.MRConfig}
 * {@link org.apache.hadoop.mapreduce.v2.jobhistory.JHAdminConfig}
 * {@link org.apache.hadoop.mapred.ShuffleHandler}
 * {@link org.apache.hadoop.mapreduce.lib.output.FileOutputFormat}
 * {@link org.apache.hadoop.mapreduce.lib.input.FileInputFormat}
 * {@link org.apache.hadoop.mapreduce.Job}
 * {@link org.apache.hadoop.mapreduce.lib.input.NLineInputFormat}
 * {@link org.apache.hadoop.mapred.JobConf}
 * <p></p>
 * against mapred-default.xml for missing properties.  Currently only
 * throws an error if the class is missing a property.
 * <p></p>
 * Refer to {@link org.apache.hadoop.conf.TestConfigurationFieldsBase}
 * for how this class works.
 */
TestJobImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestJobImpl.java)/**
 * Tests various functions of the JobImpl class
 */
MRApp (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MRApp.java)/**
 * Mock MRAppMaster. Doesn't start RPC servers.
 * No threads are started except of the event Dispatcher thread.
 */
TestFail (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestFail.java)/**
 * Tests the state machine with respect to Job/Task/TaskAttempt failure 
 * scenarios.
 */
TestJobEndNotifier (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestJobEndNotifier.java)/**
 * Tests job end notification
 *
 */
TestKill (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKill.java)/**
 * Tests the state machine with respect to Job/Task/TaskAttempt kill scenarios.
 *
 */
TestMRApp (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRApp.java)/**
 * Tests the state machine of MR App.
 */
TestFileOutputCommitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRecovery.java)/**
   * The class provides a custom implementation of output committer setupTask
   * and isRecoverySupported methods, which determines if recovery supported
   * based on config property.
   */
TestStagingCleanup (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestStagingCleanup.java)/**
 * Make sure that the job staging directory clean up happens.
 */
AppControllerForTest (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppControllerForTest.java)/**
 * Class AppControllerForTest overrides some methods of AppController for test
 */
AppForTest (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppForTest.java)/**
 *   Class AppForTest publishes a methods for test
 */
TasksBlockForTest (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TasksBlockForTest.java)/**
 *    Class TasksBlockForTest overrides some methods for test
 */
TestAMWebServices (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServices.java)/**
 * Test the MapReduce Application master info web services api's. Also test
 * non-existent urls.
 *
 *  /ws/v1/mapreduce
 *  /ws/v1/mapreduce/info
 */
TestAMWebServicesAttempt (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempt.java)/**
 * Test the app master web service Rest API for getting task attempts, a
 * specific task attempt, and task attempt counters
 *
 * /ws/v1/mapreduce/jobs/{jobid}/tasks/{taskid}/attempts/{attemptid}/state
 */
TestAMWebServicesAttempts (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempts.java)/**
 * Test the app master web service Rest API for getting task attempts, a
 * specific task attempt, and task attempt counters
 *
 * /ws/v1/mapreduce/jobs/{jobid}/tasks/{taskid}/attempts
 * /ws/v1/mapreduce/jobs/{jobid}/tasks/{taskid}/attempts/{attemptid}
 * /ws/v1/mapreduce/jobs/{jobid}/tasks/{taskid}/attempts/{attemptid}/counters
 */
TestAMWebServicesJobConf (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobConf.java)/**
 * Test the app master web service Rest API for getting the job conf. This
 * requires created a temporary configuration file.
 *
 *   /ws/v1/mapreduce/job/{jobid}/conf
 */
TestAMWebServicesJobs (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java)/**
 * Test the app master web service Rest API for getting jobs, a specific job,
 * and job counters.
 *
 * /ws/v1/mapreduce/jobs
 * /ws/v1/mapreduce/jobs/{jobid}
 * /ws/v1/mapreduce/jobs/{jobid}/counters
 * /ws/v1/mapreduce/jobs/{jobid}/jobattempts
 */
TestAMWebServicesTasks (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java)/**
 * Test the app master web service Rest API for getting tasks, a specific task,
 * and task counters.
 *
 * /ws/v1/mapreduce/jobs/{jobid}/tasks
 * /ws/v1/mapreduce/jobs/{jobid}/tasks/{taskid}
 * /ws/v1/mapreduce/jobs/{jobid}/tasks/{taskid}/counters
 */
ViewForTest (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/ViewForTest.java)/**
 *  override method render() for test
 */
BlockForTest (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/yarn/webapp/view/BlockForTest.java)/**
 *   BlockForTest publishes constructor for test
 */
LocalDistributedCacheManager (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalDistributedCacheManager.java)/**
 * A helper class for managing the distributed cache for {@link LocalJobRunner}.
 */
MapTaskRunnable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java)/**
     * A Runnable instance that handles a map task to be run by an executor.
     */
LocalJobRunner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java)/** Implements MapReduce locally, in-process, for debugging. */
MRDelegationTokenIdentifier (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/api/MRDelegationTokenIdentifier.java)/**
 * {@link TokenIdentifier} that identifies delegation tokens 
 * issued by JobHistoryServer to delegate
 * MR tasks talking to the JobHistoryServer.
 */
CancelDelegationTokenRequest (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/api/protocolrecords/CancelDelegationTokenRequest.java)/**
 * The request issued by the client to the {@code ResourceManager} to cancel a
 * delegation token.
 */
CancelDelegationTokenResponse (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/api/protocolrecords/CancelDelegationTokenResponse.java)/**
 * The response from the {@code ResourceManager} to a cancelDelegationToken
 * request.
 */
RenewDelegationTokenRequest (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/api/protocolrecords/RenewDelegationTokenRequest.java)/**
 * The request issued by the client to renew a delegation token from
 * the {@code ResourceManager}.
 */
RenewDelegationTokenResponse (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/api/protocolrecords/RenewDelegationTokenResponse.java)/**
 * The response to a renewDelegationToken call to the {@code ResourceManager}.
 */
JobId (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/api/records/JobId.java)/**
 * <p><code>JobId</code> represents the <em>globally unique</em> 
 * identifier for a MapReduce job.</p>
 * 
 * <p>The globally unique nature of the identifier is achieved by using the 
 * <em>cluster timestamp</em> from the associated ApplicationId. i.e. 
 * start-time of the <code>ResourceManager</code> along with a monotonically
 * increasing counter for the jobId.</p>
 */
TaskAttemptId (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/api/records/TaskAttemptId.java)/**
 * <p>
 * <code>TaskAttemptId</code> represents the unique identifier for a task
 * attempt. Each task attempt is one particular instance of a Map or Reduce Task
 * identified by its TaskId.
 * </p>
 * 
 * <p>
 * TaskAttemptId consists of 2 parts. First part is the <code>TaskId</code>,
 * that this <code>TaskAttemptId</code> belongs to. Second part is the task
 * attempt number.
 * </p>
 */
TaskId (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/api/records/TaskId.java)/**
 * <p>
 * <code>TaskId</code> represents the unique identifier for a Map or Reduce
 * Task.
 * </p>
 * 
 * <p>
 * TaskId consists of 3 parts. First part is <code>JobId</code>, that this Task
 * belongs to. Second part of the TaskId is either 'm' or 'r' representing
 * whether the task is a map task or a reduce task. And the third part is the
 * task number.
 * </p>
 */
JHAdminConfig (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/jobhistory/JHAdminConfig.java)/**
 * Stores Job History configuration keys that can be set by administrators of
 * the Job History server.
 */
JobIndexInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/jobhistory/JobIndexInfo.java)/**
 * Maintains information which may be used by the jobHistory indexing
 * system.
 */
LocalResourceBuilder (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/util/LocalResourceBuilder.java)/**
 * Helper class for MR applications that parses distributed cache artifacts and
 * creates a map of LocalResources.
 */
MRApps (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/util/MRApps.java)/**
 * Helper class for MR applications
 */
HSClientProtocol (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/yarn/proto/HSClientProtocol.java)/**
 * Fake protocol to differentiate the blocking interfaces in the 
 * security info class loaders.
 */
MockInputStream (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestLocalDistributedCacheManager.java)/**
   * Mock input stream based on a byte array so that it can be used by a
   * FSDataInputStream.
   */
TestMRWithDistributedCache (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestMRWithDistributedCache.java)/**
 * Tests the use of the
 * {@link org.apache.hadoop.mapreduce.filecache.DistributedCache} within the
 * full MR flow as well as the LocalJobRunner. This ought to be part of the
 * filecache package, but that package is not currently in mapred, so cannot
 * depend on MR for testing.
 * 
 * We use the distributed.* namespace for temporary files.
 * 
 * See {@link TestMiniMRLocalFS}, {@link TestMiniMRDFSCaching}, and
 * {@link MRCaching} for other tests that test the distributed cache.
 * 
 * This test is not fast: it uses MiniMRCluster.
 */
DistributedCache (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/filecache/DistributedCache.java)/**
 * Distribute application-specific large, read-only files efficiently.
 *
 * <p><code>DistributedCache</code> is a facility provided by the Map-Reduce
 * framework to cache files (text, archives, jars etc.) needed by applications.
 * </p>
 *
 * <p>Applications specify the files, via urls (hdfs:// or http://) to be cached
 * via the {@link org.apache.hadoop.mapred.JobConf}. The
 * <code>DistributedCache</code> assumes that the files specified via urls are
 * already present on the {@link FileSystem} at the path specified by the url
 * and are accessible by every machine in the cluster.</p>
 *
 * <p>The framework will copy the necessary files on to the worker node before
 * any tasks for the job are executed on that node. Its efficiency stems from
 * the fact that the files are only copied once per job and the ability to
 * cache archives which are un-archived on the workers.</p>
 *
 * <p><code>DistributedCache</code> can be used to distribute simple, read-only
 * data/text files and/or more complex types such as archives, jars etc.
 * Archives (zip, tar and tgz/tar.gz files) are un-archived at the worker nodes.
 * Jars may be optionally added to the classpath of the tasks, a rudimentary
 * software distribution mechanism.  Files have execution permissions.
 * In older version of Hadoop Map/Reduce users could optionally ask for symlinks
 * to be created in the working directory of the child task.  In the current
 * version symlinks are always created.  If the URL does not have a fragment
 * the name of the file or directory will be used. If multiple files or
 * directories map to the same link name, the last one added, will be used.  All
 * others will not even be downloaded.</p>
 *
 * <p><code>DistributedCache</code> tracks modification timestamps of the cache
 * files. Clearly the cache files should not be modified by the application
 * or externally while the job is executing.</p>
 *
 * <p>Here is an illustrative example on how to use the
 * <code>DistributedCache</code>:</p>
 * <p><blockquote><pre>
 *     // Setting up the cache for the application
 *
 *     1. Copy the requisite files to the <code>FileSystem</code>:
 *
 *     $ bin/hadoop fs -copyFromLocal lookup.dat /myapp/lookup.dat
 *     $ bin/hadoop fs -copyFromLocal map.zip /myapp/map.zip
 *     $ bin/hadoop fs -copyFromLocal mylib.jar /myapp/mylib.jar
 *     $ bin/hadoop fs -copyFromLocal mytar.tar /myapp/mytar.tar
 *     $ bin/hadoop fs -copyFromLocal mytgz.tgz /myapp/mytgz.tgz
 *     $ bin/hadoop fs -copyFromLocal mytargz.tar.gz /myapp/mytargz.tar.gz
 *
 *     2. Setup the application's <code>JobConf</code>:
 *
 *     JobConf job = new JobConf();
 *     DistributedCache.addCacheFile(new URI("/myapp/lookup.dat#lookup.dat"),
 *                                   job);
 *     DistributedCache.addCacheArchive(new URI("/myapp/map.zip"), job);
 *     DistributedCache.addFileToClassPath(new Path("/myapp/mylib.jar"), job);
 *     DistributedCache.addCacheArchive(new URI("/myapp/mytar.tar"), job);
 *     DistributedCache.addCacheArchive(new URI("/myapp/mytgz.tgz"), job);
 *     DistributedCache.addCacheArchive(new URI("/myapp/mytargz.tar.gz"), job);
 *
 *     3. Use the cached files in the {@link org.apache.hadoop.mapred.Mapper}
 *     or {@link org.apache.hadoop.mapred.Reducer}:
 *
 *     public static class MapClass extends MapReduceBase
 *     implements Mapper&lt;K, V, K, V&gt; {
 *
 *       private Path[] localArchives;
 *       private Path[] localFiles;
 *
 *       public void configure(JobConf job) {
 *         // Get the cached archives/files
 *         File f = new File("./map.zip/some/file/in/zip.txt");
 *       }
 *
 *       public void map(K key, V value,
 *                       OutputCollector&lt;K, V&gt; output, Reporter reporter)
 *       throws IOException {
 *         // Use data from the cached archives/files here
 *         // ...
 *         // ...
 *         output.collect(k, v);
 *       }
 *     }
 *
 * </pre></blockquote>
 *
 * It is also very common to use the DistributedCache by using
 * {@link org.apache.hadoop.util.GenericOptionsParser}.
 *
 * This class includes methods that should be used by users
 * (specifically those mentioned in the example above, as well
 * as {@link DistributedCache#addArchiveToClassPath(Path, Configuration)}),
 * as well as methods intended for use by the MapReduce framework
 * (e.g., {@link org.apache.hadoop.mapred.JobClient}).
 *
 * @see org.apache.hadoop.mapred.JobConf
 * @see org.apache.hadoop.mapred.JobClient
 * @see org.apache.hadoop.mapreduce.Job
 */
AMFeedback (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/AMFeedback.java)/**
 * This class is a simple struct to include both the taskFound information and
 * a possible preemption request coming from the AM.
 */
BackupStore (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/BackupStore.java)/**
 * <code>BackupStore</code> is an utility class that is used to support
 * the mark-reset functionality of values iterator
 *
 * <p>It has two caches - a memory cache and a file cache where values are
 * stored as they are iterated, after a mark. On reset, values are retrieved
 * from these caches. Framework moves from the memory cache to the 
 * file cache when the memory cache becomes full.
 * 
 */
BasicTypeSorterBase (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/BasicTypeSorterBase.java)/** This class implements the sort interface using primitive int arrays as 
 * the data structures (that is why this class is called 'BasicType'SorterBase)
 */
BufferSorter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/BufferSorter.java)/** This class provides a generic sort interface that should be implemented
 * by specific sort algorithms. The use case is the following:
 * A user class writes key/value records to a buffer, and finally wants to
 * sort the buffer. This interface defines methods by which the user class
 * can update the interface implementation with the offsets of the records
 * and the lengths of the keys/values. The user class gives a reference to
 * the buffer when the latter wishes to sort the records written to the buffer
 * so far. Typically, the user class decides the point at which sort should
 * happen based on the memory consumed so far by the buffer and the data
 * structures maintained by an implementation of this interface. That is why
 * a method is provided to get the memory consumed so far by the datastructures
 * in the interface implementation.  
 */
PathDeletionContext (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/CleanupQueue.java)/**
   * Contains info related to the path of the file/dir to be deleted
   */
Clock (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Clock.java)/**
 * A clock class - can be mocked out for testing.
 */
BlackListInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ClusterStatus.java)/**
   * Class which encapsulates information about a blacklisted tasktracker.
   *  
   * The information includes the tasktracker's name and reasons for
   * getting blacklisted. The toString method of the class will print
   * the information in a whitespace separated fashion to enable parsing.
   */
ClusterStatus (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ClusterStatus.java)/**
 * Status information on the current state of the Map-Reduce cluster.
 * 
 * <p><code>ClusterStatus</code> provides clients with information such as:
 * <ol>
 *   <li>
 *   Size of the cluster. 
 *   </li>
 *   <li>
 *   Name of the trackers. 
 *   </li>
 *   <li>
 *   Task capacity of the cluster. 
 *   </li>
 *   <li>
 *   The number of currently running map and reduce tasks.
 *   </li>
 *   <li>
 *   State of the <code>JobTracker</code>.
 *   </li>
 *   <li>
 *   Details regarding black listed trackers.
 *   </li>
 * </ol>
 * 
 * <p>Clients can query for the latest <code>ClusterStatus</code>, via 
 * {@link JobClient#getClusterStatus()}.</p>
 * 
 * @see JobClient
 */
Counter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Counters.java)/**
   * A counter record, comprising its name and value.
   */
Group (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Counters.java)/**
   *  <code>Group</code> of counters, comprising of counters from a particular
   *  counter {@link Enum} class.
   *
   *  <p><code>Group</code>handles localization of the class name and the
   *  counter names.</p>
   */
GroupFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Counters.java)/**
   * Provide factory methods for counter group factory implementation.
   * See also the GroupFactory in
   *  {@link org.apache.hadoop.mapreduce.Counters mapreduce.Counters}
   */
CountersExceededException (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Counters.java)/**
   * Counter exception thrown when the number of counters exceed the limit
   */
Counters (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Counters.java)/**
 * A set of named counters.
 *
 * <p><code>Counters</code> represent global counters, defined either by the
 * Map-Reduce framework or applications. Each <code>Counter</code> can be of
 * any {@link Enum} type.</p>
 *
 * <p><code>Counters</code> are bunched into {@link Group}s, each comprising of
 * counters from a particular <code>Enum</code> class.
 */
CumulativePeriodicStats (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/CumulativePeriodicStats.java)/**
 *
 * This class is a concrete PeriodicStatsAccumulator that deals with
 *  measurements where the raw data are a measurement of an
 *  accumulation.  The result in each bucket is the estimate 
 *  of the progress-weighted change in that quantity over the
 *  progress range covered by the bucket.
 *
 * <p>An easy-to-understand example of this kind of quantity would be
 *  a distance traveled.  It makes sense to consider that portion of
 *  the total travel that can be apportioned to each bucket.
 *
 */
DeprecatedQueueConfigurationParser (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/DeprecatedQueueConfigurationParser.java)/**
 * Class to build queue hierarchy using deprecated conf(mapred-site.xml).
 * Generates a single level of queue hierarchy. 
 * 
 */
FileAlreadyExistsException (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileAlreadyExistsException.java)/**
 * Used when target file already exists for any operation and 
 * is not configured to be overwritten.  
 */
MultiPathFilter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java)/**
   * Proxy PathFilter that accepts a path only if all filters given in the
   * constructor do. Used by the listPaths() to apply the built-in
   * hiddenFileFilter together with a user provided one (if any).
   */
FileInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java)/** 
 * A base class for file-based {@link InputFormat}.
 * 
 * <p><code>FileInputFormat</code> is the base class for all file-based 
 * <code>InputFormat</code>s. This provides a generic implementation of
 * {@link #getSplits(JobConf, int)}.
 *
 * Implementations of <code>FileInputFormat</code> can also override the
 * {@link #isSplitable(FileSystem, Path)} method to prevent input files
 * from being split-up in certain situations. Implementations that may
 * deal with non-splittable files <i>must</i> override this method, since
 * the default implementation assumes splitting is always possible.
 */
FileOutputCommitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java)/** An {@link OutputCommitter} that commits files specified 
 * in job output directory i.e. ${mapreduce.output.fileoutputformat.outputdir}. 
 **/
FileOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputFormat.java)/** A base class for {@link OutputFormat}. */
FileSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileSplit.java)/** A section of an input file.  Returned by {@link
 * InputFormat#getSplits(JobConf, int)} and passed to
 * {@link InputFormat#getRecordReader(InputSplit,JobConf,Reporter)}. 
 */
FixedLengthInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FixedLengthInputFormat.java)/**
 * FixedLengthInputFormat is an input format used to read input files
 * which contain fixed length records.  The content of a record need not be
 * text.  It can be arbitrary binary data.  Users must configure the record
 * length property by calling:
 * FixedLengthInputFormat.setRecordLength(conf, recordLength);<br><br> or
 * conf.setInt(FixedLengthInputFormat.FIXED_RECORD_LENGTH, recordLength);
 * <br><br>
 * @see FixedLengthRecordReader
 */
FixedLengthRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FixedLengthRecordReader.java)/**
 * A reader to read fixed length records from a split.  Record offset is
 * returned as key and the record as bytes is returned in value.
 */
ID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ID.java)/**
 * A general identifier, which internally stores the id
 * as an integer. This is the super class of {@link JobID}, 
 * {@link TaskID} and {@link TaskAttemptID}.
 * 
 * @see JobID
 * @see TaskID
 * @see TaskAttemptID
 */
Writer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFile.java)/**
   * <code>IFile.Writer</code> to write out intermediate map-outputs. 
   */
Reader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFile.java)/**
   * <code>IFile.Reader</code> to read intermediate map-outputs. 
   */
IFile (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFile.java)/**
 * <code>IFile</code> is the simple &lt;key-len, value-len, key, value&gt; format
 * for the intermediate map-outputs in Map-Reduce.
 *
 * There is a <code>Writer</code> to write out map-outputs in this format and 
 * a <code>Reader</code> to read files of this format.
 */
IFileInputStream (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFileInputStream.java)/**
 * A checksum input stream, used for IFiles.
 * Used to validate the checksum of files created by {@link IFileOutputStream}. 
*/
IFileOutputStream (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFileOutputStream.java)/**
 * A Checksum output stream.
 * Checksum for the contents of the file is calculated and
 * appended to the end of the file on close of the stream.
 * Used for IFiles
 */
InputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/InputFormat.java)/** 
 * <code>InputFormat</code> describes the input-specification for a 
 * Map-Reduce job. 
 * 
 * <p>The Map-Reduce framework relies on the <code>InputFormat</code> of the
 * job to:<p>
 * <ol>
 *   <li>
 *   Validate the input-specification of the job. 
 *   <li>
 *   Split-up the input file(s) into logical {@link InputSplit}s, each of 
 *   which is then assigned to an individual {@link Mapper}.
 *   </li>
 *   <li>
 *   Provide the {@link RecordReader} implementation to be used to glean
 *   input records from the logical <code>InputSplit</code> for processing by 
 *   the {@link Mapper}.
 *   </li>
 * </ol>
 * 
 * <p>The default behavior of file-based {@link InputFormat}s, typically 
 * sub-classes of {@link FileInputFormat}, is to split the 
 * input into <i>logical</i> {@link InputSplit}s based on the total size, in 
 * bytes, of the input files. However, the {@link FileSystem} blocksize of  
 * the input files is treated as an upper bound for input splits. A lower bound 
 * on the split size can be set via 
 * <a href="{@docRoot}/../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml#mapreduce.input.fileinputformat.split.minsize">
 * mapreduce.input.fileinputformat.split.minsize</a>.</p>
 * 
 * <p>Clearly, logical splits based on input-size is insufficient for many 
 * applications since record boundaries are to be respected. In such cases, the
 * application has to also implement a {@link RecordReader} on whom lies the
 * responsibilty to respect record-boundaries and present a record-oriented
 * view of the logical <code>InputSplit</code> to the individual task.
 *
 * @see InputSplit
 * @see RecordReader
 * @see JobClient
 * @see FileInputFormat
 */
InputSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/InputSplit.java)/**
 * <code>InputSplit</code> represents the data to be processed by an 
 * individual {@link Mapper}. 
 *
 * <p>Typically, it presents a byte-oriented view on the input and is the 
 * responsibility of {@link RecordReader} of the job to process this and present
 * a record-oriented view.
 * 
 * @see InputFormat
 * @see RecordReader
 */
InvalidFileTypeException (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/InvalidFileTypeException.java)/**
 * Used when file type differs from the desired file type. like 
 * getting a file when a directory is expected. Or a wrong file type. 
 */
InvalidInputException (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/InvalidInputException.java)/**
 * This class wraps a list of problems with the input, so that the user
 * can get a list of problems together instead of finding and fixing them one 
 * by one.
 */
InvalidJobConfException (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/InvalidJobConfException.java)/**
 * This exception is thrown when jobconf misses some mendatory attributes
 * or value of some attributes is invalid. 
 */
NetworkedJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java)/**
   * A NetworkedJob is an implementation of RunningJob.  It holds
   * a JobProfile object to provide some info, and interacts with the
   * remote service to provide certain functionality.
   */
JobClient (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java)/**
 * <code>JobClient</code> is the primary interface for the user-job to interact
 * with the cluster.
 * 
 * <code>JobClient</code> provides facilities to submit jobs, track their 
 * progress, access component-tasks' reports/logs, get the Map-Reduce cluster
 * status information etc.
 * 
 * <p>The job submission process involves:
 * <ol>
 *   <li>
 *   Checking the input and output specifications of the job.
 *   </li>
 *   <li>
 *   Computing the {@link InputSplit}s for the job.
 *   </li>
 *   <li>
 *   Setup the requisite accounting information for the {@link DistributedCache} 
 *   of the job, if necessary.
 *   </li>
 *   <li>
 *   Copying the job's jar and configuration to the map-reduce system directory 
 *   on the distributed file-system. 
 *   </li>
 *   <li>
 *   Submitting the job to the cluster and optionally monitoring
 *   it's status.
 *   </li>
 * </ol>
 *  
 * Normally the user creates the application, describes various facets of the
 * job via {@link JobConf} and then uses the <code>JobClient</code> to submit 
 * the job and monitor its progress.
 * 
 * <p>Here is an example on how to use <code>JobClient</code>:</p>
 * <p><blockquote><pre>
 *     // Create a new JobConf
 *     JobConf job = new JobConf(new Configuration(), MyJob.class);
 *     
 *     // Specify various job-specific parameters     
 *     job.setJobName("myjob");
 *     
 *     job.setInputPath(new Path("in"));
 *     job.setOutputPath(new Path("out"));
 *     
 *     job.setMapperClass(MyJob.MyMapper.class);
 *     job.setReducerClass(MyJob.MyReducer.class);
 *
 *     // Submit the job, then poll for progress until the job is complete
 *     JobClient.runJob(job);
 * </pre></blockquote>
 * 
 * <b id="JobControl">Job Control</b>
 * 
 * <p>At times clients would chain map-reduce jobs to accomplish complex tasks 
 * which cannot be done via a single map-reduce job. This is fairly easy since 
 * the output of the job, typically, goes to distributed file-system and that 
 * can be used as the input for the next job.</p>
 * 
 * <p>However, this also means that the onus on ensuring jobs are complete 
 * (success/failure) lies squarely on the clients. In such situations the 
 * various job-control options are:
 * <ol>
 *   <li>
 *   {@link #runJob(JobConf)} : submits the job and returns only after 
 *   the job has completed.
 *   </li>
 *   <li>
 *   {@link #submitJob(JobConf)} : only submits the job, then poll the 
 *   returned handle to the {@link RunningJob} to query status and make 
 *   scheduling decisions.
 *   </li>
 *   <li>
 *   {@link JobConf#setJobEndNotificationURI(String)} : setup a notification
 *   on job-completion, thus avoiding polling.
 *   </li>
 * </ol>
 * 
 * @see JobConf
 * @see ClusterStatus
 * @see Tool
 * @see DistributedCache
 */
JobConf (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConf.java)/** 
 * A map/reduce job configuration.
 * 
 * <p><code>JobConf</code> is the primary interface for a user to describe a 
 * map-reduce job to the Hadoop framework for execution. The framework tries to
 * faithfully execute the job as-is described by <code>JobConf</code>, however:
 * <ol>
 *   <li>
 *   Some configuration parameters might have been marked as 
 *   <a href="{@docRoot}/org/apache/hadoop/conf/Configuration.html#FinalParams">
 *   final</a> by administrators and hence cannot be altered.
 *   </li>
 *   <li>
 *   While some job parameters are straight-forward to set 
 *   (e.g. {@link #setNumReduceTasks(int)}), some parameters interact subtly
 *   with the rest of the framework and/or job-configuration and is relatively
 *   more complex for the user to control finely
 *   (e.g. {@link #setNumMapTasks(int)}).
 *   </li>
 * </ol>
 * 
 * <p><code>JobConf</code> typically specifies the {@link Mapper}, combiner 
 * (if any), {@link Partitioner}, {@link Reducer}, {@link InputFormat} and 
 * {@link OutputFormat} implementations to be used etc.
 *
 * <p>Optionally <code>JobConf</code> is used to specify other advanced facets 
 * of the job such as <code>Comparator</code>s to be used, files to be put in  
 * the {@link DistributedCache}, whether or not intermediate and/or job outputs 
 * are to be compressed (and how), debugability via user-provided scripts 
 * ( {@link #setMapDebugScript(String)}/{@link #setReduceDebugScript(String)}),
 * for doing post-processing on task logs, task's stdout, stderr, syslog. 
 * and etc.</p>
 * 
 * <p>Here is an example on how to configure a job via <code>JobConf</code>:</p>
 * <p><blockquote><pre>
 *     // Create a new JobConf
 *     JobConf job = new JobConf(new Configuration(), MyJob.class);
 *     
 *     // Specify various job-specific parameters     
 *     job.setJobName("myjob");
 *     
 *     FileInputFormat.setInputPaths(job, new Path("in"));
 *     FileOutputFormat.setOutputPath(job, new Path("out"));
 *     
 *     job.setMapperClass(MyJob.MyMapper.class);
 *     job.setCombinerClass(MyJob.MyReducer.class);
 *     job.setReducerClass(MyJob.MyReducer.class);
 *     
 *     job.setInputFormat(SequenceFileInputFormat.class);
 *     job.setOutputFormat(SequenceFileOutputFormat.class);
 * </pre></blockquote>
 * 
 * @see JobClient
 * @see ClusterStatus
 * @see Tool
 * @see DistributedCache
 */
JobConfigurable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobConfigurable.java)/** That what may be configured. */
JobID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobID.java)/**
 * JobID represents the immutable and unique identifier for 
 * the job. JobID consists of two parts. First part 
 * represents the jobtracker identifier, so that jobID to jobtracker map 
 * is defined. For cluster setup this string is the jobtracker 
 * start time, for local setting, it is "local".
 * Second part of the JobID is the job number. <br> 
 * An example JobID is : 
 * <code>job_200707121733_0003</code> , which represents the third job 
 * running at the jobtracker started at <code>200707121733</code>. 
 * <p>
 * Applications should never construct or parse JobID strings, but rather 
 * use appropriate constructors or {@link #forName(String)} method. 
 * 
 * @see TaskID
 * @see TaskAttemptID
 */
JobInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobInfo.java)/**
 * Represents the basic information that is saved per a job when the 
 * JobTracker receives a submitJob request. The information is saved
 * so that the JobTracker can recover incomplete jobs upon restart.
 */
JobProfile (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobProfile.java)/**************************************************
 * A JobProfile is a MapReduce primitive.  Tracks a job,
 * whether living or dead.
 *
 **************************************************/
JobQueueInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java)/**
 * Class that contains the information regarding the Job Queues which are 
 * maintained by the Hadoop Map/Reduce framework.
 */
JobStatus (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobStatus.java)/**************************************************
 * Describes the current status of a job.  This is
 * not intended to be a comprehensive piece of data.
 * For that, look at JobProfile.
 *************************************************
 **/
JobTracker (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobTracker.java)/**
 * <code>JobTracker</code> is no longer used since M/R 2.x. This is a dummy
 * JobTracker class, which is used to be compatible with M/R 1.x applications.
 */
ArrayListBackedIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/ArrayListBackedIterator.java)/**
 * This class provides an implementation of ResetableIterator. The
 * implementation uses an {@link java.util.ArrayList} to store elements
 * added to it, replaying them as requested.
 * Prefer {@link StreamBackedIterator}.
 */
ComposableInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/ComposableInputFormat.java)/**
 * Refinement of InputFormat requiring implementors to provide
 * ComposableRecordReader instead of RecordReader.
 */
ComposableRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/ComposableRecordReader.java)/**
 * Additional operations required of a RecordReader to participate in a join.
 */
CompositeInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/CompositeInputFormat.java)/**
 * An InputFormat capable of performing joins over a set of data sources sorted
 * and partitioned the same way.
 *
 * A user may define new join types by setting the property
 * <tt>mapred.join.define.&lt;ident&gt;</tt> to a classname. In the expression
 * <tt>mapred.join.expr</tt>, the identifier will be assumed to be a
 * ComposableRecordReader.
 * <tt>mapred.join.keycomparator</tt> can be a classname used to compare keys
 * in the join.
 * @see #setFormat
 * @see JoinRecordReader
 * @see MultiFilterRecordReader
 */
CompositeInputSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/CompositeInputSplit.java)/**
 * This InputSplit contains a set of child InputSplits. Any InputSplit inserted
 * into this collection must have a public default constructor.
 */
JoinCollector (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/CompositeRecordReader.java)/**
   * Collector for join values.
   * This accumulates values for a given key from the child RecordReaders. If
   * one or more child RR contain duplicate keys, this will emit the cross
   * product of the associated values until exhausted.
   */
CompositeRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/CompositeRecordReader.java)/**
 * A RecordReader that can effect joins of RecordReaders sharing a common key
 * type and partitioning.
 */
InnerJoinRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/InnerJoinRecordReader.java)/**
 * Full inner join.
 */
JoinDelegationIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/JoinRecordReader.java)/**
   * Since the JoinCollector is effecting our operation, we need only
   * provide an iterator proxy wrapping its operation.
   */
JoinRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/JoinRecordReader.java)/**
 * Base class for Composite joins returning Tuples of arbitrary Writables.
 */
MultiFilterDelegationIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/MultiFilterRecordReader.java)/**
   * Proxy the JoinCollector, but include callback to emit.
   */
MultiFilterRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/MultiFilterRecordReader.java)/**
 * Base class for Composite join returning values derived from multiple
 * sources, but generally not tuples.
 */
OuterJoinRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/OuterJoinRecordReader.java)/**
 * Full outer join.
 */
OverrideRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/OverrideRecordReader.java)/**
 * Prefer the &quot;rightmost&quot; data source for this key.
 * For example, <tt>override(S1,S2,S3)</tt> will prefer values
 * from S3 over S2, and values from S2 over S1 for all keys
 * emitted from all sources.
 */
Token (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/Parser.java)/**
   * Tagged-union type for tokens from the join expression.
   * @see Parser.TType
   */
Lexer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/Parser.java)/**
   * Simple lexer wrapping a StreamTokenizer.
   * This encapsulates the creation of tagged-union Tokens and initializes the
   * SteamTokenizer.
   */
WNode (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/Parser.java)/**
   * Nodetype in the parse tree for &quot;wrapped&quot; InputFormats.
   */
CNode (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/Parser.java)/**
   * Internal nodetype for &quot;composite&quot; InputFormats.
   */
Parser (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/Parser.java)/**
 * Very simple shift-reduce parser for join expressions.
 *
 * This should be sufficient for the user extension permitted now, but ought to
 * be replaced with a parser generator if more complex grammars are supported.
 * In particular, this &quot;shift-reduce&quot; parser has no states. Each set
 * of formals requires a different internal node type, which is responsible for
 * interpreting the list of tokens it receives. This is sufficient for the
 * current grammar, but it has several annoying properties that might inhibit
 * extension. In particular, parenthesis are always function calls; an
 * algebraic or filter grammar would not only require a node type, but must
 * also work around the internals of this parser.
 *
 * For most other cases, adding classes to the hierarchy- particularly by
 * extending JoinRecordReader and MultiFilterRecordReader- is fairly
 * straightforward. One need only override the relevant method(s) (usually only
 * {@link CompositeRecordReader#combine}) and include a property to map its
 * value to an identifier in the parser.
 */
ResetableIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/ResetableIterator.java)/**
 * This defines an interface to a stateful Iterator that can replay elements
 * added to it directly.
 * Note that this does not extend {@link java.util.Iterator}.
 */
StreamBackedIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/StreamBackedIterator.java)/**
 * This class provides an implementation of ResetableIterator. This
 * implementation uses a byte array to store elements added to it.
 */
TupleWritable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/TupleWritable.java)/**
 * Writable type storing multiple {@link org.apache.hadoop.io.Writable}s.
 *
 * This is *not* a general-purpose tuple type. In almost all cases, users are
 * encouraged to implement their own serializable types, which can perform
 * better validation and provide more efficient encodings than this class is
 * capable. TupleWritable relies on the join framework for type safety and
 * assumes its instances will rarely be persisted, assumptions not only
 * incompatible with, but contrary to the general case.
 *
 * @see org.apache.hadoop.io.Writable
 */
WrappedRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/WrappedRecordReader.java)/**
 * Proxy class for a RecordReader participating in the join framework.
 * This class keeps track of the &quot;head&quot; key-value pair for the
 * provided RecordReader and keeps a store of values matching a key when
 * this source is participating in a join.
 */
JvmTask (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JvmTask.java)/**
 * Task abstraction that can be serialized, implements the writable interface.
 */
KeyValueLineRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/KeyValueLineRecordReader.java)/**
 * This class treats a line in the input as a key/value pair separated by a 
 * separator character. The separator can be specified in config file 
 * under the attribute name mapreduce.input.keyvaluelinerecordreader.key.value.separator. The default
 * separator is the tab character ('\t').
 */
KeyValueTextInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/KeyValueTextInputFormat.java)/**
 * An {@link InputFormat} for plain text files. Files are broken into lines.
 * Either linefeed or carriage-return are used to signal end of line. Each line
 * is divided into key and value parts by a separator byte. If no such a byte
 * exists, the key will be the entire line and value will be empty.
 */
DoubleValueSum (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/DoubleValueSum.java)/**
 * This class implements a value aggregator that sums up a sequence of double
 * values.
 */
LongValueMax (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/LongValueMax.java)/**
 * This class implements a value aggregator that maintain the maximum of 
 * a sequence of long values.
 */
LongValueMin (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/LongValueMin.java)/**
 * This class implements a value aggregator that maintain the minimum of 
 * a sequence of long values.
 */
LongValueSum (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/LongValueSum.java)/**
 * This class implements a value aggregator that sums up 
 * a sequence of long values.
 */
StringValueMax (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/StringValueMax.java)/**
 * This class implements a value aggregator that maintain the biggest of 
 * a sequence of strings.
 */
StringValueMin (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/StringValueMin.java)/**
 * This class implements a value aggregator that maintain the smallest of 
 * a sequence of strings.
 */
UniqValueCount (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/UniqValueCount.java)/**
 * This class implements a value aggregator that dedupes a sequence of objects.
 */
UserDefinedValueAggregatorDescriptor (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/UserDefinedValueAggregatorDescriptor.java)/**
 * This class implements a wrapper for a user defined value aggregator 
 * descriptor.
 * It serves two functions: One is to create an object of 
 * ValueAggregatorDescriptor from the name of a user defined class that may be 
 * dynamically loaded. The other is to delegate invocations of 
 * generateKeyValPairs function to the created object.
 */
ValueAggregator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/ValueAggregator.java)/**
 * This interface defines the minimal protocol for value aggregators.
 */
ValueAggregatorBaseDescriptor (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorBaseDescriptor.java)/** 
 * This class implements the common functionalities of 
 * the subclasses of ValueAggregatorDescriptor class.
 */
ValueAggregatorCombiner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorCombiner.java)/**
 * This class implements the generic combiner of Aggregate.
 */
ValueAggregatorDescriptor (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorDescriptor.java)/**
 * This interface defines the contract a value aggregator descriptor must
 * support. Such a descriptor can be configured with a JobConf object. Its main
 * function is to generate a list of aggregation-id/value pairs. An aggregation
 * id encodes an aggregation type which is used to guide the way to aggregate
 * the value in the reduce/combiner phrase of an Aggregate based job.The mapper in
 * an Aggregate based map/reduce job may create one or more of
 * ValueAggregatorDescriptor objects at configuration time. For each input
 * key/value pair, the mapper will use those objects to create aggregation
 * id/value pairs.
 */
ValueAggregatorJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorJob.java)/**
 * This is the main class for creating a map/reduce job using Aggregate
 * framework. The Aggregate is a specialization of map/reduce framework,
 * specilizing for performing various simple aggregations.
 * 
 * Generally speaking, in order to implement an application using Map/Reduce
 * model, the developer is to implement Map and Reduce functions (and possibly
 * combine function). However, a lot of applications related to counting and
 * statistics computing have very similar characteristics. Aggregate abstracts
 * out the general patterns of these functions and implementing those patterns.
 * In particular, the package provides generic mapper/redducer/combiner classes,
 * and a set of built-in value aggregators, and a generic utility class that
 * helps user create map/reduce jobs using the generic class. The built-in
 * aggregators include:
 * 
 * sum over numeric values count the number of distinct values compute the
 * histogram of values compute the minimum, maximum, media,average, standard
 * deviation of numeric values
 * 
 * The developer using Aggregate will need only to provide a plugin class
 * conforming to the following interface:
 * 
 * public interface ValueAggregatorDescriptor { public ArrayList&lt;Entry&gt;
 * generateKeyValPairs(Object key, Object value); public void
 * configure(JobConfjob); }
 * 
 * The package also provides a base class, ValueAggregatorBaseDescriptor,
 * implementing the above interface. The user can extend the base class and
 * implement generateKeyValPairs accordingly.
 * 
 * The primary work of generateKeyValPairs is to emit one or more key/value
 * pairs based on the input key/value pair. The key in an output key/value pair
 * encode two pieces of information: aggregation type and aggregation id. The
 * value will be aggregated onto the aggregation id according the aggregation
 * type.
 * 
 * This class offers a function to generate a map/reduce job using Aggregate
 * framework. The function takes the following parameters: input directory spec
 * input format (text or sequence file) output directory a file specifying the
 * user plugin class
 */
ValueAggregatorJobBase (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorJobBase.java)/**
 * This abstract class implements some common functionalities of the
 * the generic mapper, reducer and combiner classes of Aggregate.
 */
ValueAggregatorMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorMapper.java)/**
 * This class implements the generic mapper of Aggregate.
 */
ValueAggregatorReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/ValueAggregatorReducer.java)/**
 * This class implements the generic reducer of Aggregate.
 */
ValueHistogram (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/aggregate/ValueHistogram.java)/**
 * This class implements a value aggregator that computes the 
 * histogram of a sequence of strings.
 */
BinaryPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/BinaryPartitioner.java)/**
 * Partition {@link BinaryComparable} keys using a configurable part of 
 * the bytes array returned by {@link BinaryComparable#getBytes()}. 
 * 
 * @see org.apache.hadoop.mapreduce.lib.partition.BinaryPartitioner
 */
ChainOutputCollector (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/Chain.java)/**
   * OutputCollector implementation used by the chain tasks.
   * <p/>
   * If it is not the end of the chain, a {@link #collect} invocation invokes
   * the next Mapper in the chain. If it is the end of the chain the task
   * OutputCollector is called.
   */
Chain (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/Chain.java)/**
 * The Chain class provides all the common functionality for the
 * {@link ChainMapper} and the {@link ChainReducer} classes.
 */
ChainMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/ChainMapper.java)/**
 * The ChainMapper class allows to use multiple Mapper classes within a single
 * Map task.
 * <p>
 * The Mapper classes are invoked in a chained (or piped) fashion, the output of
 * the first becomes the input of the second, and so on until the last Mapper,
 * the output of the last Mapper will be written to the task's output.
 * <p>
 * The key functionality of this feature is that the Mappers in the chain do not
 * need to be aware that they are executed in a chain. This enables having
 * reusable specialized Mappers that can be combined to perform composite
 * operations within a single task.
 * <p>
 * Special care has to be taken when creating chains that the key/values output
 * by a Mapper are valid for the following Mapper in the chain. It is assumed
 * all Mappers and the Reduce in the chain use maching output and input key and
 * value classes as no conversion is done by the chaining code.
 * <p>
 * Using the ChainMapper and the ChainReducer classes is possible to compose
 * Map/Reduce jobs that look like <code>[MAP+ / REDUCE MAP*]</code>. And
 * immediate benefit of this pattern is a dramatic reduction in disk IO.
 * <p>
 * IMPORTANT: There is no need to specify the output key/value classes for the
 * ChainMapper, this is done by the addMapper for the last mapper in the chain.
 * <p>
 * ChainMapper usage pattern:
 * <p>
 * <pre>
 * ...
 * conf.setJobName("chain");
 * conf.setInputFormat(TextInputFormat.class);
 * conf.setOutputFormat(TextOutputFormat.class);
 *
 * JobConf mapAConf = new JobConf(false);
 * ...
 * ChainMapper.addMapper(conf, AMap.class, LongWritable.class, Text.class,
 *   Text.class, Text.class, true, mapAConf);
 *
 * JobConf mapBConf = new JobConf(false);
 * ...
 * ChainMapper.addMapper(conf, BMap.class, Text.class, Text.class,
 *   LongWritable.class, Text.class, false, mapBConf);
 *
 * JobConf reduceConf = new JobConf(false);
 * ...
 * ChainReducer.setReducer(conf, XReduce.class, LongWritable.class, Text.class,
 *   Text.class, Text.class, true, reduceConf);
 *
 * ChainReducer.addMapper(conf, CMap.class, Text.class, Text.class,
 *   LongWritable.class, Text.class, false, null);
 *
 * ChainReducer.addMapper(conf, DMap.class, LongWritable.class, Text.class,
 *   LongWritable.class, LongWritable.class, true, null);
 *
 * FileInputFormat.setInputPaths(conf, inDir);
 * FileOutputFormat.setOutputPath(conf, outDir);
 * ...
 *
 * JobClient jc = new JobClient(conf);
 * RunningJob job = jc.submitJob(conf);
 * ...
 * </pre>
 */
ChainReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/ChainReducer.java)/**
 * The ChainReducer class allows to chain multiple Mapper classes after a
 * Reducer within the Reducer task.
 * <p>
 * For each record output by the Reducer, the Mapper classes are invoked in a
 * chained (or piped) fashion, the output of the first becomes the input of the
 * second, and so on until the last Mapper, the output of the last Mapper will
 * be written to the task's output.
 * <p>
 * The key functionality of this feature is that the Mappers in the chain do not
 * need to be aware that they are executed after the Reducer or in a chain.
 * This enables having reusable specialized Mappers that can be combined to
 * perform composite operations within a single task.
 * <p>
 * Special care has to be taken when creating chains that the key/values output
 * by a Mapper are valid for the following Mapper in the chain. It is assumed
 * all Mappers and the Reduce in the chain use maching output and input key and
 * value classes as no conversion is done by the chaining code.
 * <p>
 * Using the ChainMapper and the ChainReducer classes is possible to compose
 * Map/Reduce jobs that look like <code>[MAP+ / REDUCE MAP*]</code>. And
 * immediate benefit of this pattern is a dramatic reduction in disk IO.
 * <p>
 * IMPORTANT: There is no need to specify the output key/value classes for the
 * ChainReducer, this is done by the setReducer or the addMapper for the last
 * element in the chain.
 * <p>
 * ChainReducer usage pattern:
 * <p>
 * <pre>
 * ...
 * conf.setJobName("chain");
 * conf.setInputFormat(TextInputFormat.class);
 * conf.setOutputFormat(TextOutputFormat.class);
 *
 * JobConf mapAConf = new JobConf(false);
 * ...
 * ChainMapper.addMapper(conf, AMap.class, LongWritable.class, Text.class,
 *   Text.class, Text.class, true, mapAConf);
 *
 * JobConf mapBConf = new JobConf(false);
 * ...
 * ChainMapper.addMapper(conf, BMap.class, Text.class, Text.class,
 *   LongWritable.class, Text.class, false, mapBConf);
 *
 * JobConf reduceConf = new JobConf(false);
 * ...
 * ChainReducer.setReducer(conf, XReduce.class, LongWritable.class, Text.class,
 *   Text.class, Text.class, true, reduceConf);
 *
 * ChainReducer.addMapper(conf, CMap.class, Text.class, Text.class,
 *   LongWritable.class, Text.class, false, null);
 *
 * ChainReducer.addMapper(conf, DMap.class, LongWritable.class, Text.class,
 *   LongWritable.class, LongWritable.class, true, null);
 *
 * FileInputFormat.setInputPaths(conf, inDir);
 * FileOutputFormat.setOutputPath(conf, outDir);
 * ...
 *
 * JobClient jc = new JobClient(conf);
 * RunningJob job = jc.submitJob(conf);
 * ...
 * </pre>
 */
CombineFileInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/CombineFileInputFormat.java)/**
 * An abstract {@link org.apache.hadoop.mapred.InputFormat} that returns {@link CombineFileSplit}'s
 * in {@link org.apache.hadoop.mapred.InputFormat#getSplits(JobConf, int)} method. 
 * Splits are constructed from the files under the input paths. 
 * A split cannot have files from different pools.
 * Each split returned may contain blocks from different files.
 * If a maxSplitSize is specified, then blocks on the same node are
 * combined to form a single split. Blocks that are left over are
 * then combined with other blocks in the same rack. 
 * If maxSplitSize is not specified, then blocks from the same rack
 * are combined in a single split; no attempt is made to create
 * node-local splits.
 * If the maxSplitSize is equal to the block size, then this class
 * is similar to the default spliting behaviour in Hadoop: each
 * block is a locally processed split.
 * Subclasses implement {@link org.apache.hadoop.mapred.InputFormat#getRecordReader(InputSplit, JobConf, Reporter)}
 * to construct <code>RecordReader</code>'s for <code>CombineFileSplit</code>'s.
 * @see CombineFileSplit
 */
CombineFileRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/CombineFileRecordReader.java)/**
 * A generic RecordReader that can hand out different recordReaders
 * for each chunk in a {@link CombineFileSplit}.
 * A CombineFileSplit can combine data chunks from multiple files. 
 * This class allows using different RecordReaders for processing
 * these data chunks from different files.
 * @see CombineFileSplit
 */
CombineFileRecordReaderWrapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/CombineFileRecordReaderWrapper.java)/**
 * A wrapper class for a record reader that handles a single file split. It
 * delegates most of the methods to the wrapped instance. A concrete subclass
 * needs to provide a constructor that calls this parent constructor with the
 * appropriate input format. The subclass constructor must satisfy the specific
 * constructor signature that is required by
 * <code>CombineFileRecordReader</code>.
 *
 * Subclassing is needed to get a concrete record reader wrapper because of the
 * constructor requirement.
 *
 * @see CombineFileRecordReader
 * @see CombineFileInputFormat
 */
SequenceFileRecordReaderWrapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/CombineSequenceFileInputFormat.java)/**
   * A record reader that may be passed to <code>CombineFileRecordReader</code>
   * so that it can be used in a <code>CombineFileInputFormat</code>-equivalent
   * for <code>SequenceFileInputFormat</code>.
   *
   * @see CombineFileRecordReader
   * @see CombineFileInputFormat
   * @see SequenceFileInputFormat
   */
CombineSequenceFileInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/CombineSequenceFileInputFormat.java)/**
 * Input format that is a <code>CombineFileInputFormat</code>-equivalent for
 * <code>SequenceFileInputFormat</code>.
 *
 * @see CombineFileInputFormat
 */
TextRecordReaderWrapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/CombineTextInputFormat.java)/**
   * A record reader that may be passed to <code>CombineFileRecordReader</code>
   * so that it can be used in a <code>CombineFileInputFormat</code>-equivalent
   * for <code>TextInputFormat</code>.
   *
   * @see CombineFileRecordReader
   * @see CombineFileInputFormat
   * @see TextInputFormat
   */
CombineTextInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/CombineTextInputFormat.java)/**
 * Input format that is a <code>CombineFileInputFormat</code>-equivalent for
 * <code>TextInputFormat</code>.
 *
 * @see CombineFileInputFormat
 */
DBRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/db/DBInputFormat.java)/**
   * A RecordReader that reads records from a SQL table.
   * Emits LongWritables containing the record number as 
   * key and DBWritables as value.  
   */
DBRecordReaderWrapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/db/DBInputFormat.java)/**
   * A RecordReader implementation that just passes through to a wrapped
   * RecordReader built with the new API.
   */
NullDBWritable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/db/DBInputFormat.java)/**
   * A Class that does nothing, implementing DBWritable
   */
DBInputSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/db/DBInputFormat.java)/**
   * A InputSplit that spans a set of rows
   */
DBRecordWriter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/db/DBOutputFormat.java)/**
   * A RecordWriter that writes the reduce output to a SQL table
   */
DelegatingInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/DelegatingInputFormat.java)/**
 * An {@link InputFormat} that delegates behaviour of paths to multiple other
 * InputFormats.
 * 
 * @see MultipleInputs#addInputPath(JobConf, Path, Class, Class)
 */
DelegatingMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/DelegatingMapper.java)/**
 * An {@link Mapper} that delegates behaviour of paths to multiple other
 * mappers.
 * 
 * @see MultipleInputs#addInputPath(JobConf, Path, Class, Class)
 */
FieldSelectionMapReduce (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/FieldSelectionMapReduce.java)/**
 * This class implements a mapper/reducer class that can be used to perform
 * field selections in a manner similar to unix cut. The input data is treated
 * as fields separated by a user specified separator (the default value is
 * "\t"). The user can specify a list of fields that form the map output keys,
 * and a list of fields that form the map output values. If the inputformat is
 * TextInputFormat, the mapper will ignore the key to the map function. and the
 * fields are from the value only. Otherwise, the fields are the union of those
 * from the key and those from the value.
 * 
 * The field separator is under attribute "mapreduce.fieldsel.data.field.separator"
 * 
 * The map output field list spec is under attribute 
 * "mapreduce.fieldsel.map.output.key.value.fields.spec".
 * The value is expected to be like "keyFieldsSpec:valueFieldsSpec"
 * key/valueFieldsSpec are comma (,) separated field spec: fieldSpec,fieldSpec,fieldSpec ...
 * Each field spec can be a simple number (e.g. 5) specifying a specific field, or a range
 * (like 2-5) to specify a range of fields, or an open range (like 3-) specifying all 
 * the fields starting from field 3. The open range field spec applies value fields only.
 * They have no effect on the key fields.
 * 
 * Here is an example: "4,3,0,1:6,5,1-3,7-". It specifies to use fields 4,3,0 and 1 for keys,
 * and use fields 6,5,1,2,3,7 and above for values.
 * 
 * The reduce output field list spec is under attribute 
 * "mapreduce.fieldsel.reduce.output.key.value.fields.spec".
 * 
 * The reducer extracts output key/value pairs in a similar manner, except that
 * the key is never ignored.
 */
FilterOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/FilterOutputFormat.java)/**
 * FilterOutputFormat is a convenience class that wraps OutputFormat. 
 */
HashPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/HashPartitioner.java)/** 
 * Partition keys by their {@link Object#hashCode()}. 
 */
IdentityMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/IdentityMapper.java)/** 
 * Implements the identity function, mapping inputs directly to outputs. 
 */
IdentityReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/IdentityReducer.java)/** 
 * Performs no reduction, writing all input values directly to the output. 
 */
Sampler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/InputSampler.java)/**
   * Interface to sample using an {@link org.apache.hadoop.mapred.InputFormat}.
   */
SplitSampler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/InputSampler.java)/**
   * Samples the first n records from s splits.
   * Inexpensive way to sample random data.
   */
RandomSampler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/InputSampler.java)/**
   * Sample from random points in the input.
   * General-purpose sampler. Takes numSamples / maxSplitsSampled inputs from
   * each split.
   */
IntervalSampler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/InputSampler.java)/**
   * Sample from s splits at regular intervals.
   * Useful for sorted data.
   */
InverseMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/InverseMapper.java)/** 
 * A {@link Mapper} that swaps keys and values. 
 */
KeyFieldBasedComparator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/KeyFieldBasedComparator.java)/**
 * This comparator implementation provides a subset of the features provided
 * by the Unix/GNU Sort. In particular, the supported features are:
 * -n, (Sort numerically)
 * -r, (Reverse the result of comparison)
 * -k pos1[,pos2], where pos is of the form f[.c][opts], where f is the number
 *  of the field to use, and c is the number of the first character from the
 *  beginning of the field. Fields and character posns are numbered starting
 *  with 1; a character position of zero in pos2 indicates the field's last
 *  character. If '.c' is omitted from pos1, it defaults to 1 (the beginning
 *  of the field); if omitted from pos2, it defaults to 0 (the end of the
 *  field). opts are ordering options (any of 'nr' as described above). 
 * We assume that the fields in the key are separated by
 * {@link JobContext#MAP_OUTPUT_KEY_FIELD_SEPARATOR}
 */
KeyFieldBasedPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/KeyFieldBasedPartitioner.java)/**   
  *  Defines a way to partition keys based on certain key fields (also see
  *  {@link KeyFieldBasedComparator}.
  *  The key specification supported is of the form -k pos1[,pos2], where,
  *  pos is of the form f[.c][opts], where f is the number
  *  of the key field to use, and c is the number of the first character from
  *  the beginning of the field. Fields and character posns are numbered 
  *  starting with 1; a character position of zero in pos2 indicates the
  *  field's last character. If '.c' is omitted from pos1, it defaults to 1
  *  (the beginning of the field); if omitted from pos2, it defaults to 0 
  *  (the end of the field).
  */
LazyOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/LazyOutputFormat.java)/**
 * A Convenience class that creates output lazily. 
 */
LongSumReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/LongSumReducer.java)/** 
 * A {@link Reducer} that sums long values. 
 */
MultipleInputs (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/MultipleInputs.java)/**
 * This class supports MapReduce jobs that have multiple input paths with
 * a different {@link InputFormat} and {@link Mapper} for each path 
 */
MultipleOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/MultipleOutputFormat.java)/**
 * This abstract class extends the FileOutputFormat, allowing to write the
 * output data to different output files. There are three basic use cases for
 * this class.
 * 
 * Case one: This class is used for a map reduce job with at least one reducer.
 * The reducer wants to write data to different files depending on the actual
 * keys. It is assumed that a key (or value) encodes the actual key (value)
 * and the desired location for the actual key (value).
 * 
 * Case two: This class is used for a map only job. The job wants to use an
 * output file name that is either a part of the input file name of the input
 * data, or some derivation of it.
 * 
 * Case three: This class is used for a map only job. The job wants to use an
 * output file name that depends on both the keys and the input file name,
 */
MultipleOutputs (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/MultipleOutputs.java)/**
 * The MultipleOutputs class simplifies writing to additional outputs other
 * than the job default output via the <code>OutputCollector</code> passed to
 * the <code>map()</code> and <code>reduce()</code> methods of the
 * <code>Mapper</code> and <code>Reducer</code> implementations.
 * <p>
 * Each additional output, or named output, may be configured with its own
 * <code>OutputFormat</code>, with its own key class and with its own value
 * class.
 * <p>
 * A named output can be a single file or a multi file. The later is referred as
 * a multi named output.
 * <p>
 * A multi named output is an unbound set of files all sharing the same
 * <code>OutputFormat</code>, key class and value class configuration.
 * <p>
 * When named outputs are used within a <code>Mapper</code> implementation,
 * key/values written to a name output are not part of the reduce phase, only
 * key/values written to the job <code>OutputCollector</code> are part of the
 * reduce phase.
 * <p>
 * MultipleOutputs supports counters, by default the are disabled. The counters
 * group is the {@link MultipleOutputs} class name.
 * </p>
 * The names of the counters are the same as the named outputs. For multi
 * named outputs the name of the counter is the concatenation of the named
 * output, and underscore '_' and the multiname.
 * <p>
 * Job configuration usage pattern is:
 * <pre>
 *
 * JobConf conf = new JobConf();
 *
 * conf.setInputPath(inDir);
 * FileOutputFormat.setOutputPath(conf, outDir);
 *
 * conf.setMapperClass(MOMap.class);
 * conf.setReducerClass(MOReduce.class);
 * ...
 *
 * // Defines additional single text based output 'text' for the job
 * MultipleOutputs.addNamedOutput(conf, "text", TextOutputFormat.class,
 * LongWritable.class, Text.class);
 *
 * // Defines additional multi sequencefile based output 'sequence' for the
 * // job
 * MultipleOutputs.addMultiNamedOutput(conf, "seq",
 *   SequenceFileOutputFormat.class,
 *   LongWritable.class, Text.class);
 * ...
 *
 * JobClient jc = new JobClient();
 * RunningJob job = jc.submitJob(conf);
 *
 * ...
 * </pre>
 * <p>
 * Job configuration usage pattern is:
 * <pre>
 *
 * public class MOReduce implements
 *   Reducer&lt;WritableComparable, Writable&gt; {
 * private MultipleOutputs mos;
 *
 * public void configure(JobConf conf) {
 * ...
 * mos = new MultipleOutputs(conf);
 * }
 *
 * public void reduce(WritableComparable key, Iterator&lt;Writable&gt; values,
 * OutputCollector output, Reporter reporter)
 * throws IOException {
 * ...
 * mos.getCollector("text", reporter).collect(key, new Text("Hello"));
 * mos.getCollector("seq", "A", reporter).collect(key, new Text("Bye"));
 * mos.getCollector("seq", "B", reporter).collect(key, new Text("Chau"));
 * ...
 * }
 *
 * public void close() throws IOException {
 * mos.close();
 * ...
 * }
 *
 * }
 * </pre>
 */
MultipleSequenceFileOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/MultipleSequenceFileOutputFormat.java)/**
 * This class extends the MultipleOutputFormat, allowing to write the output data 
 * to different output files in sequence file output format. 
 */
MultipleTextOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/MultipleTextOutputFormat.java)/**
 * This class extends the MultipleOutputFormat, allowing to write the output
 * data to different output files in Text output format.
 */
BlockingArrayQueue (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/MultithreadedMapRunner.java)/**
   * A blocking array queue that replaces offer and add, which throws on a full
   * queue, to a put, which waits on a full queue.
   */
MapperInvokeRunable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/MultithreadedMapRunner.java)/**
   * Runnable to execute a single Mapper.map call from a forked thread.
   */
MultithreadedMapRunner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/MultithreadedMapRunner.java)/**
 * Multithreaded implementation for {@link MapRunnable}.
 * <p>
 * It can be used instead of the default implementation,
 * of {@link org.apache.hadoop.mapred.MapRunner}, when the Map
 * operation is not CPU bound in order to improve throughput.
 * <p>
 * Map implementations using this MapRunnable must be thread-safe.
 * <p>
 * The Map-Reduce job has to be configured to use this MapRunnable class (using
 * the JobConf.setMapRunnerClass method) and
 * the number of threads the thread-pool can use with the
 * <code>mapred.map.multithreadedrunner.threads</code> property, its default
 * value is 10 threads.
 * <p>
 */
NLineInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/NLineInputFormat.java)/**
 * NLineInputFormat which splits N lines of input as one split.
 *
 * In many "pleasantly" parallel applications, each process/mapper 
 * processes the same input file (s), but with computations are 
 * controlled by different parameters.(Referred to as "parameter sweeps").
 * One way to achieve this, is to specify a set of parameters 
 * (one set per line) as input in a control file 
 * (which is the input path to the map-reduce application,
 * where as the input dataset is specified 
 * via a config variable in JobConf.).
 * 
 * The NLineInputFormat can be used in such applications, that splits 
 * the input file such that by default, one line is fed as
 * a value to one map task, and key is the offset.
 * i.e. (k,v) is (LongWritable, Text).
 * The location hints will span the whole mapred cluster.
 */
NullOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/NullOutputFormat.java)/**
 * Consume all outputs and put them in /dev/null. 
 */
RegexMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/RegexMapper.java)/** 
 * A {@link Mapper} that extracts text matching a regular expression.
 */
TaggedInputSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/TaggedInputSplit.java)/**
 * An {@link InputSplit} that tags another InputSplit with extra data for use
 * by {@link DelegatingInputFormat}s and {@link DelegatingMapper}s.
 */
TokenCountMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/TokenCountMapper.java)/** 
 * A {@link Mapper} that maps text values into &lt;token,freq&gt; pairs.  Uses
 * {@link StringTokenizer} to break text into tokens. 
 */
TotalOrderPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/TotalOrderPartitioner.java)/**
 * Partitioner effecting a total order by reading split points from
 * an externally generated source.
 */
LineReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LineRecordReader.java)/**
   * A class that provides a line reader from an input stream.
   * @deprecated Use {@link org.apache.hadoop.util.LineReader} instead.
   */
LineRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LineRecordReader.java)/**
 * Treats keys as offset in file and value as line. 
 */
ProcessInputDirCallable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LocatedFileStatusFetcher.java)/**
   * Retrieves block locations for the given @link {@link FileStatus}, and adds
   * additional paths to the process queue if required.
   */
ProcessInputDirCallback (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LocatedFileStatusFetcher.java)/**
   * The callback handler to handle results generated by
   * {@link ProcessInputDirCallable}. This populates the final result set.
   * 
   */
ProcessInitialInputPathCallable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LocatedFileStatusFetcher.java)/**
   * Processes an initial Input Path pattern through the globber and PathFilter
   * to generate a list of files which need further processing.
   */
ProcessInitialInputPathCallback (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LocatedFileStatusFetcher.java)/**
   * The callback handler to handle results generated by
   * {@link ProcessInitialInputPathCallable}.
   * 
   */
LocatedFileStatusFetcher (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LocatedFileStatusFetcher.java)/**
 * Utility class to fetch block locations for specified Input paths using a
 * configured number of threads.
 * The thread count is determined from the value of
 * "mapreduce.input.fileinputformat.list-status.num-threads" in the
 * configuration.
 */
MapFileOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapFileOutputFormat.java)/** An {@link OutputFormat} that writes {@link MapFile}s.
 */
MapOutputFile (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapOutputFile.java)/**
 * Manipulate the working area for the transient store for maps and reduces.
 *
 * This class is used by map and reduce tasks to identify the directories that
 * they need to write to/read from for intermediate files. The callers of
 * these methods are from child space and see mapreduce.cluster.local.dir as
 * taskTracker/jobCache/jobId/attemptId
 * This class should not be used from TaskTracker space.
 */
Mapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Mapper.java)/** 
 * Maps input key/value pairs to a set of intermediate key/value pairs.  
 * 
 * <p>Maps are the individual tasks which transform input records into a 
 * intermediate records. The transformed intermediate records need not be of 
 * the same type as the input records. A given input pair may map to zero or 
 * many output pairs.</p> 
 * 
 * <p>The Hadoop Map-Reduce framework spawns one map task for each 
 * {@link InputSplit} generated by the {@link InputFormat} for the job.
 * <code>Mapper</code> implementations can access the {@link JobConf} for the 
 * job via the {@link JobConfigurable#configure(JobConf)} and initialize
 * themselves. Similarly they can use the {@link Closeable#close()} method for
 * de-initialization.</p>
 * 
 * <p>The framework then calls 
 * {@link #map(Object, Object, OutputCollector, Reporter)} 
 * for each key/value pair in the <code>InputSplit</code> for that task.</p>
 * 
 * <p>All intermediate values associated with a given output key are 
 * subsequently grouped by the framework, and passed to a {@link Reducer} to  
 * determine the final output. Users can control the grouping by specifying
 * a <code>Comparator</code> via 
 * {@link JobConf#setOutputKeyComparatorClass(Class)}.</p>
 *
 * <p>The grouped <code>Mapper</code> outputs are partitioned per 
 * <code>Reducer</code>. Users can control which keys (and hence records) go to 
 * which <code>Reducer</code> by implementing a custom {@link Partitioner}.
 * 
 * <p>Users can optionally specify a <code>combiner</code>, via 
 * {@link JobConf#setCombinerClass(Class)}, to perform local aggregation of the 
 * intermediate outputs, which helps to cut down the amount of data transferred 
 * from the <code>Mapper</code> to the <code>Reducer</code>.
 * 
 * <p>The intermediate, grouped outputs are always stored in 
 * {@link SequenceFile}s. Applications can specify if and how the intermediate
 * outputs are to be compressed and which {@link CompressionCodec}s are to be
 * used via the <code>JobConf</code>.</p>
 *  
 * <p>If the job has 
 * <a href="{@docRoot}/org/apache/hadoop/mapred/JobConf.html#ReducerNone">zero
 * reduces</a> then the output of the <code>Mapper</code> is directly written
 * to the {@link FileSystem} without grouping by keys.</p>
 * 
 * <p>Example:</p>
 * <p><blockquote><pre>
 *     public class MyMapper&lt;K extends WritableComparable, V extends Writable&gt; 
 *     extends MapReduceBase implements Mapper&lt;K, V, K, V&gt; {
 *     
 *       static enum MyCounters { NUM_RECORDS }
 *       
 *       private String mapTaskId;
 *       private String inputFile;
 *       private int noRecords = 0;
 *       
 *       public void configure(JobConf job) {
 *         mapTaskId = job.get(JobContext.TASK_ATTEMPT_ID);
 *         inputFile = job.get(JobContext.MAP_INPUT_FILE);
 *       }
 *       
 *       public void map(K key, V val,
 *                       OutputCollector&lt;K, V&gt; output, Reporter reporter)
 *       throws IOException {
 *         // Process the &lt;key, value&gt; pair (assume this takes a while)
 *         // ...
 *         // ...
 *         
 *         // Let the framework know that we are alive, and kicking!
 *         // reporter.progress();
 *         
 *         // Process some more
 *         // ...
 *         // ...
 *         
 *         // Increment the no. of &lt;key, value&gt; pairs processed
 *         ++noRecords;
 *
 *         // Increment counters
 *         reporter.incrCounter(NUM_RECORDS, 1);
 *        
 *         // Every 100 records update application-level status
 *         if ((noRecords%100) == 0) {
 *           reporter.setStatus(mapTaskId + " processed " + noRecords + 
 *                              " from input-file: " + inputFile); 
 *         }
 *         
 *         // Output the result
 *         output.collect(key, val);
 *       }
 *     }
 * </pre></blockquote>
 *
 * <p>Applications may write a custom {@link MapRunnable} to exert greater
 * control on map processing e.g. multi-threaded <code>Mapper</code>s etc.</p>
 * 
 * @see JobConf
 * @see InputFormat
 * @see Partitioner  
 * @see Reducer
 * @see MapReduceBase
 * @see MapRunnable
 * @see SequenceFile
 */
MapReduceBase (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapReduceBase.java)/** 
 * Base class for {@link Mapper} and {@link Reducer} implementations.
 * 
 * <p>Provides default no-op implementations for a few methods, most non-trivial
 * applications need to override some of them.</p>
 */
MapRunnable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapRunnable.java)/**
 * Expert: Generic interface for {@link Mapper}s.
 * 
 * <p>Custom implementations of <code>MapRunnable</code> can exert greater 
 * control on map processing e.g. multi-threaded, asynchronous mappers etc.</p>
 * 
 * @see Mapper
 */
MapRunner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapRunner.java)/** Default {@link MapRunnable} implementation.*/
TrackedRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java)/**
   * This class wraps the user's record reader to update the counters and progress
   * as records are read.
   * @param <K>
   * @param <V>
   */
SkippingRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java)/**
   * This class skips the records based on the failed ranges from previous 
   * attempts.
   */
OldOutputCollector (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java)/**
   * Since the mapred and mapreduce Partitioners don't share a common interface
   * (JobConfigurable is deprecated and a subtype of mapred.Partitioner), the
   * partitioner lives in Old/NewOutputCollector. Note that, for map-only jobs,
   * the configured partitioner should not be called. It's common for
   * partitioners to compute a result mod numReduces, which causes a div0 error
   */
BlockingBuffer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java)/**
     * Inner class managing the spill of serialized records to disk.
     */
InMemValBytes (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java)/**
     * Inner class wrapping valuebytes, used for appendRaw.
     */
MapBufferTooSmallException (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java)/**
   * Exception indicating that the allocated sort buffer is insufficient
   * to hold the current record.
   */
MapTask (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java)/** A Map task. */
MapTaskCompletionEventsUpdate (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTaskCompletionEventsUpdate.java)/**
 * A class that represents the communication between the tasktracker and child
 * tasks w.r.t the map task completion events. It also indicates whether the
 * child task should reset its events index.
 */
Merger (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Merger.java)/**
 * Merger is an utility class used by the Map and Reduce tasks for merging
 * both their memory and disk segments
 */
MergeSorter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MergeSorter.java)/** This class implements the sort method from BasicTypeSorterBase class as
 * MergeSort. Note that this class is really a wrapper over the actual
 * mergesort implementation that is there in the util package. The main intent
 * of providing this class is to setup the input data for the util.MergeSort
 * algo so that the latter doesn't need to bother about the various data 
 * structures that have been created for the Map output but rather concentrate 
 * on the core algorithm (thereby allowing easy integration of a mergesort
 * implementation). The bridge between this class and the util.MergeSort class
 * is the Comparator.
 */
MRConstants (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MRConstants.java)/*******************************
 * Some handy constants
 * 
 *******************************/
MROutputFiles (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MROutputFiles.java)/**
 * Manipulate the working area for the transient store for maps and reduces.
 *
 * This class is used by map and reduce tasks to identify the directories that
 * they need to write to/read from for intermediate files. The callers of
 * these methods are from the Child running the Task.
 */
MultiFileInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MultiFileInputFormat.java)/**
 * An abstract {@link InputFormat} that returns {@link MultiFileSplit}'s
 * in {@link #getSplits(JobConf, int)} method. Splits are constructed from 
 * the files under the input paths. Each split returned contains <i>nearly</i>
 * equal content length. <br>  
 * Subclasses implement {@link #getRecordReader(InputSplit, JobConf, Reporter)}
 * to construct <code>RecordReader</code>'s for <code>MultiFileSplit</code>'s.
 * @see MultiFileSplit
 */
MultiFileSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MultiFileSplit.java)/**
 * A sub-collection of input files. Unlike {@link FileSplit}, MultiFileSplit 
 * class does not represent a split of a file, but a split of input files 
 * into smaller sets. The atomic unit of split is a file. <br> 
 * MultiFileSplit can be used to implement {@link RecordReader}'s, with 
 * reading one record per file.
 * @see FileSplit
 * @see MultiFileInputFormat 
 */
OutputCollector (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/OutputCollector.java)/**
 * Collects the <code>&lt;key, value&gt;</code> pairs output by {@link Mapper}s
 * and {@link Reducer}s.
 *  
 * <p><code>OutputCollector</code> is the generalization of the facility 
 * provided by the Map-Reduce framework to collect data output by either the 
 * <code>Mapper</code> or the <code>Reducer</code> i.e. intermediate outputs 
 * or the output of the job.</p>  
 */
OutputCommitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/OutputCommitter.java)/**
 * <code>OutputCommitter</code> describes the commit of task output for a 
 * Map-Reduce job.
 *
 * <p>The Map-Reduce framework relies on the <code>OutputCommitter</code> of 
 * the job to:<p>
 * <ol>
 *   <li>
 *   Setup the job during initialization. For example, create the temporary 
 *   output directory for the job during the initialization of the job.
 *   </li>
 *   <li>
 *   Cleanup the job after the job completion. For example, remove the
 *   temporary output directory after the job completion. 
 *   </li>
 *   <li>
 *   Setup the task temporary output.
 *   </li> 
 *   <li>
 *   Check whether a task needs a commit. This is to avoid the commit
 *   procedure if a task does not need commit.
 *   </li>
 *   <li>
 *   Commit of the task output.
 *   </li>  
 *   <li>
 *   Discard the task commit.
 *   </li>
 * </ol>
 * The methods in this class can be called from several different processes and
 * from several different contexts.  It is important to know which process and
 * which context each is called from.  Each method should be marked accordingly
 * in its documentation.  It is also important to note that not all methods are
 * guaranteed to be called once and only once.  If a method is not guaranteed to
 * have this property the output committer needs to handle this appropriately. 
 * Also note it will only be in rare situations where they may be called 
 * multiple times for the same task.
 * 
 * @see FileOutputCommitter 
 * @see JobContext
 * @see TaskAttemptContext 
 */
OutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/OutputFormat.java)/** 
 * <code>OutputFormat</code> describes the output-specification for a 
 * Map-Reduce job.
 *
 * <p>The Map-Reduce framework relies on the <code>OutputFormat</code> of the
 * job to:<p>
 * <ol>
 *   <li>
 *   Validate the output-specification of the job. For e.g. check that the 
 *   output directory doesn't already exist. 
 *   <li>
 *   Provide the {@link RecordWriter} implementation to be used to write out
 *   the output files of the job. Output files are stored in a 
 *   {@link FileSystem}.
 *   </li>
 * </ol>
 * 
 * @see RecordWriter
 * @see JobConf
 */
OutputLogFilter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/OutputLogFilter.java)/**
 * This class filters log files from directory given
 * It doesnt accept paths having _logs.
 * This can be used to list paths of output directory as follows:
 *   Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir,
 *                                   new OutputLogFilter()));
 */
Partitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Partitioner.java)/** 
 * Partitions the key space.
 * 
 * <p><code>Partitioner</code> controls the partitioning of the keys of the 
 * intermediate map-outputs. The key (or a subset of the key) is used to derive
 * the partition, typically by a hash function. The total number of partitions
 * is the same as the number of reduce tasks for the job. Hence this controls
 * which of the <code>m</code> reduce tasks the intermediate key (and hence the 
 * record) is sent for reduction.</p>
 *
 * <p>Note: A <code>Partitioner</code> is created only when there are multiple
 * reducers.</p>
 * 
 * @see Reducer
 */
PeriodicStatsAccumulator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/PeriodicStatsAccumulator.java)/**
 *
 * This abstract class that represents a bucketed series of
 *  measurements of a quantity being measured in a running task
 *  attempt. 
 *
 * <p>The sole constructor is called with a count, which is the
 *  number of buckets into which we evenly divide the spectrum of
 *  progress from 0.0D to 1.0D .  In the future we may provide for
 *  custom split points that don't have to be uniform.
 *
 * <p>A subclass determines how we fold readings for portions of a
 *  bucket and how we interpret the readings by overriding
 *  {@code extendInternal(...)} and {@code initializeInterval()}
 */
Application (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/Application.java)/**
 * This class is responsible for launching and communicating with the child 
 * process.
 */
TeeOutputStream (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java)/**
   * An output stream that will save a copy of the data into a file.
   */
BinaryProtocol (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/BinaryProtocol.java)/**
 * This protocol is a binary implementation of the Pipes protocol.
 */
DownwardProtocol (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/DownwardProtocol.java)/**
 * The abstract description of the downward (from Java to C++) Pipes protocol.
 * All of these calls are asynchronous and return before the message has been 
 * processed.
 */
OutputHandler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/OutputHandler.java)/**
 * Handles the upward (C++ to Java) messages from the application.
 */
PipesMapRunner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/PipesMapRunner.java)/**
 * An adaptor to run a C++ mapper.
 */
PipesDummyRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/PipesNonJavaInputFormat.java)/**
   * A dummy {@link org.apache.hadoop.mapred.RecordReader} to help track the
   * progress of Hadoop Pipes' applications when they are using a non-Java
   * <code>RecordReader</code>.
   *
   * The <code>PipesDummyRecordReader</code> is informed of the 'progress' of
   * the task by the {@link OutputHandler#progress(float)} which calls the
   * {@link #next(FloatWritable, NullWritable)} with the progress as the
   * <code>key</code>.
   */
PipesNonJavaInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/PipesNonJavaInputFormat.java)/**
 * Dummy input format used when non-Java a {@link RecordReader} is used by
 * the Pipes' application.
 *
 * The only useful thing this does is set up the Map-Reduce job to get the
 * {@link PipesDummyRecordReader}, everything else left for the 'actual'
 * InputFormat specified by the user which is given by 
 * <i>mapreduce.pipes.inputformat</i>.
 */
PipesPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/PipesPartitioner.java)/**
 * This partitioner is one that can either be set manually per a record or it
 * can fall back onto a Java partitioner that was set by the user.
 */
PipesReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/PipesReducer.java)/**
 * This class is used to talk to a C++ reduce task.
 */
CommandLineParser (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/Submitter.java)/**
   * A command line parser for the CLI-based Pipes job submitter.
   */
Submitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/Submitter.java)/**
 * The main entry point and job submitter. It may either be used as a command
 * line-based or API-based method to launch Pipes jobs.
 */
UpwardProtocol (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/UpwardProtocol.java)/**
 * The interface for the messages that can come up from the child. All of these
 * calls are asynchronous and return before the message has been processed.
 */
Queue (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Queue.java)/**
 * A class for storing the properties of a job queue.
 */
QueueAclsInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/QueueAclsInfo.java)/**
 *  Class to encapsulate Queue ACLs for a particular
 *  user.
 */
QueueConfigurationParser (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/QueueConfigurationParser.java)/**
 * Class for parsing mapred-queues.xml.
 *    The format consists nesting of
 *    queues within queues - a feature called hierarchical queues.
 *    The parser expects that queues are
 *    defined within the 'queues' tag which is the top level element for
 *    XML document.
 * 
 * Creates the complete queue hieararchy
 */
QueueManager (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/QueueManager.java)/**
 * Class that exposes information about queues maintained by the Hadoop
 * Map/Reduce framework.
 * <p>
 * The Map/Reduce framework can be configured with one or more queues,
 * depending on the scheduler it is configured with. While some
 * schedulers work only with one queue, some schedulers support multiple
 * queues. Some schedulers also support the notion of queues within
 * queues - a feature called hierarchical queues.
 * <p>
 * Queue names are unique, and used as a key to lookup queues. Hierarchical
 * queues are named by a 'fully qualified name' such as q1:q2:q3, where
 * q2 is a child queue of q1 and q3 is a child queue of q2.
 * <p>
 * Leaf level queues are queues that contain no queues within them. Jobs
 * can be submitted only to leaf level queues.
 * <p>
 * Queues can be configured with various properties. Some of these
 * properties are common to all schedulers, and those are handled by this
 * class. Schedulers might also associate several custom properties with
 * queues. These properties are parsed and maintained per queue by the
 * framework. If schedulers need more complicated structure to maintain
 * configuration per queue, they are free to not use the facilities
 * provided by the framework, but define their own mechanisms. In such cases,
 * it is likely that the name of the queue will be used to relate the
 * common properties of a queue with scheduler specific properties.
 * <p>
 * Information related to a queue, such as its name, properties, scheduling
 * information and children are exposed by this class via a serializable
 * class called {@link JobQueueInfo}.
 * <p>
 * Queues are configured in the configuration file mapred-queues.xml.
 * To support backwards compatibility, queues can also be configured
 * in mapred-site.xml. However, when configured in the latter, there is
 * no support for hierarchical queues.
 */
QueueRefresher (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/QueueRefresher.java)/**
 * Abstract QueueRefresher class. Scheduler's can extend this and return an
 * instance of this in the {@link #getQueueRefresher()} method. The
 * {@link #refreshQueues(List)} method of this instance will be invoked by the
 * {@link QueueManager} whenever it gets a request from an administrator to
 * refresh its own queue-configuration. This method has a documented contract
 * between the {@link QueueManager} and the {@link TaskScheduler}.
 * 
 * Before calling QueueRefresher, the caller must hold the lock to the
 * corresponding {@link TaskScheduler} (generally in the {@link JobTracker}).
 */
RamManager (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/RamManager.java)/**
 * <code>RamManager</code> manages a memory pool of a configured limit.
 */
RawKeyValueIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/RawKeyValueIterator.java)/**
 * <code>RawKeyValueIterator</code> is an iterator used to iterate over
 * the raw keys and values during sort/merge of intermediate data. 
 */
RecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/RecordReader.java)/**
 * <code>RecordReader</code> reads &lt;key, value&gt; pairs from an 
 * {@link InputSplit}.
 *   
 * <p><code>RecordReader</code>, typically, converts the byte-oriented view of 
 * the input, provided by the <code>InputSplit</code>, and presents a 
 * record-oriented view for the {@link Mapper} and {@link Reducer} tasks for
 * processing. It thus assumes the responsibility of processing record 
 * boundaries and presenting the tasks with keys and values.</p>
 * 
 * @see InputSplit
 * @see InputFormat
 */
RecordWriter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/RecordWriter.java)/**
 * <code>RecordWriter</code> writes the output &lt;key, value&gt; pairs 
 * to an output file.
 
 * <p><code>RecordWriter</code> implementations write the job outputs to the
 * {@link FileSystem}.
 * 
 * @see OutputFormat
 */
Reducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Reducer.java)/** 
 * Reduces a set of intermediate values which share a key to a smaller set of
 * values.  
 * 
 * <p>The number of <code>Reducer</code>s for the job is set by the user via 
 * {@link JobConf#setNumReduceTasks(int)}. <code>Reducer</code> implementations 
 * can access the {@link JobConf} for the job via the 
 * {@link JobConfigurable#configure(JobConf)} method and initialize themselves. 
 * Similarly they can use the {@link Closeable#close()} method for
 * de-initialization.</p>

 * <p><code>Reducer</code> has 3 primary phases:</p>
 * <ol>
 *   <li>
 *   
 *   <b id="Shuffle">Shuffle</b>
 *   
 *   <p><code>Reducer</code> is input the grouped output of a {@link Mapper}.
 *   In the phase the framework, for each <code>Reducer</code>, fetches the 
 *   relevant partition of the output of all the <code>Mapper</code>s, via HTTP. 
 *   </p>
 *   </li>
 *   
 *   <li>
 *   <b id="Sort">Sort</b>
 *   
 *   <p>The framework groups <code>Reducer</code> inputs by <code>key</code>s 
 *   (since different <code>Mapper</code>s may have output the same key) in this
 *   stage.</p>
 *   
 *   <p>The shuffle and sort phases occur simultaneously i.e. while outputs are
 *   being fetched they are merged.</p>
 *      
 *   <b id="SecondarySort">SecondarySort</b>
 *   
 *   <p>If equivalence rules for keys while grouping the intermediates are 
 *   different from those for grouping keys before reduction, then one may 
 *   specify a <code>Comparator</code> via 
 *   {@link JobConf#setOutputValueGroupingComparator(Class)}.Since 
 *   {@link JobConf#setOutputKeyComparatorClass(Class)} can be used to 
 *   control how intermediate keys are grouped, these can be used in conjunction 
 *   to simulate <i>secondary sort on values</i>.</p>
 *   
 *   
 *   For example, say that you want to find duplicate web pages and tag them 
 *   all with the url of the "best" known example. You would set up the job 
 *   like:
 *   <ul>
 *     <li>Map Input Key: url</li>
 *     <li>Map Input Value: document</li>
 *     <li>Map Output Key: document checksum, url pagerank</li>
 *     <li>Map Output Value: url</li>
 *     <li>Partitioner: by checksum</li>
 *     <li>OutputKeyComparator: by checksum and then decreasing pagerank</li>
 *     <li>OutputValueGroupingComparator: by checksum</li>
 *   </ul>
 *   </li>
 *   
 *   <li>   
 *   <b id="Reduce">Reduce</b>
 *   
 *   <p>In this phase the 
 *   {@link #reduce(Object, Iterator, OutputCollector, Reporter)}
 *   method is called for each <code>&lt;key, (list of values)&gt;</code> pair in
 *   the grouped inputs.</p>
 *   <p>The output of the reduce task is typically written to the 
 *   {@link FileSystem} via 
 *   {@link OutputCollector#collect(Object, Object)}.</p>
 *   </li>
 * </ol>
 * 
 * <p>The output of the <code>Reducer</code> is <b>not re-sorted</b>.</p>
 * 
 * <p>Example:</p>
 * <p><blockquote><pre>
 *     public class MyReducer&lt;K extends WritableComparable, V extends Writable&gt; 
 *     extends MapReduceBase implements Reducer&lt;K, V, K, V&gt; {
 *     
 *       static enum MyCounters { NUM_RECORDS }
 *        
 *       private String reduceTaskId;
 *       private int noKeys = 0;
 *       
 *       public void configure(JobConf job) {
 *         reduceTaskId = job.get(JobContext.TASK_ATTEMPT_ID);
 *       }
 *       
 *       public void reduce(K key, Iterator&lt;V&gt; values,
 *                          OutputCollector&lt;K, V&gt; output, 
 *                          Reporter reporter)
 *       throws IOException {
 *       
 *         // Process
 *         int noValues = 0;
 *         while (values.hasNext()) {
 *           V value = values.next();
 *           
 *           // Increment the no. of values for this key
 *           ++noValues;
 *           
 *           // Process the &lt;key, value&gt; pair (assume this takes a while)
 *           // ...
 *           // ...
 *           
 *           // Let the framework know that we are alive, and kicking!
 *           if ((noValues%10) == 0) {
 *             reporter.progress();
 *           }
 *         
 *           // Process some more
 *           // ...
 *           // ...
 *           
 *           // Output the &lt;key, value&gt; 
 *           output.collect(key, value);
 *         }
 *         
 *         // Increment the no. of &lt;key, list of values&gt; pairs processed
 *         ++noKeys;
 *         
 *         // Increment counters
 *         reporter.incrCounter(NUM_RECORDS, 1);
 *         
 *         // Every 100 keys update application-level status
 *         if ((noKeys%100) == 0) {
 *           reporter.setStatus(reduceTaskId + " processed " + noKeys);
 *         }
 *       }
 *     }
 * </pre></blockquote>
 * 
 * @see Mapper
 * @see Partitioner
 * @see Reporter
 * @see MapReduceBase
 */
ReduceTask (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java)/** A Reduce task. */
Reporter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Reporter.java)/** 
 * A facility for Map-Reduce applications to report progress and update 
 * counters, status information etc.
 * 
 * <p>{@link Mapper} and {@link Reducer} can use the <code>Reporter</code>
 * provided to report progress or just indicate that they are alive. In 
 * scenarios where the application takes significant amount of time to
 * process individual key/value pairs, this is crucial since the framework 
 * might assume that the task has timed-out and kill that task.
 *
 * <p>Applications can also update {@link Counters} via the provided 
 * <code>Reporter</code> .</p>
 * 
 * @see Progressable
 * @see Counters
 */
RunningJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/RunningJob.java)/** 
 * <code>RunningJob</code> is the user-interface to query for details on a 
 * running Map-Reduce job.
 * 
 * <p>Clients can get hold of <code>RunningJob</code> via the {@link JobClient}
 * and then query the running-job for details such as name, configuration, 
 * progress etc.</p> 
 * 
 * @see JobClient
 */
SequenceFileAsBinaryRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileAsBinaryInputFormat.java)/**
   * Read records from a SequenceFile as binary (raw) bytes.
   */
SequenceFileAsBinaryInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileAsBinaryInputFormat.java)/**
 * InputFormat reading keys, values from SequenceFiles in binary (raw)
 * format.
 */
WritableValueBytes (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java)/** 
   * Inner class used for appendRaw
   */
SequenceFileAsBinaryOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileAsBinaryOutputFormat.java)/** 
 * An {@link OutputFormat} that writes keys, values to 
 * {@link SequenceFile}s in binary(raw) format
 */
SequenceFileAsTextInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileAsTextInputFormat.java)/**
 * This class is similar to SequenceFileInputFormat, 
 * except it generates SequenceFileAsTextRecordReader 
 * which converts the input keys and values to their 
 * String forms by calling toString() method.
 */
SequenceFileAsTextRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileAsTextRecordReader.java)/**
 * This class converts the input keys and values to their String forms by calling toString()
 * method. This class to SequenceFileAsTextInputFormat class is as LineRecordReader
 * class to TextInputFormat class.
 */
Filter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileInputFilter.java)/**
   * filter interface
   */
FilterBase (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileInputFilter.java)/**
   * base class for Filters
   */
RegexFilter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileInputFilter.java)/** Records filter by matching key to regex
   */
PercentFilter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileInputFilter.java)/** This class returns a percentage of records
   * The percentage is determined by a filtering frequency <i>f</i> using
   * the criteria record# % f == 0.
   * For example, if the frequency is 10, one out of 10 records is returned.
   */
MD5Filter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileInputFilter.java)/** This class returns a set of records by examing the MD5 digest of its
   * key against a filtering frequency <i>f</i>. The filtering criteria is
   * MD5(key) % f == 0.
   */
SequenceFileInputFilter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileInputFilter.java)/**
 * A class that allows a map/red job to work on a sample of sequence files.
 * The sample is decided by the filter class set by the job.
 */
SequenceFileInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileInputFormat.java)/** 
 * An {@link InputFormat} for {@link SequenceFile}s. 
 */
SequenceFileOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileOutputFormat.java)/** 
 * An {@link OutputFormat} that writes {@link SequenceFile}s. 
 */
SequenceFileRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SequenceFileRecordReader.java)/** 
 * An {@link RecordReader} for {@link SequenceFile}s. 
 */
ShuffleConsumerPlugin (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ShuffleConsumerPlugin.java)/**
 * ShuffleConsumerPlugin for serving Reducers.  It may shuffle MOF files from
 * either the built-in ShuffleHandler or from a 3rd party AuxiliaryService.
 *
 */
SkipBadRecords (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SkipBadRecords.java)/**
 * Utility class for skip bad records functionality. It contains various 
 * settings related to skipping of bad records.
 * 
 * <p>Hadoop provides an optional mode of execution in which the bad records
 * are detected and skipped in further attempts.
 * 
 * <p>This feature can be used when map/reduce tasks crashes deterministically on 
 * certain input. This happens due to bugs in the map/reduce function. The usual
 * course would be to fix these bugs. But sometimes this is not possible; 
 * perhaps the bug is in third party libraries for which the source code is 
 * not available. Due to this, the task never reaches to completion even with 
 * multiple attempts and complete data for that task is lost.</p>
 *  
 * <p>With this feature, only a small portion of data is lost surrounding 
 * the bad record, which may be acceptable for some user applications.
 * see {@link SkipBadRecords#setMapperMaxSkipRecords(Configuration, long)}</p>
 * 
 * <p>The skipping mode gets kicked off after certain no of failures 
 * see {@link SkipBadRecords#setAttemptsToStartSkipping(Configuration, int)}</p>
 *  
 * <p>In the skipping mode, the map/reduce task maintains the record range which 
 * is getting processed at all times. Before giving the input to the
 * map/reduce function, it sends this record range to the Task tracker.
 * If task crashes, the Task tracker knows which one was the last reported
 * range. On further attempts that range get skipped.</p>
 */
Range (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SortedRanges.java)/**
   * Index Range. Comprises of start index and length.
   * A Range can be of 0 length also. The Range stores indices 
   * of type long.
   */
SkipRangeIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SortedRanges.java)/**
   * Index Iterator which skips the stored ranges.
   */
SortedRanges (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/SortedRanges.java)/**
 * Keeps the Ranges sorted by startIndex.
 * The added ranges are always ensured to be non-overlapping.
 * Provides the SkipRangeIterator, which skips the Ranges 
 * stored in this object.
 */
StatePeriodicStats (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/StatePeriodicStats.java)/**
 *
 * This class is a concrete PeriodicStatsAccumulator that deals with
 *  measurements where the raw data are a measurement of a
 *  time-varying quantity.  The result in each bucket is the estimate
 *  of the progress-weighted mean value of that quantity over the
 *  progress range covered by the bucket.
 *
 * <p>An easy-to-understand example of this kind of quantity would be
 *  a temperature.  It makes sense to consider the mean temperature
 *  over a progress range.
 *
 */
TimeWindowStatUpdater (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/StatisticsCollector.java)/**
   * Updates TimeWindow statistics in buckets.
   *
   */
StatisticsCollector (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/StatisticsCollector.java)/**
 * Collects the statistics in time windows.
 */
TaskLimitException (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java)/**
     * exception thrown when the task exceeds some configured limits.
     */
DiskLimitCheck (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java)/**
     * disk limit checker, runs in separate thread when activated.
     */
GcTimeUpdater (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java)/**
   * An updater that tracks the amount of time this task has spent in GC.
   */
FileSystemStatisticUpdater (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java)/**
   * An updater that tracks the last number reported for a given file
   * system and only creates the counters when they are needed.
   */
CombineOutputCollector (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java)/**
   * OutputCollector for the combiner.
   */
ValuesIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java)/** Iterates values while keys match in sorted input. */
CombineValuesIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java)/** Iterator to return Combined values */
Task (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java)/**
 * Base class for tasks.
 */
TaskAttemptID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskAttemptID.java)/**
 * TaskAttemptID represents the immutable and unique identifier for 
 * a task attempt. Each task attempt is one particular instance of a Map or
 * Reduce Task identified by its TaskID. 
 * 
 * TaskAttemptID consists of 2 parts. First part is the 
 * {@link TaskID}, that this TaskAttemptID belongs to.
 * Second part is the task attempt number. <br> 
 * An example TaskAttemptID is : 
 * <code>attempt_200707121733_0003_m_000005_0</code> , which represents the
 * zeroth task attempt for the fifth map task in the third job 
 * running at the jobtracker started at <code>200707121733</code>.
 * <p>
 * Applications should never construct or parse TaskAttemptID strings
 * , but rather use appropriate constructors or {@link #forName(String)} 
 * method. 
 * 
 * @see JobID
 * @see TaskID
 */
TaskCompletionEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskCompletionEvent.java)/**
 * This is used to track task completion events on 
 * job tracker. 
 */
TaskID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskID.java)/**
 * TaskID represents the immutable and unique identifier for 
 * a Map or Reduce Task. Each TaskID encompasses multiple attempts made to
 * execute the Map or Reduce Task, each of which are uniquely indentified by
 * their TaskAttemptID.
 * 
 * TaskID consists of 3 parts. First part is the {@link JobID}, that this 
 * TaskInProgress belongs to. Second part of the TaskID is either 'm' or 'r' 
 * representing whether the task is a map task or a reduce task. 
 * And the third part is the task number. <br> 
 * An example TaskID is : 
 * <code>task_200707121733_0003_m_000005</code> , which represents the
 * fifth map task in the third job running at the jobtracker 
 * started at <code>200707121733</code>. 
 * <p>
 * Applications should never construct or parse TaskID strings
 * , but rather use appropriate constructors or {@link #forName(String)} 
 * method. 
 * 
 * @see JobID
 * @see TaskAttemptID
 */
TaskLog (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLog.java)/**
 * A simple logger to handle the task-specific user logs.
 * This class uses the system property <code>hadoop.log.dir</code>.
 * 
 */
TaskLogAppender (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLogAppender.java)/**
 * A simple log4j-appender for the task child's 
 * map-reduce system logs.
 * 
 */
TaskReport (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskReport.java)/** 
 * A report on the state of a task. 
 */
TaskStatus (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskStatus.java)/**************************************************
 * Describes the current status of a task.  This is
 * not intended to be a comprehensive piece of data.
 *
 **************************************************/
TaskUmbilicalProtocol (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskUmbilicalProtocol.java)/** Protocol that task child process uses to contact its parent process.  The
 * parent is a daemon which which polls the central master for a new map or
 * reduce task and runs it as a child process.  All communication between child
 * and parent is via this protocol. */
TextInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TextInputFormat.java)/** 
 * An {@link InputFormat} for plain text files.  Files are broken into lines.
 * Either linefeed or carriage-return are used to signal end of line.  Keys are
 * the position in the file, and values are the line of text.. 
 */
TextOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TextOutputFormat.java)/** 
 * An {@link OutputFormat} that writes plain text files. 
 */
OutputFilesFilter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Utils.java)/**
     * This class filters output(part) files from the given directory
     * It does not accept files with filenames _logs and _SUCCESS.
     * This can be used to list paths of output directory as follows:
     *   Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir,
     *                                         new OutputFilesFilter()));
     */
OutputLogFilter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Utils.java)/**
     * This class filters log files from directory given
     * It doesnt accept paths having _logs.
     * This can be used to list paths of output directory as follows:
     *   Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir,
     *                                   new OutputLogFilter()));
     */
Utils (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Utils.java)/**
 * A utility class. It provides
 *   A path filter utility to filter out output/part files in the output dir
 */
CheckpointID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/checkpoint/CheckpointID.java)/**
 * This class represent the identified (memento) for a checkpoint. It is allowed
 * to contain small amount of metadata about a checkpoint and must provide
 * sufficient information to the corresponding CheckpointService to locate and
 * retrieve the data contained in the checkpoint.
 */
CheckpointNamingService (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/checkpoint/CheckpointNamingService.java)/**
 * This class represent a naming service for checkpoints.
 */
CheckpointService (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/checkpoint/CheckpointService.java)/**
 * The CheckpointService provides a simple API to store and retrieve the state of a task.
 *
 * Checkpoints are atomic, single-writer, write-once, multiple-readers,
 * ready-many type of objects. This is provided by releasing the CheckpointID
 * for a checkpoint only upon commit of the checkpoint, and by preventing a
 * checkpoint to be re-opened for writes.
 *
 * Non-functional properties such as durability, availability, compression,
 * garbage collection, quotas are left to the implementation.
 *
 * This API is envisioned as the basic building block for a checkpoint service,
 * on top of which richer interfaces can be layered (e.g., frameworks providing
 * object-serialization, checkpoint metadata and provenance, etc.)
 *
 */
FSCheckpointID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointID.java)/**
 * A FileSystem based checkpoint ID contains reference to the Path
 * where the checkpoint has been saved.
 */
FSCheckpointService (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/checkpoint/FSCheckpointService.java)/**
 * A FileSystem based CheckpointService.
 */
RandomNameCNS (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/checkpoint/RandomNameCNS.java)/**
 * Simple naming service that generates a random checkpoint name.
 */
SimpleNamingService (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/checkpoint/SimpleNamingService.java)/**
 * A naming service that simply returns the name it has been initialized with.
 */
TaskCheckpointID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/checkpoint/TaskCheckpointID.java)/**
 * Implementation of CheckpointID used in MR. It contains a reference to an
 * underlying FileSsytem based checkpoint, and various metadata about the
 * cost of checkpoints and other counters. This is sent by the task to the AM
 * to be stored and provided to the next execution of the same task.
 */
Cluster (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java)/**
 * Provides a way to access information about the map/reduce cluster.
 */
ClusterMetrics (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/ClusterMetrics.java)/**
 * Status information on the current state of the Map-Reduce cluster.
 * 
 * <p><code>ClusterMetrics</code> provides clients with information such as:
 * <ol>
 *   <li>
 *   Size of the cluster.  
 *   </li>
 *   <li>
 *   Number of blacklisted and decommissioned trackers.  
 *   </li>
 *   <li>
 *   Slot capacity of the cluster. 
 *   </li>
 *   <li>
 *   The number of currently occupied/reserved map and reduce slots.
 *   </li>
 *   <li>
 *   The number of currently running map and reduce tasks.
 *   </li>
 *   <li>
 *   The number of job submissions.
 *   </li>
 * </ol>
 * 
 * <p>Clients can query for the latest <code>ClusterMetrics</code>, via 
 * {@link Cluster#getClusterStatus()}.</p>
 * 
 * @see Cluster
 */
ContextFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/ContextFactory.java)/**
 * A factory to allow applications to deal with inconsistencies between
 * MapReduce Context Objects API between hadoop-0.20 and later versions.
 */
Counter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Counter.java)/**
 * A named counter that tracks the progress of a map/reduce job.
 *
 * <p><code>Counters</code> represent global counters, defined either by the
 * Map-Reduce framework or applications. Each <code>Counter</code> is named by
 * an {@link Enum} and has a long for the value.</p>
 *
 * <p><code>Counters</code> are bunched into Groups, each comprising of
 * counters from a particular <code>Enum</code> class.
 */
CounterGroup (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/CounterGroup.java)/**
 * A group of {@link Counter}s that logically belong together. Typically,
 * it is an {@link Enum} subclass and the counters are the values.
 */
AbstractCounter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/AbstractCounter.java)/**
 * An abstract counter class to provide common implementation of
 * the counter interface in both mapred and mapreduce packages.
 */
AbstractCounterGroup (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/AbstractCounterGroup.java)/**
 * An abstract class to provide common implementation of the
 * generic counter group in both mapred and mapreduce package.
 *
 * @param <T> type of the counter for the group
 */
AbstractCounters (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java)/**
 * An abstract class to provide common implementation for the Counters
 * container in both mapred and mapreduce packages.
 *
 * @param <C> type of counter inside the counters
 * @param <G> type of group inside the counters
 */
CounterGroupBase (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/CounterGroupBase.java)/**
 * The common counter group interface.
 *
 * @param <T> type of the counter for the group
 */
CounterGroupFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/CounterGroupFactory.java)/**
 * An abstract class to provide common implementation of the
 * group factory in both mapred and mapreduce packages.
 *
 * @param <C> type of the counter
 * @param <G> type of the group
 */
FileSystemCounterGroup (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java)/**
 * An abstract class to provide common implementation of the filesystem
 * counter group in both mapred and mapreduce packages.
 *
 * @param <C> the type of the Counter for the group
 */
FrameworkCounter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java)/**
   * A counter facade for framework counters.
   * Use old (which extends new) interface to make compatibility easier.
   */
FrameworkCounterGroup (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java)/**
 * An abstract class to provide common implementation for the framework
 * counter group in both mapred and mapreduce packages.
 *
 * @param <T> type of the counter enum class
 * @param <C> type of the counter
 */
GenericCounter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/GenericCounter.java)/**
 * A generic counter implementation
 */
GroupFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Counters.java)/**
   * Provide factory methods for counter group factory implementation.
   * See also the GroupFactory in
   *  {@link org.apache.hadoop.mapred.Counters mapred.Counters}
   */
Counters (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Counters.java)/**
 * <p><code>Counters</code> holds per job/task counters, defined either by the
 * Map-Reduce framework or applications. Each <code>Counter</code> can be of
 * any {@link Enum} type.</p>
 *
 * <p><code>Counters</code> are bunched into {@link CounterGroup}s, each
 * comprising of counters from a particular <code>Enum</code> class.
 */
CryptoUtils (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/CryptoUtils.java)/**
 * This class provides utilities to make it easier to work with Cryptographic
 * Streams. Specifically for dealing with encrypting intermediate data such
 * MapReduce spill files.
 */
ClientDistributedCacheManager (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/filecache/ClientDistributedCacheManager.java)/**
 * Manages internal configuration of the cache by the client for job submission.
 */
DistributedCache (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/filecache/DistributedCache.java)/**
 * Distribute application-specific large, read-only files efficiently.
 *
 * <p><code>DistributedCache</code> is a facility provided by the Map-Reduce
 * framework to cache files (text, archives, jars etc.) needed by applications.
 * </p>
 *
 * <p>Applications specify the files, via urls (hdfs:// or http://) to be cached
 * via the {@link org.apache.hadoop.mapred.JobConf}. The
 * <code>DistributedCache</code> assumes that the files specified via urls are
 * already present on the {@link FileSystem} at the path specified by the url
 * and are accessible by every machine in the cluster.</p>
 *
 * <p>The framework will copy the necessary files on to the worker node before
 * any tasks for the job are executed on that node. Its efficiency stems from
 * the fact that the files are only copied once per job and the ability to
 * cache archives which are un-archived on the workers.</p>
 *
 * <p><code>DistributedCache</code> can be used to distribute simple, read-only
 * data/text files and/or more complex types such as archives, jars etc.
 * Archives (zip, tar and tgz/tar.gz files) are un-archived at the worker nodes.
 * Jars may be optionally added to the classpath of the tasks, a rudimentary
 * software distribution mechanism.  Files have execution permissions.
 * In older version of Hadoop Map/Reduce users could optionally ask for symlinks
 * to be created in the working directory of the child task.  In the current
 * version symlinks are always created.  If the URL does not have a fragment
 * the name of the file or directory will be used. If multiple files or
 * directories map to the same link name, the last one added, will be used.  All
 * others will not even be downloaded.</p>
 *
 * <p><code>DistributedCache</code> tracks modification timestamps of the cache
 * files. Clearly the cache files should not be modified by the application
 * or externally while the job is executing.</p>
 *
 * <p>Here is an illustrative example on how to use the
 * <code>DistributedCache</code>:</p>
 * <p><blockquote><pre>
 *     // Setting up the cache for the application
 *
 *     1. Copy the requisite files to the <code>FileSystem</code>:
 *
 *     $ bin/hadoop fs -copyFromLocal lookup.dat /myapp/lookup.dat
 *     $ bin/hadoop fs -copyFromLocal map.zip /myapp/map.zip
 *     $ bin/hadoop fs -copyFromLocal mylib.jar /myapp/mylib.jar
 *     $ bin/hadoop fs -copyFromLocal mytar.tar /myapp/mytar.tar
 *     $ bin/hadoop fs -copyFromLocal mytgz.tgz /myapp/mytgz.tgz
 *     $ bin/hadoop fs -copyFromLocal mytargz.tar.gz /myapp/mytargz.tar.gz
 *
 *     2. Setup the application's <code>JobConf</code>:
 *
 *     JobConf job = new JobConf();
 *     DistributedCache.addCacheFile(new URI("/myapp/lookup.dat#lookup.dat"),
 *                                   job);
 *     DistributedCache.addCacheArchive(new URI("/myapp/map.zip"), job);
 *     DistributedCache.addFileToClassPath(new Path("/myapp/mylib.jar"), job);
 *     DistributedCache.addCacheArchive(new URI("/myapp/mytar.tar"), job);
 *     DistributedCache.addCacheArchive(new URI("/myapp/mytgz.tgz"), job);
 *     DistributedCache.addCacheArchive(new URI("/myapp/mytargz.tar.gz"), job);
 *
 *     3. Use the cached files in the {@link org.apache.hadoop.mapred.Mapper}
 *     or {@link org.apache.hadoop.mapred.Reducer}:
 *
 *     public static class MapClass extends MapReduceBase
 *     implements Mapper&lt;K, V, K, V&gt; {
 *
 *       private Path[] localArchives;
 *       private Path[] localFiles;
 *
 *       public void configure(JobConf job) {
 *         // Get the cached archives/files
 *         File f = new File("./map.zip/some/file/in/zip.txt");
 *       }
 *
 *       public void map(K key, V value,
 *                       OutputCollector&lt;K, V&gt; output, Reporter reporter)
 *       throws IOException {
 *         // Use data from the cached archives/files here
 *         // ...
 *         // ...
 *         output.collect(k, v);
 *       }
 *     }
 *
 * </pre></blockquote>
 *
 * It is also very common to use the DistributedCache by using
 * {@link org.apache.hadoop.util.GenericOptionsParser}.
 *
 * This class includes methods that should be used by users
 * (specifically those mentioned in the example above, as well
 * as {@link DistributedCache#addArchiveToClassPath(Path, Configuration)}),
 * as well as methods intended for use by the MapReduce framework
 * (e.g., {@link org.apache.hadoop.mapred.JobClient}).
 *
 * @see org.apache.hadoop.mapreduce.Job
 * @see org.apache.hadoop.mapred.JobConf
 * @see org.apache.hadoop.mapred.JobClient
 */
ID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/ID.java)/**
 * A general identifier, which internally stores the id
 * as an integer. This is the super class of {@link JobID}, 
 * {@link TaskID} and {@link TaskAttemptID}.
 * 
 * @see JobID
 * @see TaskID
 * @see TaskAttemptID
 */
InputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/InputFormat.java)/** 
 * <code>InputFormat</code> describes the input-specification for a 
 * Map-Reduce job. 
 * 
 * <p>The Map-Reduce framework relies on the <code>InputFormat</code> of the
 * job to:<p>
 * <ol>
 *   <li>
 *   Validate the input-specification of the job. 
 *   <li>
 *   Split-up the input file(s) into logical {@link InputSplit}s, each of 
 *   which is then assigned to an individual {@link Mapper}.
 *   </li>
 *   <li>
 *   Provide the {@link RecordReader} implementation to be used to glean
 *   input records from the logical <code>InputSplit</code> for processing by 
 *   the {@link Mapper}.
 *   </li>
 * </ol>
 * 
 * <p>The default behavior of file-based {@link InputFormat}s, typically 
 * sub-classes of {@link FileInputFormat}, is to split the 
 * input into <i>logical</i> {@link InputSplit}s based on the total size, in 
 * bytes, of the input files. However, the {@link FileSystem} blocksize of  
 * the input files is treated as an upper bound for input splits. A lower bound 
 * on the split size can be set via 
 * <a href="{@docRoot}/../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml#mapreduce.input.fileinputformat.split.minsize">
 * mapreduce.input.fileinputformat.split.minsize</a>.</p>
 * 
 * <p>Clearly, logical splits based on input-size is insufficient for many 
 * applications since record boundaries are to respected. In such cases, the
 * application has to also implement a {@link RecordReader} on whom lies the
 * responsibility to respect record-boundaries and present a record-oriented
 * view of the logical <code>InputSplit</code> to the individual task.
 *
 * @see InputSplit
 * @see RecordReader
 * @see FileInputFormat
 */
InputSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/InputSplit.java)/**
 * <code>InputSplit</code> represents the data to be processed by an 
 * individual {@link Mapper}. 
 *
 * <p>Typically, it presents a byte-oriented view on the input and is the 
 * responsibility of {@link RecordReader} of the job to process this and present
 * a record-oriented view.
 * 
 * @see InputFormat
 * @see RecordReader
 */
Job (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java)/**
 * The job submitter's view of the Job.
 * 
 * <p>It allows the user to configure the
 * job, submit it, control its execution, and query the state. The set methods
 * only work until the job is submitted, afterwards they will throw an 
 * IllegalStateException. </p>
 * 
 * <p>
 * Normally the user creates the application, describes various facets of the
 * job via {@link Job} and then submits the job and monitor its progress.</p>
 * 
 * <p>Here is an example on how to submit a job:</p>
 * <p><blockquote><pre>
 *     // Create a new Job
 *     Job job = Job.getInstance();
 *     job.setJarByClass(MyJob.class);
 *     
 *     // Specify various job-specific parameters     
 *     job.setJobName("myjob");
 *     
 *     job.setInputPath(new Path("in"));
 *     job.setOutputPath(new Path("out"));
 *     
 *     job.setMapperClass(MyJob.MyMapper.class);
 *     job.setReducerClass(MyJob.MyReducer.class);
 *
 *     // Submit the job, then poll for progress until the job is complete
 *     job.waitForCompletion(true);
 * </pre></blockquote>
 * 
 * 
 */
JobContext (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobContext.java)/**
 * A read-only view of the job that is provided to the tasks while they
 * are running.
 */
AMStartedEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/AMStartedEvent.java)/**
 * Event to record start of a task attempt
 * 
 */
EventWriter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/EventWriter.java)/**
 * Event Writer is an utility class used to write events to the underlying
 * stream. Typically, one event writer (which translates to one stream) 
 * is created per job 
 * 
 */
HistoryEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryEvent.java)/**
 * Interface for event wrapper classes.  Implementations each wrap an
 * Avro-generated class, adding constructors and accessor methods.
 */
SummarizedJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryViewer.java)/**
   * Utility class used the summarize the job. 
   * Used by HistoryViewer and the JobHistory UI.
   *
   */
AnalyzedJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryViewer.java)/**
   * Utility class used while analyzing the job. 
   * Used by HistoryViewer and the JobHistory UI.
   */
FilteredJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryViewer.java)/**
   * Utility to filter out events based on the task status
   */
HistoryViewer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryViewer.java)/**
 * HistoryViewer is used to parse and view the JobHistory files.  They can be
 * printed in human-readable format or machine-readable JSON format using the
 * {@link HistoryViewerPrinter}.
 */
HistoryViewerPrinter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryViewerPrinter.java)/**
 * Used by the {@link HistoryViewer} to print job history in different formats.
 */
HumanReadableHistoryViewerPrinter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HumanReadableHistoryViewerPrinter.java)/**
 * Used by the {@link HistoryViewer} to print job history in a human-readable
 * format.
 */
JobFinishedEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobFinishedEvent.java)/**
 * Event to record successful completion of job
 *
 */
JobInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryParser.java)/**
   * The class where job information is aggregated into after parsing
   */
TaskInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryParser.java)/**
   * TaskInformation is aggregated in this class after parsing
   */
TaskAttemptInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryParser.java)/**
   * Task Attempt Information is aggregated in this class after parsing
   */
AMInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryParser.java)/**
   * Stores AM information
   */
JobHistoryParser (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryParser.java)/**
 * Default Parser for the JobHistory files. Typical usage is
 * JobHistoryParser parser = new JobHistoryParser(fs, historyFile);
 * job = parser.parse();
 *
 */
JobInfoChangeEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobInfoChangeEvent.java)/**
 * Event to record changes in the submit and launch time of
 * a job
 */
JobInitedEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobInitedEvent.java)/**
 * Event to record the initialization of a job
 *
 */
JobPriorityChangeEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobPriorityChangeEvent.java)/**
 * Event to record the change of priority of a job
 *
 */
JobStatusChangedEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobStatusChangedEvent.java)/**
 * Event to record the change of status for a job
 *
 */
JobSubmittedEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobSubmittedEvent.java)/**
 * Event to record the submission of a job
 *
 */
JobUnsuccessfulCompletionEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobUnsuccessfulCompletionEvent.java)/**
 * Event to record Failed and Killed completion of jobs
 *
 */
JSONHistoryViewerPrinter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JSONHistoryViewerPrinter.java)/**
 * Used by the {@link HistoryViewer} to print job history in a machine-readable
 * JSON format.
 */
MapAttemptFinishedEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/MapAttemptFinishedEvent.java)/**
 * Event to record successful completion of a map attempt.
 *
 */
NormalizedResourceEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/NormalizedResourceEvent.java)/**
 * Event to record the normalized map/reduce requirements.
 * 
 */
ReduceAttemptFinishedEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/ReduceAttemptFinishedEvent.java)/**
 * Event to record successful completion of a reduce attempt
 *
 */
TaskAttemptFinishedEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/TaskAttemptFinishedEvent.java)/**
 * Event to record successful task completion
 *
 */
TaskAttemptStartedEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/TaskAttemptStartedEvent.java)/**
 * Event to record start of a task attempt
 *
 */
TaskAttemptUnsuccessfulCompletionEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/TaskAttemptUnsuccessfulCompletionEvent.java)/**
 * Event to record unsuccessful (Killed/Failed) completion of task attempts
 *
 */
TaskFailedEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/TaskFailedEvent.java)/**
 * Event to record the failure of a task
 *
 */
TaskFinishedEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/TaskFinishedEvent.java)/**
 * Event to record the successful completion of a task
 *
 */
TaskStartedEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/TaskStartedEvent.java)/**
 * Event to record the start of a task
 *
 */
TaskUpdatedEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/TaskUpdatedEvent.java)/**
 * Event to record updates to a task
 *
 */
JobID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobID.java)/**
 * JobID represents the immutable and unique identifier for 
 * the job. JobID consists of two parts. First part 
 * represents the jobtracker identifier, so that jobID to jobtracker map 
 * is defined. For cluster setup this string is the jobtracker 
 * start time, for local setting, it is "local" and a random number.
 * Second part of the JobID is the job number. <br> 
 * An example JobID is : 
 * <code>job_200707121733_0003</code> , which represents the third job 
 * running at the jobtracker started at <code>200707121733</code>. 
 * <p>
 * Applications should never construct or parse JobID strings, but rather 
 * use appropriate constructors or {@link #forName(String)} method. 
 * 
 * @see TaskID
 * @see TaskAttemptID
 */
JobResourceUploader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java)/**
 * This class is responsible for uploading resources from the client to HDFS
 * that are associated with a MapReduce job.
 */
JobStatus (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobStatus.java)/**************************************************
 * Describes the current status of a job.
 **************************************************/
JobSubmissionFiles (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmissionFiles.java)/**
 * A utility to manage job submission files.
 */
DoubleValueSum (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/DoubleValueSum.java)/**
 * This class implements a value aggregator that sums up a sequence of double
 * values.
 * 
 */
LongValueMax (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/LongValueMax.java)/**
 * This class implements a value aggregator that maintain the maximum of 
 * a sequence of long values.
 * 
 */
LongValueMin (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/LongValueMin.java)/**
 * This class implements a value aggregator that maintain the minimum of 
 * a sequence of long values.
 * 
 */
LongValueSum (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/LongValueSum.java)/**
 * This class implements a value aggregator that sums up 
 * a sequence of long values.
 * 
 */
StringValueMax (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/StringValueMax.java)/**
 * This class implements a value aggregator that maintain the biggest of 
 * a sequence of strings.
 * 
 */
StringValueMin (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/StringValueMin.java)/**
 * This class implements a value aggregator that maintain the smallest of 
 * a sequence of strings.
 * 
 */
UniqValueCount (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/UniqValueCount.java)/**
 * This class implements a value aggregator that dedupes a sequence of objects.
 * 
 */
UserDefinedValueAggregatorDescriptor (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/UserDefinedValueAggregatorDescriptor.java)/**
 * This class implements a wrapper for a user defined value 
 * aggregator descriptor.
 * It serves two functions: One is to create an object of 
 * ValueAggregatorDescriptor from the name of a user defined class
 * that may be dynamically loaded. The other is to
 * delegate invocations of generateKeyValPairs function to the created object.
 * 
 */
ValueAggregator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/ValueAggregator.java)/**
 * This interface defines the minimal protocol for value aggregators.
 * 
 */
ValueAggregatorBaseDescriptor (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/ValueAggregatorBaseDescriptor.java)/** 
 * This class implements the common functionalities of 
 * the subclasses of ValueAggregatorDescriptor class.
 */
ValueAggregatorCombiner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/ValueAggregatorCombiner.java)/**
 * This class implements the generic combiner of Aggregate.
 */
ValueAggregatorDescriptor (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/ValueAggregatorDescriptor.java)/**
 * This interface defines the contract a value aggregator descriptor must
 * support. Such a descriptor can be configured with a {@link Configuration}
 * object. Its main function is to generate a list of aggregation-id/value 
 * pairs. An aggregation id encodes an aggregation type which is used to 
 * guide the way to aggregate the value in the reduce/combiner phrase of an
 * Aggregate based job. 
 * The mapper in an Aggregate based map/reduce job may create one or more of
 * ValueAggregatorDescriptor objects at configuration time. For each input
 * key/value pair, the mapper will use those objects to create aggregation
 * id/value pairs.
 * 
 */
ValueAggregatorJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/ValueAggregatorJob.java)/**
 * This is the main class for creating a map/reduce job using Aggregate
 * framework. The Aggregate is a specialization of map/reduce framework,
 * specializing for performing various simple aggregations.
 * 
 * Generally speaking, in order to implement an application using Map/Reduce
 * model, the developer is to implement Map and Reduce functions (and possibly
 * combine function). However, a lot of applications related to counting and
 * statistics computing have very similar characteristics. Aggregate abstracts
 * out the general patterns of these functions and implementing those patterns.
 * In particular, the package provides generic mapper/redducer/combiner 
 * classes, and a set of built-in value aggregators, and a generic utility 
 * class that helps user create map/reduce jobs using the generic class. 
 * The built-in aggregators include:
 * 
 * sum over numeric values count the number of distinct values compute the
 * histogram of values compute the minimum, maximum, media,average, standard
 * deviation of numeric values
 * 
 * The developer using Aggregate will need only to provide a plugin class
 * conforming to the following interface:
 * 
 * public interface ValueAggregatorDescriptor { public ArrayList&lt;Entry&gt;
 * generateKeyValPairs(Object key, Object value); public void
 * configure(Configuration conf); }
 * 
 * The package also provides a base class, ValueAggregatorBaseDescriptor,
 * implementing the above interface. The user can extend the base class and
 * implement generateKeyValPairs accordingly.
 * 
 * The primary work of generateKeyValPairs is to emit one or more key/value
 * pairs based on the input key/value pair. The key in an output key/value pair
 * encode two pieces of information: aggregation type and aggregation id. The
 * value will be aggregated onto the aggregation id according the aggregation
 * type.
 * 
 * This class offers a function to generate a map/reduce job using Aggregate
 * framework. The function takes the following parameters: input directory spec
 * input format (text or sequence file) output directory a file specifying the
 * user plugin class
 * 
 */
ValueAggregatorJobBase (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/ValueAggregatorJobBase.java)/**
 * This abstract class implements some common functionalities of the
 * the generic mapper, reducer and combiner classes of Aggregate.
 */
ValueAggregatorMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/ValueAggregatorMapper.java)/**
 * This class implements the generic mapper of Aggregate.
 */
ValueAggregatorReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/ValueAggregatorReducer.java)/**
 * This class implements the generic reducer of Aggregate.
 */
ValueHistogram (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/aggregate/ValueHistogram.java)/**
 * This class implements a value aggregator that computes the 
 * histogram of a sequence of strings.
 * 
 */
ChainBlockingQueue (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/chain/Chain.java)/**
   * A blocking queue with one element.
   *   
   * @param <E>
   */
Chain (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/chain/Chain.java)/**
 * The Chain class provides all the common functionality for the
 * {@link ChainMapper} and the {@link ChainReducer} classes.
 */
ChainMapContextImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/chain/ChainMapContextImpl.java)/**
 * A simple wrapper class that delegates most of its functionality to the
 * underlying context, but overrides the methods to do with record readers ,
 * record writers and configuration.
 */
ChainMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/chain/ChainMapper.java)/**
 * The ChainMapper class allows to use multiple Mapper classes within a single
 * Map task.
 * 
 * <p>
 * The Mapper classes are invoked in a chained (or piped) fashion, the output of
 * the first becomes the input of the second, and so on until the last Mapper,
 * the output of the last Mapper will be written to the task's output.
 * </p>
 * <p>
 * The key functionality of this feature is that the Mappers in the chain do not
 * need to be aware that they are executed in a chain. This enables having
 * reusable specialized Mappers that can be combined to perform composite
 * operations within a single task.
 * </p>
 * <p>
 * Special care has to be taken when creating chains that the key/values output
 * by a Mapper are valid for the following Mapper in the chain. It is assumed
 * all Mappers and the Reduce in the chain use matching output and input key and
 * value classes as no conversion is done by the chaining code.
 * </p>
 * <p>
 * Using the ChainMapper and the ChainReducer classes is possible to compose
 * Map/Reduce jobs that look like <code>[MAP+ / REDUCE MAP*]</code>. And
 * immediate benefit of this pattern is a dramatic reduction in disk IO.
 * </p>
 * <p>
 * IMPORTANT: There is no need to specify the output key/value classes for the
 * ChainMapper, this is done by the addMapper for the last mapper in the chain.
 * </p>
 * ChainMapper usage pattern:
 * <p>
 * 
 * <pre>
 * ...
 * Job = new Job(conf);
 *
 * Configuration mapAConf = new Configuration(false);
 * ...
 * ChainMapper.addMapper(job, AMap.class, LongWritable.class, Text.class,
 *   Text.class, Text.class, true, mapAConf);
 *
 * Configuration mapBConf = new Configuration(false);
 * ...
 * ChainMapper.addMapper(job, BMap.class, Text.class, Text.class,
 *   LongWritable.class, Text.class, false, mapBConf);
 *
 * ...
 *
 * job.waitForComplettion(true);
 * ...
 * </pre>
 */
ChainReduceContextImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/chain/ChainReduceContextImpl.java)/**
 * A simple wrapper class that delegates most of its functionality to the
 * underlying context, but overrides the methods to do with record writer and
 * configuration
 */
ChainReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/chain/ChainReducer.java)/**
 * The ChainReducer class allows to chain multiple Mapper classes after a
 * Reducer within the Reducer task.
 * 
 * <p>
 * For each record output by the Reducer, the Mapper classes are invoked in a
 * chained (or piped) fashion. The output of the reducer becomes the input of
 * the first mapper and output of first becomes the input of the second, and so
 * on until the last Mapper, the output of the last Mapper will be written to
 * the task's output.
 * </p>
 * <p>
 * The key functionality of this feature is that the Mappers in the chain do not
 * need to be aware that they are executed after the Reducer or in a chain. This
 * enables having reusable specialized Mappers that can be combined to perform
 * composite operations within a single task.
 * </p>
 * <p>
 * Special care has to be taken when creating chains that the key/values output
 * by a Mapper are valid for the following Mapper in the chain. It is assumed
 * all Mappers and the Reduce in the chain use matching output and input key and
 * value classes as no conversion is done by the chaining code.
 * </p>
 * <p> Using the ChainMapper and the ChainReducer classes is possible to
 * compose Map/Reduce jobs that look like <code>[MAP+ / REDUCE MAP*]</code>. And
 * immediate benefit of this pattern is a dramatic reduction in disk IO. </p>
 * <p>
 * IMPORTANT: There is no need to specify the output key/value classes for the
 * ChainReducer, this is done by the setReducer or the addMapper for the last
 * element in the chain.
 * </p>
 * ChainReducer usage pattern:
 * <p>
 * 
 * <pre>
 * ...
 * Job = new Job(conf);
 * ....
 *
 * Configuration reduceConf = new Configuration(false);
 * ...
 * ChainReducer.setReducer(job, XReduce.class, LongWritable.class, Text.class,
 *   Text.class, Text.class, true, reduceConf);
 *
 * ChainReducer.addMapper(job, CMap.class, Text.class, Text.class,
 *   LongWritable.class, Text.class, false, null);
 *
 * ChainReducer.addMapper(job, DMap.class, LongWritable.class, Text.class,
 *   LongWritable.class, LongWritable.class, true, null);
 *
 * ...
 *
 * job.waitForCompletion(true);
 * ...
 * </pre>
 */
BigDecimalSplitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/BigDecimalSplitter.java)/**
 * Implement DBSplitter over BigDecimal values.
 */
BooleanSplitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/BooleanSplitter.java)/**
 * Implement DBSplitter over boolean values.
 */
DataDrivenDBInputSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java)/**
   * A InputSplit that spans a set of rows
   */
DataDrivenDBInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBInputFormat.java)/**
 * A InputFormat that reads input data from an SQL table.
 * Operates like DBInputFormat, but instead of using LIMIT and OFFSET to demarcate
 * splits, it tries to generate WHERE clauses which separate the data into roughly
 * equivalent shards.
 */
DataDrivenDBRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DataDrivenDBRecordReader.java)/**
 * A RecordReader that reads records from a SQL table,
 * using data-driven WHERE clause splits.
 * Emits LongWritables containing the record number as
 * key and DBWritables as value.
 */
DateSplitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DateSplitter.java)/**
 * Implement DBSplitter over date/time values.
 * Make use of logic from IntegerSplitter, since date/time are just longs
 * in Java.
 */
DBConfiguration (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DBConfiguration.java)/**
 * A container for configuration property names for jobs with DB input/output.
 *  
 * The job can be configured using the static methods in this class, 
 * {@link DBInputFormat}, and {@link DBOutputFormat}. 
 * Alternatively, the properties can be set in the configuration with proper
 * values. 
 *   
 * @see DBConfiguration#configureDB(Configuration, String, String, String, String)
 * @see DBInputFormat#setInput(Job, Class, String, String)
 * @see DBInputFormat#setInput(Job, Class, String, String, String, String...)
 * @see DBOutputFormat#setOutput(Job, String, String...)
 */
NullDBWritable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java)/**
   * A Class that does nothing, implementing DBWritable
   */
DBInputSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java)/**
   * A InputSplit that spans a set of rows
   */
DBInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DBInputFormat.java)/**
 * A InputFormat that reads input data from an SQL table.
 * <p>
 * DBInputFormat emits LongWritables containing the record number as 
 * key and DBWritables as value. 
 * 
 * The SQL query, and input class can be using one of the two 
 * setInput methods.
 */
DBRecordWriter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java)/**
   * A RecordWriter that writes the reduce output to a SQL table
   */
DBOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DBOutputFormat.java)/**
 * A OutputFormat that sends the reduce output to a SQL table.
 * <p> 
 * {@link DBOutputFormat} accepts &lt;key,value&gt; pairs, where 
 * key has a type extending DBWritable. Returned {@link RecordWriter} 
 * writes <b>only the key</b> to the database with a batch SQL query.  
 * 
 */
DBRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DBRecordReader.java)/**
 * A RecordReader that reads records from a SQL table.
 * Emits LongWritables containing the record number as 
 * key and DBWritables as value.  
 */
DBSplitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DBSplitter.java)/**
 * DBSplitter will generate DBInputSplits to use with DataDrivenDBInputFormat.
 * DataDrivenDBInputFormat needs to interpolate between two values that
 * represent the lowest and highest valued records to import. Depending
 * on the data-type of the column, this requires different behavior.
 * DBSplitter implementations should perform this for a data type or family
 * of data types.
 */
DBWritable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DBWritable.java)/**
 * Objects that are read from/written to a database should implement
 * <code>DBWritable</code>. DBWritable, is similar to {@link Writable} 
 * except that the {@link #write(PreparedStatement)} method takes a 
 * {@link PreparedStatement}, and {@link #readFields(ResultSet)} 
 * takes a {@link ResultSet}. 
 * <p>
 * Implementations are responsible for writing the fields of the object 
 * to PreparedStatement, and reading the fields of the object from the 
 * ResultSet. 
 * 
 * <p>Example:</p>
 * If we have the following table in the database :
 * <pre>
 * CREATE TABLE MyTable (
 *   counter        INTEGER NOT NULL,
 *   timestamp      BIGINT  NOT NULL,
 * );
 * </pre>
 * then we can read/write the tuples from/to the table with :
 * <p><pre>
 * public class MyWritable implements Writable, DBWritable {
 *   // Some data     
 *   private int counter;
 *   private long timestamp;
 *       
 *   //Writable#write() implementation
 *   public void write(DataOutput out) throws IOException {
 *     out.writeInt(counter);
 *     out.writeLong(timestamp);
 *   }
 *       
 *   //Writable#readFields() implementation
 *   public void readFields(DataInput in) throws IOException {
 *     counter = in.readInt();
 *     timestamp = in.readLong();
 *   }
 *       
 *   public void write(PreparedStatement statement) throws SQLException {
 *     statement.setInt(1, counter);
 *     statement.setLong(2, timestamp);
 *   }
 *       
 *   public void readFields(ResultSet resultSet) throws SQLException {
 *     counter = resultSet.getInt(1);
 *     timestamp = resultSet.getLong(2);
 *   } 
 * }
 * </pre>
 */
FloatSplitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/FloatSplitter.java)/**
 * Implement DBSplitter over floating-point values.
 */
IntegerSplitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/IntegerSplitter.java)/**
 * Implement DBSplitter over integer values.
 */
MySQLDataDrivenDBRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/MySQLDataDrivenDBRecordReader.java)/**
 * A RecordReader that reads records from a MySQL table via DataDrivenDBRecordReader
 */
MySQLDBRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/MySQLDBRecordReader.java)/**
 * A RecordReader that reads records from a MySQL table.
 */
OracleDataDrivenDBInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/OracleDataDrivenDBInputFormat.java)/**
 * A InputFormat that reads input data from an SQL table in an Oracle db.
 */
OracleDataDrivenDBRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/OracleDataDrivenDBRecordReader.java)/**
 * A RecordReader that reads records from a Oracle table via DataDrivenDBRecordReader
 */
OracleDateSplitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/OracleDateSplitter.java)/**
 * Implement DBSplitter over date/time values returned by an Oracle db.
 * Make use of logic from DateSplitter, since this just needs to use
 * some Oracle-specific functions on the formatting end when generating
 * InputSplits.
 */
OracleDBRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/OracleDBRecordReader.java)/**
 * A RecordReader that reads records from an Oracle SQL table.
 */
TextSplitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/TextSplitter.java)/**
 * Implement DBSplitter over text strings.
 */
FieldSelectionHelper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/fieldsel/FieldSelectionHelper.java)/**
 * This class implements a mapper/reducer class that can be used to perform
 * field selections in a manner similar to unix cut. The input data is treated
 * as fields separated by a user specified separator (the default value is
 * "\t"). The user can specify a list of fields that form the map output keys,
 * and a list of fields that form the map output values. If the inputformat is
 * TextInputFormat, the mapper will ignore the key to the map function. and the
 * fields are from the value only. Otherwise, the fields are the union of those
 * from the key and those from the value.
 * 
 * The field separator is under attribute "mapreduce.fieldsel.data.field.separator"
 * 
 * The map output field list spec is under attribute 
 * "mapreduce.fieldsel.map.output.key.value.fields.spec".
 * The value is expected to be like "keyFieldsSpec:valueFieldsSpec"
 * key/valueFieldsSpec are comma (,) separated field spec: fieldSpec,fieldSpec,fieldSpec ...
 * Each field spec can be a simple number (e.g. 5) specifying a specific field, or a range
 * (like 2-5) to specify a range of fields, or an open range (like 3-) specifying all 
 * the fields starting from field 3. The open range field spec applies value fields only.
 * They have no effect on the key fields.
 * 
 * Here is an example: "4,3,0,1:6,5,1-3,7-". It specifies to use fields 4,3,0 and 1 for keys,
 * and use fields 6,5,1,2,3,7 and above for values.
 * 
 * The reduce output field list spec is under attribute 
 * "mapreduce.fieldsel.reduce.output.key.value.fields.spec".
 * 
 * The reducer extracts output key/value pairs in a similar manner, except that
 * the key is never ignored.
 * 
 */
FieldSelectionMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/fieldsel/FieldSelectionMapper.java)/**
 * This class implements a mapper class that can be used to perform
 * field selections in a manner similar to unix cut. The input data is treated
 * as fields separated by a user specified separator (the default value is
 * "\t"). The user can specify a list of fields that form the map output keys,
 * and a list of fields that form the map output values. If the inputformat is
 * TextInputFormat, the mapper will ignore the key to the map function. and the
 * fields are from the value only. Otherwise, the fields are the union of those
 * from the key and those from the value.
 * 
 * The field separator is under attribute "mapreduce.fieldsel.data.field.separator"
 * 
 * The map output field list spec is under attribute 
 * "mapreduce.fieldsel.map.output.key.value.fields.spec". 
 * The value is expected to be like
 * "keyFieldsSpec:valueFieldsSpec" key/valueFieldsSpec are comma (,) separated
 * field spec: fieldSpec,fieldSpec,fieldSpec ... Each field spec can be a 
 * simple number (e.g. 5) specifying a specific field, or a range (like 2-5)
 * to specify a range of fields, or an open range (like 3-) specifying all 
 * the fields starting from field 3. The open range field spec applies value
 * fields only. They have no effect on the key fields.
 * 
 * Here is an example: "4,3,0,1:6,5,1-3,7-". It specifies to use fields
 * 4,3,0 and 1 for keys, and use fields 6,5,1,2,3,7 and above for values.
 */
FieldSelectionReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/fieldsel/FieldSelectionReducer.java)/**
 * This class implements a reducer class that can be used to perform field
 * selections in a manner similar to unix cut. 
 * 
 * The input data is treated as fields separated by a user specified
 * separator (the default value is "\t"). The user can specify a list of
 * fields that form the reduce output keys, and a list of fields that form
 * the reduce output values. The fields are the union of those from the key
 * and those from the value.
 * 
 * The field separator is under attribute "mapreduce.fieldsel.data.field.separator"
 * 
 * The reduce output field list spec is under attribute 
 * "mapreduce.fieldsel.reduce.output.key.value.fields.spec". 
 * The value is expected to be like
 * "keyFieldsSpec:valueFieldsSpec" key/valueFieldsSpec are comma (,) 
 * separated field spec: fieldSpec,fieldSpec,fieldSpec ... Each field spec
 * can be a simple number (e.g. 5) specifying a specific field, or a range
 * (like 2-5) to specify a range of fields, or an open range (like 3-) 
 * specifying all the fields starting from field 3. The open range field
 * spec applies value fields only. They have no effect on the key fields.
 * 
 * Here is an example: "4,3,0,1:6,5,1-3,7-". It specifies to use fields
 * 4,3,0 and 1 for keys, and use fields 6,5,1,2,3,7 and above for values.
 */
OneFileInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java)/**
   * information about one file from the File System
   */
OneBlockInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java)/**
   * information about one block from the File System
   */
MultiPathFilter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java)/**
   * Accept a path only if any one of filters given in the
   * constructor do. 
   */
CombineFileInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileInputFormat.java)/**
 * An abstract {@link InputFormat} that returns {@link CombineFileSplit}'s in 
 * {@link InputFormat#getSplits(JobContext)} method. 
 * 
 * Splits are constructed from the files under the input paths. 
 * A split cannot have files from different pools.
 * Each split returned may contain blocks from different files.
 * If a maxSplitSize is specified, then blocks on the same node are
 * combined to form a single split. Blocks that are left over are
 * then combined with other blocks in the same rack. 
 * If maxSplitSize is not specified, then blocks from the same rack
 * are combined in a single split; no attempt is made to create
 * node-local splits.
 * If the maxSplitSize is equal to the block size, then this class
 * is similar to the default splitting behavior in Hadoop: each
 * block is a locally processed split.
 * Subclasses implement 
 * {@link InputFormat#createRecordReader(InputSplit, TaskAttemptContext)}
 * to construct <code>RecordReader</code>'s for 
 * <code>CombineFileSplit</code>'s.
 * 
 * @see CombineFileSplit
 */
CombineFileRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileRecordReader.java)/**
 * A generic RecordReader that can hand out different recordReaders
 * for each chunk in a {@link CombineFileSplit}.
 * A CombineFileSplit can combine data chunks from multiple files. 
 * This class allows using different RecordReaders for processing
 * these data chunks from different files.
 * @see CombineFileSplit
 */
CombineFileRecordReaderWrapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileRecordReaderWrapper.java)/**
 * A wrapper class for a record reader that handles a single file split. It
 * delegates most of the methods to the wrapped instance. A concrete subclass
 * needs to provide a constructor that calls this parent constructor with the
 * appropriate input format. The subclass constructor must satisfy the specific
 * constructor signature that is required by
 * <code>CombineFileRecordReader</code>.
 *
 * Subclassing is needed to get a concrete record reader wrapper because of the
 * constructor requirement.
 *
 * @see CombineFileRecordReader
 * @see CombineFileInputFormat
 */
CombineFileSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineFileSplit.java)/**
 * A sub-collection of input files. 
 * 
 * Unlike {@link FileSplit}, CombineFileSplit class does not represent 
 * a split of a file, but a split of input files into smaller sets. 
 * A split may contain blocks from different file but all 
 * the blocks in the same split are probably local to some rack <br> 
 * CombineFileSplit can be used to implement {@link RecordReader}'s, 
 * with reading one record per file.
 * 
 * @see FileSplit
 * @see CombineFileInputFormat 
 */
SequenceFileRecordReaderWrapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineSequenceFileInputFormat.java)/**
   * A record reader that may be passed to <code>CombineFileRecordReader</code>
   * so that it can be used in a <code>CombineFileInputFormat</code>-equivalent
   * for <code>SequenceFileInputFormat</code>.
   *
   * @see CombineFileRecordReader
   * @see CombineFileInputFormat
   * @see SequenceFileInputFormat
   */
CombineSequenceFileInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineSequenceFileInputFormat.java)/**
 * Input format that is a <code>CombineFileInputFormat</code>-equivalent for
 * <code>SequenceFileInputFormat</code>.
 *
 * @see CombineFileInputFormat
 */
TextRecordReaderWrapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineTextInputFormat.java)/**
   * A record reader that may be passed to <code>CombineFileRecordReader</code>
   * so that it can be used in a <code>CombineFileInputFormat</code>-equivalent
   * for <code>TextInputFormat</code>.
   *
   * @see CombineFileRecordReader
   * @see CombineFileInputFormat
   * @see TextInputFormat
   */
CombineTextInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CombineTextInputFormat.java)/**
 * Input format that is a <code>CombineFileInputFormat</code>-equivalent for
 * <code>TextInputFormat</code>.
 *
 * @see CombineFileInputFormat
 */
CompressedSplitLineReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/CompressedSplitLineReader.java)/**
 * Line reader for compressed splits
 *
 * Reading records from a compressed split is tricky, as the
 * LineRecordReader is using the reported compressed input stream
 * position directly to determine when a split has ended.  In addition the
 * compressed input stream is usually faking the actual byte position, often
 * updating it only after the first compressed block after the split is
 * accessed.
 *
 * Depending upon where the last compressed block of the split ends relative
 * to the record delimiters it can be easy to accidentally drop the last
 * record or duplicate the last record between this split and the next.
 *
 * Split end scenarios:
 *
 * 1) Last block of split ends in the middle of a record
 *      Nothing special that needs to be done here, since the compressed input
 *      stream will report a position after the split end once the record
 *      is fully read.  The consumer of the next split will discard the
 *      partial record at the start of the split normally, and no data is lost
 *      or duplicated between the splits.
 *
 * 2) Last block of split ends in the middle of a delimiter
 *      The line reader will continue to consume bytes into the next block to
 *      locate the end of the delimiter.  If a custom delimiter is being used
 *      then the next record must be read by this split or it will be dropped.
 *      The consumer of the next split will not recognize the partial
 *      delimiter at the beginning of its split and will discard it along with
 *      the next record.
 *
 *      However for the default delimiter processing there is a special case
 *      because CR, LF, and CRLF are all valid record delimiters.  If the
 *      block ends with a CR then the reader must peek at the next byte to see
 *      if it is an LF and therefore part of the same record delimiter.
 *      Peeking at the next byte is an access to the next block and triggers
 *      the stream to report the end of the split.  There are two cases based
 *      on the next byte:
 *
 *      A) The next byte is LF
 *           The split needs to end after the current record is returned.  The
 *           consumer of the next split will discard the first record, which
 *           is degenerate since LF is itself a delimiter, and start consuming
 *           records after that byte.  If the current split tries to read
 *           another record then the record will be duplicated between splits.
 *
 *      B) The next byte is not LF
 *           The current record will be returned but the stream will report
 *           the split has ended due to the peek into the next block.  If the
 *           next record is not read then it will be lost, as the consumer of
 *           the next split will discard it before processing subsequent
 *           records.  Therefore the next record beyond the reported split end
 *           must be consumed by this split to avoid data loss.
 *
 * 3) Last block of split ends at the beginning of a delimiter
 *      This is equivalent to case 1, as the reader will consume bytes into
 *      the next block and trigger the end of the split.  No further records
 *      should be read as the consumer of the next split will discard the
 *      (degenerate) record at the beginning of its split.
 *
 * 4) Last block of split ends at the end of a delimiter
 *      Nothing special needs to be done here. The reader will not start
 *      examining the bytes into the next block until the next record is read,
 *      so the stream will not report the end of the split just yet.  Once the
 *      next record is read then the next block will be accessed and the
 *      stream will indicate the end of the split.  The consumer of the next
 *      split will correctly discard the first record of its split, and no
 *      data is lost or duplicated.
 *
 *      If the default delimiter is used and the block ends at a CR then this
 *      is treated as case 2 since the reader does not yet know without
 *      looking at subsequent bytes whether the delimiter has ended.
 *
 * NOTE: It is assumed that compressed input streams *never* return bytes from
 *       multiple compressed blocks from a single read.  Failure to do so will
 *       violate the buffering performed by this class, as it will access
 *       bytes into the next block after the split before returning all of the
 *       records from the previous block.
 */
DelegatingInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/DelegatingInputFormat.java)/**
 * An {@link InputFormat} that delegates behavior of paths to multiple other
 * InputFormats.
 * 
 * @see MultipleInputs#addInputPath(Job, Path, Class, Class)
 */
DelegatingMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/DelegatingMapper.java)/**
 * An {@link Mapper} that delegates behavior of paths to multiple other
 * mappers.
 * 
 * @see MultipleInputs#addInputPath(Job, Path, Class, Class)
 */
DelegatingRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/DelegatingRecordReader.java)/**
 * This is a delegating RecordReader, which delegates the functionality to the
 * underlying record reader in {@link TaggedInputSplit}  
 */
MultiPathFilter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.java)/**
   * Proxy PathFilter that accepts a path only if all filters given in the
   * constructor do. Used by the listPaths() to apply the built-in
   * hiddenFileFilter together with a user provided one (if any).
   */
FileInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.java)/** 
 * A base class for file-based {@link InputFormat}s.
 *
 * <p><code>FileInputFormat</code> is the base class for all file-based 
 * <code>InputFormat</code>s. This provides a generic implementation of
 * {@link #getSplits(JobContext)}.
 *
 * Implementations of <code>FileInputFormat</code> can also override the
 * {@link #isSplitable(JobContext, Path)} method to prevent input files
 * from being split-up in certain situations. Implementations that may
 * deal with non-splittable files <i>must</i> override this method, since
 * the default implementation assumes splitting is always possible.
 */
FileSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FileSplit.java)/** A section of an input file.  Returned by {@link
 * InputFormat#getSplits(JobContext)} and passed to
 * {@link InputFormat#createRecordReader(InputSplit,TaskAttemptContext)}. */
FixedLengthInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FixedLengthInputFormat.java)/**
 * FixedLengthInputFormat is an input format used to read input files
 * which contain fixed length records.  The content of a record need not be
 * text.  It can be arbitrary binary data.  Users must configure the record
 * length property by calling:
 * FixedLengthInputFormat.setRecordLength(conf, recordLength);<br><br> or
 * conf.setInt(FixedLengthInputFormat.FIXED_RECORD_LENGTH, recordLength);
 * <br><br>
 * @see FixedLengthRecordReader
 */
FixedLengthRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FixedLengthRecordReader.java)/**
 * A reader to read fixed length records from a split.  Record offset is
 * returned as key and the record as bytes is returned in value.
 */
InvalidInputException (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/InvalidInputException.java)/**
 * This class wraps a list of problems with the input, so that the user
 * can get a list of problems together instead of finding and fixing them one 
 * by one.
 */
KeyValueLineRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/KeyValueLineRecordReader.java)/**
 * This class treats a line in the input as a key/value pair separated by a 
 * separator character. The separator can be specified in config file 
 * under the attribute name mapreduce.input.keyvaluelinerecordreader.key.value.separator. The default
 * separator is the tab character ('\t').
 */
KeyValueTextInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/KeyValueTextInputFormat.java)/**
 * An {@link InputFormat} for plain text files. Files are broken into lines.
 * Either line feed or carriage-return are used to signal end of line. 
 * Each line is divided into key and value parts by a separator byte. If no
 * such a byte exists, the key will be the entire line and value will be empty.
 * The separator byte can be specified in config file under the attribute name
 * mapreduce.input.keyvaluelinerecordreader.key.value.separator. The default
 * is the tab character ('\t').
 */
LineRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java)/**
 * Treats keys as offset in file and value as line. 
 */
MultipleInputs (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/MultipleInputs.java)/**
 * This class supports MapReduce jobs that have multiple input paths with
 * a different {@link InputFormat} and {@link Mapper} for each path 
 */
NLineInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/NLineInputFormat.java)/**
 * NLineInputFormat which splits N lines of input as one split.
 *
 * In many "pleasantly" parallel applications, each process/mapper 
 * processes the same input file (s), but with computations are 
 * controlled by different parameters.(Referred to as "parameter sweeps").
 * One way to achieve this, is to specify a set of parameters 
 * (one set per line) as input in a control file 
 * (which is the input path to the map-reduce application,
 * where as the input dataset is specified 
 * via a config variable in JobConf.).
 * 
 * The NLineInputFormat can be used in such applications, that splits 
 * the input file such that by default, one line is fed as
 * a value to one map task, and key is the offset.
 * i.e. (k,v) is (LongWritable, Text).
 * The location hints will span the whole mapred cluster.
 */
SequenceFileAsBinaryRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileAsBinaryInputFormat.java)/**
   * Read records from a SequenceFile as binary (raw) bytes.
   */
SequenceFileAsBinaryInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileAsBinaryInputFormat.java)/**
 * InputFormat reading keys, values from SequenceFiles in binary (raw)
 * format.
 */
SequenceFileAsTextInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileAsTextInputFormat.java)/**
 * This class is similar to SequenceFileInputFormat, except it generates
 * SequenceFileAsTextRecordReader which converts the input keys and values
 * to their String forms by calling toString() method. 
 */
SequenceFileAsTextRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileAsTextRecordReader.java)/**
 * This class converts the input keys and values to their String forms by
 * calling toString() method. This class to SequenceFileAsTextInputFormat
 * class is as LineRecordReader class to TextInputFormat class.
 */
Filter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFilter.java)/**
   * filter interface
   */
FilterBase (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFilter.java)/**
   * base class for Filters
   */
RegexFilter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFilter.java)/** Records filter by matching key to regex
   */
PercentFilter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFilter.java)/** This class returns a percentage of records
   * The percentage is determined by a filtering frequency <i>f</i> using
   * the criteria record# % f == 0.
   * For example, if the frequency is 10, one out of 10 records is returned.
   */
MD5Filter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFilter.java)/** This class returns a set of records by examing the MD5 digest of its
   * key against a filtering frequency <i>f</i>. The filtering criteria is
   * MD5(key) % f == 0.
   */
SequenceFileInputFilter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFilter.java)/**
 * A class that allows a map/red job to work on a sample of sequence files.
 * The sample is decided by the filter class set by the job.
 */
SequenceFileInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFormat.java)/** An {@link InputFormat} for {@link SequenceFile}s. */
SequenceFileRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileRecordReader.java)/** An {@link RecordReader} for {@link SequenceFile}s. */
TaggedInputSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java)/**
 * An {@link InputSplit} that tags another InputSplit with extra data for use
 * by {@link DelegatingInputFormat}s and {@link DelegatingMapper}s.
 */
TextInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/TextInputFormat.java)/** An {@link InputFormat} for plain text files.  Files are broken into lines.
 * Either linefeed or carriage-return are used to signal end of line.  Keys are
 * the position in the file, and values are the line of text.. */
UncompressedSplitLineReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/UncompressedSplitLineReader.java)/**
 * SplitLineReader for uncompressed files.
 * This class can split the file correctly even if the delimiter is multi-bytes.
 */
ControlledJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/jobcontrol/ControlledJob.java)/** 
 *  This class encapsulates a MapReduce job and its dependency. It monitors 
 *  the states of the depending jobs and updates the state of this job.
 *  A job starts in the WAITING state. If it does not have any depending jobs,
 *  or all of the depending jobs are in SUCCESS state, then the job state 
 *  will become READY. If any depending jobs fail, the job will fail too. 
 *  When in READY state, the job can be submitted to Hadoop for execution, with
 *  the state changing into RUNNING state. From RUNNING state, the job 
 *  can get into SUCCESS or FAILED state, depending 
 *  the status of the job execution.
 */
JobControl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/jobcontrol/JobControl.java)/** 
 *  This class encapsulates a set of MapReduce jobs and its dependency.
 *   
 *  It tracks the states of the jobs by placing them into different tables
 *  according to their states. 
 *  
 *  This class provides APIs for the client app to add a job to the group 
 *  and to get the jobs in the group in different states. When a job is 
 *  added, an ID unique to the group is assigned to the job. 
 *  
 *  This class has a thread that submits jobs when they become ready, 
 *  monitors the states of the running jobs, and updates the states of jobs
 *  based on the state changes of their depending jobs states. The class 
 *  provides APIs for suspending/resuming the thread, and 
 *  for stopping the thread.
 *  
 */
ArrayListBackedIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/ArrayListBackedIterator.java)/**
 * This class provides an implementation of ResetableIterator. The
 * implementation uses an {@link java.util.ArrayList} to store elements
 * added to it, replaying them as requested.
 * Prefer {@link StreamBackedIterator}.
 */
ComposableInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/ComposableInputFormat.java)/**
 * Refinement of InputFormat requiring implementors to provide
 * ComposableRecordReader instead of RecordReader.
 */
ComposableRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/ComposableRecordReader.java)/**
 * Additional operations required of a RecordReader to participate in a join.
 */
CompositeInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/CompositeInputFormat.java)/**
 * An InputFormat capable of performing joins over a set of data sources sorted
 * and partitioned the same way.
 *
 * A user may define new join types by setting the property
 * <tt>mapreduce.join.define.&lt;ident&gt;</tt> to a classname. 
 * In the expression <tt>mapreduce.join.expr</tt>, the identifier will be
 * assumed to be a ComposableRecordReader.
 * <tt>mapreduce.join.keycomparator</tt> can be a classname used to compare 
 * keys in the join.
 * @see #setFormat
 * @see JoinRecordReader
 * @see MultiFilterRecordReader
 */
CompositeInputSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/CompositeInputSplit.java)/**
 * This InputSplit contains a set of child InputSplits. Any InputSplit inserted
 * into this collection must have a public default constructor.
 */
JoinCollector (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/CompositeRecordReader.java)/**
   * Collector for join values.
   * This accumulates values for a given key from the child RecordReaders. If
   * one or more child RR contain duplicate keys, this will emit the cross
   * product of the associated values until exhausted.
   */
CompositeRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/CompositeRecordReader.java)/**
 * A RecordReader that can effect joins of RecordReaders sharing a common key
 * type and partitioning.
 */
InnerJoinRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/InnerJoinRecordReader.java)/**
 * Full inner join.
 */
JoinDelegationIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/JoinRecordReader.java)/**
   * Since the JoinCollector is effecting our operation, we need only
   * provide an iterator proxy wrapping its operation.
   */
JoinRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/JoinRecordReader.java)/**
 * Base class for Composite joins returning Tuples of arbitrary Writables.
 */
MultiFilterDelegationIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/MultiFilterRecordReader.java)/**
   * Proxy the JoinCollector, but include callback to emit.
   */
MultiFilterRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/MultiFilterRecordReader.java)/**
 * Base class for Composite join returning values derived from multiple
 * sources, but generally not tuples.
 */
OuterJoinRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/OuterJoinRecordReader.java)/**
 * Full outer join.
 */
OverrideRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/OverrideRecordReader.java)/**
 * Prefer the &quot;rightmost&quot; data source for this key.
 * For example, <tt>override(S1,S2,S3)</tt> will prefer values
 * from S3 over S2, and values from S2 over S1 for all keys
 * emitted from all sources.
 */
Token (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/Parser.java)/**
   * Tagged-union type for tokens from the join expression.
   * @see Parser.TType
   */
Lexer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/Parser.java)/**
   * Simple lexer wrapping a StreamTokenizer.
   * This encapsulates the creation of tagged-union Tokens and initializes the
   * SteamTokenizer.
   */
WNode (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/Parser.java)/**
   * Nodetype in the parse tree for &quot;wrapped&quot; InputFormats.
   */
CNode (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/Parser.java)/**
   * Internal nodetype for &quot;composite&quot; InputFormats.
   */
Parser (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/Parser.java)/**
 * Very simple shift-reduce parser for join expressions.
 *
 * This should be sufficient for the user extension permitted now, but ought to
 * be replaced with a parser generator if more complex grammars are supported.
 * In particular, this &quot;shift-reduce&quot; parser has no states. Each set
 * of formals requires a different internal node type, which is responsible for
 * interpreting the list of tokens it receives. This is sufficient for the
 * current grammar, but it has several annoying properties that might inhibit
 * extension. In particular, parenthesis are always function calls; an
 * algebraic or filter grammar would not only require a node type, but must
 * also work around the internals of this parser.
 *
 * For most other cases, adding classes to the hierarchy- particularly by
 * extending JoinRecordReader and MultiFilterRecordReader- is fairly
 * straightforward. One need only override the relevant method(s) (usually only
 * {@link CompositeRecordReader#combine}) and include a property to map its
 * value to an identifier in the parser.
 */
ResetableIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/ResetableIterator.java)/**
 * This defines an interface to a stateful Iterator that can replay elements
 * added to it directly.
 * Note that this does not extend {@link java.util.Iterator}.
 */
StreamBackedIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/StreamBackedIterator.java)/**
 * This class provides an implementation of ResetableIterator. This
 * implementation uses a byte array to store elements added to it.
 */
TupleWritable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/TupleWritable.java)/**
 * Writable type storing multiple {@link org.apache.hadoop.io.Writable}s.
 *
 * This is *not* a general-purpose tuple type. In almost all cases, users are
 * encouraged to implement their own serializable types, which can perform
 * better validation and provide more efficient encodings than this class is
 * capable. TupleWritable relies on the join framework for type safety and
 * assumes its instances will rarely be persisted, assumptions not only
 * incompatible with, but contrary to the general case.
 *
 * @see org.apache.hadoop.io.Writable
 */
WrappedRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/WrappedRecordReader.java)/**
 * Proxy class for a RecordReader participating in the join framework.
 * 
 * This class keeps track of the &quot;head&quot; key-value pair for the
 * provided RecordReader and keeps a store of values matching a key when
 * this source is participating in a join.
 */
InverseMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/map/InverseMapper.java)/** A {@link Mapper} that swaps keys and values. */
MultithreadedMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/map/MultithreadedMapper.java)/**
 * Multithreaded implementation for @link org.apache.hadoop.mapreduce.Mapper.
 * <p>
 * It can be used instead of the default implementation,
 * {@link org.apache.hadoop.mapred.MapRunner}, when the Map operation is not CPU
 * bound in order to improve throughput.
 * <p>
 * Mapper implementations using this MapRunnable must be thread-safe.
 * <p>
 * The Map-Reduce job has to be configured with the mapper to use via 
 * {@link #setMapperClass(Job, Class)} and
 * the number of thread the thread-pool can use with the
 * {@link #getNumberOfThreads(JobContext)} method. The default
 * value is 10 threads.
 * <p>
 */
RegexMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/map/RegexMapper.java)/** A {@link Mapper} that extracts text matching a regular expression. */
TokenCounterMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/map/TokenCounterMapper.java)/**
 * Tokenize the input values and emit each word with a count of 1.
 */
WrappedMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/map/WrappedMapper.java)/**
 * A {@link Mapper} which wraps a given one to allow custom 
 * {@link Mapper.Context} implementations.
 */
BindingPathOutputCommitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/BindingPathOutputCommitter.java)/**
 * This is a special committer which creates the factory for the committer and
 * runs off that. Why does it exist? So that you can explicitly instantiate
 * a committer by classname and yet still have the actual implementation
 * driven dynamically by the factory options and destination filesystem.
 * This simplifies integration
 * with existing code which takes the classname of a committer.
 * There's no factory for this, as that would lead to a loop.
 *
 * All commit protocol methods and accessors are delegated to the
 * wrapped committer.
 *
 * How to use:
 *
 * <ol>
 *   <li>
 *     In applications which take a classname of committer in
 *     a configuration option, set it to the canonical name of this class
 *     (see {@link #NAME}). When this class is instantiated, it will
 *     use the factory mechanism to locate the configured committer for the
 *     destination.
 *   </li>
 *   <li>
 *     In code, explicitly create an instance of this committer through
 *     its constructor, then invoke commit lifecycle operations on it.
 *     The dynamically configured committer will be created in the constructor
 *     and have the lifecycle operations relayed to it.
 *   </li>
 * </ol>
 *
 */
FileOutputCommitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java)/** An {@link OutputCommitter} that commits files specified 
 * in job output directory i.e. ${mapreduce.output.fileoutputformat.outputdir}.
 **/
FileOutputCommitterFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitterFactory.java)/**
 * Creates a {@link FileOutputCommitter}, always.
 */
FileOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputFormat.java)/** A base class for {@link OutputFormat}s that read from {@link FileSystem}s.*/
FilterOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FilterOutputFormat.java)/**
 * FilterOutputFormat is a convenience class that wraps OutputFormat. 
 */
LazyRecordWriter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java)/**
   * A convenience class to be used with LazyOutputFormat
   */
LazyOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/LazyOutputFormat.java)/**
 * A Convenience class that creates output lazily.
 * Use in conjuction with org.apache.hadoop.mapreduce.lib.output.MultipleOutputs to recreate the
 * behaviour of org.apache.hadoop.mapred.lib.MultipleTextOutputFormat (etc) of the old Hadoop API.
 * See {@link MultipleOutputs} documentation for more information.
 */
MapFileOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/MapFileOutputFormat.java)/** 
 * An {@link org.apache.hadoop.mapreduce.OutputFormat} that writes 
 * {@link MapFile}s.
 */
RecordWriterWithCounter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java)/**
   * Wraps RecordWriter to increment counters. 
   */
MultipleOutputs (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java)/**
 * The MultipleOutputs class simplifies writing output data 
 * to multiple outputs
 * 
 * <p> 
 * Case one: writing to additional outputs other than the job default output.
 *
 * Each additional output, or named output, may be configured with its own
 * <code>OutputFormat</code>, with its own key class and with its own value
 * class.
 * </p>
 * 
 * <p>
 * Case two: to write data to different files provided by user
 * </p>
 * 
 * <p>
 * MultipleOutputs supports counters, by default they are disabled. The 
 * counters group is the {@link MultipleOutputs} class name. The names of the 
 * counters are the same as the output name. These count the number records 
 * written to each output name.
 * </p>
 * 
 * Usage pattern for job submission:
 * <pre>
 *
 * Job job = new Job();
 *
 * FileInputFormat.setInputPath(job, inDir);
 * FileOutputFormat.setOutputPath(job, outDir);
 *
 * job.setMapperClass(MOMap.class);
 * job.setReducerClass(MOReduce.class);
 * ...
 *
 * // Defines additional single text based output 'text' for the job
 * MultipleOutputs.addNamedOutput(job, "text", TextOutputFormat.class,
 * LongWritable.class, Text.class);
 *
 * // Defines additional sequence-file based output 'sequence' for the job
 * MultipleOutputs.addNamedOutput(job, "seq",
 *   SequenceFileOutputFormat.class,
 *   LongWritable.class, Text.class);
 * ...
 *
 * job.waitForCompletion(true);
 * ...
 * </pre>
 * <p>
 * Usage in Reducer:
 * <pre>
 * &lt;K, V&gt; String generateFileName(K k, V v) {
 *   return k.toString() + "_" + v.toString();
 * }
 * 
 * public class MOReduce extends
 *   Reducer&lt;WritableComparable, Writable,WritableComparable, Writable&gt; {
 * private MultipleOutputs mos;
 * public void setup(Context context) {
 * ...
 * mos = new MultipleOutputs(context);
 * }
 *
 * public void reduce(WritableComparable key, Iterator&lt;Writable&gt; values,
 * Context context)
 * throws IOException {
 * ...
 * mos.write("text", , key, new Text("Hello"));
 * mos.write("seq", LongWritable(1), new Text("Bye"), "seq_a");
 * mos.write("seq", LongWritable(2), key, new Text("Chau"), "seq_b");
 * mos.write(key, new Text("value"), generateFileName(key, new Text("value")));
 * ...
 * }
 *
 * public void cleanup(Context) throws IOException {
 * mos.close();
 * ...
 * }
 *
 * }
 * </pre>
 * 
 * <p>
 * When used in conjuction with org.apache.hadoop.mapreduce.lib.output.LazyOutputFormat,
 * MultipleOutputs can mimic the behaviour of MultipleTextOutputFormat and MultipleSequenceFileOutputFormat
 * from the old Hadoop API - ie, output can be written from the Reducer to more than one location.
 * </p>
 * 
 * <p>
 * Use <code>MultipleOutputs.write(KEYOUT key, VALUEOUT value, String baseOutputPath)</code> to write key and 
 * value to a path specified by <code>baseOutputPath</code>, with no need to specify a named output.
 * <b>Warning</b>: when the baseOutputPath passed to MultipleOutputs.write
 * is a path that resolves outside of the final job output directory, the
 * directory is created immediately and then persists through subsequent
 * task retries, breaking the concept of output committing:
 * </p>
 * 
 * <pre>
 * private MultipleOutputs&lt;Text, Text&gt; out;
 * 
 * public void setup(Context context) {
 *   out = new MultipleOutputs&lt;Text, Text&gt;(context);
 *   ...
 * }
 * 
 * public void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {
 * for (Text t : values) {
 *   out.write(key, t, generateFileName(&lt;<i>parameter list...</i>&gt;));
 *   }
 * }
 * 
 * protected void cleanup(Context context) throws IOException, InterruptedException {
 *   out.close();
 * }
 * </pre>
 * 
 * <p>
 * Use your own code in <code>generateFileName()</code> to create a custom path to your results. 
 * '/' characters in <code>baseOutputPath</code> will be translated into directory levels in your file system. 
 * Also, append your custom-generated path with "part" or similar, otherwise your output will be -00000, -00001 etc. 
 * No call to <code>context.write()</code> is necessary. See example <code>generateFileName()</code> code below. 
 * </p>
 * 
 * <pre>
 * private String generateFileName(Text k) {
 *   // expect Text k in format "Surname|Forename"
 *   String[] kStr = k.toString().split("\\|");
 *   
 *   String sName = kStr[0];
 *   String fName = kStr[1];
 *
 *   // example for k = Smith|John
 *   // output written to /user/hadoop/path/to/output/Smith/John-r-00000 (etc)
 *   return sName + "/" + fName;
 * }
 * </pre>
 * 
 * <p>
 * Using MultipleOutputs in this way will still create zero-sized default output, eg part-00000.
 * To prevent this use <code>LazyOutputFormat.setOutputFormatClass(job, TextOutputFormat.class);</code>
 * instead of <code>job.setOutputFormatClass(TextOutputFormat.class);</code> in your Hadoop job configuration.
 * </p> 
 * 
 */
NamedCommitterFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/NamedCommitterFactory.java)/**
 * A factory which creates any named committer identified
 * in the option {@link PathOutputCommitterFactory#NAMED_COMMITTER_CLASS}.
 */
NullOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/NullOutputFormat.java)/**
 * Consume all outputs and put them in /dev/null. 
 */
PartialFileOutputCommitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/PartialFileOutputCommitter.java)/** An {@link OutputCommitter} that commits files specified
 * in job output directory i.e. ${mapreduce.output.fileoutputformat.outputdir}.
 **/
PartialOutputCommitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/PartialOutputCommitter.java)/**
 * Interface for an {@link org.apache.hadoop.mapreduce.OutputCommitter}
 * implementing partial commit of task output, as during preemption.
 */
PathOutputCommitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/PathOutputCommitter.java)/**
 * A committer which somehow commits data written to a working directory
 * to the final directory during the commit process. The reference
 * implementation of this is the {@link FileOutputCommitter}.
 *
 * There are two constructors, both of which do nothing but long and
 * validate their arguments.
 */
PathOutputCommitterFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/PathOutputCommitterFactory.java)/**
 * A factory for committers implementing the {@link PathOutputCommitter}
 * methods, and so can be used from {@link FileOutputFormat}.
 * The base implementation returns {@link FileOutputCommitter} instances.
 *
 * Algorithm:
 * <ol>
 *   <li>If an explicit committer factory is named, it is used.</li>
 *   <li>The output path is examined.
 *   If is non null and there is an explicit schema for that filesystem,
 *   its factory is instantiated.</li>
 *   <li>Otherwise, an instance of {@link FileOutputCommitter} is
 *   created.</li>
 * </ol>
 *
 * In {@link FileOutputFormat}, the created factory has its method
 * {@link #createOutputCommitter(Path, TaskAttemptContext)} with a task
 * attempt context and a possibly null path.
 *
 */
WritableValueBytes (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java)/** 
   * Inner class used for appendRaw
   */
SequenceFileAsBinaryOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/SequenceFileAsBinaryOutputFormat.java)/** 
 * An {@link org.apache.hadoop.mapreduce.OutputFormat} that writes keys, 
 * values to {@link SequenceFile}s in binary(raw) format
 */
SequenceFileOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/SequenceFileOutputFormat.java)/** An {@link OutputFormat} that writes {@link SequenceFile}s. */
TextOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/TextOutputFormat.java)/** An {@link OutputFormat} that writes plain text files. */
BinaryPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/BinaryPartitioner.java)/**
 * <p>Partition {@link BinaryComparable} keys using a configurable part of 
 * the bytes array returned by {@link BinaryComparable#getBytes()}.</p>
 * 
 * <p>The subarray to be used for the partitioning can be defined by means
 * of the following properties:
 * <ul>
 *   <li>
 *     <i>mapreduce.partition.binarypartitioner.left.offset</i>:
 *     left offset in array (0 by default)
 *   </li>
 *   <li>
 *     <i>mapreduce.partition.binarypartitioner.right.offset</i>: 
 *     right offset in array (-1 by default)
 *   </li>
 * </ul>
 * Like in Python, both negative and positive offsets are allowed, but
 * the meaning is slightly different. In case of an array of length 5,
 * for instance, the possible offsets are:
 * <pre><code>
 *  +---+---+---+---+---+
 *  | B | B | B | B | B |
 *  +---+---+---+---+---+
 *    0   1   2   3   4
 *   -5  -4  -3  -2  -1
 * </code></pre>
 * The first row of numbers gives the position of the offsets 0...5 in 
 * the array; the second row gives the corresponding negative offsets. 
 * Contrary to Python, the specified subarray has byte <code>i</code> 
 * and <code>j</code> as first and last element, repectively, when 
 * <code>i</code> and <code>j</code> are the left and right offset.
 * 
 * <p>For Hadoop programs written in Java, it is advisable to use one of 
 * the following static convenience methods for setting the offsets:
 * <ul>
 *   <li>{@link #setOffsets}</li>
 *   <li>{@link #setLeftOffset}</li>
 *   <li>{@link #setRightOffset}</li>
 * </ul>
 */
HashPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/HashPartitioner.java)/** Partition keys by their {@link Object#hashCode()}. */
Sampler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/InputSampler.java)/**
   * Interface to sample using an 
   * {@link org.apache.hadoop.mapreduce.InputFormat}.
   */
SplitSampler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/InputSampler.java)/**
   * Samples the first n records from s splits.
   * Inexpensive way to sample random data.
   */
RandomSampler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/InputSampler.java)/**
   * Sample from random points in the input.
   * General-purpose sampler. Takes numSamples / maxSplitsSampled inputs from
   * each split.
   */
IntervalSampler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/InputSampler.java)/**
   * Sample from s splits at regular intervals.
   * Useful for sorted data.
   */
InputSampler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/InputSampler.java)/**
 * Utility for collecting samples and writing a partition file for
 * {@link TotalOrderPartitioner}.
 */
KeyFieldBasedComparator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.java)/**
 * This comparator implementation provides a subset of the features provided
 * by the Unix/GNU Sort. In particular, the supported features are:
 * -n, (Sort numerically)
 * -r, (Reverse the result of comparison)
 * -k pos1[,pos2], where pos is of the form f[.c][opts], where f is the number
 *  of the field to use, and c is the number of the first character from the
 *  beginning of the field. Fields and character posns are numbered starting
 *  with 1; a character position of zero in pos2 indicates the field's last
 *  character. If '.c' is omitted from pos1, it defaults to 1 (the beginning
 *  of the field); if omitted from pos2, it defaults to 0 (the end of the
 *  field). opts are ordering options (any of 'nr' as described above). 
 * We assume that the fields in the key are separated by 
 * {@link JobContext#MAP_OUTPUT_KEY_FIELD_SEPARATOR}.
 */
KeyFieldBasedPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedPartitioner.java)/**   
  *  Defines a way to partition keys based on certain key fields (also see
  *  {@link KeyFieldBasedComparator}.
  *  The key specification supported is of the form -k pos1[,pos2], where,
  *  pos is of the form f[.c][opts], where f is the number
  *  of the key field to use, and c is the number of the first character from
  *  the beginning of the field. Fields and character posns are numbered 
  *  starting with 1; a character position of zero in pos2 indicates the
  *  field's last character. If '.c' is omitted from pos1, it defaults to 1
  *  (the beginning of the field); if omitted from pos2, it defaults to 0 
  *  (the end of the field).
  * 
  */
RehashPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/RehashPartitioner.java)/**
  *  This partitioner rehashes values returned by {@link Object#hashCode()}
  *  to get smoother distribution between partitions which may improve
  *  reduce reduce time in some cases and should harm things in no cases.
  *  This partitioner is suggested with Integer and Long keys with simple
  *  patterns in their distributions.
  *  @since 2.0.3
 */
Node (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/TotalOrderPartitioner.java)/**
   * Interface to the partitioner to locate a key in the partition keyset.
   */
TrieNode (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/TotalOrderPartitioner.java)/**
   * Base class for trie nodes. If the keytype is memcomp-able, this builds
   * tries of the first <tt>total.order.partitioner.max.trie.depth</tt>
   * bytes.
   */
BinarySearchNode (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/TotalOrderPartitioner.java)/**
   * For types that are not {@link org.apache.hadoop.io.BinaryComparable} or
   * where disabled by <tt>total.order.partitioner.natural.order</tt>,
   * search the partition keyset with a binary search.
   */
InnerTrieNode (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/TotalOrderPartitioner.java)/**
   * An inner trie node that contains 256 children based on the next
   * character.
   */
LeafTrieNode (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/TotalOrderPartitioner.java)/**
   * A leaf trie node that scans for the key between lower..upper.
   * 
   * We don't generate many of these now, since we usually continue trie-ing 
   * when more than one split point remains at this level. and we make different
   * objects for nodes with 0 or 1 split point.
   */
CarriedTrieNodeRef (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/TotalOrderPartitioner.java)/**
   * 
   * This object contains a TrieNodeRef if there is such a thing that
   * can be repeated.  Two adjacent trie node slots that contain no 
   * split points can be filled with the same trie node, even if they
   * are not on the same level.  See buildTreeRec, below.
   *
   */
TotalOrderPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/TotalOrderPartitioner.java)/**
 * Partitioner effecting a total order by reading split points from
 * an externally generated source.
 */
WrappedReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/reduce/WrappedReducer.java)/**
 * A {@link Reducer} which wraps a given one to allow for custom 
 * {@link Reducer.Context} implementations.
 */
MapContext (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MapContext.java)/**
 * The context that is given to the {@link Mapper}.
 * @param <KEYIN> the key input type to the Mapper
 * @param <VALUEIN> the value input type to the Mapper
 * @param <KEYOUT> the key output type from the Mapper
 * @param <VALUEOUT> the value output type from the Mapper
 */
Context (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Mapper.java)/**
   * The <code>Context</code> passed on to the {@link Mapper} implementations.
   */
Mapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Mapper.java)/** 
 * Maps input key/value pairs to a set of intermediate key/value pairs.  
 * 
 * <p>Maps are the individual tasks which transform input records into a 
 * intermediate records. The transformed intermediate records need not be of 
 * the same type as the input records. A given input pair may map to zero or 
 * many output pairs.</p> 
 * 
 * <p>The Hadoop Map-Reduce framework spawns one map task for each 
 * {@link InputSplit} generated by the {@link InputFormat} for the job.
 * <code>Mapper</code> implementations can access the {@link Configuration} for 
 * the job via the {@link JobContext#getConfiguration()}.
 * 
 * <p>The framework first calls 
 * {@link #setup(org.apache.hadoop.mapreduce.Mapper.Context)}, followed by
 * {@link #map(Object, Object, org.apache.hadoop.mapreduce.Mapper.Context)}
 * for each key/value pair in the <code>InputSplit</code>. Finally 
 * {@link #cleanup(org.apache.hadoop.mapreduce.Mapper.Context)} is called.</p>
 * 
 * <p>All intermediate values associated with a given output key are 
 * subsequently grouped by the framework, and passed to a {@link Reducer} to  
 * determine the final output. Users can control the sorting and grouping by 
 * specifying two key {@link RawComparator} classes.</p>
 *
 * <p>The <code>Mapper</code> outputs are partitioned per 
 * <code>Reducer</code>. Users can control which keys (and hence records) go to 
 * which <code>Reducer</code> by implementing a custom {@link Partitioner}.
 * 
 * <p>Users can optionally specify a <code>combiner</code>, via 
 * {@link Job#setCombinerClass(Class)}, to perform local aggregation of the 
 * intermediate outputs, which helps to cut down the amount of data transferred 
 * from the <code>Mapper</code> to the <code>Reducer</code>.
 * 
 * <p>Applications can specify if and how the intermediate
 * outputs are to be compressed and which {@link CompressionCodec}s are to be
 * used via the <code>Configuration</code>.</p>
 *  
 * <p>If the job has zero
 * reduces then the output of the <code>Mapper</code> is directly written
 * to the {@link OutputFormat} without sorting by keys.</p>
 * 
 * <p>Example:</p>
 * <p><blockquote><pre>
 * public class TokenCounterMapper 
 *     extends Mapper&lt;Object, Text, Text, IntWritable&gt;{
 *    
 *   private final static IntWritable one = new IntWritable(1);
 *   private Text word = new Text();
 *   
 *   public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
 *     StringTokenizer itr = new StringTokenizer(value.toString());
 *     while (itr.hasMoreTokens()) {
 *       word.set(itr.nextToken());
 *       context.write(word, one);
 *     }
 *   }
 * }
 * </pre></blockquote>
 *
 * <p>Applications may override the
 * {@link #run(org.apache.hadoop.mapreduce.Mapper.Context)} method to exert
 * greater control on map processing e.g. multi-threaded <code>Mapper</code>s 
 * etc.</p>
 * 
 * @see InputFormat
 * @see JobContext
 * @see Partitioner  
 * @see Reducer
 */
MarkableIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MarkableIterator.java)/**
 * <code>MarkableIterator</code> is a wrapper iterator class that 
 * implements the {@link MarkableIteratorInterface}.
 * 
 */
MarkableIteratorInterface (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MarkableIteratorInterface.java)/**
 * <code>MarkableIteratorInterface</code> is an interface for a iterator that 
 * supports mark-reset functionality. 
 *
 * <p>Mark can be called at any point during the iteration process and a reset
 * will go back to the last record before the call to the previous mark.
 * 
 */
MRConfig (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/MRConfig.java)/**
 * Place holder for cluster level configuration keys.
 * 
 * The keys should have "mapreduce.cluster." as the prefix. 
 *
 */
OutputCommitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/OutputCommitter.java)/**
 * <code>OutputCommitter</code> describes the commit of task output for a 
 * Map-Reduce job.
 *
 * <p>The Map-Reduce framework relies on the <code>OutputCommitter</code> of 
 * the job to:<p>
 * <ol>
 *   <li>
 *   Setup the job during initialization. For example, create the temporary 
 *   output directory for the job during the initialization of the job.
 *   </li>
 *   <li>
 *   Cleanup the job after the job completion. For example, remove the
 *   temporary output directory after the job completion. 
 *   </li>
 *   <li>
 *   Setup the task temporary output.
 *   </li> 
 *   <li>
 *   Check whether a task needs a commit. This is to avoid the commit
 *   procedure if a task does not need commit.
 *   </li>
 *   <li>
 *   Commit of the task output.
 *   </li>  
 *   <li>
 *   Discard the task commit.
 *   </li>
 * </ol>
 * The methods in this class can be called from several different processes and
 * from several different contexts.  It is important to know which process and
 * which context each is called from.  Each method should be marked accordingly
 * in its documentation.  It is also important to note that not all methods are
 * guaranteed to be called once and only once.  If a method is not guaranteed to
 * have this property the output committer needs to handle this appropriately. 
 * Also note it will only be in rare situations where they may be called 
 * multiple times for the same task.
 * 
 * @see org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter 
 * @see JobContext
 * @see TaskAttemptContext 
 */
OutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/OutputFormat.java)/** 
 * <code>OutputFormat</code> describes the output-specification for a 
 * Map-Reduce job.
 *
 * <p>The Map-Reduce framework relies on the <code>OutputFormat</code> of the
 * job to:<p>
 * <ol>
 *   <li>
 *   Validate the output-specification of the job. For e.g. check that the 
 *   output directory doesn't already exist. 
 *   <li>
 *   Provide the {@link RecordWriter} implementation to be used to write out
 *   the output files of the job. Output files are stored in a 
 *   {@link FileSystem}.
 *   </li>
 * </ol>
 * 
 * @see RecordWriter
 */
Partitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Partitioner.java)/** 
 * Partitions the key space.
 * 
 * <p><code>Partitioner</code> controls the partitioning of the keys of the 
 * intermediate map-outputs. The key (or a subset of the key) is used to derive
 * the partition, typically by a hash function. The total number of partitions
 * is the same as the number of reduce tasks for the job. Hence this controls
 * which of the <code>m</code> reduce tasks the intermediate key (and hence the 
 * record) is sent for reduction.</p>
 *
 * <p>Note: A <code>Partitioner</code> is created only when there are multiple
 * reducers.</p>
 *
 * <p>Note: If you require your Partitioner class to obtain the Job's
 * configuration object, implement the {@link Configurable} interface.</p>
 * 
 * @see Reducer
 */
ClientProtocol (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java)/** 
 * Protocol that a JobClient and the central JobTracker use to communicate.  The
 * JobClient can use these methods to submit a Job for execution, and learn about
 * the current system status.
 */
QueueAclsInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/QueueAclsInfo.java)/**
 *  Class to encapsulate Queue ACLs for a particular
 *  user.
 * 
 */
QueueInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/QueueInfo.java)/**
 * Class that contains the information regarding the Job Queues which are 
 * maintained by the Hadoop Map/Reduce framework.
 * 
 */
RecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/RecordReader.java)/**
 * The record reader breaks the data into key/value pairs for input to the
 * {@link Mapper}.
 * @param <KEYIN>
 * @param <VALUEIN>
 */
RecordWriter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/RecordWriter.java)/**
 * <code>RecordWriter</code> writes the output &lt;key, value&gt; pairs 
 * to an output file.
 
 * <p><code>RecordWriter</code> implementations write the job outputs to the
 * {@link FileSystem}.
 * 
 * @see OutputFormat
 */
ValueIterator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/ReduceContext.java)/**
   * {@link Iterator} to iterate over values for a given group of records.
   */
ReduceContext (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/ReduceContext.java)/**
 * The context passed to the {@link Reducer}.
 * @param <KEYIN> the class of the input keys
 * @param <VALUEIN> the class of the input values
 * @param <KEYOUT> the class of the output keys
 * @param <VALUEOUT> the class of the output values
 */
Context (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Reducer.java)/**
   * The <code>Context</code> passed on to the {@link Reducer} implementations.
   */
Reducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Reducer.java)/** 
 * Reduces a set of intermediate values which share a key to a smaller set of
 * values.  
 * 
 * <p><code>Reducer</code> implementations 
 * can access the {@link Configuration} for the job via the 
 * {@link JobContext#getConfiguration()} method.</p>

 * <p><code>Reducer</code> has 3 primary phases:</p>
 * <ol>
 *   <li>
 *   
 *   <b id="Shuffle">Shuffle</b>
 *   
 *   <p>The <code>Reducer</code> copies the sorted output from each 
 *   {@link Mapper} using HTTP across the network.</p>
 *   </li>
 *   
 *   <li>
 *   <b id="Sort">Sort</b>
 *   
 *   <p>The framework merge sorts <code>Reducer</code> inputs by 
 *   <code>key</code>s 
 *   (since different <code>Mapper</code>s may have output the same key).</p>
 *   
 *   <p>The shuffle and sort phases occur simultaneously i.e. while outputs are
 *   being fetched they are merged.</p>
 *      
 *   <b id="SecondarySort">SecondarySort</b>
 *   
 *   <p>To achieve a secondary sort on the values returned by the value 
 *   iterator, the application should extend the key with the secondary
 *   key and define a grouping comparator. The keys will be sorted using the
 *   entire key, but will be grouped using the grouping comparator to decide
 *   which keys and values are sent in the same call to reduce.The grouping 
 *   comparator is specified via 
 *   {@link Job#setGroupingComparatorClass(Class)}. The sort order is
 *   controlled by 
 *   {@link Job#setSortComparatorClass(Class)}.</p>
 *   
 *   
 *   For example, say that you want to find duplicate web pages and tag them 
 *   all with the url of the "best" known example. You would set up the job 
 *   like:
 *   <ul>
 *     <li>Map Input Key: url</li>
 *     <li>Map Input Value: document</li>
 *     <li>Map Output Key: document checksum, url pagerank</li>
 *     <li>Map Output Value: url</li>
 *     <li>Partitioner: by checksum</li>
 *     <li>OutputKeyComparator: by checksum and then decreasing pagerank</li>
 *     <li>OutputValueGroupingComparator: by checksum</li>
 *   </ul>
 *   </li>
 *   
 *   <li>   
 *   <b id="Reduce">Reduce</b>
 *   
 *   <p>In this phase the 
 *   {@link #reduce(Object, Iterable, org.apache.hadoop.mapreduce.Reducer.Context)}
 *   method is called for each <code>&lt;key, (collection of values)&gt;</code> in
 *   the sorted inputs.</p>
 *   <p>The output of the reduce task is typically written to a 
 *   {@link RecordWriter} via 
 *   {@link Context#write(Object, Object)}.</p>
 *   </li>
 * </ol>
 * 
 * <p>The output of the <code>Reducer</code> is <b>not re-sorted</b>.</p>
 * 
 * <p>Example:</p>
 * <p><blockquote><pre>
 * public class IntSumReducer&lt;Key&gt; extends Reducer&lt;Key,IntWritable,
 *                                                 Key,IntWritable&gt; {
 *   private IntWritable result = new IntWritable();
 * 
 *   public void reduce(Key key, Iterable&lt;IntWritable&gt; values,
 *                      Context context) throws IOException, InterruptedException {
 *     int sum = 0;
 *     for (IntWritable val : values) {
 *       sum += val.get();
 *     }
 *     result.set(sum);
 *     context.write(key, result);
 *   }
 * }
 * </pre></blockquote>
 * 
 * @see Mapper
 * @see Partitioner
 */
SecureShuffleUtils (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/SecureShuffleUtils.java)/**
 * 
 * utilities for generating kyes, hashes and verifying them for shuffle
 *
 */
DelegationTokenIdentifier (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/token/delegation/DelegationTokenIdentifier.java)/**
 * A delegation token identifier that is specific to MapReduce.
 */
DelegationTokenSecretManager (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/token/delegation/DelegationTokenSecretManager.java)/**
 * A MapReduce specific delegation token secret manager.
 * The secret manager is responsible for generating and accepting the password
 * for each token.
 */
DelegationTokenSelector (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/token/delegation/DelegationTokenSelector.java)/**
 * A delegation token that is specialized for MapReduce
 */
JobTokenIdentifier (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/token/JobTokenIdentifier.java)/**
 * The token identifier for job token
 */
JobTokenSecretManager (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/token/JobTokenSecretManager.java)/**
 * SecretManager for job token. It can be used to cache generated job tokens.
 */
JobTokenSelector (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/token/JobTokenSelector.java)/**
 * Look through tokens to find the first job token that matches the service
 * and return it.
 */
TokenCache (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/TokenCache.java)/**
 * This class provides user facing APIs for transferring secrets from
 * the job client to the tasks.
 * The secrets can be stored just before submission of jobs and read during
 * the task execution.  
 */
JTConfig (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/server/jobtracker/JTConfig.java)/**
 * Place holder for JobTracker server-level configuration.
 * 
 * The keys should have "mapreduce.jobtracker." as the prefix
 */
TTConfig (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/server/tasktracker/TTConfig.java)/**
 * Place holder for MapReduce server-level configuration.
 * (formerly TaskTracker configuration)
 */
SharedCacheConfig (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/SharedCacheConfig.java)/**
 * A class for parsing configuration parameters associated with the shared
 * cache.
 */
SplitMetaInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/JobSplit.java)/**
   * This represents the meta information about the task split.
   * The main fields are 
   *     - start offset in actual split
   *     - data length that will be processed in this split
   *     - hosts on which this split is local
   */
TaskSplitMetaInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/JobSplit.java)/**
   * This represents the meta information about the task split that the 
   * JobTracker creates
   */
TaskSplitIndex (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/JobSplit.java)/**
   * This represents the meta information about the task split that the 
   * task gets
   */
JobSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/JobSplit.java)/**
 * This class groups the fundamental classes associated with
 * reading/writing splits. The split information is divided into
 * two parts based on the consumer of the information. The two
 * parts are the split meta information, and the raw split 
 * information. The first part is consumed by the JobTracker to
 * create the tasks' locality data structures. The second part is
 * used by the maps at runtime to know what to do!
 * These pieces of information are written to two separate files.
 * The metainformation file is slurped by the JobTracker during 
 * job initialization. A map task gets the meta information during
 * the launch and it reads the raw split bytes directly from the 
 * file.
 */
JobSplitWriter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/JobSplitWriter.java)/**
 * The class that is used by the Job clients to write splits (both the meta
 * and the raw bytes parts)
 */
SplitMetaInfoReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java)/**
 * A utility that reads the split meta info and creates
 * split meta info objects
 */
JobContextImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/JobContextImpl.java)/**
 * A read-only view of the job that is provided to the tasks while they
 * are running.
 */
MapContextImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/MapContextImpl.java)/**
 * The context that is given to the {@link Mapper}.
 * @param <KEYIN> the key input type to the Mapper
 * @param <VALUEIN> the value input type to the Mapper
 * @param <KEYOUT> the key output type from the Mapper
 * @param <VALUEOUT> the value output type from the Mapper
 */
ExceptionReporter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/ExceptionReporter.java)/**
 * An interface for reporting exceptions to other threads
 */
IFileWrappedMapOutput (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/IFileWrappedMapOutput.java)/**
 * Common code for allowing MapOutput classes to handle streams.
 *
 * @param <K> key type for map output
 * @param <V> value type for map output
 */
InMemoryReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/InMemoryReader.java)/**
 * <code>IFile.InMemoryReader</code> to read map-outputs present in-memory.
 */
LocalFetcher (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/LocalFetcher.java)/**
 * LocalFetcher is used by LocalJobRunner to perform a local filesystem
 * fetch.
 */
MergeManager (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java)/**
 * An interface for a reduce side merge that works with the default Shuffle
 * implementation.
 */
ShuffleClientMetrics (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/ShuffleClientMetrics.java)/**
 * Metric for Shuffle client.
 */
ShuffleHeader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/ShuffleHeader.java)/**
 * Shuffle Header information that is sent by the TaskTracker and 
 * deciphered by the Fetcher thread of Reduce task
 *
 */
Penalty (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/ShuffleSchedulerImpl.java)/**
   * A structure that records the penalty for a host.
   */
Referee (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/ShuffleSchedulerImpl.java)/**
   * A thread that takes hosts off of the penalty list when the timer expires.
   */
ReduceContextImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/ReduceContextImpl.java)/**
 * The context passed to the {@link Reducer}.
 * @param <KEYIN> the class of the input keys
 * @param <VALUEIN> the class of the input values
 * @param <KEYOUT> the class of the output keys
 * @param <VALUEOUT> the class of the output values
 */
TaskAttemptContextImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl.java)/**
 * The context for task attempts.
 */
TaskInputOutputContextImpl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/TaskInputOutputContextImpl.java)/**
 * A context object that allows input and output from the task. It is only
 * supplied to the {@link Mapper} or {@link Reducer}.
 * @param <KEYIN> the input key type for the task
 * @param <VALUEIN> the input value type for the task
 * @param <KEYOUT> the output key type for the task
 * @param <VALUEOUT> the output value type for the task
 */
TaskAttemptContext (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskAttemptContext.java)/**
 * The context for task attempts.
 */
TaskAttemptID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskAttemptID.java)/**
 * TaskAttemptID represents the immutable and unique identifier for 
 * a task attempt. Each task attempt is one particular instance of a Map or
 * Reduce Task identified by its TaskID. 
 * 
 * TaskAttemptID consists of 2 parts. First part is the 
 * {@link TaskID}, that this TaskAttemptID belongs to.
 * Second part is the task attempt number. <br> 
 * An example TaskAttemptID is : 
 * <code>attempt_200707121733_0003_m_000005_0</code> , which represents the
 * zeroth task attempt for the fifth map task in the third job 
 * running at the jobtracker started at <code>200707121733</code>.
 * <p>
 * Applications should never construct or parse TaskAttemptID strings
 * , but rather use appropriate constructors or {@link #forName(String)} 
 * method. 
 * 
 * @see JobID
 * @see TaskID
 */
TaskCompletionEvent (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskCompletionEvent.java)/**
 * This is used to track task completion events on 
 * job tracker. 
 */
CharTaskTypeMaps (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java)/**
   * Maintains the mapping from the character representation of a task type to 
   * the enum class TaskType constants
   */
TaskID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java)/**
 * TaskID represents the immutable and unique identifier for 
 * a Map or Reduce Task. Each TaskID encompasses multiple attempts made to
 * execute the Map or Reduce Task, each of which are uniquely indentified by
 * their TaskAttemptID.
 * 
 * TaskID consists of 3 parts. First part is the {@link JobID}, that this 
 * TaskInProgress belongs to. Second part of the TaskID is either 'm' or 'r' 
 * representing whether the task is a map task or a reduce task. 
 * And the third part is the task number. <br> 
 * An example TaskID is : 
 * <code>task_200707121733_0003_m_000005</code> , which represents the
 * fifth map task in the third job running at the jobtracker 
 * started at <code>200707121733</code>. 
 * <p>
 * Applications should never construct or parse TaskID strings
 * , but rather use appropriate constructors or {@link #forName(String)} 
 * method. 
 * 
 * @see JobID
 * @see TaskAttemptID
 */
TaskInputOutputContext (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskInputOutputContext.java)/**
 * A context object that allows input and output from the task. It is only
 * supplied to the {@link Mapper} or {@link Reducer}.
 * @param <KEYIN> the input key type for the task
 * @param <VALUEIN> the input value type for the task
 * @param <KEYOUT> the output key type for the task
 * @param <VALUEOUT> the output value type for the task
 */
TaskReport (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskReport.java)/** A report on the state of a task. */
TaskTrackerInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskTrackerInfo.java)/**
 * Information about TaskTracker.
 */
CLI (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java)/**
 * Interprets the map reduce cli options 
 */
ConfigUtil (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/util/ConfigUtil.java)/**
 * Place holder for deprecated keys in the framework 
 */
CountersStrings (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/util/CountersStrings.java)/**
 * String conversion utilities for counters.
 * Candidate for deprecation since we start to use JSON in 0.21+
 */
JobHistoryEventUtils (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/util/JobHistoryEventUtils.java)/**
 * Class containing utility methods to be used by JobHistoryEventHandler.
 */
MRJobConfUtil (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/util/MRJobConfUtil.java)/**
 * A class that contains utility methods for MR Job configuration.
 */
SigKillThread (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/util/ProcessTree.java)/**
   * Helper thread class that kills process-tree with SIGKILL in background
   */
ProcessTree (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/util/ProcessTree.java)/** 
 * Process tree related operations
 */
ResourceBundles (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/util/ResourceBundles.java)/**
 * Helper class to handle resource bundles in a saner way
 */
TestClock (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestClock.java)/**
 *  test Clock class
 *
 */
TestCounters (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestCounters.java)/**
 * TestCounters checks the sanity and recoverability of {@code Counters}
 */
CommitterWithFailedThenSucceed (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java)/**
   * The class provides a overrided implementation of commitJobInternal which
   * causes the commit failed for the first time then succeed.
   */
TestJobAclsManager (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestJobAclsManager.java)/**
 * Test the job acls manager
 */
TestJobConf (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestJobConf.java)/**
 * test JobConf
 * 
 */
TestJobInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestJobInfo.java)/**
 * test class JobInfo
 * 
 * 
 */
TestOldMethodsJobID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestOldMethodsJobID.java)/**
 * Test deprecated methods
 *
 */
TestQueue (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestQueue.java)/**
 * TestCounters checks the sanity and recoverability of Queue
 */
TestSkipBadRecords (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestSkipBadRecords.java)/**
 * test SkipBadRecords
 * 
 * 
 */
TestTaskLog (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestTaskLog.java)/**
 * TestCounters checks the sanity and recoverability of Queue
 */
TestDistributedCache (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/filecache/TestDistributedCache.java)/**
 * Test the {@link DistributedCache} class.
 */
DriverForTest (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/db/DriverForTest.java)/**
 * class emulates a connection to database
 * 
 */
TestSplitters (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/db/TestSplitters.java)/**
 * Test Splitters. Splitters should build parts of sql sentences for split result. 
 */
CommitterWithFailedThenSucceed (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestFileOutputCommitter.java)/**
   * The class provides a overrided implementation of commitJobInternal which
   * causes the commit failed for the first time then succeed.
   */
TaskContext (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestPathOutputCommitter.java)/**
   * Stub task context.
   * The {@link #getConfiguration()} method returns the configuration supplied
   * in the constructor; while {@link #setOutputCommitter(OutputCommitter)}
   * sets the committer returned in {@link #getOutputCommitter()}.
   * Otherwise, the methods are all no-ops.
   */
TestPathOutputCommitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestPathOutputCommitter.java)/**
 * Test the path output committer binding to FileOutputFormat.
 */
SimpleCommitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestPathOutputCommitterFactory.java)/**
   * A simple committer.
   */
SimpleCommitterFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestPathOutputCommitterFactory.java)/**
   * The simple committer factory.
   */
OtherFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestPathOutputCommitterFactory.java)/**
   * Some other factory.
   */
TestPathOutputCommitterFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestPathOutputCommitterFactory.java)/**
 * Test the committer factory logic, looking at the override
 * and fallback behavior.
 */
TestJobSplitWriterWithEC (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/split/TestJobSplitWriterWithEC.java)/**
 * Tests that maxBlockLocations default value is sufficient for RS-10-4.
 */
TestFetcher (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestFetcher.java)/**
 * Test that the Fetcher does what we expect it to.
 */
TestShuffleClientMetrics (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestShuffleClientMetrics.java)/**
 * Unit test for {@link TestShuffleClientMetrics}.
 */
TestCluster (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestCluster.java)/**
 * Testing the Cluster initialization.
 */
TestJobMonitorAndPrint (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJobMonitorAndPrint.java)/**
 * Test to make sure that command line output for 
 * job monitoring is correct and prints 100% for map and reduce before 
 * successful completion.
 */
TestJobResourceUploader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJobResourceUploader.java)/**
 * A class for unit testing JobResourceUploader.
 */
TestJobResourceUploaderWithSharedCache (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJobResourceUploaderWithSharedCache.java)/**
 * Tests the JobResourceUploader class with the shared cache.
 */
TestJobSubmissionFiles (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJobSubmissionFiles.java)/**
 * Tests for JobSubmissionFiles Utility class.
 */
TestShufflePlugin (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestShufflePlugin.java)/**
  * A JUnit for testing availability and accessibility of shuffle related API.
  * It is needed for maintaining comptability with external sub-classes of
  * ShuffleConsumerPlugin and AuxiliaryService(s) like ShuffleHandler.
  *
  * The importance of this test is for preserving API with 3rd party plugins.
  */
TestTaskID (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestTaskID.java)/**
 * Test the {@link TaskID} class.
 */
CachedHistoryStorage (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CachedHistoryStorage.java)/**
 * Manages an in memory cache of parsed Job History files.
 */
CompletedJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedJob.java)/**
 * Loads the basic job level data upfront.
 * Data from job history file is loaded lazily.
 */
HistoryClientService (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryClientService.java)/**
 * This module is responsible for talking to the
 * JobClient (user facing).
 *
 */
SerialNumberIndex (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java)/**
   * Maps between a serial number (generated based on jobId) and the timestamp
   * component(s) to which it belongs. Facilitates jobId based searches. If a
   * jobId is not found in this list - it will not be found.
   */
JobIdHistoryFileInfoMap (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java)/**
   * Wrapper around {@link ConcurrentSkipListMap} that maintains size along
   * side for O(1) size() implementation for use in JobListCache.
   *
   * Note: The size is not updated atomically with changes additions/removals.
   * This race can lead to size() returning an incorrect size at times.
   */
UserLogDir (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java)/**
   * This class represents a user dir in the intermediate done directory.  This
   * is mostly for locking purposes. 
   */
HistoryFileManager (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java)/**
 * This class provides a way to interact with history files in a thread safe
 * manor.
 */
HistoryStorage (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryStorage.java)/**
 * Provides an API to query jobs that have finished.
 * 
 * For those implementing this API be aware that there is no feedback when
 * files are removed from HDFS.  You may rely on HistoryFileManager to help
 * you know when that has happened if you have not made a complete backup of
 * the data stored on HDFS.
 */
JHSDelegationTokenSecretManager (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JHSDelegationTokenSecretManager.java)/**
 * A MapReduce specific delegation token secret manager.
 * The secret manager is responsible for generating and accepting the password
 * for each token.
 */
JobHistory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistory.java)/**
 * Loads and manages the Job history cache.
 */
JobHistoryServer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistoryServer.java)/******************************************************************
 * {@link JobHistoryServer} is responsible for servicing all job history
 * related requests from client.
 *
 *****************************************************************/
UnparsedJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/UnparsedJob.java)/**
 * A job that has too many tasks associated with it, of which we do not parse
 * its job history file, to prevent the Job History Server from hanging on
 * parsing the file. It is meant to be used only by JHS to indicate if the
 * history file of a job is fully parsed or not.
 */
HsAboutPage (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsAboutPage.java)/**
 * A Page the shows info about the history server
 */
HsAttemptsPage (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsAttemptsPage.java)/**
 * Render a page showing the attempts made of a given type and a given job.
 */
HsConfPage (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsConfPage.java)/**
 * Render a page with the configuration for a give job in it.
 */
HsController (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsController.java)/**
 * This class renders the various pages that the History Server WebApp supports
 */
HsCountersPage (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsCountersPage.java)/**
 * Render the counters page
 */
HsJobBlock (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsJobBlock.java)/**
 * Render a block of HTML for a give job.
 */
HsJobPage (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsJobPage.java)/**
 * Render a page that describes a specific job.
 */
HsJobsBlock (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsJobsBlock.java)/**
 * Render all of the jobs that the history server is aware of.
 */
HsNavBlock (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsNavBlock.java)/**
 * The navigation block for the history server
 */
HsSingleCounterPage (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsSingleCounterPage.java)/**
 * Render the counters page
 */
AttemptsBlock (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTaskPage.java)/**
   * A Block of HTML that will render a given task attempt. 
   */
HsTaskPage (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTaskPage.java)/**
 * A Page the shows the status of a given task
 */
HsTasksBlock (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTasksBlock.java)/**
 * Render the a table of tasks for a given type.
 */
HsTasksPage (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTasksPage.java)/**
 * A page showing the tasks for a given application.
 */
HsView (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsView.java)/**
 * A view that should be used as the base class for all history server pages.
 */
HistoryServerMemStateStoreService (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/HistoryServerMemStateStoreService.java)/**
 * A state store backed by memory for unit tests
 */
TestHistoryFileManagerInitWithNonRunningDFS (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestHistoryFileManagerInitWithNonRunningDFS.java)/**
 * Test service initialization of HistoryFileManager when
 * HDFS is not running normally (either in start phase or
 * in safe mode).
 */
MRAppWithSpecialHistoryHandler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryEvents.java)/**
   * MRapp with special HistoryEventHandler that writes events only during stop.
   * This is to simulate events that don't get written by the eventHandling
   * thread due to say a slow DFS and verify that they are flushed during stop.
   */
HistoryFileInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestUnnecessaryBlockingOnHistoryFileInfo.java)/**
     * A HistoryFileInfo implementation that takes forever to parse the
     * associated job files. This mimics the behavior of parsing huge job files.
     */
HistoryFileManagerUnderContention (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestUnnecessaryBlockingOnHistoryFileInfo.java)/**
   * A test implementation of HistoryFileManager that does not move files
   * from intermediate directory to done directory and hangs up parsing
   * job history files.
   */
TestUnnecessaryBlockingOnHistoryFileInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestUnnecessaryBlockingOnHistoryFileInfo.java)/**
 * The test in this class is created specifically to address the issue in
 * MAPREDUCE-6684. In cases where there are two threads trying to load different
 * jobs through job history file manager, one thread could be blocked by the
 * other that is loading a huge job file, which is undesirable.
 *
 */
JobHistoryStubWithAllOversizeJobs (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsJobBlock.java)/**
   * A JobHistory stub that treat all jobs as oversized and therefore will
   * not parse their job history files but return a UnparseJob instance.
   */
JobHitoryStubWithAllNormalSizeJobs (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsJobBlock.java)/**
   * A JobHistory stub that treats all jobs as normal size and therefore will
   * return a CompletedJob on HistoryFileInfo.loadJob().
   */
TestHsJobBlock (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsJobBlock.java)/**
 * Test the HsJobBlock generated for oversized jobs in JHS.
 */
TestHsWebServices (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java)/**
 * Test the History Server info web services api's. Also test non-existent urls.
 *
 *  /ws/v1/history
 *  /ws/v1/history/info
 */
TestHsWebServicesAttempts (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesAttempts.java)/**
 * Test the history server Rest API for getting task attempts, a
 * specific task attempt, and task attempt counters
 *
 * /ws/v1/history/mapreduce/jobs/{jobid}/tasks/{taskid}/attempts
 * /ws/v1/history/mapreduce/jobs/{jobid}/tasks/{taskid}/attempts/{attemptid}
 * /ws/v1/history/mapreduce/jobs/{jobid}/tasks/{taskid}/attempts/{attemptid}/
 * counters
 */
TestHsWebServicesJobConf (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobConf.java)/**
 * Test the history server Rest API for getting the job conf. This
 * requires created a temporary configuration file.
 *
 *   /ws/v1/history/mapreduce/jobs/{jobid}/conf
 */
TestHsWebServicesJobs (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobs.java)/**
 * Test the history server Rest API for getting jobs, a specific job, job
 * counters, and job attempts.
 *
 * /ws/v1/history/mapreduce/jobs /ws/v1/history/mapreduce/jobs/{jobid}
 * /ws/v1/history/mapreduce/jobs/{jobid}/counters
 * /ws/v1/history/mapreduce/jobs/{jobid}/jobattempts
 */
TestHsWebServicesJobsQuery (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobsQuery.java)/**
 * Test the history server Rest API for getting jobs with various query
 * parameters.
 *
 * /ws/v1/history/mapreduce/jobs?{query=value}
 */
TestHsWebServicesTasks (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesTasks.java)/**
 * Test the history server Rest API for getting tasks, a specific task,
 * and task counters.
 *
 * /ws/v1/history/mapreduce/jobs/{jobid}/tasks
 * /ws/v1/history/mapreduce/jobs/{jobid}/tasks/{taskid}
 * /ws/v1/history/mapreduce/jobs/{jobid}/tasks/{taskid}/counters
 */
YARNRunner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java)/**
 * This class enables the current JobClient (0.22 hadoop) to run on YARN.
 */
TestNoDefaultsJobConf (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/conf/TestNoDefaultsJobConf.java)/**
 * This testcase tests that a JobConf without default values submits jobs
 * properly and the JT applies its own default values to it to make the job
 * run properly.
 */
FailingMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/FailingMapper.java)/**
 * Fails the Mapper. First attempt throws exception. Rest do System.exit.
 *
 */
FiConfig (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fi/FiConfig.java)/**
 * This class wraps the logic around fault injection configuration file
 * Default file is expected to be found in src/test/fi-site.xml
 * This default file should be copied by JUnit Ant's tasks to 
 * build/test/extraconf folder before tests are ran
 * An alternative location can be set through
 *   -Dfi.config=<file_name>
 */
ProbabilityModel (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fi/ProbabilityModel.java)/**
 * This class is responsible for the decision of when a fault 
 * has to be triggered within a class of Hadoop
 * 
 *  Default probability of injection is set to 0%. To change it
 *  one can set the sys. prop. -Dfi.*=<new probability level>
 *  Another way to do so is to set this level through FI config file,
 *  located under src/test/fi-site.conf
 *  
 *  To change the level one has to specify the following sys,prop.:
 *  -Dfi.<name of fault location>=<probability level> in the runtime
 *  Probability level is specified by a float between 0.0 and 1.0
 *  
 *  <name of fault location> might be represented by a short classname
 *  or otherwise. This decision is left up to the discretion of aspects
 *  developer, but has to be consistent through the code 
 */
AccumulatingReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/AccumulatingReducer.java)/**
 * Reducer that accumulates values based on their type.
 * <p>
 * The type is specified in the key part of the key-value pair 
 * as a prefix to the key in the following way
 * <p>
 * <tt>type:key</tt>
 * <p>
 * The values are accumulated according to the types:
 * <ul>
 * <li><tt>s:</tt> - string, concatenate</li>
 * <li><tt>f:</tt> - float, summ</li>
 * <li><tt>l:</tt> - long, summ</li>
 * </ul>
 * 
 */
IOStatMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/DFSCIOTest.java)/**
   * Write/Read mapper base class.
   * <p>
   * Collects the following statistics per task:
   * <ul>
   * <li>number of tasks completed</li>
   * <li>number of bytes written/read</li>
   * <li>execution time</li>
   * <li>i/o rate</li>
   * <li>i/o rate squared</li>
   * </ul>
   */
WriteMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/DFSCIOTest.java)/**
   * Write mapper class.
   */
ReadMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/DFSCIOTest.java)/**
   * Read mapper class.
   */
DFSCIOTest (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/DFSCIOTest.java)/**
 * Distributed i/o benchmark.
 * <p>
 * This test writes into or reads from a specified number of files.
 * File size is specified as a parameter to the test. 
 * Each file is accessed in a separate map task.
 * <p>
 * The reducer collects the following statistics:
 * <ul>
 * <li>number of tasks completed</li>
 * <li>number of bytes written/read</li>
 * <li>execution time</li>
 * <li>io rate</li>
 * <li>io rate squared</li>
 * </ul>
 *    
 * Finally, the following information is appended to a local file
 * <ul>
 * <li>read or write test</li>
 * <li>date and time the test finished</li>   
 * <li>number of files</li>
 * <li>total number of bytes processed</li>
 * <li>throughput in mb/sec (total number of bytes / sum of processing times)</li>
 * <li>average i/o rate in mb/sec per file</li>
 * <li>standard i/o rate deviation</li>
 * </ul>
 */
DistributedFSCheckMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/DistributedFSCheck.java)/**
   * DistributedFSCheck mapper class.
   */
DistributedFSCheck (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/DistributedFSCheck.java)/**
 * Distributed checkup of the file system consistency.
 * <p>
 * Test file system consistency by reading each block of each file
 * of the specified file tree. 
 * Report corrupted blocks and general file statistics.
 * <p>
 * Optionally displays statistics on read performance.
 * 
 */
IOMapperBase (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/IOMapperBase.java)/**
 * Base mapper class for IO operations.
 * <p>
 * Two abstract method {@link #doIO(Reporter, String, long)} and 
 * {@link #collectStats(OutputCollector,String,long,Object)} should be
 * overloaded in derived classes to define the IO operation and the
 * statistics data to be collected by subsequent reducers.
 * 
 */
JobHistoryLog (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/JHLogAnalyzer.java)/**
   * JobHistory log record.
   */
TaskHistoryLog (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/JHLogAnalyzer.java)/**
   * TaskHistory log record.
   */
TaskAttemptHistoryLog (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/JHLogAnalyzer.java)/**
   * TaskAttemptHistory log record.
   */
IntervalKey (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/JHLogAnalyzer.java)/**
   * Key = statName*date-time*taskType
   * Value = number of msec for the our
   */
JHLAMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/JHLogAnalyzer.java)/**
   * Mapper class.
   */
JHLogAnalyzer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/JHLogAnalyzer.java)/**
 * Job History Log Analyzer.
 * 
 * <h3>Description.</h3>
 * This a tool for parsing and analyzing history logs of map-reduce jobs.
 * History logs contain information about execution of jobs, tasks, and 
 * attempts. This tool focuses on submission, launch, start, and finish times,
 * as well as the success or failure of jobs, tasks, and attempts.
 * <p>
 * The analyzer calculates <em>per hour slot utilization</em> for the cluster 
 * as follows.
 * For each task attempt it divides the time segment from the start of the 
 * attempt t<sub>S</sub> to the finish t<sub>F</sub> into whole hours 
 * [t<sub>0</sub>, ..., t<sub>n</sub>], where t<sub>0</sub> <= t<sub>S</sub> 
 * is the maximal whole hour preceding t<sub>S</sub>, and
 * t<sub>n</sub> >= t<sub>F</sub> is the minimal whole hour after t<sub>F</sub>. 
 * Thus, [t<sub>0</sub>, ..., t<sub>n</sub>] covers the segment 
 * [t<sub>S</sub>, t<sub>F</sub>], during which the attempt was executed.
 * Each interval [t<sub>i</sub>, t<sub>i+1</sub>] fully contained in 
 * [t<sub>S</sub>, t<sub>F</sub>] corresponds to exactly one slot on
 * a map-reduce cluster (usually MAP-slot or REDUCE-slot).
 * If interval [t<sub>i</sub>, t<sub>i+1</sub>] only intersects with 
 * [t<sub>S</sub>, t<sub>F</sub>] then we say that the task 
 * attempt used just a fraction of the slot during this hour.
 * The fraction equals the size of the intersection.
 * Let slotTime(A, h) denote the number of slots calculated that way for a 
 * specific attempt A during hour h.
 * The tool then sums all slots for all attempts for every hour.
 * The result is the slot hour utilization of the cluster:
 * <tt>slotTime(h) = SUM<sub>A</sub> slotTime(A,h)</tt>.
 * <p>
 * Log analyzer calculates slot hours for <em>MAP</em> and <em>REDUCE</em> 
 * attempts separately.
 * <p>
 * Log analyzer distinguishes between <em>successful</em> and <em>failed</em>
 * attempts. Task attempt is considered successful if its own status is SUCCESS
 * and the statuses of the task and the job it is a part of are also SUCCESS.
 * Otherwise the task attempt is considered failed.
 * <p>
 * Map-reduce clusters are usually configured to have a fixed number of MAP 
 * and REDUCE slots per node. Thus the maximal possible number of slots on
 * the cluster is <tt>total_slots = total_nodes * slots_per_node</tt>.
 * Effective slot hour cannot exceed <tt>total_slots</tt> for successful
 * attempts.
 * <p>
 * <em>Pending time</em> characterizes the wait time of attempts.
 * It is calculated similarly to the slot hour except that the wait interval
 * starts when the job is submitted and ends when an attempt starts execution.
 * In addition to that pending time also includes intervals between attempts
 * of the same task if it was re-executed.
 * <p>
 * History log analyzer calculates two pending time variations. First is based
 * on job submission time as described above, second, starts the wait interval
 * when the job is launched rather than submitted.
 * 
 * <h3>Input.</h3>
 * The following input parameters can be specified in the argument string
 * to the job log analyzer:
 * <ul>
 * <li><tt>-historyDir inputDir</tt> specifies the location of the directory
 * where analyzer will be looking for job history log files.</li>
 * <li><tt>-resFile resultFile</tt> the name of the result file.</li>
 * <li><tt>-usersIncluded | -usersExcluded userList</tt> slot utilization and 
 * pending time can be calculated for all or for all but the specified users.
 * <br>
 * <tt>userList</tt> is a comma or semicolon separated list of users.</li>
 * <li><tt>-gzip</tt> is used if history log files are compressed.
 * Only {@link GzipCodec} is currently supported.</li>
 * <li><tt>-jobDelimiter pattern</tt> one can concatenate original log files into 
 * larger file(s) with the specified delimiter to recognize the end of the log
 * for one job from the next one.<br>
 * <tt>pattern</tt> is a java regular expression
 * {@link java.util.regex.Pattern}, which should match only the log delimiters.
 * <br>
 * E.g. pattern <tt>".!!FILE=.*!!"</tt> matches delimiters, which contain
 * the original history log file names in the following form:<br>
 * <tt>"$!!FILE=my.job.tracker.com_myJobId_user_wordcount.log!!"</tt></li>
 * <li><tt>-clean</tt> cleans up default directories used by the analyzer.</li>
 * <li><tt>-test</tt> test one file locally and exit;
 * does not require map-reduce.</li>
 * <li><tt>-help</tt> print usage.</li>
 * </ul>
 * 
 * <h3>Output.</h3>
 * The output file is formatted as a tab separated table consisting of four
 * columns: <tt>SERIES, PERIOD, TYPE, SLOT_HOUR</tt>.
 * <ul>
 * <li><tt>SERIES</tt> one of the four statistical series;</li>
 * <li><tt>PERIOD</tt> the start of the time interval in the following format:
 * <tt>"yyyy-mm-dd hh:mm:ss"</tt>;</li>
 * <li><tt>TYPE</tt> the slot type, e.g. MAP or REDUCE;</li>
 * <li><tt>SLOT_HOUR</tt> the value of the slot usage during this 
 * time interval.</li>
 * </ul>
 */
LoadGeneratorMR (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/loadGenerator/LoadGeneratorMR.java)/** The load generator is a tool for testing NameNode behavior under
 * different client loads.
 * The main code is in HadoopCommon, @LoadGenerator. This class, LoadGeneratorMR
 * lets you run that LoadGenerator as a MapReduce job.
 * 
 * The synopsis of the command is
 * java LoadGeneratorMR
 *   -mr <numMapJobs> <outputDir> : results in outputDir/Results
 *   the rest of the args are the same as the original LoadGenerator.
 *
 */
AppendOp (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/AppendOp.java)/**
 * Operation which selects a random file and appends a random amount of bytes
 * (selected from the configuration for append size) to that file if it exists.
 * 
 * This operation will capture statistics on success for bytes written, time
 * taken (milliseconds), and success count and on failure it will capture the
 * number of failures and the time taken (milliseconds) to fail.
 */
ParsedOutput (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ArgumentParser.java)/**
   * Result of a parse is the following object
   */
ArgumentParser (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ArgumentParser.java)/**
 * Class which abstracts the parsing of command line arguments for slive test
 */
BadFileException (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/BadFileException.java)/**
 * Exception used to signify file reading failures where headers are bad or an
 * unexpected EOF occurs when it should not.
 */
ConfigExtractor (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigExtractor.java)/**
 * Simple access layer onto of a configuration object that extracts the slive
 * specific configuration values needed for slive running
 */
ConfigException (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigMerger.java)/**
   * Exception that represents config problems...
   */
ConfigMerger (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigMerger.java)/**
 * Class which merges options given from a config file and the command line and
 * performs some basic verification of the data retrieved and sets the verified
 * values back into the configuration object for return
 */
ConfigOption (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ConfigOption.java)/**
 * Class which extends the basic option object and adds in the configuration id
 * and a default value so a central place can be used for retrieval of these as
 * needed
 */
Constants (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Constants.java)/**
 * Constants used in various places in slive
 */
CreateOp (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/CreateOp.java)/**
 * Operation which selects a random file and a random number of bytes to create
 * that file with (from the write size option) and selects a random block size
 * (from the block size option) and a random replication amount (from the
 * replication option) and attempts to create a file with those options.
 * 
 * This operation will capture statistics on success for bytes written, time
 * taken (milliseconds), and success count and on failure it will capture the
 * number of failures and the time taken (milliseconds) to fail.
 */
DataHasher (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/DataHasher.java)/**
 * Class which is used to create the data to write for a given path and offset
 * into that file for writing and later verification that the expected value is
 * read at that file bytes offset
 */
VerifyOutput (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/DataVerifier.java)/**
   * The output from verification includes the number of chunks that were the
   * same as expected and the number of segments that were different than what
   * was expected and the number of total bytes read
   */
ReadInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/DataVerifier.java)/**
   * Class used to hold the result of a read on a header
   */
VerifyInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/DataVerifier.java)/**
   * Storage class used to hold the chunks same and different for buffered reads
   * and the resultant verification
   */
DataVerifier (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/DataVerifier.java)/**
 * Class which reads in and verifies bytes that have been read in
 */
GenerateOutput (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/DataWriter.java)/**
   * Class used to hold the number of bytes written and time taken for write
   * operations for callers to use
   */
GenerateResult (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/DataWriter.java)/**
   * Class used to hold a byte buffer and offset position for generating data
   */
WriteInfo (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/DataWriter.java)/**
   * What a header write output returns need the hash value to use and the time
   * taken to perform the write + bytes written
   */
DataWriter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/DataWriter.java)/**
 * Class which handles generating data (creating and appending) along with
 * ensuring the correct data is written out for the given path name so that it
 * can be later verified
 */
DeleteOp (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/DeleteOp.java)/**
 * Operation which selects a random file and attempts to delete that file (if it
 * exists)
 * 
 * This operation will capture statistics on success the time taken to delete
 * and the number of successful deletions that occurred and on failure or error
 * it will capture the number of failures and the amount of time taken to fail
 */
DummyInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/DummyInputFormat.java)/**
 * A input format which returns one dummy key and value
 */
Formatter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Formatter.java)/**
 * Simple class that holds the number formatters used in the slive application
 */
Helper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Helper.java)/**
 * Simple slive helper methods (may not exist in 0.20)
 */
ListOp (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ListOp.java)/**
 * Operation which selects a random directory and attempts to list that
 * directory (if it exists)
 * 
 * This operation will capture statistics on success the time taken to list that
 * directory and the number of successful listings that occurred as well as the
 * number of entries in the selected directory and on failure or error it will
 * capture the number of failures and the amount of time taken to fail
 */
MkdirOp (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/MkdirOp.java)/**
 * Operation which selects a random directory and attempts to create that
 * directory.
 * 
 * This operation will capture statistics on success the time taken to create
 * that directory and the number of successful creations that occurred and on
 * failure or error it will capture the number of failures and the amount of
 * time taken to fail
 */
Observer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ObserveableOp.java)/**
   * The observation interface which class that wish to monitor starting and
   * ending events must implement.
   */
ObserveableOp (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ObserveableOp.java)/**
 * Operation which wraps a given operation and allows an observer to be notified
 * when the operation is about to start and when the operation has finished
 */
Operation (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Operation.java)/**
 * An operation provides these abstractions and if it desires to perform any
 * operations it must implement a override of the run() function to provide
 * varying output to be captured.
 */
OperationData (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/OperationData.java)/**
 * This class holds the data representing what an operations distribution and
 * its percentage is (between 0 and 1) and provides operations to access those
 * types and parse and unparse from and into strings
 */
OperationFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/OperationFactory.java)/**
 * Factory class which returns instances of operations given there operation
 * type enumeration (in string or enumeration format).
 */
OperationOutput (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/OperationOutput.java)/**
 * An operation output has the following object format whereby simple types are
 * represented as a key of dataType:operationType*measurementType and these
 * simple types can be combined (mainly in the reducer) using there given types
 * into a single operation output.
 * 
 * Combination is done based on the data types and the following convention is
 * followed (in the following order). If one is a string then the other will be
 * concated as a string with a ";" separator. If one is a double then the other
 * will be added as a double and the output will be a double. If one is a float
 * then the other will be added as a float and the the output will be a float.
 * Following this if one is a long the other will be added as a long and the
 * output type will be a long and if one is a integer the other will be added as
 * a integer and the output type will be an integer.
 */
OperationWeight (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/OperationWeight.java)/**
 * Class which holds an operation and its weight (used in operation selection)
 */
PathFinder (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/PathFinder.java)/**
 * Class which generates a file or directory path using a simple random
 * generation algorithm stated in http://issues.apache.org/jira/browse/HDFS-708
 */
Range (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Range.java)/**
 * Class that represents a numeric minimum and a maximum
 * 
 * @param <T>
 *          the type of number being used
 */
ReadOp (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ReadOp.java)/**
 * Operation which selects a random file and selects a random read size (from
 * the read size option) and reads from the start of that file to the read size
 * (or the full file) and verifies the bytes that were written there.
 * 
 * This operation will capture statistics on success the time taken to read that
 * file and the number of successful readings that occurred as well as the
 * number of bytes read and the number of chunks verified and the number of
 * chunks which failed verification and on failure or error it will capture the
 * number of failures and the amount of time taken to fail
 */
SrcTarget (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/RenameOp.java)/**
   * Class that holds the src and target for renames
   */
RenameOp (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/RenameOp.java)/**
 * Operation which selects a random file and a second random file and attempts
 * to rename that first file into the second file.
 * 
 * This operation will capture statistics on success the time taken to rename
 * those files and the number of successful renames that occurred and on failure
 * or error it will capture the number of failures and the amount of time taken
 * to fail
 */
ReportWriter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/ReportWriter.java)/**
 * Class which provides a report for the given operation output
 */
RouletteSelector (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/RouletteSelector.java)/**
 * A selection object which simulates a roulette wheel whereby all operation
 * have a weight and the total value of the wheel is the combined weight and
 * during selection a random number (0, total weight) is selected and then the
 * operation that is at that value will be selected. So for a set of operations
 * with uniform weight they will all have the same probability of being
 * selected. Operations which choose to have higher weights will have higher
 * likelihood of being selected (and the same goes for lower weights).
 */
SleepOp (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/SleepOp.java)/**
 * Operation which sleeps for a given number of milliseconds according to the
 * config given, and reports on the sleep time overall
 */
SliveMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/SliveMapper.java)/**
 * The slive class which sets up the mapper to be used which itself will receive
 * a single dummy key and value and then in a loop run the various operations
 * that have been selected and upon operation completion output the collected
 * output from that operation (and repeat until finished).
 */
SlivePartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/SlivePartitioner.java)/**
 * The partitioner partitions the map output according to the operation type.
 * The partition number is the hash of the operation type modular the total
 * number of the reducers.
 */
SliveReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/SliveReducer.java)/**
 * The slive reducer which iterates over the given input values and merges them
 * together into a final output value.
 */
SliveTest (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/SliveTest.java)/**
 * Slive test entry point + main program
 * 
 * This program will output a help message given -help which can be used to
 * determine the program options and configuration which will affect the program
 * runtime. The program will take these options, either from configuration or
 * command line and process them (and merge) and then establish a job which will
 * thereafter run a set of mappers & reducers and then the output of the
 * reduction will be reported on.
 * 
 * The number of maps is specified by "slive.maps".
 * The number of reduces is specified by "slive.reduces".
 */
TestSlive (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TestSlive.java)/**
 * Junit 4 test for slive
 */
Timer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Timer.java)/**
 * Simple timer class that abstracts time access
 */
TruncateOp (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/TruncateOp.java)/**
 * Operation which selects a random file and truncates a random amount of bytes
 * (selected from the configuration for truncate size) from that file,
 * if it exists.
 * 
 * This operation will capture statistics on success for bytes written, time
 * taken (milliseconds), and success count and on failure it will capture the
 * number of failures and the time taken (milliseconds) to fail.
 */
UniformWeight (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Weights.java)/**
   * A weight which always returns the same weight (1/3). Which will have an
   * overall area of (1/3) unless otherwise provided.
   */
MidWeight (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Weights.java)/**
   * A weight which normalized the elapsed time and the duration to a value
   * between 0 and 1 and applies the algorithm to form an output using the
   * function (-2 * (x-0.5)^2) + 0.5 which initially (close to 0) has a value
   * close to 0 and near input being 1 has a value close to 0 and near 0.5 has a
   * value close to 0.5 (with overall area 0.3).
   */
EndWeight (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Weights.java)/**
   * A weight which normalized the elapsed time and the duration to a value
   * between 0 and 1 and applies the algorithm to form an output using the
   * function (x)^2 which initially (close to 0) has a value close to 0 and near
   * input being 1 has a value close to 1 (with overall area 1/3).
   */
BeginWeight (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Weights.java)/**
   * A weight which normalized the elapsed time and the duration to a value
   * between 0 and 1 and applies the algorithm to form an output using the
   * function (x-1)^2 which initially (close to 0) has a value close to 1 and
   * near input being 1 has a value close to 0 (with overall area 1/3).
   */
Weights (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/Weights.java)/**
 * Class to isolate the various weight algorithms we use.
 */
WeightSelector (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/slive/WeightSelector.java)/**
 * This class is the main handler that selects operations to run using the
 * currently held selection object. It configures and weights each operation and
 * then hands the operations + weights off to the selector object to determine
 * which one should be ran. If no operations are left to be ran then it will
 * return null.
 */
IOStatMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java)/**
   * Write/Read mapper base class.
   * <p>
   * Collects the following statistics per task:
   * <ul>
   * <li>number of tasks completed</li>
   * <li>number of bytes written/read</li>
   * <li>execution time</li>
   * <li>i/o rate</li>
   * <li>i/o rate squared</li>
   * </ul>
   */
WriteMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java)/**
   * Write mapper class.
   */
AppendMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java)/**
   * Append mapper class.
   */
ReadMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java)/**
   * Read mapper class.
   */
RandomReadMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java)/**
   * Mapper class for random reads.
   * The mapper chooses a position in the file and reads bufferSize
   * bytes starting at the chosen position.
   * It stops after reading the totalSize bytes, specified by -size.
   * 
   * There are three type of reads.
   * 1) Random read always chooses a random position to read from: skipSize = 0
   * 2) Backward read reads file in reverse order                : skipSize < 0
   * 3) Skip-read skips skipSize bytes after every read          : skipSize > 0
   */
TruncateMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java)/**
   * Truncate mapper class.
   * The mapper truncates given file to the newLength, specified by -size.
   */
TestDFSIO (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java)/**
 * Distributed i/o benchmark.
 * <p>
 * This test writes into or reads from a specified number of files.
 * Number of bytes to write or read is specified as a parameter to the test. 
 * Each file is accessed in a separate map task.
 * <p>
 * The reducer collects the following statistics:
 * <ul>
 * <li>number of tasks completed</li>
 * <li>number of bytes written/read</li>
 * <li>execution time</li>
 * <li>io rate</li>
 * <li>io rate squared</li>
 * </ul>
 *    
 * Finally, the following information is appended to a local file
 * <ul>
 * <li>read or write test</li>
 * <li>date and time the test finished</li>   
 * <li>number of files</li>
 * <li>total number of bytes processed</li>
 * <li>throughput in mb/sec (total number of bytes / sum of processing times)</li>
 * <li>average i/o rate in mb/sec per file</li>
 * <li>standard deviation of i/o rate </li>
 * </ul>
 */
TestJHLA (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestJHLA.java)/**
 * Test Job History Log Analyzer.
 *
 * @see JHLogAnalyzer
 */
NNBenchMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/hdfs/NNBench.java)/**
   * Mapper class
   */
NNBenchReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/hdfs/NNBench.java)/**
   * Reducer class
   */
NNBenchWithoutMR (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/hdfs/NNBenchWithoutMR.java)/**
 * This program executes a specified operation that applies load to 
 * the NameNode. Possible operations include create/writing files,
 * opening/reading files, renaming files, and deleting files.
 * 
 * When run simultaneously on multiple nodes, this program functions 
 * as a stress-test and benchmark for namenode, especially when 
 * the number of bytes written to each file is small.
 * 
 * This version does not use the map reduce framework
 * 
 */
TestMRCJCSocketFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/ipc/TestMRCJCSocketFactory.java)/**
 * This class checks that RPCs can use specialized socket factories.
 */
DummySocketFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/ipc/TestMRCJCSocketFactory.java)/**
 * Dummy socket factory which shift TPC ports by subtracting 10 when
 * establishing a connection
 */
ClusterMapReduceTestCase (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/ClusterMapReduceTestCase.java)/**
 * Test case to run a MapReduce job.
 * <p/>
 * It runs a 2 node cluster Hadoop with a 2 node DFS.
 * <p/>
 * The JobConf to use must be obtained via the creatJobConf() method.
 * <p/>
 * It creates a temporary directory -accessible via getTestRootDir()-
 * for both input and output.
 * <p/>
 * The input directory is accesible via getInputDir() and the output
 * directory via getOutputDir()
 * <p/>
 * The DFS filesystem is formated before the testcase starts and after it ends.
 */
EmptyInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/EmptyInputFormat.java)/**
  * InputFormat which simulates the absence of input data
  * by returning zero split.
  */
IndirectInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/GenericMRLoadGenerator.java)/**
   * Obscures the InputFormat and location information to simulate maps
   * reading input from arbitrary locations (&quot;indirect&quot; reads).
   */
HadoopTestCase (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/HadoopTestCase.java)/**
 * Abstract Test case class to run MR in local or cluster mode and in local FS
 * or DFS.
 *
 * The Hadoop instance is started and stopped on each test method.
 *
 * If using DFS the filesystem is reformated at each start (test method).
 *
 * Job Configurations should be created using a configuration returned by the
 * 'createJobConf()' method.
 */
DataCopy (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/jobcontrol/JobControlTestUtils.java)/**
   * Simple Mapper and Reducer implementation which copies data it reads in.
   */
JobControlTestUtils (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/jobcontrol/JobControlTestUtils.java)/**
 * Utility methods used in various Job Control unit tests.
 */
TestJobControl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/jobcontrol/TestJobControl.java)/**
 * This class performs unit test for Job/JobControl classes.
 *  
 */
TestLocalJobControl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/jobcontrol/TestLocalJobControl.java)/**
 * HadoopTestCase that tests the local job runner.
 */
PreVersion21TupleWritable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/join/TestTupleWritable.java)/**
   * Writes to the DataOutput stream in the same way as pre-0.21 versions of
   * {@link TupleWritable#write(DataOutput)}
   */
TestMultipleInputs (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/lib/TestMultipleInputs.java)/**
 * @see TestDelegatingInputFormat
 */
MiniMRClientClusterFactory (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/MiniMRClientClusterFactory.java)/**
 * A MiniMRCluster factory. In MR2, it provides a wrapper MiniMRClientCluster
 * interface around the MiniMRYarnCluster. While in MR1, it provides such
 * wrapper around MiniMRCluster. This factory should be used in tests to provide
 * an easy migration of tests across MR1 and MR2.
 */
MiniMRCluster (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/MiniMRCluster.java)/**
 * This class is an MR2 replacement for older MR1 MiniMRCluster, that was used
 * by tests prior to MR2. This replacement class uses the new MiniMRYarnCluster
 * in MR2 but provides the same old MR1 interface, so tests can be migrated from
 * MR1 to MR2 with minimal changes.
 *
 * Due to major differences between MR1 and MR2, a number of methods are either
 * unimplemented/unsupported or were re-implemented to provide wrappers around
 * MR2 functionality.
 *
 * @deprecated Use {@link org.apache.hadoop.mapred.MiniMRClientClusterFactory}
 * instead
 */
MiniMRYarnClusterAdapter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/MiniMRYarnClusterAdapter.java)/**
 * An adapter for MiniMRYarnCluster providing a MiniMRClientCluster interface.
 * This interface could be used by tests across both MR1 and MR2.
 */
Map (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/MRBench.java)/**
   * Takes input format as text lines, runs some processing on it and 
   * writes out data as text again. 
   */
Reduce (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/MRBench.java)/**
   * Ignores the key and writes values to the output. 
   */
MRBench (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/MRBench.java)/**
 * Runs a job multiple times and takes average of all runs.
 */
MapClass (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/MRCaching.java)/**
   * Using the wordcount example and adding caching to it. The cache
   * archives/files are set and then are checked in the map if they have been
   * localized or not.
   */
MapClass2 (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/MRCaching.java)/**
   * Using the wordcount example and adding caching to it. The cache
   * archives/files are set and then are checked in the map if they have been
   * symlinked or not.
   */
ReduceClass (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/MRCaching.java)/**
   * A reducer class that just emits the sum of the input values.
   */
NotificationTestCase (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/NotificationTestCase.java)/**
 * Base class to test Job end notification in local and cluster mode.
 *
 * Starts up hadoop on Local or Cluster mode (by extending of the
 * HadoopTestCase class) and it starts a servlet engine that hosts
 * a servlet that will receive the notification of job finalization.
 *
 * The notification servlet returns a HTTP 400 the first time is called
 * and a HTTP 200 the second time, thus testing retry.
 *
 * In both cases local file system is used (this is irrelevant for
 * the tested functionality)
 *
 * 
 */
WordCountInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/pipes/WordCountInputFormat.java)/**
 * This is a support class to test Hadoop Pipes when using C++ RecordReaders.
 * It defines an InputFormat with InputSplits that are just strings. The
 * RecordReaders are not implemented in Java, naturally...
 */
Raw (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/SortValidator.java)/**
     * Generic way to get <b>raw</b> data from a {@link Writable}.
     */
RawBytesWritable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/SortValidator.java)/**
     * Specialization of {@link Raw} for {@link BytesWritable}.
     */
RawText (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/SortValidator.java)/**
     * Specialization of {@link Raw} for {@link Text}.
     */
RecordStatsChecker (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/SortValidator.java)/**
   * A simple map-reduce job which checks consistency of the
   * MapReduce framework's sort by checking:
   * a) Records are sorted correctly
   * b) Keys are partitioned correctly
   * c) The input and output have same no. of bytes and records.
   * d) The input and output have the correct 'checksum' by xor'ing 
   *    the md5 of each record.
   *    
   */
RecordChecker (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/SortValidator.java)/**
   * A simple map-reduce task to check if the input and the output
   * of the framework's sort is consistent by ensuring each record 
   * is present in both the input and the output.
   * 
   */
SortValidator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/SortValidator.java)/**
 * A set of utilities to validate the <b>sort</b> of the map-reduce framework.
 * This utility program has 2 main parts:
 * 1. Checking the records' statistics
 *   a) Validates the no. of bytes and records in sort's input & output. 
 *   b) Validates the xor of the md5's of each key/value pair.
 *   c) Ensures same key/value is present in both input and output.
 * 2. Check individual records  to ensure each record is present in both
 *    the input and the output of the sort (expensive on large data-sets). 
 *    
 * To run: bin/hadoop jar build/hadoop-examples.jar sortvalidate
 *            [-m <i>maps</i>] [-r <i>reduces</i>] [-deep] 
 *            -sortInput <i>sort-in-dir</i> -sortOutput <i>sort-out-dir</i> 
 */
TestClusterMRNotification (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestClusterMRNotification.java)/**
 * Tests Job end notification in cluster mode.
 */
TestCollect (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestCollect.java)/** 
 * TestCollect checks if the collect can handle simultaneous invocations.
 */
TestCommandLineJobSubmission (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestCommandLineJobSubmission.java)/**
 * check for the job submission  options of 
 * -libjars -files -archives
 */
RandomGenMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestComparators.java)/** 
   * RandomGen is a mapper that generates 5 random values for each key
   * in the input. The values are in the range [0-4]. The mapper also
   * generates a composite key. If the input key is x and the generated
   * value is y, the composite key is x0y (x-zero-y). Therefore, the inter-
   * mediate key value pairs are ordered by {input key, value}.
   * Think of the random value as a timestamp associated with the record. 
   */
IdentityMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestComparators.java)/** 
   * Your basic identity mapper. 
   */
AscendingKeysReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestComparators.java)/** 
   * Checks whether keys are in ascending order.  
   */
DescendingKeysReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestComparators.java)/** 
   * Checks whether keys are in ascending order.  
   */
AscendingGroupReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestComparators.java)/** The reducer checks whether the input values are in ascending order and
   * whether they are correctly grouped by key (i.e. each call to reduce
   * should have 5 values if the grouping is correct). It also checks whether
   * the keys themselves are in ascending order.
   */
DescendingGroupReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestComparators.java)/** The reducer checks whether the input values are in descending order and
   * whether they are correctly grouped by key (i.e. each call to reduce
   * should have 5 values if the grouping is correct). 
   */
DecreasingIntComparator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestComparators.java)/** 
   * A decreasing Comparator for IntWritable 
   */
CompositeIntGroupFn (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestComparators.java)/** Grouping function for values based on the composite key. This
   * comparator strips off the secondary key part from the x0y composite
   * and only compares the primary key value (x).
   */
CompositeIntReverseGroupFn (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestComparators.java)/** Reverse grouping function for values based on the composite key. 
   */
TestComparators (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestComparators.java)/**
 * Two different types of comparators can be used in MapReduce. One is used
 * during the Map and Reduce phases, to sort/merge key-value pairs. Another
 * is used to group values for a particular key, when calling the user's 
 * reducer. A user can override both of these two. 
 * This class has tests for making sure we use the right comparators at the 
 * right places. See Hadoop issues 485 and 1535. Our tests: 
 * 1. Test that the same comparator is used for all sort/merge operations 
 * during the Map and Reduce phases.  
 * 2. Test the common use case where values are grouped by keys but values 
 * within each key are grouped by a secondary key (a timestamp, for example). 
 */
TestConcatenatedCompressedInput (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestConcatenatedCompressedInput.java)/**
 * Test class for concatenated {@link CompressionInputStream}.
 */
CommitterWithCustomDeprecatedCleanup (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCleanup.java)/**
   * Committer with deprecated
   * {@link FileOutputCommitter#cleanupJob(JobContext)} making a _failed/_killed
   * in the output folder
   */
CommitterWithCustomAbort (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCleanup.java)/**
   * Committer with abort making a _failed/_killed in the output folder
   */
TestJobCleanup (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCleanup.java)/**
 * A JUnit test to test Map-Reduce job cleanup.
 */
MemoryLoader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCounters.java)/** 
   * Increases the JVM's heap usage to the specified target value.
   */
MemoryLoaderMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCounters.java)/**
   * A mapper that increases the JVM's heap usage to a target value configured 
   * via {@link MemoryLoaderMapper#TARGET_VALUE} using a {@link MemoryLoader}.
   */
MemoryLoaderReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCounters.java)/** 
   * A reducer that increases the JVM's heap usage to a target value configured 
   * via {@link MemoryLoaderReducer#TARGET_VALUE} using a {@link MemoryLoader}.
   */
TokenizerMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCounters.java)/**
   * Test mapper.
   */
IntSumReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCounters.java)/**
   * Test reducer.
   */
MockResourceCalculatorProcessTree (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCounters.java)/**
   * Mock resource reporting.
   */
TestJobCounters (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCounters.java)/**
 * This is an wordcount application that tests the count of records
 * got spilled to disk. It generates simple text input files. Then
 * runs the wordcount map/reduce application on (1) 3 i/p files(with 3 maps
 * and 1 reduce) and verifies the counters and (2) 4 i/p files(with 4 maps
 * and 1 reduce) and verifies counters. Wordcount application reads the
 * text input files, breaks each line into words and counts them. The output
 * is a locally sorted list of words and the count of how often they occurred.
 *
 */
TestJobSysDirWithDFS (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobSysDirWithDFS.java)/**
 * A JUnit test to test Job System Directory with Mini-DFS.
 */
TestLazyOutput (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestLazyOutput.java)/**
 * A JUnit test to test the Map-Reduce framework's feature to create part
 * files only if there is an explicit output.collect. This helps in preventing
 * 0 byte files
 */
TestLocalJobSubmission (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestLocalJobSubmission.java)/**
 * check for the job submission options of
 * -jt local -libjars
 */
TestLocalMRNotification (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestLocalMRNotification.java)/**
 * Tests Job end notification in local mode.
 */
TextReduce (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapOutputType.java)/** A do-nothing reducer class. We won't get this far, really.
   *
   */
TestMapOutputType (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapOutputType.java)/** 
 * TestMapOutputType checks whether the Map task handles type mismatch
 * between mapper output and the type specified in
 * JobConf.MapOutputKeyType and JobConf.MapOutputValueType.
 */
TestTaskReporter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapProgress.java)/**
   *  Task Reporter that validates map phase progress after each record is
   *  processed by map task
   */
TestMapTask (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapProgress.java)/**
   * Map Task that overrides run method and uses TestTaskReporter instead of
   * TaskReporter and uses FakeUmbilical.
   */
TestMapProgress (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapProgress.java)/**
 *  Validates map phase progress.
 *  Testcase uses newApi.
 *  We extend Task.TaskReporter class and override setProgress()
 *  to validate the map phase progress being set.
 *  We extend MapTask and override startReporter() method that creates
 *  TestTaskReporter instead of TaskReporter and call mapTask.run().
 *  Similar to LocalJobRunner, we set up splits and call mapTask.run()
 *  directly. No job is run, only map task is run.
 *  As the reporter's setProgress() validates progress after
 *  every record is read, we are done with the validation of map phase progress
 *  once mapTask.run() is finished. Sort phase progress in map task is not
 *  validated here.
 */
RandomGenReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapRed.java)/**
   */
RandomCheckMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapRed.java)/**
   * The RandomCheck Job does a lot of our work.  It takes
   * in a num/string keyspace, and transforms it into a
   * key/count(int) keyspace.
   *
   * The map() function just emits a num/1 pair for every
   * num/string input pair.
   *
   * The reduce() function sums up all the 1s that were
   * emitted for a single key.  It then emits the key/total
   * pair.
   *
   * This is used to regenerate the random number "answer key".
   * Each key here is a random number, and the count is the
   * number of times the number was emitted.
   */
RandomCheckReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapRed.java)/**
   */
MergeMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapRed.java)/**
   * The Merge Job is a really simple one.  It takes in
   * an int/int key-value set, and emits the same set.
   * But it merges identical keys by adding their values.
   *
   * Thus, the map() function is just the identity function
   * and reduce() just sums.  Nothing to see here!
   */
TestMapRed (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapRed.java)/**********************************************************
 * MapredLoadTest generates a bunch of work that exercises
 * a Hadoop Map-Reduce system (and DFS, too).  It goes through
 * the following steps:
 *
 * 1) Take inputs 'range' and 'counts'.
 * 2) Generate 'counts' random integers between 0 and range-1.
 * 3) Create a file that lists each integer between 0 and range-1,
 *    and lists the number of times that integer was generated.
 * 4) Emit a (very large) file that contains all the integers
 *    in the order generated.
 * 5) After the file has been generated, read it back and count
 *    how many times each int was generated.
 * 6) Compare this big count-map against the original one.  If
 *    they match, then SUCCESS!  Otherwise, FAILURE!
 *
 * OK, that's how we can think about it.  What are the map-reduce
 * steps that get the job done?
 *
 * 1) In a non-mapred thread, take the inputs 'range' and 'counts'.
 * 2) In a non-mapread thread, generate the answer-key and write to disk.
 * 3) In a mapred job, divide the answer key into K jobs.
 * 4) A mapred 'generator' task consists of K map jobs.  Each reads
 *    an individual "sub-key", and generates integers according to
 *    to it (though with a random ordering).
 * 5) The generator's reduce task agglomerates all of those files
 *    into a single one.
 * 6) A mapred 'reader' task consists of M map jobs.  The output
 *    file is cut into M pieces. Each of the M jobs counts the 
 *    individual ints in its chunk and creates a map of all seen ints.
 * 7) A mapred job integrates all the count files into a single one.
 *
 **********************************************************/
MyMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMerge.java)/**
   * A mapper implementation that assumes that key text contains valid integers
   * in displayable form.
   */
MyPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMerge.java)/**
   * Partitioner implementation to make sure that output is in total sorted
   * order.  We basically route key ranges to different reducers such that
   * key values monotonically increase with the partition number.  For example,
   * in this test, the keys are numbers from 1 to 1000 in the form "000000001"
   * to "000001000" in each input file.  The keys "000000001" to "000000250" are
   * routed to partition 0, "000000251" to "000000500" are routed to partition 1
   * and so on since we have 4 reducers.
   */
MapOutputCopier (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMerge.java)/**
   * Implementation of map output copier(that avoids sorting) on the map side.
   * It maintains keys in the input order within each partition created for
   * reducers.
   */
TestMiniMRBringup (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMiniMRBringup.java)/**
 * A Unit-test to test bringup and shutdown of Mini Map-Reduce Cluster.
 */
MapClass (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMiniMRChildTask.java)/**
   * Map class which checks whether temp directory exists
   * and check the value of java.io.tmpdir
   * Creates a tempfile and checks whether that is created in 
   * temp directory specified.
   */
TestMiniMRChildTask (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMiniMRChildTask.java)/**
 * Class to test mapred task's 
 *   - temp directory
 *   - child env
 */
TestMiniMRClasspath (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMiniMRClasspath.java)/**
 * A JUnit test to test Mini Map-Reduce Cluster with multiple directories
 * and check for correct classpath
 */
TestMiniMRClientCluster (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMiniMRClientCluster.java)/**
 * Basic testing for the MiniMRClientCluster. This test shows an example class
 * that can be used in MR1 or MR2, without any change to the test. The test will
 * use MiniMRYarnCluster in MR2, and MiniMRCluster in MR1.
 */
TestMiniMRDFSCaching (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMiniMRDFSCaching.java)/**
 * A JUnit test to test caching with DFS
 * 
 */
TestMiniMRWithDFSWithDistinctUsers (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMiniMRWithDFSWithDistinctUsers.java)/**
 * A JUnit test to test Mini Map-Reduce Cluster with Mini-DFS.
 */
MyMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMRIntermediateDataEncryption.java)/**
   * A mapper implementation that assumes that key text contains valid integers
   * in displayable form.
   */
MyPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMRIntermediateDataEncryption.java)/**
   * Partitioner implementation to make sure that output is in total sorted
   * order.  We basically route key ranges to different reducers such that
   * key values monotonically increase with the partition number.  For example,
   * in this test, the keys are numbers from 1 to 1000 in the form "000000001"
   * to "000001000" in each input file.  The keys "000000001" to "000000250" are
   * routed to partition 0, "000000251" to "000000500" are routed to partition 1
   * and so on since we have 4 reducers.
   */
MyMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMROpportunisticMaps.java)/**
   * A mapper implementation that assumes that key text contains valid integers
   * in displayable form.
   */
MyPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMROpportunisticMaps.java)/**
   * Partitioner implementation to make sure that output is in total sorted
   * order.  We basically route key ranges to different reducers such that
   * key values monotonically increase with the partition number.  For example,
   * in a test with 4 reducers, the keys are numbers from 1 to 1000 in the
   * form "000000001" to "000001000" in each input file. The keys "000000001"
   * to "000000250" are routed to partition 0, "000000251" to "000000500" are
   * routed to partition 1.
   */
TestMROpportunisticMaps (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMROpportunisticMaps.java)/**
 * Simple MapReduce to test ability of the MRAppMaster to request and use
 * OPPORTUNISTIC containers.
 * This test runs a simple external merge sort using MapReduce.
 * The Hadoop framework's merge on the reduce side will merge the partitions
 * created to generate the final output which is sorted on the key.
 */
DummyMultiFileInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMultiFileInputFormat.java)/** Dummy class to extend MultiFileInputFormat*/
TestMultiFileSplit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMultiFileSplit.java)/**
 * 
 * test MultiFileSplit class
 */
TestMultipleLevelCaching (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMultipleLevelCaching.java)/**
 * This test checks whether the task caches are created and used properly.
 */
MapMB (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestReduceFetchFromPartialMem.java)/**
   * Emit 4096 small keys, 2 &quot;tagged&quot; keys. Emits a fixed amount of
   * data so the in-memory fetch semantics can be tested.
   */
MBValidate (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestReduceFetchFromPartialMem.java)/**
   * Confirm that each small key is emitted once by all maps, each tagged key
   * is emitted by only one map, all IDs are consistent with record data, and
   * all non-ID record data is consistent.
   */
TestReduceTask (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestReduceTask.java)/**
 * This test exercises the ValueIterator.
 */
ProgressTestingReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestReporter.java)/**
   * A {@link Reducer} implementation that checks the progress on every call
   * to {@link Reducer#reduce(Object, Iterator, OutputCollector, Reporter)}.
   */
TestReporter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestReporter.java)/**
 * Tests the old mapred APIs with {@link Reporter#getProgress()}.
 */
SpecialTextOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSpecialCharactersInOutputPath.java)/** generates output filenames with special characters */
TestSpecialCharactersInOutputPath (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestSpecialCharactersInOutputPath.java)/**
 * A JUnit test to test that jobs' output filenames are not HTML-encoded (cf HADOOP-1795).
 */
CommitterWithoutCleanup (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestTaskCommit.java)/**
   * Special Committer that does not cleanup temporary files in
   * abortTask
   * 
   * The framework's FileOutputCommitter cleans up any temporary
   * files left behind in abortTask. We want the test case to
   * find these files and hence short-circuit abortTask.
   */
CommitterThatAlwaysRequiresCommit (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestTaskCommit.java)/**
   * Special committer that always requires commit.
   */
TestYARNRunner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java)/**
 * Test YarnRunner and make sure the client side plugin works
 * fine
 */
Map (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/ThreadedMapBenchmark.java)/**
   * Generates random input data of given size with keys and values of given 
   * sizes. By default it generates 128mb input data with 10 byte keys and 10 
   * byte values.
   */
HalfWaitingMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/UtilsForTests.java)/** Only the later half of the maps wait for the signal while the rest 
   * complete immediately.
   */
RandomInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/UtilsForTests.java)/**
   * A custom input format that creates virtual inputs of a single string
   * for each map. 
   */
InlineCleanupQueue (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/UtilsForTests.java)/**
   * Cleans up files/dirs inline. CleanupQueue deletes in a separate thread
   * asynchronously.
   */
UtilsForTests (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/UtilsForTests.java)/** 
 * Utilities used in unit test.
 *  
 */
MapClass (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/WordCount.java)/**
   * Counts the words in each line.
   * For each line of input, break the line into words and emit them as
   * (<b>word</b>, <b>1</b>).
   */
Reduce (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/WordCount.java)/**
   * A reducer class that just emits the sum of the input values.
   */
WordCount (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/WordCount.java)/**
 * This is an example Hadoop Map/Reduce application.
 * It reads the text input files, breaks each line into words
 * and counts them. The output is a locally sorted list of words and the 
 * count of how often they occurred.
 *
 * To run: bin/hadoop jar build/hadoop-examples.jar wordcount
 *            [-m <i>maps</i>] [-r <i>reduces</i>] <i>in-dir</i> <i>out-dir</i> 
 */
EntityWriterV2 (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/EntityWriterV2.java)/**
 * Base mapper for writing entities to the timeline service. Subclasses
 * override {@link #writeEntities(Configuration, TimelineCollectorManager,
 * org.apache.hadoop.mapreduce.Mapper.Context)} to create and write entities
 * to the timeline service.
 */
FailJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/FailJob.java)/**
 * Dummy class for testing failed mappers and/or reducers.
 * 
 * Mappers emit a token amount of data.
 */
IndirectInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/GenericMRLoadGenerator.java)/**
   * Obscures the InputFormat and location information to simulate maps
   * reading input from arbitrary locations (&quot;indirect&quot; reads).
   */
GrowingSleepJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/GrowingSleepJob.java)/**
 * A sleep job whose mappers create 1MB buffer for every record.
 */
JobHistoryFileParser (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/JobHistoryFileParser.java)/**
 * Used to parse job history and configuration files.
 */
JobHistoryFileReplayMapperV1 (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/JobHistoryFileReplayMapperV1.java)/**
 * Mapper for TimelineServicePerformanceV1 that replays job history files to the
 * timeline service.
 *
 */
JobHistoryFileReplayMapperV2 (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/JobHistoryFileReplayMapperV2.java)/**
 * Mapper for TimelineServicePerformance that replays job history files to the
 * timeline service v.2.
 *
 */
RandomRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/LargeSorter.java)/**
     * Return a single record (filename, "") where the filename is taken from
     * the file split.
     */
RandomInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/LargeSorter.java)/**
   * A custom input format that creates virtual inputs of a single string
   * for each map.
   */
LargeSorter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/LargeSorter.java)/**
 * A sample MR job that helps with testing large sorts in the MapReduce
 * framework. Mapper generates the specified number of bytes and pipes them
 * to the reducers.
 *
 * <code>mapreduce.large-sorter.mbs-per-map</code> specifies the amount
 * of data (in MBs) to generate per map. By default, this is twice the value
 * of <code>mapreduce.task.io.sort.mb</code> or 1 GB if that is not specified
 * either.
 * <code>mapreduce.large-sorter.map-tasks</code> specifies the number of map
 * tasks to run.
 * <code>mapreduce.large-sorter.reduce-tasks</code> specifies the number of
 * reduce tasks to run.
 */
TestChainErrors (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/chain/TestChainErrors.java)/**
 * Tests error conditions in ChainMapper/ChainReducer.
 */
TestSingleElementChain (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/chain/TestSingleElementChain.java)/**
 * Runs wordcount by adding single mapper and single reducer to chain
 */
TestDataDrivenDBInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/db/TestDataDrivenDBInputFormat.java)/**
 * Test aspects of DataDrivenDBInputFormat
 */
DummyInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestCombineFileInputFormat.java)/** Dummy class to extend CombineFileInputFormat*/
DummyInputFormat1 (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestCombineFileInputFormat.java)/** Dummy class to extend CombineFileInputFormat. It allows 
   * non-existent files to be passed into the CombineFileInputFormat, allows
   * for easy testing without having to create real files.
   */
MissingBlockFileSystem (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestCombineFileInputFormat.java)/** Dummy class to extend CombineFileInputFormat. It allows
   * testing with files having missing blocks without actually removing replicas.
   */
ChildRRInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestCombineFileInputFormat.java)/** Extend CFIF to use CFRR with DummyRecordReader */
Split (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestCombineFileInputFormat.java)/**
   * For testing each split has the expected name, length, and offset.
   */
DummyInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestMRCJCFileInputFormat.java)/** Dummy class to extend FileInputFormat*/
TestMultipleInputs (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestMultipleInputs.java)/**
 * @see TestDelegatingInputFormat
 */
TestControlledJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/jobcontrol/TestControlledJob.java)/**
 */
TestMapReduceJobControl (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/jobcontrol/TestMapReduceJobControl.java)/**
 * This class performs unit test for Job/JobControl classes.
 *  
 */
TestMapReduceJobControlWithMocks (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/jobcontrol/TestMapReduceJobControlWithMocks.java)/**
 * Tests the JobControl API using mock and stub Job instances.
 */
CommitterWithCustomDeprecatedCleanup (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestJobOutputCommitter.java)/** 
   * Committer with deprecated {@link FileOutputCommitter#cleanupJob(JobContext)}
   * making a _failed/_killed in the output folder
   */
CommitterWithCustomAbort (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestJobOutputCommitter.java)/**
   * Committer with abort making a _failed/_killed in the output folder
   */
TestJobOutputCommitter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestJobOutputCommitter.java)/**
 * A JUnit test to test Map-Reduce job committer.
 */
DataCopyMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/MapReduceTestUtil.java)/**
   * Simple Mapper and Reducer implementation which copies data it reads in.
   */
MapReduceTestUtil (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/MapReduceTestUtil.java)/**
 * Utility methods used in various Job Control unit tests.
 */
MiniHadoopClusterManager (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/MiniHadoopClusterManager.java)/**
 * This class drives the creation of a mini-cluster on the local machine. By
 * default, a MiniDFSCluster and MiniMRCluster are spawned on the first
 * available ports that are found.
 *
 * A series of command line flags controls the startup cluster options.
 *
 * This class can dump a Hadoop configuration and some basic metadata (in JSON)
 * into a text file.
 *
 * To shutdown the cluster, kill the process.
 */
RandomTextWriter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/RandomTextWriter.java)/**
 * This program uses map/reduce to just run a distributed job where there is
 * no interaction between the tasks and each task writes a large unsorted
 * random sequence of words.
 * In order for this program to generate data for terasort with a 5-10 words
 * per key and 20-100 words per value, have the following config:
 * <pre>{@code
 * <?xml version="1.0"?>
 * <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
 * <configuration>
 *   <property>
 *     <name>mapreduce.randomtextwriter.minwordskey</name>
 *     <value>5</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomtextwriter.maxwordskey</name>
 *     <value>10</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomtextwriter.minwordsvalue</name>
 *     <value>20</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomtextwriter.maxwordsvalue</name>
 *     <value>100</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomtextwriter.totalbytes</name>
 *     <value>1099511627776</value>
 *   </property>
 * </configuration>}</pre>
 * 
 * Equivalently, {@link RandomTextWriter} also supports all the above options
 * and ones supported by {@link Tool} via the command-line.
 * 
 * To run: bin/hadoop jar hadoop-${version}-examples.jar randomtextwriter
 *            [-outFormat <i>output format class</i>] <i>output</i> 
 */
RandomRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/RandomWriter.java)/**
     * Return a single record (filename, "") where the filename is taken from
     * the file split.
     */
RandomInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/RandomWriter.java)/**
   * A custom input format that creates virtual inputs of a single string
   * for each map.
   */
RandomWriter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/RandomWriter.java)/**
 * This program uses map/reduce to just run a distributed job where there is
 * no interaction between the tasks and each task write a large unsorted
 * random binary sequence file of BytesWritable.
 * In order for this program to generate data for terasort with 10-byte keys
 * and 90-byte values, have the following config:
 * <pre>{@code
 * <?xml version="1.0"?>
 * <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
 * <configuration>
 *   <property>
 *     <name>mapreduce.randomwriter.minkey</name>
 *     <value>10</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomwriter.maxkey</name>
 *     <value>10</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomwriter.minvalue</name>
 *     <value>90</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomwriter.maxvalue</name>
 *     <value>90</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomwriter.totalbytes</name>
 *     <value>1099511627776</value>
 *   </property>
 * </configuration>}</pre>
 * Equivalently, {@link RandomWriter} also supports all the above options
 * and ones supported by {@link GenericOptionsParser} via the command-line.
 */
CredentialsTestJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/security/CredentialsTestJob.java)/**
 * class for testing transport of keys via Credentials . 
 * Client passes a list of keys in the Credentials object. 
 * The mapper and reducer checks whether it can access the keys
 * from Credentials.
 */
TestUmbilicalProtocolWithJobToken (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/security/TestUmbilicalProtocolWithJobToken.java)/** Unit tests for using Job Token over RPC. 
 * 
 * System properties required:
 * -Djava.security.krb5.conf=.../hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/test-classes/krb5.conf 
 * -Djava.net.preferIPv4Stack=true
 */
SimpleEntityWriterConstants (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/SimpleEntityWriterConstants.java)/**
 * Constants for simple entity writers.
 */
SimpleEntityWriterV1 (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/SimpleEntityWriterV1.java)/**
   * Adds simple entities with random string payload, events, metrics, and
   * configuration.
   */
SimpleEntityWriterV2 (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/SimpleEntityWriterV2.java)/**
 * Adds simple entities with random string payload, events, metrics, and
 * configuration.
 */
SleepJob (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/SleepJob.java)/**
 * Dummy class for testing MR framefork. Sleeps for a defined period 
 * of time in mapper and reducer. Generates fake input for map / reduce 
 * jobs. Note that generated number of input pairs is in the order 
 * of <code>numMappers * mapSleepTime / 100</code>, so the job uses
 * some disk space.
 */
TestCounters (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestCounters.java)/**
 * TestCounters checks the sanity and recoverability of {@code Counters}
 */
EmptyInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestLocalRunner.java)/** An IF that creates no splits */
SequenceMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestLocalRunner.java)/**
   * Each record received by this mapper is a number 'n'.
   * Emit the values [0..n-1]
   */
TestLocalRunner (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestLocalRunner.java)/**
 * Stress tests for the LocalJobRunner
 */
RandomGenMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestMapReduce.java)/**
   * Modified to make it a junit test.
   * The RandomGen Job does the actual work of creating
   * a huge file of assorted numbers.  It receives instructions
   * as to how many times each number should be counted.  Then
   * it emits those numbers in a crazy order.
   *
   * The map() function takes a key/val pair that describes
   * a value-to-be-emitted (the key) and how many times it 
   * should be emitted (the value), aka "numtimes".  map() then
   * emits a series of intermediate key/val pairs.  It emits
   * 'numtimes' of these.  The key is a random number and the
   * value is the 'value-to-be-emitted'.
   *
   * The system collates and merges these pairs according to
   * the random number.  reduce() function takes in a key/value
   * pair that consists of a crazy random number and a series
   * of values that should be emitted.  The random number key
   * is now dropped, and reduce() emits a pair for every intermediate value.
   * The emitted key is an intermediate value.  The emitted value
   * is just a blank string.  Thus, we've created a huge file
   * of numbers in random order, but where each number appears
   * as many times as we were instructed.
   */
RandomGenReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestMapReduce.java)/**
   */
RandomCheckMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestMapReduce.java)/**
   * The RandomCheck Job does a lot of our work.  It takes
   * in a num/string keyspace, and transforms it into a
   * key/count(int) keyspace.
   *
   * The map() function just emits a num/1 pair for every
   * num/string input pair.
   *
   * The reduce() function sums up all the 1s that were
   * emitted for a single key.  It then emits the key/total
   * pair.
   *
   * This is used to regenerate the random number "answer key".
   * Each key here is a random number, and the count is the
   * number of times the number was emitted.
   */
RandomCheckReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestMapReduce.java)/**
   */
MergeMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestMapReduce.java)/**
   * The Merge Job is a really simple one.  It takes in
   * an int/int key-value set, and emits the same set.
   * But it merges identical keys by adding their values.
   *
   * Thus, the map() function is just the identity function
   * and reduce() just sums.  Nothing to see here!
   */
TestMapReduce (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestMapReduce.java)/**********************************************************
 * MapredLoadTest generates a bunch of work that exercises
 * a Hadoop Map-Reduce system (and DFS, too).  It goes through
 * the following steps:
 *
 * 1) Take inputs 'range' and 'counts'.
 * 2) Generate 'counts' random integers between 0 and range-1.
 * 3) Create a file that lists each integer between 0 and range-1,
 *    and lists the number of times that integer was generated.
 * 4) Emit a (very large) file that contains all the integers
 *    in the order generated.
 * 5) After the file has been generated, read it back and count
 *    how many times each int was generated.
 * 6) Compare this big count-map against the original one.  If
 *    they match, then SUCCESS!  Otherwise, FAILURE!
 *
 * OK, that's how we can think about it.  What are the map-reduce
 * steps that get the job done?
 *
 * 1) In a non-mapred thread, take the inputs 'range' and 'counts'.
 * 2) In a non-mapread thread, generate the answer-key and write to disk.
 * 3) In a mapred job, divide the answer key into K jobs.
 * 4) A mapred 'generator' task consists of K map jobs.  Each reads
 *    an individual "sub-key", and generates integers according to
 *    to it (though with a random ordering).
 * 5) The generator's reduce task agglomerates all of those files
 *    into a single one.
 * 6) A mapred 'reader' task consists of M map jobs.  The output
 *    file is cut into M pieces. Each of the M jobs counts the 
 *    individual ints in its chunk and creates a map of all seen ints.
 * 7) A mapred job integrates all the count files into a single one.
 *
 **********************************************************/
TestMapper (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestMapReduceLazyOutput.java)/**
   * Test mapper.
   */
TestReducer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestMapReduceLazyOutput.java)/**
   * Test Reducer.
   */
TestMapReduceLazyOutput (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestMapReduceLazyOutput.java)/**
 * A JUnit test to test the Map-Reduce framework's feature to create part
 * files only if there is an explicit output.collect. This helps in preventing
 * 0 byte files
 */
TestMRJobClient (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestMRJobClient.java)/**
 test CLI class. CLI class implemented  the Tool interface. 
 Here test that CLI sends correct command with options and parameters. 
 */
TestTaskContext (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestTaskContext.java)/**
 * Tests context api and {@link StatusReporter#getProgress()} via 
 * {@link TaskAttemptContext#getProgress()} API . 
 */
TestValueIterReset (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestValueIterReset.java)/**
 * A JUnit test to test the Map-Reduce framework's support for the
 * "mark-reset" functionality in Reduce Values Iterator
 */
DeleteTask (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/util/MRAsyncDiskService.java)/** A task for deleting a pathName from a volume.
   */
MRAsyncDiskService (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/util/MRAsyncDiskService.java)/**
 * This class is a container of multiple thread pools, each for a volume,
 * so that we can schedule async disk operations easily.
 * 
 * Examples of async disk operations are deletion of files.
 * We can move the files to a "toBeDeleted" folder before asychronously
 * deleting it, to make sure the caller can run it faster.
 * 
 * Users should not write files into the "toBeDeleted" folder, otherwise
 * the files can be gone any time we restart the MRAsyncDiskService.  
 * 
 * This class also contains all operations that will be performed by the
 * thread pools. 
 */
TestMRAsyncDiskService (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/util/TestMRAsyncDiskService.java)/**
 * A test for MRAsyncDiskService.
 */
MiniMRYarnCluster (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/MiniMRYarnCluster.java)/**
 * Configures and starts the MR-specific components in the YARN cluster.
 *
 */
SharedCacheChecker (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobs.java)/**
   * An identity mapper for testing the shared cache.
   */
RandomRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/RandomTextWriterJob.java)/**
     * Return a single record (filename, "") where the filename is taken from
     * the file split.
     */
MapredTestDriver (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/test/MapredTestDriver.java)/**
 * Driver for Map-reduce tests.
 *
 */
Hello (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/util/Hello.java)/**
 * A simple Hello class that is called from TestRunJar
 * 
 */
TestMRCJCReflectionUtils (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/util/TestMRCJCReflectionUtils.java)/**
 * Test for the JobConf-related parts of common's ReflectionUtils
 * class.
 */
TestMRCJCRunJar (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/util/TestMRCJCRunJar.java)/**
 * A test to rest the RunJar class.
 */
MapClass (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/testjar/ClassWordCount.java)/**
   * Counts the words in each line.
   * For each line of input, break the line into words and emit them as
   * (<b>word</b>, <b>1</b>).
   */
Reduce (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/testjar/ClassWordCount.java)/**
   * A reducer class that just emits the sum of the input values.
   */
ClassWordCount (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/testjar/ClassWordCount.java)/**
 * This is an example Hadoop Map/Reduce application being used for 
 * TestMiniMRClasspath. Uses the WordCount examples in hadoop.
 */
ExternalWritable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/testjar/ExternalWritable.java)/**
 * This is an example simple writable class.  This is used as a class external 
 * to the Hadoop IO classes for testing of user Writable classes.
 * 
 */
CommitterWithFailSetup (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/testjar/JobKillCommitter.java)/**
   * The class provides a overrided implementation of output committer
   * set up method, which causes the job to fail during set up.
   */
CommitterWithNoError (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/testjar/JobKillCommitter.java)/**
   * The class provides a dummy implementation of outputcommitter
   * which does nothing
   */
CommitterWithFailCleanup (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/testjar/JobKillCommitter.java)/**
   * The class provides a overrided implementation of commitJob which
   * causes the clean up method to fail.
   */
MapperPass (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/testjar/JobKillCommitter.java)/**
   * The class is used provides a dummy implementation for mapper method which
   * does nothing.
   */
MapperPassSleep (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/testjar/JobKillCommitter.java)/**
  * The class provides a sleep implementation for mapper method.
  */
MapperFail (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/testjar/JobKillCommitter.java)/**
   * The class  provides a way for the mapper function to fail by
   * intentionally throwing an IOException
   */
ReducerFail (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/testjar/JobKillCommitter.java)/**
   * The class provides a way for the reduce function to fail by
   * intentionally throwing an IOException
   */
ReducerPass (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/testjar/JobKillCommitter.java)/**
   * The class provides a empty implementation of reducer method that
   * does nothing
   */
ExternalMapReduce (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/testshell/ExternalMapReduce.java)/**
 * will be in an external jar and used for 
 * test in TestJobShell.java.
 */
ByteBufferDataReader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/buffer/ByteBufferDataReader.java)/**
 * read data from a input buffer
 */
ByteBufferDataWriter (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/buffer/ByteBufferDataWriter.java)/**
 * DataOutputStream implementation which buffers data in a fixed-size
 * ByteBuffer.
 * When the byte buffer has filled up, synchronously passes the buffer
 * to a downstream NativeDataTarget.
 */
CommandDispatcher (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/CommandDispatcher.java)/**
 * a CommandDispatcher receives {@link Command} from upstream
 * and performs corresponding operations
 */
DataReceiver (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/DataReceiver.java)/**
 * a DataReceiver pulls in arriving data, an example
 * is {@link org.apache.hadoop.mapred.nativetask.handlers.BufferPuller}
 */
BufferPullee (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/handlers/BufferPullee.java)/**
 * load data into a buffer signaled by a {@link BufferPuller}
 */
BufferPuller (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/handlers/BufferPuller.java)/**
 * actively signal a {@link BufferPullee} to load data into buffer and receive
 */
BufferPushee (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/handlers/BufferPushee.java)/**
 * collect data when signaled
 */
BufferPusher (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/handlers/BufferPusher.java)/**
 * actively push data into a buffer and signal a {@link BufferPushee} to collect it
 */
IDataLoader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/handlers/IDataLoader.java)/**
 * an IDataLoader loads data on demand
 */
NativeCollectorOnlyHandler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/handlers/NativeCollectorOnlyHandler.java)/**
 * Java Record Reader + Java Mapper + Native Collector
 */
ICombineHandler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/ICombineHandler.java)/**
 * interacts with native side to support Java Combiner
 */
INativeComparable (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/INativeComparable.java)/**
 * Any key type that is comparable at native side must implement this interface.
 *
 * A native comparator function should have the ComparatorPtr type:
 * <code>
 *   typedef int (*ComparatorPtr)(const char * src, uint32_t srcLength,
 *   const char * dest,  uint32_t destLength);
 * </code>
 * Keys are in serialized format at native side. The function has passed in
 * the keys' locations and lengths such that we can compare them in the same
 * logic as their Java comparator.
 *
 * For example, a HiveKey serialized as an int field (containing the length of
 * raw bytes) + raw bytes.
 * When comparing two HiveKeys, we first read the length field and then
 * compare the raw bytes by invoking the BytesComparator provided by our library.
 * We pass the location and length of raw bytes into BytesComparator.
 *
 * <code>
 *   int HivePlatform::HiveKeyComparator(const char * src, uint32_t srcLength,
 *   const char * dest, uint32_t destLength) {
 *     uint32_t sl = bswap(*(uint32_t*)src);
 *     uint32_t dl = bswap(*(uint32_t*)dest);
 *     return NativeObjectFactory::BytesComparator(src + 4, sl, dest + 4, dl);
 *   }
 * </code>
 */
INativeHandler (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/INativeHandler.java)/**
 * A Handler accept input, and give output can be used to transfer command and data
 */
NativeBatchProcessor (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/NativeBatchProcessor.java)/**
 * used to create channel, transfer data and command between Java and native
 */
NativeDataSource (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/NativeDataSource.java)/**
 * NativeDataSource loads data from upstream
 */
NativeDataTarget (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/NativeDataTarget.java)/**
 * NativeDataTarge sends data to downstream
 */
NativeMapOutputCollectorDelegator (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/NativeMapOutputCollectorDelegator.java)/**
 * native map output collector wrapped in Java interface
 */
NativeRuntime (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/NativeRuntime.java)/**
 * This class stands for the native runtime It has three functions:
 * 1. Create native handlers for map, reduce, outputcollector, etc
 * 2. Configure native task with provided MR configs
 * 3. Provide file system api to native space, so that it can use File system like HDFS.
 */
Platform (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/Platform.java)/**
 * Base class for platforms. A platform is a framework running on top of
 * MapReduce, like Hadoop, Hive, Pig, Mahout. Each framework defines its
 * own key type and value type across a MapReduce job. For each platform,
 * we should implement serializers such that we could communicate data with
 * native side and native comparators so our native output collectors could
 * sort them and write out. We've already provided the {@link HadoopPlatform}
 * that supports all key types of Hadoop and users could implement their custom
 * platform.
 */
Platforms (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/Platforms.java)/**
 * this class will load in and init all platforms on classpath
 * it is also the facade to check for key type support and other
 * platform methods
 */
IKVSerializer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/serde/IKVSerializer.java)/**
 * serializes key-value pair
 */
INativeSerializer (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/serde/INativeSerializer.java)/**
 * an INativeSerializer serializes and deserializes data transferred between
 * Java and native. {@link DefaultSerializer} provides default implementations.
 *
 * Note: if you implemented your customized NativeSerializer instead of DefaultSerializer,
 * you have to make sure the native side can serialize it correctly.
 * 
 */
StatusReportChecker (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/StatusReportChecker.java)/**
 * Will periodically check status from native and report to MR framework.
 * 
 */
NativeTaskOutput (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/util/NativeTaskOutput.java)/**
 * base class of output files manager.
 */
NativeTaskOutputFiles (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-nativetask/src/main/java/org/apache/hadoop/mapred/nativetask/util/NativeTaskOutputFiles.java)/**
 * Manipulate the working area for the transient store for maps and reduces.
 *
 * This class is used by map and reduce tasks to identify the directories that they need
 * to write to/read from for intermediate files. The callers of these methods are from
 * child space and see mapreduce.cluster.local.dir as
 * taskTracker/jobCache/jobId/attemptId.
 *
 * This class should not be used from TaskTracker space.
 */
ReduceContext (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java)/**
   * Maintain parameters per messageReceived() Netty context.
   * Allows sendMapOutput calls from operationComplete()
   */
TestFadvisedChunkedFile (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/test/java/org/apache/hadoop/mapred/TestFadvisedChunkedFile.java)/**
 * Unit test for FadvisedChunkedFile.
 */
DefaultJars (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-uploader/src/main/java/org/apache/hadoop/mapred/uploader/DefaultJars.java)/**
 * Default white list and black list implementations.
 */
FrameworkUploader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-uploader/src/main/java/org/apache/hadoop/mapred/uploader/FrameworkUploader.java)/**
 * Upload a MapReduce framework tarball to HDFS.
 * Usage:
 * sudo -u mapred mapred frameworkuploader -fs hdfs://`hostname`:8020 -target
 * /tmp/upload.tar.gz#mr-framework
*/
UploaderException (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-uploader/src/main/java/org/apache/hadoop/mapred/uploader/UploaderException.java)/**
 * Framework uploaded exception type.
 */
TestFrameworkUploader (/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-uploader/src/test/java/org/apache/hadoop/mapred/uploader/TestFrameworkUploader.java)/**
 * Unit test class for FrameworkUploader.
 */
AggregateWordCount (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/AggregateWordCount.java)/**
 * This is an example Aggregated Hadoop Map/Reduce application. It reads the
 * text input files, breaks each line into words and counts them. The output is
 * a locally sorted list of words and the count of how often they occurred.
 * 
 * To run: bin/hadoop jar hadoop-*-examples.jar aggregatewordcount 
 * <i>in-dir</i> <i>out-dir</i> <i>numOfReducers</i> textinputformat
 * 
 */
AggregateWordHistogram (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/AggregateWordHistogram.java)/**
 * This is an example Aggregated Hadoop Map/Reduce application. Computes the
 * histogram of the words in the input texts.
 * 
 * To run: bin/hadoop jar hadoop-*-examples.jar aggregatewordhist <i>in-dir</i>
 * <i>out-dir</i> <i>numOfReducers</i> textinputformat
 * 
 */
BbpMapper (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/BaileyBorweinPlouffe.java)/** Mapper class computing digits of Pi. */
BbpReducer (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/BaileyBorweinPlouffe.java)/** Reducer for concatenating map outputs. */
BbpSplit (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/BaileyBorweinPlouffe.java)/** Input split for the {@link BbpInputFormat}. */
BbpInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/BaileyBorweinPlouffe.java)/**
   * Input format for the {@link BbpMapper}.
   * Keys and values represent offsets and sizes, respectively.
   */
Fraction (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/BaileyBorweinPlouffe.java)/** Represent a number x in hex for 1 > x >= 0 */
BaileyBorweinPlouffe (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/BaileyBorweinPlouffe.java)/**
 * A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact 
 * digits of Pi.
 * This program is able to calculate digit positions
 * lower than a certain limit, which is roughly 10^8.
 * If the limit is exceeded,
 * the corresponding results may be incorrect due to overflow errors.
 * For computing higher bits of Pi, consider using distbbp. 
 * 
 * Reference:
 *
 * [1] David H. Bailey, Peter B. Borwein and Simon Plouffe.  On the Rapid
 *     Computation of Various Polylogarithmic Constants.
 *     Math. Comp., 66:903-913, 1996.
 */
Node (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/DancingLinks.java)/**
   * A cell in the table with up/down and left/right links that form doubly
   * linked lists in both directions. It also includes a link to the column
   * head.
   */
ColumnHeader (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/DancingLinks.java)/**
   * Column headers record the name of the column and the number of rows that 
   * satisfy this column. The names are provided by the application and can 
   * be anything. The size is used for the heuristic for picking the next 
   * column to explore.
   */
SolutionAcceptor (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/DancingLinks.java)/**
   * Applications should implement this to receive the solutions to their 
   * problems.
   */
DancingLinks (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/DancingLinks.java)/**
 * A generic solver for tile laying problems using Knuth's dancing link
 * algorithm. It provides a very fast backtracking data structure for problems
 * that can expressed as a sparse boolean matrix where the goal is to select a
 * subset of the rows such that each column has exactly 1 true in it.
 * 
 * The application gives each column a name and each row is named after the
 * set of columns that it has as true. Solutions are passed back by giving the 
 * selected rows' names.
 * 
 * The type parameter ColumnName is the class of application's column names.
 */
SolutionCatcher (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/DistributedPentomino.java)/**
     * For each solution, generate the prefix and a string representation
     * of the solution. The solution starts with a newline, so that the output
     * looks like:
     * <prefix>,
     * <solution>
     * 
     */
PentMap (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/DistributedPentomino.java)/**
   * Each map takes a line, which represents a prefix move and finds all of 
   * the solutions that start with that prefix. The output is the prefix as
   * the key and the solution as the value.
   */
DistributedPentomino (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/DistributedPentomino.java)/**
 * Launch a distributed pentomino solver.
 * It generates a complete list of prefixes of length N with each unique prefix
 * as a separate line. A prefix is a sequence of N integers that denote the 
 * index of the row that is chosen for each column in order. Note that the
 * next column is heuristically chosen by the solver, so it is dependant on
 * the previous choice. That file is given as the input to
 * map/reduce. The output key/value are the move prefix/solution as Text/Text.
 */
OneSidedPentomino (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/OneSidedPentomino.java)/**
 * Of the "normal" 12 pentominos, 6 of them have distinct shapes when flipped.
 * This class includes both variants of the "flippable" shapes and the
 * unflippable shapes for a total of 18 pieces. Clearly, the boards must have
 * 18*5=90 boxes to hold all of the solutions.
 */
ColumnName (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/Pentomino.java)/**
   * This interface just is a marker for what types I expect to get back
   * as column names.
   */
Piece (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/Pentomino.java)/**
   * Maintain information about a puzzle piece.
   */
Point (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/Pentomino.java)/**
   * A point in the puzzle board. This represents a placement of a piece into
   * a given point on the board.
   */
SolutionPrinter (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/Pentomino.java)/**
   * A solution printer that just writes the solution to stdout.
   */
ColumnName (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/Sudoku.java)/**
   * This interface is a marker class for the columns created for the
   * Sudoku solver.
   */
SolutionPrinter (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/Sudoku.java)/**
   * An acceptor to get the solutions to the puzzle as they are generated and
   * print them to the console.
   */
ColumnConstraint (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/Sudoku.java)/**
   * A constraint that each number can appear just once in a column.
   */
RowConstraint (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/Sudoku.java)/**
   * A constraint that each number can appear just once in a row.
   */
SquareConstraint (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/Sudoku.java)/**
   * A constraint that each number can appear just once in a square.
   */
CellConstraint (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/Sudoku.java)/**
   * A constraint that each cell can only be used once.
   */
Sudoku (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/Sudoku.java)/**
 * This class uses the dancing links algorithm from Knuth to solve sudoku
 * puzzles. It has solved 42x42 puzzles in 1.02 seconds.
 */
AccessRecord (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/DBCountPageView.java)/** Holds a &lt;url, referrer, time &gt; tuple */
PageviewRecord (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/DBCountPageView.java)/** Holds a &lt;url, pageview &gt; tuple */
PageviewMapper (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/DBCountPageView.java)/**
   * Mapper extracts URLs from the AccessRecord (tuples from db), 
   * and emits a &lt;url,1&gt; pair for each access record. 
   */
PageviewReducer (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/DBCountPageView.java)/**
   * Reducer sums up the pageviews and emits a PageviewRecord, 
   * which will correspond to one tuple in the db.
   */
DBCountPageView (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/DBCountPageView.java)/**
 * This is a demonstrative program, which uses DBInputFormat for reading
 * the input data from a database, and DBOutputFormat for writing the data 
 * to the database. 
 * <br>
 * The Program first creates the necessary tables, populates the input table 
 * and runs the mapred job. 
 * <br> 
 * The input data is a mini access log, with a <code>&lt;url,referrer,time&gt;
 * </code> schema.The output is the number of pageviews of each url in the log, 
 * having the schema <code>&lt;url,pageview&gt;</code>.  
 * 
 * When called with no arguments the program starts a local HSQLDB server, and 
 * uses this database for storing/retrieving the data. 
 * <br>
 * This program requires some additional configuration relating to HSQLDB.  
 * The the hsqldb jar should be added to the classpath:
 * <br>
 * <code>export HADOOP_CLASSPATH=share/hadoop/mapreduce/lib-examples/hsqldb-2.0.0.jar</code>
 * <br>
 * And the hsqldb jar should be included with the <code>-libjars</code> 
 * argument when executing it with hadoop:
 * <br>
 * <code>-libjars share/hadoop/mapreduce/lib-examples/hsqldb-2.0.0.jar</code>
 */
ExampleDriver (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/ExampleDriver.java)/**
 * A description of an example program based on its class and a 
 * human-readable description.
 */
Join (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/Join.java)/**
 * Given a set of sorted datasets keyed with the same class and yielding
 * equal partitions, it is possible to effect a join of those datasets 
 * prior to the map. The example facilitates the same.
 *
 * To run: bin/hadoop jar build/hadoop-examples.jar join
 *            [-r <i>reduces</i>]
 *            [-inFormat <i>input format class</i>] 
 *            [-outFormat <i>output format class</i>] 
 *            [-outKey <i>output key class</i>] 
 *            [-outValue <i>output value class</i>] 
 *            [-joinOp &lt;inner|outer|override&gt;]
 *            [<i>in-dir</i>]* <i>in-dir</i> <i>out-dir</i> 
 */
WordOffset (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/MultiFileWordCount.java)/**
   * This record keeps &lt;filename,offset&gt; pairs.
   */
MyInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/MultiFileWordCount.java)/**
   * To use {@link CombineFileInputFormat}, one should extend it, to return a 
   * (custom) {@link RecordReader}. CombineFileInputFormat uses 
   * {@link CombineFileSplit}s. 
   */
CombineFileLineRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/MultiFileWordCount.java)/**
   * RecordReader is responsible from extracting records from a chunk
   * of the CombineFileSplit. 
   */
MapClass (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/MultiFileWordCount.java)/**
   * This Mapper is similar to the one in {@link WordCount.TokenizerMapper}.
   */
MultiFileWordCount (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/MultiFileWordCount.java)/**
 * MultiFileWordCount is an example to demonstrate the usage of 
 * MultiFileInputFormat. This examples counts the occurrences of
 * words in the text files under the given input directory.
 */
Combinable (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/Combinable.java)/**
 * A class is Combinable if its object can be combined with other objects.
 * @param <T> The generic type
 */
Container (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/Container.java)/**
 * A class is a Container if it contains an element. 
 * @param <T> The generic type
 */
Parameters (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/** DistSum job parameters */
SummationSplit (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/** Split for the summations */
AbstractInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/** An abstract InputFormat for the jobs */
Machine (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/** Abstract machine for job execution. */
PartitionInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/** An InputFormat which partitions a summation */
SummingMapper (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/** A mapper which computes sums */
MapSide (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/**
   * A machine which does computation on the map side.
   */
SummationInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/** An InputFormat which returns a single summation. */
PartitionMapper (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/** A Mapper which partitions a summation */
IndexPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/** Use the index for partitioning. */
SummingReducer (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/** A Reducer which computes sums */
ReduceSide (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/**
   * A machine which does computation on the reduce side.
   */
MixMachine (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/**
   * A machine which chooses Machine in runtime according to the cluster status
   */
Computation (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/** Callable computation */
DistSum (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java)/**
 * The main class for computing sums using map/reduce jobs.
 * A sum is partitioned into jobs.
 * A job may be executed on the map-side or on the reduce-side.
 * A map-side job has multiple maps and zero reducer.
 * A reduce-side job has one map and multiple reducers.
 * Depending on the clusters status in runtime,
 * a mix-type job may be executed on either side.
 */
ArithmeticProgression (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/math/ArithmeticProgression.java)/** An arithmetic progression */
Tail (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/math/Bellard.java)/** The sum tail */
Sum (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/math/Bellard.java)/** The sums in the Bellard's formula */
Bellard (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/math/Bellard.java)/**
 * Bellard's BBP-type Pi formula
 * 1/2^6 \sum_{n=0}^\infty (-1)^n/2^{10n}
 * (-2^5/(4n+1) -1/(4n+3) +2^8/(10n+1) -2^6/(10n+3) -2^2/(10n+5)
 *  -2^2/(10n+7) +1/(10n+9))
 *  
 * References:
 *
 * [1] David H. Bailey, Peter B. Borwein and Simon Plouffe.  On the Rapid
 *     Computation of Various Polylogarithmic Constants.
 *     Math. Comp., 66:903-913, 1996.
 *     
 * [2] Fabrice Bellard.  A new formula to compute the n'th binary digit of pi,
 *     1997.  Available at http://fabrice.bellard.free.fr/pi .
 */
LongLong (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/math/LongLong.java)/** Support 124-bit integer arithmetic. */
Modular (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/math/Modular.java)/** Modular arithmetics */
Montgomery (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/math/Montgomery.java)/** Montgomery method.
 * 
 * References:
 * 
 * [1] Richard Crandall and Carl Pomerance.  Prime Numbers: A Computational 
 *     Perspective.  Springer-Verlag, 2001. 
 * 
 * [2] Peter Montgomery.  Modular multiplication without trial division.
 *     Math. Comp., 44:519-521, 1985.
 */
Summation (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/math/Summation.java)/** Represent the summation \sum \frac{2^e \mod n}{n}. */
Parser (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/Parser.java)/** A class for parsing outputs */
ArithmeticProgressionWritable (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/SummationWritable.java)/** A writable class for ArithmeticProgression */
SummationWritable (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/SummationWritable.java)/** A Writable class for Summation */
TaskResult (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/TaskResult.java)/** A class for map task results or reduce task results. */
Timer (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/Util.java)/** Timer */
Util (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/Util.java)/** Utility methods */
HaltonSequence (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/QuasiMonteCarlo.java)/** 2-dimensional Halton sequence {H(i)},
   * where H(i) is a 2-dimensional point and i >= 1 is the index.
   * Halton sequence is used to generate sample points for Pi estimation. 
   */
QmcMapper (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/QuasiMonteCarlo.java)/**
   * Mapper class for Pi estimation.
   * Generate points in a unit square
   * and then count points inside/outside of the inscribed circle of the square.
   */
QmcReducer (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/QuasiMonteCarlo.java)/**
   * Reducer class for Pi estimation.
   * Accumulate points inside/outside results from the mappers.
   */
QuasiMonteCarlo (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/QuasiMonteCarlo.java)/**
 * A map/reduce program that estimates the value of Pi
 * using a quasi-Monte Carlo (qMC) method.
 * Arbitrary integrals can be approximated numerically by qMC methods.
 * In this example,
 * we use a qMC method to approximate the integral $I = \int_S f(x) dx$,
 * where $S=[0,1)^2$ is a unit square,
 * $x=(x_1,x_2)$ is a 2-dimensional point,
 * and $f$ is a function describing the inscribed circle of the square $S$,
 * $f(x)=1$ if $(2x_1-1)^2+(2x_2-1)^2 &lt;= 1$ and $f(x)=0$, otherwise.
 * It is easy to see that Pi is equal to $4I$.
 * So an approximation of Pi is obtained once $I$ is evaluated numerically.
 * 
 * There are better methods for computing Pi.
 * We emphasize numerical approximation of arbitrary integrals in this example.
 * For computing many digits of Pi, consider using bbp.
 *
 * The implementation is discussed below.
 *
 * Mapper:
 *   Generate points in a unit square
 *   and then count points inside/outside of the inscribed circle of the square.
 *
 * Reducer:
 *   Accumulate points inside/outside results from the mappers.
 *
 * Let numTotal = numInside + numOutside.
 * The fraction numInside/numTotal is a rational approximation of
 * the value (Area of the circle)/(Area of the square) = $I$,
 * where the area of the inscribed circle is Pi/4
 * and the area of unit square is 1.
 * Finally, the estimated value of Pi is 4(numInside/numTotal).  
 */
RandomTextWriter (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/RandomTextWriter.java)/**
 * This program uses map/reduce to just run a distributed job where there is
 * no interaction between the tasks and each task writes a large unsorted
 * random sequence of words.
 * In order for this program to generate data for terasort with a 5-10 words
 * per key and 20-100 words per value, have the following config:
 * <pre>{@code
 * <?xml version="1.0"?>
 * <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
 * <configuration>
 *   <property>
 *     <name>mapreduce.randomtextwriter.minwordskey</name>
 *     <value>5</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomtextwriter.maxwordskey</name>
 *     <value>10</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomtextwriter.minwordsvalue</name>
 *     <value>20</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomtextwriter.maxwordsvalue</name>
 *     <value>100</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomtextwriter.totalbytes</name>
 *     <value>1099511627776</value>
 *   </property>
 * </configuration>}</pre>
 * 
 * Equivalently, {@link RandomTextWriter} also supports all the above options
 * and ones supported by {@link Tool} via the command-line.
 * 
 * To run: bin/hadoop jar hadoop-${version}-examples.jar randomtextwriter
 *            [-outFormat <i>output format class</i>] <i>output</i> 
 */
RandomRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/RandomWriter.java)/**
     * Return a single record (filename, "") where the filename is taken from
     * the file split.
     */
RandomInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/RandomWriter.java)/**
   * A custom input format that creates virtual inputs of a single string
   * for each map.
   */
RandomWriter (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/RandomWriter.java)/**
 * This program uses map/reduce to just run a distributed job where there is
 * no interaction between the tasks and each task write a large unsorted
 * random binary sequence file of BytesWritable.
 * In order for this program to generate data for terasort with 10-byte keys
 * and 90-byte values, have the following config:
 * <pre>{@code
 * <?xml version="1.0"?>
 * <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
 * <configuration>
 *   <property>
 *     <name>mapreduce.randomwriter.minkey</name>
 *     <value>10</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomwriter.maxkey</name>
 *     <value>10</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomwriter.minvalue</name>
 *     <value>90</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomwriter.maxvalue</name>
 *     <value>90</value>
 *   </property>
 *   <property>
 *     <name>mapreduce.randomwriter.totalbytes</name>
 *     <value>1099511627776</value>
 *   </property>
 * </configuration>}</pre>
 * Equivalently, {@link RandomWriter} also supports all the above options
 * and ones supported by {@link GenericOptionsParser} via the command-line.
 */
Comparator (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/SecondarySort.java)/** A Comparator that compares serialized IntPair. */
IntPair (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/SecondarySort.java)/**
   * Define a pair of integers that are writable.
   * They are serialized in a byte comparable format.
   */
FirstPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/SecondarySort.java)/**
   * Partition based on the first part of the pair.
   */
FirstGroupingComparator (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/SecondarySort.java)/**
   * Compare only the first part of the pair, so that reduce is called once
   * for each value of the first part.
   */
MapClass (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/SecondarySort.java)/**
   * Read two integers from each line and generate a key, value pair
   * as ((left, right), right).
   */
Reduce (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/SecondarySort.java)/**
   * A reducer class that just emits the sum of the input values.
   */
SecondarySort (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/SecondarySort.java)/**
 * This is an example Hadoop Map/Reduce application.
 * It reads the text input files that must contain two integers per a line.
 * The output is sorted by the first and second number and grouped on the 
 * first number.
 *
 * To run: bin/hadoop jar build/hadoop-examples.jar secondarysort
 *            <i>in-dir</i> <i>out-dir</i> 
 */
Sort (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/Sort.java)/**
 * This is the trivial map/reduce program that does absolutely nothing
 * other than use the framework to fragment and sort the input values.
 *
 * To run: bin/hadoop jar build/hadoop-examples.jar sort
 *            [-r <i>reduces</i>]
 *            [-inFormat <i>input format class</i>] 
 *            [-outFormat <i>output format class</i>] 
 *            [-outKey <i>output key class</i>] 
 *            [-outValue <i>output value class</i>] 
 *            [-totalOrder <i>pcnt</i> <i>num samples</i> <i>max splits</i>]
 *            <i>in-dir</i> <i>out-dir</i> 
 */
GenSort (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/GenSort.java)/** 
 * A single process data generator for the terasort data. Based on gensort.c 
 * version 1.1 (3 Mar 2009) from Chris Nyberg &lt;chris.nyberg@ordinal.com&gt;.
 */
RandomConstant (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/Random16.java)/** 
   * The "Gen" array contain powers of 2 of the linear congruential generator.
   * The index 0 struct contain the "a" coefficient and "c" constant for the
   * generator.  That is, the generator is:
   *    f(x) = (Gen[0].a * x + Gen[0].c) mod 2**128
   *
   * All structs after the first contain an "a" and "c" that
   * comprise the square of the previous function.
   *
   * f**2(x) = (Gen[1].a * x + Gen[1].c) mod 2**128
   * f**4(x) = (Gen[2].a * x + Gen[2].c) mod 2**128
   * f**8(x) = (Gen[3].a * x + Gen[3].c) mod 2**128
   * ...

   */
Random16 (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/Random16.java)/**
 * This class implements a 128-bit linear congruential generator.
 * Specifically, if X0 is the most recently issued 128-bit random
 * number (or a seed of 0 if no random number has already been generated,
 * the next number to be generated, X1, is equal to:
 * X1 = (a * X0 + c) mod 2**128
 * where a is 47026247687942121848144207491837523525
 *            or 0x2360ed051fc65da44385df649fccf645
 *   and c is 98910279301475397889117759788405497857
 *            or 0x4a696d47726179524950202020202001
 * The coefficient "a" is suggested by:
 * Pierre L'Ecuyer, "Tables of linear congruential generators of different
 * sizes and good lattice structure", Mathematics of Computation, 68
 * pp. 249 - 260 (1999)
 * http://www.ams.org/mcom/1999-68-225/S0025-5718-99-00996-5/S0025-5718-99-00996-5.pdf
 * The constant "c" meets the simple suggestion by the same reference that
 * it be odd.
 *
 * There is also a facility for quickly advancing the state of the
 * generator by a fixed number of steps - this facilitates parallel
 * generation.
 *
 * This is based on 1.0 of rand16.c from Chris Nyberg 
 * <chris.nyberg@ordinal.com>.
 */
RangeInputSplit (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraGen.java)/**
     * An input split consisting of a range on numbers.
     */
RangeRecordReader (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraGen.java)/**
     * A record reader that will generate a range of numbers.
     */
RangeInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraGen.java)/**
   * An input format that assigns ranges of longs to each mapper.
   */
SortGenMapper (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraGen.java)/**
   * The Mapper class that given a row number, will generate the appropriate 
   * output line.
   */
TeraGen (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraGen.java)/**
 * Generate the official GraySort input data set.
 * The user specifies the number of rows and the output directory and this
 * class runs a map/reduce program to generate the data.
 * The format of the data is:
 * <ul>
 * <li>(10 bytes key) (constant 2 bytes) (32 bytes rowid) 
 *     (constant 4 bytes) (48 bytes filler) (constant 4 bytes)
 * <li>The rowid is the right justified row id as a hex number.
 * </ul>
 *
 * <p>
 * To run the program: 
 * <b>bin/hadoop jar hadoop-*-examples.jar teragen 10000000000 in-dir</b>
 */
TeraInputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraInputFormat.java)/**
 * An input format that reads the first 10 characters of each line as the key
 * and the rest of the line as the value. Both key and value are represented
 * as Text.
 */
TeraOutputFormat (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraOutputFormat.java)/**
 * An output format that writes the key and value appended together.
 */
TrieNode (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraSort.java)/**
     * A generic trie node
     */
InnerTrieNode (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraSort.java)/**
     * An inner trie node that contains 256 children based on the next
     * character.
     */
LeafTrieNode (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraSort.java)/**
     * A leaf trie node that does string compares to figure out where the given
     * key belongs between lower..upper.
     */
TotalOrderPartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraSort.java)/**
   * A partitioner that splits text keys into roughly equal partitions
   * in a global sorted order.
   */
SimplePartitioner (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraSort.java)/**
   * A total order partitioner that assigns keys based on their first 
   * PREFIX_LENGTH bytes, assuming a flat distribution.
   */
TeraSort (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraSort.java)/**
 * Generates the sampled split points, launches the job, and waits for it to
 * finish. 
 * <p>
 * To run the program: 
 * <b>bin/hadoop jar hadoop-*-examples.jar terasort in-dir out-dir</b>
 */
ValidateReducer (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraValidate.java)/**
   * Check the boundaries between the output files by making sure that the
   * boundary keys are always increasing.
   * Also passes any error reports along intact.
   */
TeraValidate (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraValidate.java)/**
 * Generate 1 mapper per a file that checks to make sure the keys
 * are sorted within each file. The mapper also generates 
 * "$file:begin", first key and "$file:end", last key. The reduce verifies that
 * all of the start/end items are in order.
 * Any output from the reduce is problem report.
 * <p>
 * To run the program: 
 * <b>bin/hadoop jar hadoop-*-examples.jar teravalidate out-dir report-dir</b>
 * <p>
 * If there is any output, something is wrong and the output of the reduce
 * will have the problem report.
 */
Unsigned16 (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/Unsigned16.java)/**
 * An unsigned 16 byte integer class that supports addition, multiplication,
 * and left shifts.
 */
WordMeanMapper (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordMean.java)/**
   * Maps words from line of text into 2 key-value pairs; one key-value pair for
   * counting the word, another for counting its length.
   */
WordMeanReducer (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordMean.java)/**
   * Performs integer summation of all the values for each key.
   */
WordMedianMapper (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordMedian.java)/**
   * Maps words from line of text into a key-value pair; the length of the word
   * as the key, and 1 as the value.
   */
WordMedianReducer (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordMedian.java)/**
   * Performs integer summation of all the values for each key.
   */
WordStandardDeviationMapper (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordStandardDeviation.java)/**
   * Maps words from line of text into 3 key-value pairs; one key-value pair for
   * counting the word, one for counting its length, and one for counting the
   * square of its length.
   */
WordStandardDeviationReducer (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordStandardDeviation.java)/**
   * Performs integer summation of all the values for each key.
   */
TestBaileyBorweinPlouffe (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/test/java/org/apache/hadoop/examples/TestBaileyBorweinPlouffe.java)/** Tests for BaileyBorweinPlouffe */
WordStdDevReader (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/test/java/org/apache/hadoop/examples/TestWordStats.java)/**
   * Modified internal test class that is designed to read all the files in the
   * input directory, and find the standard deviation between all of the word
   * lengths.
   */
WordMedianReader (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/test/java/org/apache/hadoop/examples/TestWordStats.java)/**
   * Modified internal test class that is designed to read all the files in the
   * input directory, and find the median length of all the words.
   */
WordMeanReader (/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/test/java/org/apache/hadoop/examples/TestWordStats.java)/**
   * Modified internal test class that is designed to read all the files in the
   * input directory, and find the mean length of all the words.
   */
CompileMojo (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/cmakebuilder/CompileMojo.java)/**
 * Goal which builds the native sources.
 */
TestThread (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/cmakebuilder/TestMojo.java)/**
   * The test thread waits for the process to terminate.
   *
   * Since Process#waitFor doesn't take a timeout argument, we simulate one by
   * interrupting this thread after a certain amount of time has elapsed.
   */
TestMojo (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/cmakebuilder/TestMojo.java)/**
 * Goal which runs a native unit test.
 */
CreateDirsMojo (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/paralleltests/CreateDirsMojo.java)/**
 * Goal which creates the parallel-test directories.
 */
ProtocMojo (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/protoc/ProtocMojo.java)/**
 * Mojo to generate java classes from .proto files using protoc.
 * See package info for examples of use in a maven pom.
 */
ChecksumComparator (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/protoc/ProtocRunner.java)/**
   * Compares include and source file checksums against previously computed
   * checksums stored in a json file in the build directory.
   */
ProtocRunner (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/protoc/ProtocRunner.java)/**
 * Common execution for both the main and test protoc mojos.
 */
ProtocTestMojo (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/protoc/ProtocTestMojo.java)/**
 * Mojo to generate java test classes from .proto files using protoc.
 * See package info for examples of use in a maven pom.
 */
ResourceGzMojo (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/resourcegz/ResourceGzMojo.java)/**
 * ResourceGzMojo will gzip files.
 * It is meant to be used for gzipping website resource files (e.g. .js, .css,
 * etc).  It takes an input directory, output directory, and extensions to
 * process and will generate the .gz files. Any additional directory structure
 * beyond the input directory is preserved in the output directory.
 */
ServicesResourceTransformer (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/shade/resource/ServicesResourceTransformer.java)/**
 * Resources transformer that appends entries in META-INF/services resources
 * into a single resource. For example, if there are several
 * META-INF/services/org.apache.maven.project.ProjectBuilder resources spread
 * across many JARs the individual entries will all be concatenated into a
 * single META-INF/services/org.apache.maven.project.ProjectBuilder resource
 * packaged into the resultant JAR produced by the shading process.
 *
 * From following sources, only needed until MSHADE-182 gets released
 * * https://s.apache.org/vwjl (source in maven-shade-plugin repo)
 * * https://issues.apache.org/jira/secure/attachment/12718938/MSHADE-182.patch
 *
 * Has been reformatted according to Hadoop checkstyle rules and modified
 * to meet Hadoop's threshold for Findbugs problems.
 */
OutputBufferThread (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/util/Exec.java)/**
   * OutputBufferThread is a background thread for consuming and storing output
   * of the external process.
   */
Exec (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/util/Exec.java)/**
 * Exec is a helper class for executing an external process from a mojo.
 */
FileSetUtils (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/util/FileSetUtils.java)/**
 * FileSetUtils contains helper methods for mojo implementations that need to
 * work with a Maven FileSet.
 */
VersionInfoMojo (/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/versioninfo/VersionInfoMojo.java)/**
 * VersionInfoMojo calculates information about the current version of the
 * codebase and exports the information as properties for further use in a Maven
 * build.  The version information includes build time, SCM URI, SCM branch, SCM
 * commit, and an MD5 checksum of the contents of the files in the codebase.
 */
Builder (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/BucketArgs.java)/**
   * Builder for OmBucketInfo.
   */
BucketArgs (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/BucketArgs.java)/**
 * This class encapsulates the arguments that are
 * required for creating a bucket.
 */
Builder (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/io/BlockOutputStreamEntry.java)/**
   * Builder class for ChunkGroupOutputStreamEntry.
   * */
BlockOutputStreamEntry (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/io/BlockOutputStreamEntry.java)/**
 * Helper class used inside {@link BlockOutputStream}.
 * */
BlockOutputStreamEntryPool (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/io/BlockOutputStreamEntryPool.java)/**
 * This class manages the stream entries list and handles block allocation
 * from OzoneManager.
 */
KeyInputStream (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/io/KeyInputStream.java)/**
 * Maintaining a list of BlockInputStream. Read based on offset.
 */
Builder (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/io/KeyOutputStream.java)/**
   * Builder class of KeyOutputStream.
   */
KeyOutputStream (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/io/KeyOutputStream.java)/**
 * Maintaining a list of BlockInputStream. Write based on offset.
 *
 * Note that this may write to multiple containers in one write call. In case
 * that first container succeeded but later ones failed, the succeeded writes
 * are not rolled back.
 *
 * TODO : currently not support multi-thread access.
 */
OzoneInputStream (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/io/OzoneInputStream.java)/**
 * OzoneInputStream is used to read data from Ozone.
 * It uses {@link KeyInputStream} for reading the data.
 */
OzoneOutputStream (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/io/OzoneOutputStream.java)/**
 * OzoneOutputStream is used to write data into Ozone.
 * It uses SCM's {@link KeyOutputStream} for writing the data.
 */
VolumeIterator (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/ObjectStore.java)/**
   * An Iterator to iterate over {@link OzoneVolume} list.
   */
S3BucketIterator (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/ObjectStore.java)/**
   * An Iterator to iterate over {@link OzoneBucket} list.
   */
ObjectStore (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/ObjectStore.java)/**
 * ObjectStore class is responsible for the client operations that can be
 * performed on Ozone Object Store.
 */
KeyIterator (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneBucket.java)/**
   * An Iterator to iterate over {@link OzoneKey} list.
   */
OzoneBucket (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneBucket.java)/**
 * A class that encapsulates OzoneBucket.
 */
OzoneClient (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneClient.java)/**
 * OzoneClient connects to Ozone Cluster and
 * perform basic operations.
 */
OzoneClientException (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneClientException.java)/**
 * This exception is thrown by the Ozone Clients.
 */
OzoneClientFactory (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneClientFactory.java)/**
 * Factory class to create OzoneClients.
 */
OzoneClientInvocationHandler (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneClientInvocationHandler.java)/**
 * Invocation Handler for ozone client which dispatches the call to underlying
 * ClientProtocol implementation.
 */
OzoneClientUtils (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneClientUtils.java)/** A utility class for OzoneClient. */
OzoneKey (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneKey.java)/**
 * A class that encapsulates OzoneKey.
 */
OzoneKeyDetails (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneKeyDetails.java)/**
 * A class that encapsulates OzoneKeyLocation.
 */
OzoneKeyLocation (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneKeyLocation.java)/**
 * One key can be stored in one or more containers as one or more blocks.
 * This class represents one such block instance.
 */
OzoneMultipartUpload (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneMultipartUpload.java)/**
 * Information about one initialized upload.
 */
OzoneMultipartUploadList (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneMultipartUploadList.java)/**
 * List of in-flight MPU upoads.
 */
PartInfo (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneMultipartUploadPartListParts.java)/**
   * Class that represents each Part information of a multipart upload part.
   */
OzoneMultipartUploadPartListParts (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneMultipartUploadPartListParts.java)/**
 * Class that represents Multipart upload List parts response.
 */
BucketIterator (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneVolume.java)/**
   * An Iterator to iterate over {@link OzoneBucket} list.
   */
OzoneVolume (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneVolume.java)/**
 * A class that encapsulates OzoneVolume.
 */
ClientProtocol (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/protocol/ClientProtocol.java)/**
 * An implementer of this interface is capable of connecting to Ozone Cluster
 * and perform client operations. The protocol used for communication is
 * determined by the implementation class specified by
 * property <code>ozone.client.protocol</code>. The build-in implementation
 * includes: {@link org.apache.hadoop.ozone.client.rpc.RpcClient} for RPC and
 * {@link  org.apache.hadoop.ozone.client.rest.RestClient} for REST.
 */
OzoneKMSUtil (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/OzoneKMSUtil.java)/**
 * KMS utility class for Ozone Data Encryption At-Rest.
 */
RpcClient (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/RpcClient.java)/**
 * Ozone RPC Client Implementation, it connects to OM, SCM and DataNode
 * to execute client calls. This uses RPC protocol for communication
 * with the servers.
 */
Builder (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/VolumeArgs.java)/**
   * Builder for OmVolumeArgs.
   */
VolumeArgs (/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/VolumeArgs.java)/**
 * This class encapsulates the arguments that are
 * required for creating a volume.
 */
TestOzoneKMSUtil (/hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneKMSUtil.java)/**
 * Test class for {@link OzoneKMSUtil}.
 * */
TestHddsClientUtils (/hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/TestHddsClientUtils.java)/**
 * This test class verifies the parsing of SCM endpoint config settings. The
 * parsing logic is in
 * {@link org.apache.hadoop.hdds.scm.client.HddsClientUtils}.
 */
LengthInputStream (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/client/io/LengthInputStream.java)/**
 * An input stream with length.
 */
CommandHandler (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/freon/OzoneGetConf.java)/**
   * Handler to return value for key corresponding to the
   * {@link OzoneGetConf.Command}.
   */
StorageContainerManagersCommandHandler (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/freon/OzoneGetConf.java)/**
   * Handler for {@link Command#STORAGECONTAINERMANAGER}.
   */
OzoneManagersCommandHandler (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/freon/OzoneGetConf.java)/**
   * Handler for {@link Command#OZONEMANAGER}.
   */
OzoneGetConf (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/freon/OzoneGetConf.java)/**
 * CLI utility to print out ozone related configuration.
 */
OmBucketInfoCodec (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/codec/OmBucketInfoCodec.java)/**
 * Codec to encode OmBucketInfo as byte array.
 */
OmKeyInfoCodec (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/codec/OmKeyInfoCodec.java)/**
 * Codec to encode OmKeyInfo as byte array.
 */
OmMultipartKeyInfoCodec (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/codec/OmMultipartKeyInfoCodec.java)/**
 * Codec Registry for OmMultipartKeyInfo.
 */
OmPrefixInfoCodec (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/codec/OmPrefixInfoCodec.java)/**
 * Codec to encode PrefixAcl as byte array.
 */
OmVolumeArgsCodec (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/codec/OmVolumeArgsCodec.java)/**
 * Codec to encode OmVolumeArgsCodec as byte array.
 */
RepeatedOmKeyInfoCodec (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/codec/RepeatedOmKeyInfoCodec.java)/**
 * Codec to encode RepeatedOmKeyInfo as byte array.
 */
S3SecretValueCodec (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/codec/S3SecretValueCodec.java)/**
 * Codec to encode S3SecretValue as byte array.
 */
TokenIdentifierCodec (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/codec/TokenIdentifierCodec.java)/**
 * Codec to encode TokenIdentifierCodec as byte array.
 */
UserVolumeInfoCodec (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/codec/UserVolumeInfoCodec.java)/**
 * Codec to encode UserVolumeInfo as byte array.
 */
NotLeaderException (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/exceptions/NotLeaderException.java)/**
 * Exception thrown by
 * {@link org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolPB} when
 * a read request is received by a non leader OM node.
 */
OMException (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/exceptions/OMException.java)/**
 * Exception thrown by Ozone Manager.
 */
OMFailoverProxyProvider (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/ha/OMFailoverProxyProvider.java)/**
 * A failover proxy provider implementation which allows clients to configure
 * multiple OMs to connect to. In case of OM failover, client can try
 * connecting to another OM node from the list of proxies.
 */
OMProxyInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/ha/OMProxyInfo.java)/**
 * Class to store OM proxy information.
 */
Builder (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/BucketEncryptionKeyInfo.java)/**
   * Builder for BucketEncryptionKeyInfo.
   */
BucketEncryptionKeyInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/BucketEncryptionKeyInfo.java)/**
 * Encryption key info for bucket encryption key.
 */
EncryptionBucketInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/EncryptionBucketInfo.java)/**
 * A simple class for representing an encryption bucket. Presently an encryption
 * bucket only has a path (the root of the encryption zone), a key name, and a
 * unique id. The id is used to implement batched listing of encryption zones.
 */
KeyValueUtil (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/KeyValueUtil.java)/**
 * Convert from/to hdds KeyValue protobuf structure.
 */
Builder (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketArgs.java)/**
   * Builder for OmBucketArgs.
   */
OmBucketArgs (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketArgs.java)/**
 * A class that encapsulates Bucket Arguments.
 */
Builder (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java)/**
   * Builder for OmBucketInfo.
   */
OmBucketInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java)/**
 * A class that encapsulates Bucket Info.
 */
Builder (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmKeyArgs.java)/**
   * Builder class of OmKeyArgs.
   */
OmKeyArgs (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmKeyArgs.java)/**
 * Args for key. Client use this to specify key's attributes on  key creation
 * (putKey()).
 */
Builder (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmKeyInfo.java)/**
   * Builder of OmKeyInfo.
   */
OmKeyInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmKeyInfo.java)/**
 * Args for key block. The block instance for the key requested in putKey.
 * This is returned from OM to client, and client use class to talk to
 * datanode. Also, this is the metadata written to om.db on server side.
 */
Builder (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmKeyLocationInfo.java)/**
   * Builder of OmKeyLocationInfo.
   */
OmKeyLocationInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmKeyLocationInfo.java)/**
 * One key can be too huge to fit in one container. In which case it gets split
 * into a number of subkeys. This class represents one such subkey instance.
 */
OmKeyLocationInfoGroup (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmKeyLocationInfoGroup.java)/**
 * A list of key locations. This class represents one single version of the
 * blocks of a key.
 */
OmMultipartCommitUploadPartInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartCommitUploadPartInfo.java)/**
 * This class holds information about the response from commit multipart
 * upload part request.
 */
OmMultipartInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartInfo.java)/**
 * Class which holds information about the response of initiate multipart
 * upload request.
 */
OmMultipartKeyInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartKeyInfo.java)/**
 * This class represents multipart upload information for a key, which holds
 * upload part information of the key.
 */
OmMultipartUpload (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartUpload.java)/**
 * Information about one initialized upload.
 */
OmMultipartUploadCompleteInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartUploadCompleteInfo.java)/**
 * This class holds information about the response of complete Multipart
 * upload request.
 */
OmMultipartUploadCompleteList (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartUploadCompleteList.java)/**
 * This class represents multipart list, which is required for
 * CompleteMultipart upload request.
 */
OmMultipartUploadList (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartUploadList.java)/**
 * List of in-flight MPU uploads.
 */
OmMultipartUploadListParts (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartUploadListParts.java)/**
 * Class which is response for the list parts of a multipart upload key.
 */
OmOzoneAclMap (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmOzoneAclMap.java)/**
 * This helper class keeps a map of all user and their permissions.
 */
OmPartInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmPartInfo.java)/**
 * Class that defines information about each part of a multipart upload key.
 */
Builder (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmPrefixInfo.java)/**
   * Builder for OmPrefixInfo.
   */
OMRatisHelper (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OMRatisHelper.java)/**
 * Ratis helper methods for OM Ratis server and client.
 */
Builder (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmVolumeArgs.java)/**
   * Builder for OmVolumeArgs.
   */
OmVolumeArgs (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmVolumeArgs.java)/**
 * A class that encapsulates the OmVolumeArgs Args.
 */
OpenKeySession (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OpenKeySession.java)/**
 * This class represents a open key "session". A session here means a key is
 * opened by a specific client, the client sends the handler to server, such
 * that servers can recognize this client, and thus know how to close the key.
 */
OzoneAclUtil (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneAclUtil.java)/**
 * Helper class for ozone acls operations.
 */
OzoneFileStatus (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFileStatus.java)/**
 * File Status of the Ozone Key.
 */
OzoneFSUtils (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OzoneFSUtils.java)/**
 * Utility class for OzoneFileSystem.
 */
Builder (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/RepeatedOmKeyInfo.java)/**
   * Builder of RepeatedOmKeyInfo.
   */
RepeatedOmKeyInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/RepeatedOmKeyInfo.java)/**
 * Args for deleted keys. This is written to om metadata deletedTable.
 * Once a key is deleted, it is moved to om metadata deletedTable. Having a
 * {label: List<OMKeyInfo>} ensures that if users create & delete keys with
 * exact same uri multiple times, all the delete instances are bundled under
 * the same key name. This is useful as part of GDPR compliance where an
 * admin wants to confirm if a given key is deleted from deletedTable metadata.
 */
S3SecretValue (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/S3SecretValue.java)/**
 * S3Secret to be saved in database.
 */
Builder (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/ServiceInfo.java)/**
   * Builder used to build/construct {@link ServiceInfo}.
   */
ServiceInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/ServiceInfo.java)/**
 * ServiceInfo holds the config details of Ozone services.
 */
ServiceInfoEx (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/ServiceInfoEx.java)/**
 * Wrapper class for service discovery, design for broader usage such as
 * security, etc.
 */
VolumeArgs (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/VolumeArgs.java)/**
 * A class that encapsulates the createVolume Args.
 */
WithMetadata (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/WithMetadata.java)/**
 * Mixin class to handle custom metadata.
 */
OzoneManagerLockUtil (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/lock/OzoneManagerLockUtil.java)/**
 * Utility class contains helper functions required for OM lock.
 */
OMConfigKeys (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/OMConfigKeys.java)/**
 * Ozone Manager Constants.
 */
OMMetadataManager (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/OMMetadataManager.java)/**
 * OM metadata manager interface.
 */
OzoneManagerHAProtocol (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/protocol/OzoneManagerHAProtocol.java)/**
 * Protocol to talk to OM HA. These methods are needed only called from
 * OmRequestHandler.
 */
OzoneManagerProtocol (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/protocol/OzoneManagerProtocol.java)/**
 * Protocol to talk to OM.
 */
OzoneManagerSecurityProtocol (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/protocol/OzoneManagerSecurityProtocol.java)/**
 * Security protocol for a secure OzoneManager.
 */
OzoneManagerServerProtocol (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/protocol/OzoneManagerServerProtocol.java)/**
 * This will be used in the OzoneManager Server, as few of the methods in
 * OzoneManagerHAProtocol need not be exposed to Om clients. This interface
 * extends both OzoneManagerHAProtocol and OzoneManagerProtocol.
 */
OzoneManagerProtocolPB (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/protocolPB/OzoneManagerProtocolPB.java)/**
 * Protocol used to communicate with OM.
 */
S3SecretManager (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/S3SecretManager.java)/**
 * Interface to manager s3 secret.
 */
S3SecretManagerImpl (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/S3SecretManagerImpl.java)/**
 * S3 Secret manager.
 */
OmUtils (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/OmUtils.java)/**
 * Stateless helper functions for the server and client side of OM
 * communication.
 */
OzoneAcl (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/OzoneAcl.java)/**
 * OzoneACL classes define bucket ACLs used in OZONE.
 *
 * ACLs in Ozone follow this pattern.
 * <ul>
 * <li>user:name:rw
 * <li>group:name:rw
 * <li>world::rw
 * </ul>
 */
OzoneIllegalArgumentException (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/OzoneIllegalArgumentException.java)/**
 * Indicates that a method has been passed illegal or invalid argument. This
 * exception is thrown instead of IllegalArgumentException to differentiate the
 * exception thrown in Hadoop implementation from the one thrown in JDK.
 */
OMPBHelper (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/protocolPB/OMPBHelper.java)/**
 * Utilities for converting protobuf classes.
 */
OzonePBHelper (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/protocolPB/OzonePBHelper.java)/**
 * Helper class for converting protobuf objects.
 */
IAccessAuthorizer (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/acl/IAccessAuthorizer.java)/**
 * Public API for Ozone ACLs. Security providers providing support for Ozone
 * ACLs should implement this.
 */
IOzoneObj (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/acl/IOzoneObj.java)/**
 * Marker interface for objects supported by Ozone.
 * */
OzoneAccessAuthorizer (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/acl/OzoneAccessAuthorizer.java)/**
 * Default implementation for {@link IAccessAuthorizer}.
 * */
OzoneAclConfig (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/acl/OzoneAclConfig.java)/**
 * Ozone ACL config pojo.
 * */
OzoneObj (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/acl/OzoneObj.java)/**
 * Class representing an unique ozone object.
 * */
Builder (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/acl/OzoneObjInfo.java)/**
   * Inner builder class.
   */
OzoneObjInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/acl/OzoneObjInfo.java)/**
 * Class representing an ozone object.
 * It can be a volume with non-null volumeName (bucketName=null & name=null)
 * or a bucket with non-null volumeName and bucketName (name=null)
 * or a key with non-null volumeName, bucketName and key name
 * (via getKeyName)
 * or a prefix with non-null volumeName, bucketName and prefix name
 * (via getPrefixName)
 */
Builder (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/acl/RequestContext.java)/**
   * Builder class for @{@link RequestContext}.
   */
RequestContext (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/acl/RequestContext.java)/**
 * This class encapsulates information required for Ozone ACLs.
 * */
AWSV4AuthValidator (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/AWSV4AuthValidator.java)/**
 * AWS v4 authentication payload validator. For more details refer to AWS
 * documentation https://docs.aws.amazon.com/general/latest/gr/
 * sigv4-create-canonical-request.html.
 **/
GDPRSymmetricKey (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/GDPRSymmetricKey.java)/**
 * Symmetric Key structure for GDPR.
 */
OzoneBlockTokenSecretManager (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneBlockTokenSecretManager.java)/**
 * SecretManager for Ozone Master block tokens.
 */
OzoneDelegationTokenSecretManager (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneDelegationTokenSecretManager.java)/**
 * SecretManager for Ozone Master. Responsible for signing identifiers with
 * private key,
 */
OzoneDelegationTokenSelector (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneDelegationTokenSelector.java)/**
 * A delegation token selector that is specialized for Ozone.
 */
OzoneSecretKey (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneSecretKey.java)/**
 * Wrapper class for Ozone/Hdds secret keys. Used in delegation tokens and block
 * tokens.
 */
OzoneSecretManager (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneSecretManager.java)/**
 * SecretManager for Ozone Master. Responsible for signing identifiers with
 * private key,
 */
OzoneManagerSecretState (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneSecretStore.java)/**
   * Support class to maintain state of OzoneSecretStore.
   */
OzoneSecretStore (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneSecretStore.java)/**
 * SecretStore for Ozone Master.
 */
OzoneSecurityException (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneSecurityException.java)/**
 * Security exceptions thrown at Ozone layer.
 */
TokenInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneTokenIdentifier.java)/**
   * Class to encapsulate a token's renew date and password.
   */
OzoneTokenIdentifier (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneTokenIdentifier.java)/**
 * The token identifier for Ozone Master.
 */
BooleanBiFunction (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/util/BooleanBiFunction.java)/**
 * Defines a functional interface having two inputs and returns boolean as
 * output.
 */
OzoneVersionInfo (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/util/OzoneVersionInfo.java)/**
 * This class returns build information about Hadoop components.
 */
RadixNode (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/util/RadixNode.java)/**
 * Wrapper class for Radix tree node representing Ozone prefix path segment
 * separated by "/".
 */
RadixTree (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/util/RadixTree.java)/**
 * Wrapper class for handling Ozone prefix path lookup of ACL APIs
 * with radix tree.
 */
OzoneUtils (/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/web/utils/OzoneUtils.java)/**
 * Set of Utility functions used in ozone.
 */
TestOmMultipartKeyInfoCodec (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/om/codec/TestOmMultipartKeyInfoCodec.java)/**
 * This class tests OmMultipartKeyInfoCodec.
 */
TestOmPrefixInfoCodec (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/om/codec/TestOmPrefixInfoCodec.java)/**
 * This class test OmPrefixInfoCodec.
 */
TestS3SecretValueCodec (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/om/codec/TestS3SecretValueCodec.java)/**
 * This class test S3SecretValueCodec.
 */
TestResultCodes (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/om/exceptions/TestResultCodes.java)/**
 * Test code mappping.
 */
TestOmBucketInfo (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/om/helpers/TestOmBucketInfo.java)/**
 * Test BucketInfo.
 */
TestOmKeyInfo (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/om/helpers/TestOmKeyInfo.java)/**
 * Test OmKeyInfo.
 */
TestOmMultipartUpload (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/om/helpers/TestOmMultipartUpload.java)/**
 * Test utilities inside OmMutipartUpload.
 */
TestOzoneAclUtil (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/om/helpers/TestOzoneAclUtil.java)/**
 * Test for OzoneAcls utility class.
 */
ResourceInfo (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/om/lock/TestOzoneManagerLock.java)/**
   * Class used to store locked resource info.
   */
TestOzoneManagerLock (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/om/lock/TestOzoneManagerLock.java)/**
 * Class tests OzoneManagerLock.
 */
TestOzoneObjInfo (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/security/acl/TestOzoneObjInfo.java)/**
 * Test class for {@link OzoneObjInfo}.
 * */
TestAWSV4AuthValidator (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/security/TestAWSV4AuthValidator.java)/**
 * Test for {@link AWSV4AuthValidator}.
 * */
TestGDPRSymmetricKey (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/security/TestGDPRSymmetricKey.java)/**
 * Tests GDPRSymmetricKey structure.
 */
TestOzoneDelegationTokenSelector (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/security/TestOzoneDelegationTokenSelector.java)/**
 * Class to test OzoneDelegationTokenSelector.
 */
TestOmUtils (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/TestOmUtils.java)/**
 * Unit tests for {@link OmUtils}.
 */
TestOzoneAcls (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/TestOzoneAcls.java)/**
 * This class is to test acl storage and retrieval in ozone store.
 */
TestRadixTree (/hadoop-ozone/common/src/test/java/org/apache/hadoop/ozone/util/TestRadixTree.java)/**
 * Test Ozone Radix tree operations.
 */
ControllerService (/hadoop-ozone/csi/src/main/java/org/apache/hadoop/ozone/csi/ControllerService.java)/**
 * CSI controller service.
 * <p>
 * This service usually runs only once and responsible for the creation of
 * the volume.
 */
CsiConfig (/hadoop-ozone/csi/src/main/java/org/apache/hadoop/ozone/csi/CsiServer.java)/**
   * Configuration settings specific to the CSI server.
   */
CsiServer (/hadoop-ozone/csi/src/main/java/org/apache/hadoop/ozone/csi/CsiServer.java)/**
 * CLI entrypoint of the CSI service daemon.
 */
IdentitiyService (/hadoop-ozone/csi/src/main/java/org/apache/hadoop/ozone/csi/IdentitiyService.java)/**
 * Implementation of the CSI identity service.
 */
NodeService (/hadoop-ozone/csi/src/main/java/org/apache/hadoop/ozone/csi/NodeService.java)/**
 * Implementation of the CSI node service.
 */
BaseInsightPoint (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/BaseInsightPoint.java)/**
 * Default implementation of Insight point logic.
 */
BaseInsightSubCommand (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/BaseInsightSubCommand.java)/**
 * Parent class for all the insight subcommands.
 */
Component (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/Component.java)/**
 * Identifier an ozone component.
 */
ConfigurationSubCommand (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/ConfigurationSubCommand.java)/**
 * Subcommand to show configuration values/documentation.
 */
RatisInsight (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/datanode/RatisInsight.java)/**
 * Insight definition for datanode/pipline metrics.
 */
Insight (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/Insight.java)/**
 * Command line utility to check logs/metrics of internal ozone components.
 */
InsightPoint (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/InsightPoint.java)/**
 * Definition of a specific insight points.
 */
ListSubCommand (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/ListSubCommand.java)/**
 * Subcommand to list of the available insight points.
 */
LoggerSource (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/LoggerSource.java)/**
 * Definition of a log source.
 */
LogSubcommand (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/LogSubcommand.java)/**
 * Subcommand to display log.
 */
MetricDisplay (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/MetricDisplay.java)/**
 * Definition of one displayable hadoop metrics.
 */
MetricGroupDisplay (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/MetricGroupDisplay.java)/**
 * Definition of a group of metrics which can be displayed.
 */
MetricsSubCommand (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/MetricsSubCommand.java)/**
 * Command line interface to show metrics for a specific component.
 */
KeyManagerInsight (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/om/KeyManagerInsight.java)/**
 * Insight implementation for the key management related operations.
 */
OmProtocolInsight (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/om/OmProtocolInsight.java)/**
 * Insight definition for the OM RPC server.
 */
EventQueueInsight (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/scm/EventQueueInsight.java)/**
 * Insight definition to check internal events.
 */
NodeManagerInsight (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/scm/NodeManagerInsight.java)/**
 * Insight definition to check node manager / node report events.
 */
ReplicaManagerInsight (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/scm/ReplicaManagerInsight.java)/**
 * Insight definition to chech the replication manager internal state.
 */
ScmProtocolBlockLocationInsight (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/scm/ScmProtocolBlockLocationInsight.java)/**
 * Insight metric to check the SCM block location protocol behaviour.
 */
ScmProtocolContainerLocationInsight (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/scm/ScmProtocolContainerLocationInsight.java)/**
 * Insight metric to check the SCM block location protocol behaviour.
 */
ScmProtocolDatanodeInsight (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/scm/ScmProtocolDatanodeInsight.java)/**
 * Insight metric to check the SCM datanode protocol behaviour.
 */
ScmProtocolSecurityInsight (/hadoop-ozone/insight/src/main/java/org/apache/hadoop/ozone/insight/scm/ScmProtocolSecurityInsight.java)/**
 * Insight metric to check the SCM block location protocol behaviour.
 */
LogSubcommandTest (/hadoop-ozone/insight/src/test/java/org/apache/hadoop/ozone/insight/LogSubcommandTest.java)/**
 * Testing utility methods of the log subcommand test.
 */
TestSCMContainerManagerMetrics (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/container/metrics/TestSCMContainerManagerMetrics.java)/**
 * Class used to test {@link SCMContainerManagerMetrics}.
 */
TestContainerStateManagerIntegration (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/container/TestContainerStateManagerIntegration.java)/**
 * Tests for ContainerStateManager.
 */
TestNode2PipelineMap (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestNode2PipelineMap.java)/**
 * Test for the Node2Pipeline map.
 */
TestNodeFailure (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestNodeFailure.java)/**
 * Test Node failure detection and handling in Ratis.
 */
TestPipelineClose (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestPipelineClose.java)/**
 * Tests for Pipeline Closing.
 */
TestPipelineStateManager (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestPipelineStateManager.java)/**
 * Test for PipelineStateManager.
 */
TestRatisPipelineCreateAndDestroy (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestRatisPipelineCreateAndDestroy.java)/**
 * Tests for RatisPipelineUtils.
 */
TestRatisPipelineProvider (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestRatisPipelineProvider.java)/**
 * Test for RatisPipelineProvider.
 */
TestSCMPipelineManager (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestSCMPipelineManager.java)/**
 * Test cases to verify PipelineManager.
 */
TestSCMRestart (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestSCMRestart.java)/**
 * Test SCM restart and recovery wrt pipelines.
 */
TestSimplePipelineProvider (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestSimplePipelineProvider.java)/**
 * Test for SimplePipelineProvider.
 */
TestProbability (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/chaos/TestProbability.java)/**
 * This class is used to find out if a certain event is true.
 * Every event is assigned a propbability and the isTrue function returns true
 * when the probability has been met.
 */
Test2WayCommitInRatis (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/Test2WayCommitInRatis.java)/**
 * This class tests the 2 way commit in Ratis.
 */
TestBCSID (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestBCSID.java)/**
 * Tests the validity BCSID of a container.
 */
TestBlockOutputStream (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestBlockOutputStream.java)/**
 * Tests BlockOutputStream class.
 */
TestBlockOutputStreamWithFailures (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestBlockOutputStreamWithFailures.java)/**
 * Tests failure detection and handling in BlockOutputStream Class.
 */
TestCloseContainerHandlingByClient (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestCloseContainerHandlingByClient.java)/**
 * Tests Close Container Exception handling by Ozone Client.
 */
TestCommitWatcher (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestCommitWatcher.java)/**
 * Class to test CommitWatcher functionality.
 */
TestContainerReplicationEndToEnd (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestContainerReplicationEndToEnd.java)/**
 * Tests delete key operation with a slow follower in the datanode
 * pipeline.
 */
TestDeleteWithSlowFollower (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestDeleteWithSlowFollower.java)/**
 * Tests delete key operation with a slow follower in the datanode
 * pipeline.
 */
TestFailureHandlingByClient (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestFailureHandlingByClient.java)/**
 * Tests Close Container Exception handling by Ozone Client.
 */
TestHybridPipelineOnDatanode (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestHybridPipelineOnDatanode.java)/**
 * Tests Hybrid Pipeline Creation and IO on same set of Datanodes.
 */
TestKeyInputStream (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java)/**
 * Tests {@link KeyInputStream}.
 */
TestMultiBlockWritesWithDnFailures (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestMultiBlockWritesWithDnFailures.java)/**
 * Tests MultiBlock Writes with Dn failures by Ozone Client.
 */
TestOzoneAtRestEncryption (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneAtRestEncryption.java)/**
 * This class is to test all the public facing APIs of Ozone Client.
 */
TestOzoneClientRetriesOnException (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneClientRetriesOnException.java)/**
 * Tests failure detection and handling in BlockOutputStream Class.
 */
TestOzoneRpcClient (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneRpcClient.java)/**
 * This class is to test all the public facing APIs of Ozone Client.
 */
TestOzoneRpcClientAbstract (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneRpcClientAbstract.java)/**
 * This is an abstract class to test all the public facing APIs of Ozone
 * Client, w/o OM Ratis server.
 * {@link TestOzoneRpcClient} tests the Ozone Client by submitting the
 * requests directly to OzoneManager. {@link TestOzoneRpcClientWithRatis}
 * tests the Ozone Client by submitting requests to OM's Ratis server.
 */
TestOzoneRpcClientForAclAuditLog (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneRpcClientForAclAuditLog.java)/**
 * This class is to test audit logs for xxxACL APIs of Ozone Client.
 * It is annotated as NotThreadSafe intentionally since this test reads from
 * the generated audit logs to verify the operations. Since the
 * maven test plugin will trigger parallel test execution, there is a
 * possibility of other audit events being logged and leading to failure of
 * all assertion based test in this class.
 */
TestOzoneRpcClientWithRatis (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneRpcClientWithRatis.java)/**
 * This class is to test all the public facing APIs of Ozone Client with an
 * active OM Ratis server.
 */
TestReadRetries (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestReadRetries.java)/**
 * Test read retries from multiple nodes in the pipeline.
 */
TestSecureOzoneRpcClient (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestSecureOzoneRpcClient.java)/**
 * This class is to test all the public facing APIs of Ozone Client.
 */
TestWatchForCommit (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestWatchForCommit.java)/**
 * This class verifies the watchForCommit Handling by xceiverClient.
 */
TestBlockData (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/common/helpers/TestBlockData.java)/**
 * Tests to test block deleting service.
 */
TestContainerDeletionChoosingPolicy (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestContainerDeletionChoosingPolicy.java)/**
 * The class for testing container deletion choosing policy.
 */
TestContainerPersistence (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestContainerPersistence.java)/**
 * Simple tests to verify that container persistence works as expected. Some of
 * these tests are specific to {@link KeyValueContainer}. If a new {@link
 * ContainerProtos.ContainerType} is added, the tests need to be modified.
 */
TestBlockDeletion (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/TestBlockDeletion.java)/**
 * Tests for Block deletion.
 */
TestCloseContainerByPipeline (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/TestCloseContainerByPipeline.java)/**
 * Test container closing.
 */
TestCloseContainerHandler (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/TestCloseContainerHandler.java)/**
 * Test to behaviour of the datanode when receive close container command.
 */
TestDeleteContainerHandler (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/TestDeleteContainerHandler.java)/**
 * Tests DeleteContainerCommand Handler.
 */
TestBlockDeletingService (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/common/TestBlockDeletingService.java)/**
 * Tests to test block deleting service.
 */
TestCSMMetrics (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/TestCSMMetrics.java)/**
 * This class tests the metrics of ContainerStateMachine.
 */
ContainerTestHelper (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ContainerTestHelper.java)/**
 * Helpers for container tests.
 */
TestContainerMetrics (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/metrics/TestContainerMetrics.java)/**
 * Test for metrics published by storage containers.
 */
TestOzoneContainer (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestOzoneContainer.java)/**
 * Tests ozone containers.
 */
TestOzoneContainerRatis (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestOzoneContainerRatis.java)/**
 * Tests ozone containers with Apache Ratis.
 */
TestOzoneContainerWithTLS (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestOzoneContainerWithTLS.java)/**
 * Tests ozone containers via secure grpc/netty.
 */
TestRatisManager (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestRatisManager.java)/**
 * Tests ozone containers with Apache Ratis.
 */
TestSecureOzoneContainer (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestSecureOzoneContainer.java)/**
 * Tests ozone containers via secure grpc/netty.
 */
TestContainerServer (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestContainerServer.java)/**
 * Test Containers.
 */
TestSecureContainerServer (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestSecureContainerServer.java)/**
 * Test Container servers when security is enabled.
 */
TestContainerReplication (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/TestContainerReplication.java)/**
 * Tests ozone containers replication.
 */
TestDataScrubber (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/dn/scrubber/TestDataScrubber.java)/**
 * This class tests the data scrubber functionality.
 */
Builder (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneChaosCluster.java)/**
   * Builder for configuring the MiniOzoneChaosCluster to run.
   */
MiniOzoneChaosCluster (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneChaosCluster.java)/**
 * This class causes random failures in the chaos cluster.
 */
Builder (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneCluster.java)/**
   * Builder class for MiniOzoneCluster.
   */
MiniOzoneCluster (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneCluster.java)/**
 * Interface used for MiniOzoneClusters.
 */
Builder (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneClusterImpl.java)/**
   * Builder for configuring the MiniOzoneCluster to run.
   */
MiniOzoneClusterImpl (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneClusterImpl.java)/**
 * MiniOzoneCluster creates a complete in-process Ozone cluster suitable for
 * running tests.  The cluster consists of a OzoneManager,
 * StorageContainerManager and multiple DataNodes.
 */
Builder (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneHAClusterImpl.java)/**
   * Builder for configuring the MiniOzoneCluster to run.
   */
MiniOzoneHAClusterImpl (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneHAClusterImpl.java)/**
 * MiniOzoneHAClusterImpl creates a complete in-process Ozone cluster
 * with OM HA suitable for running tests.  The cluster consists of a set of
 * OzoneManagers, StorageContainerManager and multiple DataNodes.
 */
MiniOzoneLoadGenerator (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneLoadGenerator.java)/**
 * A Simple Load generator for testing.
 */
TestOMRatisSnapshotInfo (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/snapshot/TestOMRatisSnapshotInfo.java)/**
 * Tests {@link org.apache.hadoop.ozone.om.ratis.OMRatisSnapshotInfo}.
 */
TestOzoneManagerSnapshotProvider (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/snapshot/TestOzoneManagerSnapshotProvider.java)/**
 * Test OM's snapshot provider service.
 */
TestContainerReportWithKeys (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestContainerReportWithKeys.java)/**
 * This class tests container report with DN container state info.
 */
TestKeyManagerImpl (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestKeyManagerImpl.java)/**
 * Test class for @{@link KeyManagerImpl}.
 */
TestKeyPurging (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestKeyPurging.java)/**
 * Test OM's {@link KeyDeletingService}.
 */
OzoneAccessAuthorizerTest (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOmAcls.java)/**
   * Test implementation to negative case.
   */
TestOmAcls (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOmAcls.java)/**
 * Test for Ozone Manager ACLs.
 */
TestOmBlockVersioning (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOmBlockVersioning.java)/**
 * This class tests the versioning of blocks from OM side.
 */
TestOMDbCheckpointServlet (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOMDbCheckpointServlet.java)/**
 * Class used for testing the OM DB Checkpoint provider servlet.
 */
TestOmInit (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOmInit.java)/**
 * Test Ozone Manager Init.
 */
TestOmMetrics (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOmMetrics.java)/**
 * Test for OM metrics.
 */
TestOMRatisSnapshots (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOMRatisSnapshots.java)/**
 * Tests the Ratis snaphsots feature in OM.
 */
TestOzoneManagerConfiguration (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOzoneManagerConfiguration.java)/**
 * Tests OM related configurations.
 */
TestOzoneManagerHA (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOzoneManagerHA.java)/**
 * Test Ozone Manager operation in distributed handler scenario.
 */
TestOzoneManagerRestart (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOzoneManagerRestart.java)/**
 * Test some client operations after cluster starts. And perform restart and
 * then performs client operations and check the behavior is expected or not.
 */
TestOzoneManagerRestInterface (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOzoneManagerRestInterface.java)/**
 * This class is to test the REST interface exposed by OzoneManager.
 */
TestOzoneManagerRocksDBLogging (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOzoneManagerRocksDBLogging.java)/**
 * Test RocksDB logging for Ozone Manager.
 */
TestScmSafeMode (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestScmSafeMode.java)/**
 * Test Ozone Manager operation in distributed handler scenario.
 */
TestSecureOzoneManager (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestSecureOzoneManager.java)/**
 * Test secure Ozone Manager operation in distributed handler scenario.
 */
OzoneTestUtils (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/OzoneTestUtils.java)/**
 * Helper class for Tests.
 */
TestOzoneDatanodeShell (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/ozShell/TestOzoneDatanodeShell.java)/**
 * This test class specified for testing Ozone datanode shell command.
 */
TestOzoneShellHA (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/ozShell/TestOzoneShellHA.java)/**
 * This class tests Ozone sh shell command.
 * Inspired by TestS3Shell
 */
TestS3Shell (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/ozShell/TestS3Shell.java)/**
 * This test class specified for testing Ozone s3Shell command.
 */
RatisTestSuite (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/RatisTestHelper.java)/** For testing Ozone with Ratis. */
RatisTestHelper (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/RatisTestHelper.java)/**
 * Helpers for Ratis tests.
 */
TestQueryNode (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/node/TestQueryNode.java)/**
 * Test Query Node Operation.
 */
TestSCMNodeMetrics (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/node/TestSCMNodeMetrics.java)/**
 * Test cases to verify the metrics exposed by SCMNodeManager.
 */
TestPipelineManagerMXBean (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/pipeline/TestPipelineManagerMXBean.java)/**
 * Test cases to verify the metrics exposed by SCMPipelineManager via MXBean.
 */
TestSCMPipelineMetrics (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/pipeline/TestSCMPipelineMetrics.java)/**
 * Test cases to verify the metrics exposed by SCMPipelineManager.
 */
TestAllocateContainer (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/TestAllocateContainer.java)/**
 * Test allocate container calls.
 */
TestContainerSmallFile (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/TestContainerSmallFile.java)/**
 * Test Container calls.
 */
TestGetCommittedBlockLengthAndPutKey (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/TestGetCommittedBlockLengthAndPutKey.java)/**
 * Test Container calls.
 */
TestSCMContainerPlacementPolicyMetrics (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/TestSCMContainerPlacementPolicyMetrics.java)/**
 * Test cases to verify the metrics exposed by SCMPipelineManager.
 */
TestSCMMXBean (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/TestSCMMXBean.java)/**
 *
 * This class is to test JMX management interface for scm information.
 */
TestSCMNodeManagerMXBean (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/TestSCMNodeManagerMXBean.java)/**
 * Class which tests the SCMNodeManagerInfo Bean.
 */
TestXceiverClientManager (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/TestXceiverClientManager.java)/**
 * Test for XceiverClientManager caching and eviction.
 */
TestXceiverClientMetrics (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/scm/TestXceiverClientMetrics.java)/**
 * This class tests the metrics of XceiverClient.
 */
TestOzoneNativeAuthorizer (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/security/acl/TestOzoneNativeAuthorizer.java)/**
 * Test class for {@link OzoneNativeAuthorizer}.
 */
TestContainerOperations (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/TestContainerOperations.java)/**
 * This class tests container operations (TODO currently only supports create)
 * from cblock clients.
 */
TestContainerStateMachineIdempotency (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/TestContainerStateMachineIdempotency.java)/**
 * Tests the idempotent operations in ContainerStateMachine.
 */
TestDataUtil (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/TestDataUtil.java)/**
 * Utility to help to generate test data.
 */
TestMiniChaosOzoneCluster (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/TestMiniChaosOzoneCluster.java)/**
 * Test Read Write with Mini Ozone Chaos Cluster.
 */
TestMiniOzoneCluster (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/TestMiniOzoneCluster.java)/**
 * Test cases for mini ozone cluster.
 */
TestOzoneConfigurationFields (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/TestOzoneConfigurationFields.java)/**
 * Tests if configuration constants documented in ozone-defaults.xml.
 */
TestSecureOzoneCluster (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/TestSecureOzoneCluster.java)/**
 * Test class to for security enabled Ozone cluster.
 */
TestStorageContainerManager (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/TestStorageContainerManager.java)/**
 * Test class that exercises the StorageContainerManager.
 */
TestStorageContainerManagerHelper (/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/TestStorageContainerManagerHelper.java)/**
 * A helper class used by {@link TestStorageContainerManager} to generate
 * some keys and helps to verify containers and blocks locations.
 */
BucketManager (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/BucketManager.java)/**
 * BucketManager handles all the bucket level operations.
 */
BucketManagerImpl (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/BucketManagerImpl.java)/**
 * OM bucket manager.
 */
OzoneManagerFS (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/fs/OzoneManagerFS.java)/**
 * Ozone Manager FileSystem interface.
 */
OMHANodeDetails (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ha/OMHANodeDetails.java)/**
 * Class which maintains peer information and it's own OM node information.
 */
Builder (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ha/OMNodeDetails.java)/**
   * Builder class for OMNodeDetails.
   */
OMNodeDetails (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ha/OMNodeDetails.java)/**
 * This class stores OM node details.
 */
IOzoneAcl (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/IOzoneAcl.java)/**
 * Interface for Ozone Acl management.
 */
KeyDeletingTask (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyDeletingService.java)/**
   * A key deleting task scans OM DB and looking for a certain number of
   * pending-deletion keys, sends these keys along with their associated blocks
   * to SCM for deletion. Once SCM confirms keys are deleted (once SCM persisted
   * the blocks info in its deletedBlockLog), it removes these keys from the
   * DB.
   */
KeyDeletingService (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyDeletingService.java)/**
 * This is the background service to delete keys. Scan the metadata of om
 * periodically to get the keys from DeletedTable and ask scm to delete
 * metadata accordingly, if scm returns success for keys, then clean up those
 * keys.
 */
KeyManager (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManager.java)/**
 * Handles key level commands.
 */
KeyManagerImpl (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java)/**
 * Implementation of keyManager.
 */
OMDBCheckpointServlet (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMDBCheckpointServlet.java)/**
 * Provides the current checkpoint Snapshot of the OM DB. (tar.gz)
 */
OmMetadataManagerImpl (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OmMetadataManagerImpl.java)/**
 * Ozone metadata manager interface.
 */
OMMetrics (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMMetrics.java)/**
 * This class is for maintaining Ozone Manager statistics.
 */
OmMetricsInfo (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OmMetricsInfo.java)/**
 * OmMetricsInfo stored in a file, which will be used during OM restart to
 * initialize the metrics. Currently this stores only numKeys.
 */
OMMXBean (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMMXBean.java)/**
 * This is the JMX management interface for OM information.
 */
OMPolicyProvider (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMPolicyProvider.java)/**
 * {@link PolicyProvider} for OM protocols.
 */
OMStarterInterface (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMStarterInterface.java)/**
 * This interface is used by the OzoneManagerStarter class to allow the
 * dependencies to be injected to the CLI class.
 */
OMStorage (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMStorage.java)/**
 * OMStorage is responsible for management of the StorageDirectories used by
 * the Ozone Manager.
 */
OpenKeyCleanupService (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OpenKeyCleanupService.java)/**
 * This is the background service to delete hanging open keys.
 * Scan the metadata of om periodically to get
 * the keys with prefix "#open#" and ask scm to
 * delete metadata accordingly, if scm returns
 * success for keys, then clean up those keys.
 */
ScheduleOMMetricsWriteTask (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java)/**
   * Class which schedule saving metrics to a file.
   */
OzoneManager (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java)/**
 * Ozone Manager is the metadata manager of ozone.
 */
OzoneManagerHttpServer (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManagerHttpServer.java)/**
 * HttpServer wrapper for the OzoneManager.
 */
OMStarterHelper (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManagerStarter.java)/**
   * This static class wraps the external dependencies needed for this command
   * to execute its tasks. This allows the dependency to be injected for unit
   * testing.
   */
OzoneManagerStarter (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManagerStarter.java)/**
 * This class provides a command line interface to start the OM
 * using Picocli.
 */
PrefixManager (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/PrefixManager.java)/**
 * Handles prefix commands.
 * //TODO: support OzoneManagerFS for ozfs optimization using prefix tree.
 */
OMPrefixAclOpResult (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/PrefixManagerImpl.java)/**
   * Result of the prefix acl operation.
   */
PrefixManagerImpl (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/PrefixManagerImpl.java)/**
 * Implementation of PrefixManager.
 */
DoubleBufferEntry (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/helpers/DoubleBufferEntry.java)/**
 * Entry in OzoneManagerDouble Buffer.
 * @param <Response>
 */
OzoneManagerDoubleBufferMetrics (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/metrics/OzoneManagerDoubleBufferMetrics.java)/**
 * Class which maintains metrics related to OzoneManager DoubleBuffer.
 */
RatisSnapshotYaml (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OMRatisSnapshotInfo.java)/**
   * Ratis Snapshot details to be written to the yaml file.
   */
OMRatisSnapshotInfo (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OMRatisSnapshotInfo.java)/**
 * This class captures the snapshotIndex and term of the latest snapshot in
 * the OM.
 * Ratis server loads the snapshotInfo during startup and updates the
 * lastApplied index to this snapshotIndex. OM SnapshotInfo does not contain
 * any files. It is used only to store/ update the last applied index and term.
 */
OzoneManagerDoubleBuffer (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerDoubleBuffer.java)/**
 * This class implements DoubleBuffer implementation of OMClientResponse's. In
 * DoubleBuffer it has 2 buffers one is currentBuffer and other is
 * readyBuffer. The current OM requests will be always added to currentBuffer.
 * Flush thread will be running in background, it check's if currentBuffer has
 * any entries, it swaps the buffer and creates a batch and commit to DB.
 * Adding OM request to doubleBuffer and swap of buffer are synchronized
 * methods.
 *
 */
OzoneManagerRatisClient (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerRatisClient.java)/**
 * OM Ratis client to interact with OM Ratis server endpoint.
 */
OzoneManagerRatisServer (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerRatisServer.java)/**
 * Creates a Ratis server endpoint for OM.
 */
OzoneManagerStateMachine (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerStateMachine.java)/**
 * The OM StateMachine is the state machine for OM Ratis server. It is
 * responsible for applying ratis committed transactions to
 * {@link OzoneManager}.
 */
OzoneManagerDoubleBufferHelper (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/utils/OzoneManagerDoubleBufferHelper.java)/**
 * Helper interface for OzoneManagerDoubleBuffer.
 *
 */
OzoneManagerRatisUtils (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/utils/OzoneManagerRatisUtils.java)/**
 * Utility class used by OzoneManager HA.
 */
OMBucketAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/bucket/acl/OMBucketAclRequest.java)/**
 * Base class for Bucket acl request.
 */
OMBucketAddAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/bucket/acl/OMBucketAddAclRequest.java)/**
 * Handle add Acl request for bucket.
 */
OMBucketRemoveAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/bucket/acl/OMBucketRemoveAclRequest.java)/**
 * Handle removeAcl request for bucket.
 */
OMBucketSetAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/bucket/acl/OMBucketSetAclRequest.java)/**
 * Handle setAcl request for bucket.
 */
OMBucketCreateRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/bucket/OMBucketCreateRequest.java)/**
 * Handles CreateBucket Request.
 */
OMBucketDeleteRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/bucket/OMBucketDeleteRequest.java)/**
 * Handles DeleteBucket Request.
 */
OMBucketSetPropertyRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/bucket/OMBucketSetPropertyRequest.java)/**
 * Handle SetBucketProperty Request.
 */
OMDirectoryCreateRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/file/OMDirectoryCreateRequest.java)/**
 * Handle create directory request.
 */
OMFileCreateRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/file/OMFileCreateRequest.java)/**
 * Handles create file request.
 */
OMFileRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/file/OMFileRequest.java)/**
 * Base class for file requests.
 */
OMKeyAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/acl/OMKeyAclRequest.java)/**
 * Base class for Bucket acl request.
 */
OMKeyAddAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/acl/OMKeyAddAclRequest.java)/**
 * Handle add Acl request for bucket.
 */
OMKeyRemoveAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/acl/OMKeyRemoveAclRequest.java)/**
 * Handle add Acl request for bucket.
 */
OMKeySetAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/acl/OMKeySetAclRequest.java)/**
 * Handle add Acl request for bucket.
 */
OMPrefixAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/acl/prefix/OMPrefixAclRequest.java)/**
 * Base class for Prefix acl request.
 */
OMPrefixAddAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/acl/prefix/OMPrefixAddAclRequest.java)/**
 * Handle add Acl request for prefix.
 */
OMPrefixRemoveAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/acl/prefix/OMPrefixRemoveAclRequest.java)/**
 * Handle add Acl request for prefix.
 */
OMPrefixSetAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/acl/prefix/OMPrefixSetAclRequest.java)/**
 * Handle add Acl request for prefix.
 */
OMAllocateBlockRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMAllocateBlockRequest.java)/**
 * Handles allocate block request.
 */
OMKeyCommitRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMKeyCommitRequest.java)/**
 * Handles CommitKey request.
 */
OMKeyDeleteRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMKeyDeleteRequest.java)/**
 * Handles DeleteKey request.
 */
OMKeyPurgeRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMKeyPurgeRequest.java)/**
 * Handles purging of keys from OM DB.
 */
OMKeyRenameRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMKeyRenameRequest.java)/**
 * Handles rename key request.
 */
OMKeyRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/key/OMKeyRequest.java)/**
 * Interface for key write requests.
 */
OMClientRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/OMClientRequest.java)/**
 * OMClientRequest provides methods which every write OM request should
 * implement.
 */
RequestAuditor (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/RequestAuditor.java)/**
 * Interface for OM Requests to convert to audit objects.
 */
S3BucketCreateRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/s3/bucket/S3BucketCreateRequest.java)/**
 * Handles S3 Bucket create request.
 */
S3BucketDeleteRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/s3/bucket/S3BucketDeleteRequest.java)/**
 * Handle Create S3Bucket request.
 */
S3InitiateMultipartUploadRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/s3/multipart/S3InitiateMultipartUploadRequest.java)/**
 * Handles initiate multipart upload request.
 */
S3MultipartUploadAbortRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/s3/multipart/S3MultipartUploadAbortRequest.java)/**
 * Handles Abort of multipart upload request.
 */
S3MultipartUploadCommitPartRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/s3/multipart/S3MultipartUploadCommitPartRequest.java)/**
 * Handle Multipart upload commit upload part file.
 */
S3MultipartUploadCompleteRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/s3/multipart/S3MultipartUploadCompleteRequest.java)/**
 * Handle Multipart upload complete request.
 */
S3GetSecretRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/s3/security/S3GetSecretRequest.java)/**
 * Handles GetS3Secret request.
 */
OMCancelDelegationTokenRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/security/OMCancelDelegationTokenRequest.java)/**
 * Handle CancelDelegationToken Request.
 */
OMGetDelegationTokenRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/security/OMGetDelegationTokenRequest.java)/**
 * Handle GetDelegationToken Request.
 */
OMRenewDelegationTokenRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/security/OMRenewDelegationTokenRequest.java)/**
 * Handle RenewDelegationToken Request.
 */
ObjectParser (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/util/ObjectParser.java)/**
 * Utility class to parse {@link OzoneObj#getPath()}.
 */
OMVolumeAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/acl/OMVolumeAclRequest.java)/**
 * Base class for OMVolumeAcl Request.
 */
OMVolumeAddAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/acl/OMVolumeAddAclRequest.java)/**
 * Handles volume add acl request.
 */
OMVolumeRemoveAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/acl/OMVolumeRemoveAclRequest.java)/**
 * Handles volume remove acl request.
 */
OMVolumeSetAclRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/acl/OMVolumeSetAclRequest.java)/**
 * Handles volume set acl request.
 */
OMVolumeCreateRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/OMVolumeCreateRequest.java)/**
 * Handles volume create request.
 */
OMVolumeDeleteRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/OMVolumeDeleteRequest.java)/**
 * Handles volume delete request.
 */
OMVolumeRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/OMVolumeRequest.java)/**
 * Defines common methods required for volume requests.
 */
OMVolumeSetOwnerRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/OMVolumeSetOwnerRequest.java)/**
 * Handle set owner request for volume.
 */
OMVolumeSetQuotaRequest (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/volume/OMVolumeSetQuotaRequest.java)/**
 * Handles set Quota request for volume.
 */
OMBucketAclResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/bucket/acl/OMBucketAclResponse.java)/**
 * Response for Bucket acl request.
 */
OMBucketCreateResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/bucket/OMBucketCreateResponse.java)/**
 * Response for CreateBucket request.
 */
OMBucketDeleteResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/bucket/OMBucketDeleteResponse.java)/**
 * Response for DeleteBucket request.
 */
OMBucketSetPropertyResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/bucket/OMBucketSetPropertyResponse.java)/**
 * Response for SetBucketProperty request.
 */
OMDirectoryCreateResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/file/OMDirectoryCreateResponse.java)/**
 * Response for create directory request.
 */
OMFileCreateResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/file/OMFileCreateResponse.java)/**
 * Response for crate file request.
 */
OMKeyAclResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/key/acl/OMKeyAclResponse.java)/**
 * Response for Bucket acl request.
 */
OMPrefixAclResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/key/acl/prefix/OMPrefixAclResponse.java)/**
 * Response for Prefix Acl request.
 */
OMAllocateBlockResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/key/OMAllocateBlockResponse.java)/**
 * Response for AllocateBlock request.
 */
OMKeyCommitResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/key/OMKeyCommitResponse.java)/**
 * Response for CommitKey request.
 */
OMKeyCreateResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/key/OMKeyCreateResponse.java)/**
 * Response for CreateKey request.
 */
OMKeyDeleteResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/key/OMKeyDeleteResponse.java)/**
 * Response for DeleteKey request.
 */
OMKeyPurgeResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/key/OMKeyPurgeResponse.java)/**
 * Response for {@link OMKeyPurgeRequest} request.
 */
OMKeyRenameResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/key/OMKeyRenameResponse.java)/**
 * Response for RenameKey request.
 */
OMClientResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/OMClientResponse.java)/**
 * Interface for OM Responses, each OM response should implement this interface.
 */
S3BucketCreateResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/s3/bucket/S3BucketCreateResponse.java)/**
 * Response for S3Bucket create request.
 */
S3BucketDeleteResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/s3/bucket/S3BucketDeleteResponse.java)/**
 * Response for S3Bucket Delete request.
 */
S3InitiateMultipartUploadResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/s3/multipart/S3InitiateMultipartUploadResponse.java)/**
 * Response for S3 Initiate Multipart Upload request.
 */
S3MultipartUploadAbortResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/s3/multipart/S3MultipartUploadAbortResponse.java)/**
 * Response for Multipart Abort Request.
 */
S3MultipartUploadCommitPartResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/s3/multipart/S3MultipartUploadCommitPartResponse.java)/**
 * Response for S3MultipartUploadCommitPart request.
 */
S3MultipartUploadCompleteResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/s3/multipart/S3MultipartUploadCompleteResponse.java)/**
 * Response for Multipart Upload Complete request.
 */
S3GetSecretResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/s3/security/S3GetSecretResponse.java)/**
 * Response for GetS3Secret request.
 */
OMCancelDelegationTokenResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/security/OMCancelDelegationTokenResponse.java)/**
 * Handle response for CancelDelegationToken request.
 */
OMGetDelegationTokenResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/security/OMGetDelegationTokenResponse.java)/**
 * Handle response for GetDelegationToken request.
 */
OMRenewDelegationTokenResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/security/OMRenewDelegationTokenResponse.java)/**
 * Handle response for RenewDelegationToken request.
 */
OMVolumeAclOpResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/volume/OMVolumeAclOpResponse.java)/**
 * Response for om volume acl operation request.
 */
OMVolumeCreateResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/volume/OMVolumeCreateResponse.java)/**
 * Response for CreateBucket request.
 */
OMVolumeDeleteResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/volume/OMVolumeDeleteResponse.java)/**
 * Response for CreateVolume request.
 */
OMVolumeSetOwnerResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/volume/OMVolumeSetOwnerResponse.java)/**
 * Response for set owner request.
 */
OMVolumeSetQuotaResponse (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/response/volume/OMVolumeSetQuotaResponse.java)/**
 * Response for set quota request.
 */
S3BucketManager (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/S3BucketManager.java)/**
 * An interface that maps S3 buckets to Ozone
 * volume/bucket.
 */
S3BucketManagerImpl (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/S3BucketManagerImpl.java)/**
 * S3 Bucket Manager, this class maintains a mapping between S3 Bucket and Ozone
 * Volume/bucket.
 */
ScmClient (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ScmClient.java)/**
 * Wrapper class for Scm protocol clients.
 */
ServiceListJSONServlet (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ServiceListJSONServlet.java)/**
 * Provides REST access to Ozone Service List.
 * <p>
 * This servlet generally will be placed under the /serviceList URL of
 * OzoneManager HttpServer.
 *
 * The return format is of JSON and in the form
 * <p>
 *  <pre><code>
 *  {
 *    "services" : [
 *      {
 *        "NodeType":"OM",
 *        "Hostname" "$hostname",
 *        "ports" : {
 *          "$PortType" : "$port",
 *          ...
 *        }
 *      }
 *    ]
 *  }
 *  </code></pre>
 *  <p>
 *
 */
OzoneManagerSnapshotProvider (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/snapshot/OzoneManagerSnapshotProvider.java)/**
 * OzoneManagerSnapshotProvider downloads the latest checkpoint from the
 * leader OM and loads the checkpoint into State Machine.
 */
VolumeManager (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/VolumeManager.java)/**
 * OM volume manager interface.
 */
VolumeManagerImpl (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/VolumeManagerImpl.java)/**
 * OM volume management code.
 */
OzoneManagerHARequestHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/protocolPB/OzoneManagerHARequestHandler.java)/**
 * Handler to handle OM requests in OM HA.
 */
OzoneManagerHARequestHandlerImpl (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/protocolPB/OzoneManagerHARequestHandlerImpl.java)/**
 * Command Handler for OM requests. OM State Machine calls this handler for
 * deserializing the client request and sending it to OM.
 */
OzoneManagerProtocolServerSideTranslatorPB (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/protocolPB/OzoneManagerProtocolServerSideTranslatorPB.java)/**
 * This class is the server-side translator that forwards requests received on
 * {@link OzoneManagerProtocolPB}
 * to the OzoneManagerService server implementation.
 */
OzoneManagerRequestHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/protocolPB/OzoneManagerRequestHandler.java)/**
 * Command Handler for OM requests. OM State Machine calls this handler for
 * deserializing the client request and sending it to OM.
 */
RequestHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/protocolPB/RequestHandler.java)/**
 * Handler to handle the OmRequests.
 */
OzoneNativeAuthorizer (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/security/acl/OzoneNativeAuthorizer.java)/**
 * Public API for Ozone ACLs. Security providers providing support for Ozone
 * ACLs should implement this.
 */
AddAclBucketHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/bucket/AddAclBucketHandler.java)/**
 * Add acl handler for bucket.
 */
BucketCommands (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/bucket/BucketCommands.java)/**
 * Subcommands for the bucket related operations.
 */
CreateBucketHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/bucket/CreateBucketHandler.java)/**
 * create bucket handler.
 */
DeleteBucketHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/bucket/DeleteBucketHandler.java)/**
 * Delete bucket Handler.
 */
GetAclBucketHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/bucket/GetAclBucketHandler.java)/**
 * Get acl handler for bucket.
 */
InfoBucketHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/bucket/InfoBucketHandler.java)/**
 * Executes Info bucket.
 */
ListBucketHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/bucket/ListBucketHandler.java)/**
 * Executes List Bucket.
 */
RemoveAclBucketHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/bucket/RemoveAclBucketHandler.java)/**
 * Executes Info bucket.
 */
S3BucketMapping (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/bucket/S3BucketMapping.java)/**
 * S3Bucket mapping handler, which returns volume name and Ozone fs uri for
 * that bucket.
 */
SetAclBucketHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/bucket/SetAclBucketHandler.java)/**
 * Set acl handler for bucket.
 */
Handler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/Handler.java)/**
 * Common interface for command handling.
 */
AddAclKeyHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/keys/AddAclKeyHandler.java)/**
 * Add  acl handler for key.
 */
DeleteKeyHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/keys/DeleteKeyHandler.java)/**
 * Executes Delete Key.
 */
GetAclKeyHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/keys/GetAclKeyHandler.java)/**
 * Get acl handler for Key.
 */
GetKeyHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/keys/GetKeyHandler.java)/**
 * Gets an existing key.
 */
InfoKeyHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/keys/InfoKeyHandler.java)/**
 * Executes Info Object.
 */
KeyCommands (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/keys/KeyCommands.java)/**
 * Subcommand to group key related operations.
 */
ListKeyHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/keys/ListKeyHandler.java)/**
 * Executes List Keys.
 */
PutKeyHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/keys/PutKeyHandler.java)/**
 * Puts a file into an ozone bucket.
 */
RemoveAclKeyHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/keys/RemoveAclKeyHandler.java)/**
 * Remove acl handler for key.
 */
RenameKeyHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/keys/RenameKeyHandler.java)/**
 * Renames an existing key.
 */
SetAclKeyHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/keys/SetAclKeyHandler.java)/**
 * Set acl handler for Key.
 */
ObjectPrinter (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/ObjectPrinter.java)/**
 * Utility to print out response object in human readable form.
 */
OzoneAddress (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/OzoneAddress.java)/**
 * Address of an ozone object for ozone shell.
 */
OzoneShell (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/OzoneShell.java)/**
 * Shell commands for native rpc object manipulation.
 */
GetS3SecretHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/s3/GetS3SecretHandler.java)/**
 * Executes getsecret calls.
 */
S3Shell (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/s3/S3Shell.java)/**
 * Shell for s3 related operations.
 */
Shell (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/Shell.java)/**
 * Ozone user interface commands.
 *
 * This class uses dispatch method to make calls
 * to appropriate handlers that execute the ozone functions.
 */
CancelTokenHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/token/CancelTokenHandler.java)/**
 * Executes cancelDelegationToken api.
 */
GetTokenHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/token/GetTokenHandler.java)/**
 * Executes getDelegationToken api.
 */
PrintTokenHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/token/PrintTokenHandler.java)/**
 * Executes getDelegationToken api.
 */
RenewTokenHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/token/RenewTokenHandler.java)/**
 * Executes renewDelegationToken api.
 */
TokenCommands (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/token/TokenCommands.java)/**
 * Sub-command to group token related operations.
 */
AddAclVolumeHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/volume/AddAclVolumeHandler.java)/**
 * Add acl handler for volume.
 */
CreateVolumeHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/volume/CreateVolumeHandler.java)/**
 * Executes the create volume call for the shell.
 */
DeleteVolumeHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/volume/DeleteVolumeHandler.java)/**
 * Executes deleteVolume call for the shell.
 */
GetAclVolumeHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/volume/GetAclVolumeHandler.java)/**
 * Get acl handler for volume.
 */
InfoVolumeHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/volume/InfoVolumeHandler.java)/**
 * Executes volume Info calls.
 */
ListVolumeHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/volume/ListVolumeHandler.java)/**
 * Executes List Volume call.
 */
RemoveAclVolumeHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/volume/RemoveAclVolumeHandler.java)/**
 * Remove acl handler for volume.
 */
SetAclVolumeHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/volume/SetAclVolumeHandler.java)/**
 * Set acl handler for volume.
 */
UpdateVolumeHandler (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/volume/UpdateVolumeHandler.java)/**
 * Executes update volume calls.
 */
VolumeCommands (/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/web/ozShell/volume/VolumeCommands.java)/**
 * Subcommand to group volume related operations.
 */
OMDummyCreateBucketResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/ratis/TestOzoneManagerDoubleBufferWithDummyResponse.java)/**
   * DummyCreatedBucket Response class used in testing.
   */
TestOzoneManagerDoubleBufferWithDummyResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/ratis/TestOzoneManagerDoubleBufferWithDummyResponse.java)/**
 * This class tests OzoneManagerDoubleBuffer implementation with
 * dummy response class.
 */
TestOzoneManagerDoubleBufferWithOMResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/ratis/TestOzoneManagerDoubleBufferWithOMResponse.java)/**
 * This class tests OzoneManagerDouble Buffer with actual OMResponse classes.
 */
TestOzoneManagerRatisServer (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/ratis/TestOzoneManagerRatisServer.java)/**
 * Test OM Ratis server.
 */
TestBucketRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/bucket/TestBucketRequest.java)/**
 * Base test class for Bucket request.
 */
TestOMBucketCreateRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/bucket/TestOMBucketCreateRequest.java)/**
 * Tests OMBucketCreateRequest class, which handles CreateBucket request.
 */
TestOMBucketDeleteRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/bucket/TestOMBucketDeleteRequest.java)/**
 * Tests OMBucketDeleteRequest class which handles DeleteBucket request.
 */
TestOMBucketSetPropertyRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/bucket/TestOMBucketSetPropertyRequest.java)/**
 * Tests OMBucketSetPropertyRequest class which handles OMSetBucketProperty
 * request.
 */
TestOMDirectoryCreateRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/file/TestOMDirectoryCreateRequest.java)/**
 * Test OM directory create request.
 */
TestOMFileCreateRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/file/TestOMFileCreateRequest.java)/**
 * Tests OMFileCreateRequest.
 */
TestOMAllocateBlockRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/key/TestOMAllocateBlockRequest.java)/**
 * Tests OMAllocateBlockRequest class.
 */
TestOMKeyCommitRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/key/TestOMKeyCommitRequest.java)/**
 * Class tests OMKeyCommitRequest class.
 */
TestOMKeyCreateRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/key/TestOMKeyCreateRequest.java)/**
 * Tests OMCreateKeyRequest class.
 */
TestOMKeyDeleteRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/key/TestOMKeyDeleteRequest.java)/**
 * Tests OmKeyDelete request.
 */
TestOMKeyPurgeRequestAndResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/key/TestOMKeyPurgeRequestAndResponse.java)/**
 * Tests {@link OMKeyPurgeRequest} and {@link OMKeyPurgeResponse}.
 */
TestOMKeyRenameRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/key/TestOMKeyRenameRequest.java)/**
 * Tests RenameKey request.
 */
TestOMKeyRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/key/TestOMKeyRequest.java)/**
 * Base test class for key request.
 */
TestS3BucketCreateRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/s3/bucket/TestS3BucketCreateRequest.java)/**
 * Tests S3BucketCreateRequest class, which handles S3 CreateBucket request.
 */
TestS3BucketDeleteRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/s3/bucket/TestS3BucketDeleteRequest.java)/**
 * Tests S3BucketDelete Request.
 */
TestS3BucketRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/s3/bucket/TestS3BucketRequest.java)/**
 * Base test class for S3 Bucket request.
 */
TestS3InitiateMultipartUploadRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/s3/multipart/TestS3InitiateMultipartUploadRequest.java)/**
 * Tests S3 Initiate Multipart Upload request.
 */
TestS3MultipartRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/s3/multipart/TestS3MultipartRequest.java)/**
 * Base test class for S3 Multipart upload request.
 */
TestS3MultipartUploadAbortRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/s3/multipart/TestS3MultipartUploadAbortRequest.java)/**
 * Test Multipart upload abort request.
 */
TestS3MultipartUploadCommitPartRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/s3/multipart/TestS3MultipartUploadCommitPartRequest.java)/**
 * Tests S3 Multipart upload commit part request.
 */
TestS3MultipartUploadCompleteRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/s3/multipart/TestS3MultipartUploadCompleteRequest.java)/**
 * Tests S3 Multipart Upload Complete request.
 */
TestOMClientRequestWithUserInfo (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/TestOMClientRequestWithUserInfo.java)/**
 * Test OMClient Request with user information.
 */
TestOMRequestUtils (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/TestOMRequestUtils.java)/**
 * Helper class to test OMClientRequest classes.
 */
TestOMVolumeAddAclRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/volume/acl/TestOMVolumeAddAclRequest.java)/**
 * Tests volume addAcl request.
 */
TestOMVolumeRemoveAclRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/volume/acl/TestOMVolumeRemoveAclRequest.java)/**
 * Tests volume removeAcl request.
 */
TestOMVolumeSetAclRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/volume/acl/TestOMVolumeSetAclRequest.java)/**
 * Tests volume setAcl request.
 */
TestOMVolumeDeleteRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/volume/TestOMVolumeDeleteRequest.java)/**
 * Tests delete volume request.
 */
TestOMVolumeRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/volume/TestOMVolumeRequest.java)/**
 * Base test class for Volume request.
 */
TestOMVolumeSetOwnerRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/volume/TestOMVolumeSetOwnerRequest.java)/**
 * Tests set volume property request.
 */
TestOMVolumeSetQuotaRequest (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/request/volume/TestOMVolumeSetQuotaRequest.java)/**
 * Tests set volume property request.
 */
TestOMBucketCreateResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/bucket/TestOMBucketCreateResponse.java)/**
 * This class tests OMBucketCreateResponse.
 */
TestOMBucketDeleteResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/bucket/TestOMBucketDeleteResponse.java)/**
 * This class tests OMBucketDeleteResponse.
 */
TestOMBucketSetPropertyResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/bucket/TestOMBucketSetPropertyResponse.java)/**
 * This class tests OMBucketSetPropertyResponse.
 */
TestOMDirectoryCreateResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/file/TestOMDirectoryCreateResponse.java)/**
 * Tests OMDirectoryCreateResponse.
 */
TestOMAllocateBlockResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/key/TestOMAllocateBlockResponse.java)/**
 * Tests OMAllocateBlockResponse.
 */
TestOMKeyCommitResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/key/TestOMKeyCommitResponse.java)/**
 * Tests OMKeyCommitResponse.
 */
TestOMKeyCreateResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/key/TestOMKeyCreateResponse.java)/**
 * Tests MKeyCreateResponse.
 */
TestOMKeyDeleteResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/key/TestOMKeyDeleteResponse.java)/**
 * Tests OMKeyDeleteResponse.
 */
TestOMKeyRenameResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/key/TestOMKeyRenameResponse.java)/**
 * Tests OMKeyRenameResponse.
 */
TestOMKeyResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/key/TestOMKeyResponse.java)/**
 * Base test class for key response.
 */
TestS3BucketCreateResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/s3/bucket/TestS3BucketCreateResponse.java)/**
 * Class to test S3BucketCreateResponse.
 */
TestS3BucketDeleteResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/s3/bucket/TestS3BucketDeleteResponse.java)/**
 * Tests S3BucketDeleteResponse.
 */
TestS3InitiateMultipartUploadResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/s3/multipart/TestS3InitiateMultipartUploadResponse.java)/**
 * Class tests S3 Initiate MPU response.
 */
TestS3MultipartUploadAbortResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/s3/multipart/TestS3MultipartUploadAbortResponse.java)/**
 * Test multipart upload abort response.
 */
TestOMResponseUtils (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/TestOMResponseUtils.java)/**
 * Helper class to test OMClientResponse classes.
 */
TestOMVolumeCreateResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/volume/TestOMVolumeCreateResponse.java)/**
 * This class tests OMVolumeCreateResponse.
 */
TestOMVolumeDeleteResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/volume/TestOMVolumeDeleteResponse.java)/**
 * This class tests OMVolumeCreateResponse.
 */
TestOMVolumeSetOwnerResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/volume/TestOMVolumeSetOwnerResponse.java)/**
 * This class tests OMVolumeCreateResponse.
 */
TestOMVolumeSetQuotaResponse (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/response/volume/TestOMVolumeSetQuotaResponse.java)/**
 * This class tests OMVolumeCreateResponse.
 */
ScmBlockLocationTestingClient (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/ScmBlockLocationTestingClient.java)/**
 * This is a testing client that allows us to intercept calls from OzoneManager
 * to SCM.
 * <p>
 * TODO: OzoneManager#getScmBlockClient -- so that we can load this class up via
 * config setting into OzoneManager. Right now, we just pass this to
 * KeyDeletingService only.
 * <p>
 * TODO: Move this class to a generic test utils so we can use this class in
 * other Ozone Manager tests.
 */
TestBucketManagerImpl (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestBucketManagerImpl.java)/**
 * Tests BucketManagerImpl, mocks OMMetadataManager for testing.
 */
TestChunkStreams (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestChunkStreams.java)/**
 * This class tests KeyInputStream and KeyOutputStream.
 */
TestKeyDeletingService (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestKeyDeletingService.java)/**
 * Test Key Deleting Service.
 * <p>
 * This test does the following things.
 * <p>
 * 1. Creates a bunch of keys. 2. Then executes delete key directly using
 * Metadata Manager. 3. Waits for a while for the KeyDeleting Service to pick up
 * and call into SCM. 4. Confirms that calls have been successful.
 */
TestKeyManagerUnit (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestKeyManagerUnit.java)/**
 * Unit test key manager.
 */
TestOmMetadataManager (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestOmMetadataManager.java)/**
 * Tests OzoneManager MetadataManager.
 */
TestOzoneManagerHttpServer (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestOzoneManagerHttpServer.java)/**
 * Test http server of OM with various HTTP option.
 */
TestOzoneManagerStarter (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestOzoneManagerStarter.java)/**
 * This class is used to test the CLI provided by OzoneManagerStarter, which is
 * used to start and init the OzoneManager. The calls to the Ozone Manager are
 * mocked so the tests only validate the CLI calls the correct methods are
 * invoked.
 */
TestS3BucketManager (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestS3BucketManager.java)/**
 * Tests for S3 Bucket Manager.
 */
TestOzoneBlockTokenSecretManager (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/security/TestOzoneBlockTokenSecretManager.java)/**
 * Test class for {@link OzoneBlockTokenSecretManager}.
 */
TestOzoneDelegationTokenSecretManager (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/security/TestOzoneDelegationTokenSecretManager.java)/**
 * Test class for {@link OzoneDelegationTokenSecretManager}.
 */
TestOzoneManagerBlockToken (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/security/TestOzoneManagerBlockToken.java)/**
 * Test class for OzoneManagerDelegationToken.
 */
TestOzoneTokenIdentifier (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/security/TestOzoneTokenIdentifier.java)/**
 * Test class for {@link OzoneTokenIdentifier}.
 */
TestObjectPrinter (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/web/ozShell/TestObjectPrinter.java)/**
 * Test the json object printer.
 */
TestOzoneAddress (/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/web/ozShell/TestOzoneAddress.java)/**
 * Test ozone URL parsing.
 */
BasicKeyInfo (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicKeyInfo.java)/**
 * Minimum set of Ozone key information attributes.
 * <p>
 * This class doesn't depend on any other ozone class just on primitive
 * java types. It could be used easily in the signature of OzoneClientAdapter
 * as even if a separated class loader is loaded it it won't cause any
 * dependency problem.
 */
BasicOzFs (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicOzFs.java)/**
 * ozone implementation of AbstractFileSystem.
 * This impl delegates to the OzoneFileSystem
 */
Renewer (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicOzoneClientAdapterImpl.java)/**
   * Ozone Delegation Token Renewer.
   */
IteratorAdapter (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicOzoneClientAdapterImpl.java)/**
   * Adapter to convert OzoneKey to a safe and simple Key implementation.
   */
BasicOzoneClientAdapterImpl (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicOzoneClientAdapterImpl.java)/**
 * Basic Implementation of the OzoneFileSystem calls.
 * <p>
 * This is the minimal version which doesn't include any statistics.
 * <p>
 * For full featured version use OzoneClientAdapterImpl.
 */
OzoneListingIterator (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicOzoneFileSystem.java)/**
   * This class provides an interface to iterate through all the keys in the
   * bucket prefixed with the input path key and process them.
   * <p>
   * Each implementing class should define how the keys should be processed
   * through the processKey() function.
   */
BasicOzoneFileSystem (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicOzoneFileSystem.java)/**
 * The minimal Ozone Filesystem implementation.
 * <p>
 * This is a basic version which doesn't extend
 * KeyProviderTokenIssuer and doesn't include statistics. It can be used
 * from older hadoop version. For newer hadoop version use the full featured
 * OzoneFileSystem.
 */
Constants (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/Constants.java)/**
 * Constants for Ozone FileSystem implementation.
 */
FileStatusAdapter (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/FileStatusAdapter.java)/**
 * Class to hold the internal information of a FileStatus.
 * <p>
 * As FileStatus class is not compatible between 3.x and 2.x hadoop we can
 * use this adapter to hold all the required information. Hadoop 3.x FileStatus
 * information can be converted to this class, and this class can be used to
 * create hadoop 2.x FileStatus.
 * <p>
 * FileStatus (Hadoop 3.x) --> FileStatusAdapter --> FileStatus (Hadoop 2.x)
 */
FilteredClassLoader (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/FilteredClassLoader.java)/**
 * Class loader which delegates the loading only for the selected class.
 *
 * <p>
 * By default java classloader delegates first all the class loading to the
 * parent, and loads the class only if it's not found in the class.
 * <p>
 * This simple class loader do the opposit. Everything is loaded with this
 * class loader without delegation _except_ the few classes which are defined
 * in the constructor.
 * <p>
 * With this method we can use two separated class loader (the original main
 * classloader and instance of this which loaded separated classes, but the
 * few selected classes are shared between the two class loaders.
 * <p>
 * With this approach it's possible to use any older hadoop version
 * (main classloader) together with ozonefs (instance of this classloader) as
 * only the selected classes are selected between the class loaders.
 */
O3fsDtFetcher (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/O3fsDtFetcher.java)/**
 * A DT fetcher for OzoneFileSystem.
 * It is only needed for the `hadoop dtutil` command.
 */
OzFs (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzFs.java)/**
 * ozone implementation of AbstractFileSystem.
 * This impl delegates to the OzoneFileSystem
 */
OzoneClientAdapter (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneClientAdapter.java)/**
 * Lightweight adapter to separate hadoop/ozone classes.
 * <p>
 * This class contains only the bare minimum Ozone classes in the signature.
 * It could be loaded by a different classloader because only the objects in
 * the method signatures should be shared between the classloader.
 */
OzoneClientAdapterCreator (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneClientAdapterFactory.java)/**
   * Interface to create OzoneClientAdapter implementation with reflection.
   */
OzoneClientAdapterFactory (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneClientAdapterFactory.java)/**
 * Creates OzoneClientAdapter with classloader separation.
 */
OzoneClientAdapterImpl (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneClientAdapterImpl.java)/**
 * Implementation of the OzoneFileSystem calls.
 */
OzoneFileSystem (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java)/**
 * The Ozone Filesystem implementation.
 * <p>
 * This subclass is marked as private as code should not be creating it
 * directly; use {@link FileSystem#get(Configuration)} and variants to create
 * one. If cast to {@link OzoneFileSystem}, extra methods and features may be
 * accessed. Consider those private and unstable.
 */
OzoneFSInputStream (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFSInputStream.java)/**
 * The input stream for Ozone file system.
 *
 * TODO: Make inputStream generic for both rest and rpc clients
 * This class is not thread safe.
 */
OzoneFSOutputStream (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFSOutputStream.java)/**
 * The output stream for Ozone file system.
 *
 * TODO: Make outputStream generic for both rest and rpc clients
 * This class is not thread safe.
 */
OzoneFsShell (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFsShell.java)/** Provide command line access to a Ozone FileSystem. */
OzoneFSStorageStatistics (/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFSStorageStatistics.java)/**
 * Storage statistics for OzoneFileSystem.
 */
ITestOzoneContractCreate (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/contract/ITestOzoneContractCreate.java)/**
 * Ozone contract tests creating files.
 */
ITestOzoneContractDelete (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/contract/ITestOzoneContractDelete.java)/**
 * Ozone contract tests covering deletes.
 */
ITestOzoneContractDistCp (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/contract/ITestOzoneContractDistCp.java)/**
 * Contract test suite covering S3A integration with DistCp.
 * Uses the block output stream, buffered to disk. This is the
 * recommended output mechanism for DistCP due to its scalability.
 */
ITestOzoneContractGetFileStatus (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/contract/ITestOzoneContractGetFileStatus.java)/**
 * Ozone contract tests covering getFileStatus.
 */
ITestOzoneContractMkdir (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/contract/ITestOzoneContractMkdir.java)/**
 * Test dir operations on Ozone.
 */
ITestOzoneContractOpen (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/contract/ITestOzoneContractOpen.java)/**
 * Ozone contract tests opening files.
 */
ITestOzoneContractRename (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/contract/ITestOzoneContractRename.java)/**
 * Ozone contract tests covering rename.
 */
ITestOzoneContractRootDir (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/contract/ITestOzoneContractRootDir.java)/**
 * Ozone contract test for ROOT directory operations.
 */
ITestOzoneContractSeek (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/contract/ITestOzoneContractSeek.java)/**
 * Ozone contract tests covering file seek.
 */
OzoneContract (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/contract/OzoneContract.java)/**
 * The contract of Ozone: only enabled if the test bucket is provided.
 */
TestFilteredClassLoader (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestFilteredClassLoader.java)/**
 * FilteredClassLoader test using mocks.
 */
TestOzoneFileInterfaces (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileInterfaces.java)/**
 * Test OzoneFileSystem Interfaces.
 *
 * This test will test the various interfaces i.e.
 * create, read, write, getFileStatus
 */
TestOzoneFileSystem (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystem.java)/**
 * Ozone file system tests that are not covered by contract tests.
 */
TestOzoneFileSystemWithMocks (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystemWithMocks.java)/**
 * Ozone File system tests that are light weight and use mocks.
 */
TestOzoneFsHAURLs (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFsHAURLs.java)/**
 * Test client-side URI handling with Ozone Manager HA.
 */
TestOzoneFSInputStream (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFSInputStream.java)/**
 * Test OzoneFSInputStream by reading through multiple interfaces.
 */
TestOzoneFsRenameDir (/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFsRenameDir.java)/**
 * Unit Test for verifying directory rename operation through OzoneFS.
 */
ContainerKeyService (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/api/ContainerKeyService.java)/**
 * Endpoint for querying keys that belong to a container.
 */
ContainerKeyPrefix (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/api/types/ContainerKeyPrefix.java)/**
 * Class to encapsulate the Key information needed for the Recon container DB.
 * Currently, it is the containerId and the whole key + key version.
 */
ContainerMetadata (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/api/types/ContainerMetadata.java)/**
 * Metadata object that represents a Container.
 */
ContainersResponseData (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/api/types/ContainersResponse.java)/**
   * Class that encapsulates the data presented in Containers API Response.
   */
ContainersResponse (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/api/types/ContainersResponse.java)/**
 * Class that represents the API Response structure of Containers.
 */
IsoDateAdapter (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/api/types/IsoDateAdapter.java)/**
 * A converter to convert Instant to standard date string.
 */
ContainerBlockMetadata (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/api/types/KeyMetadata.java)/**
   * Class to hold ContainerID and BlockID.
   */
KeyMetadata (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/api/types/KeyMetadata.java)/**
 * Metadata object represents one key in the object store.
 */
KeysResponseData (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/api/types/KeysResponse.java)/**
   * Class that encapsulates the data presented in Keys API Response.
   */
KeysResponse (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/api/types/KeysResponse.java)/**
 * Class that represents the API Response structure of Keys within a container.
 */
UtilizationService (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/api/UtilizationService.java)/**
 * Endpoint for querying the counts of a certain file Size.
 */
ConfigurationProvider (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ConfigurationProvider.java)/**
 * Ozone Configuration Provider.
 * <p>
 * As the OzoneConfiguration is created by the CLI application here we inject
 * it via a singleton instance to the Jax-RS/CDI instances.
 */
DataSourceConfiguration (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/persistence/DataSourceConfiguration.java)/**
 * Common configuration needed to instantiate {@link javax.sql.DataSource}.
 */
DefaultDataSourceProvider (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/persistence/DefaultDataSourceProvider.java)/**
 * Provide a {@link javax.sql.DataSource} for the application.
 */
SpringConnectionProvider (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/persistence/JooqPersistenceModule.java)/**
   * This connection provider uses Spring to extract the
   * {@link TransactionAwareDataSourceProxy} from our BoneCP pooled connection
   * {@link DataSource}.
   */
JooqPersistenceModule (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/persistence/JooqPersistenceModule.java)/**
 * Persistence module that provides binding for {@link DataSource} and
 * a MethodInterceptor for nested transactions support.
 */
TransactionalMethodInterceptor (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/persistence/TransactionalMethodInterceptor.java)/**
 * A {@link MethodInterceptor} that implements nested transactions.
 * <p>
 * Only the outermost transactional method will <code>commit()</code> or
 * <code>rollback()</code> the contextual transaction. This can be verified
 * through {@link TransactionStatus#isNewTransaction()}, which returns
 * <code>true</code> only for the outermost transactional method call.
 * <p>
 */
ReconConstants (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconConstants.java)/**
 * Recon Server constants file.
 */
ReconControllerModule (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconControllerModule.java)/**
 * Guice controller that defines concrete bindings.
 */
ReconGuiceServletContextListener (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconGuiceServletContextListener.java)/**
 * Servlet Context Listener that provides the Guice injector.
 */
RestKeyBindingBuilder (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconRestServletModule.java)/**
   * Interface to provide packages for scanning.
   */
ReconRestServletModule (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconRestServletModule.java)/**
 * Class to scan API Service classes and bind them to the injector.
 */
GuiceResourceConfig (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconRestServletModule.java)/**
 * Class to bridge Guice bindings to Jersey hk2 bindings.
 */
ReconServer (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconServer.java)/**
 * Recon server main class that stops and starts recon services.
 */
ReconServerConfigKeys (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconServerConfigKeys.java)/**
 * This class contains constants for Recon configuration keys.
 */
ReconTaskBindingModule (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconTaskBindingModule.java)/**
 * Binds the various Recon Tasks.
 */
ReconUtils (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconUtils.java)/**
 * Recon Utility class.
 */
ReconOMMetadataManager (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/recovery/ReconOMMetadataManager.java)/**
 * Interface for the OM Metadata Manager + DB store maintained by
 * Recon.
 */
ReconOmMetadataManagerImpl (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/recovery/ReconOmMetadataManagerImpl.java)/**
 * Recon's implementation of the OM Metadata manager. By extending and
 * relying on the OmMetadataManagerImpl, we can make sure all changes made to
 * schema in OM will be automatically picked up by Recon.
 */
ContainerDBServiceProvider (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/spi/ContainerDBServiceProvider.java)/**
 * The Recon Container DB Service interface.
 */
HddsDatanodeServiceProvider (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/spi/HddsDatanodeServiceProvider.java)/**
 * Interface to access datanode endpoints.
 */
ContainerDBServiceProviderImpl (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/spi/impl/ContainerDBServiceProviderImpl.java)/**
 * Implementation of the Recon Container DB Service.
 */
ContainerKeyPrefixCodec (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/spi/impl/ContainerKeyPrefixCodec.java)/**
 * Codec to encode ContainerKeyPrefix as byte array.
 */
OzoneManagerServiceProviderImpl (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/spi/impl/OzoneManagerServiceProviderImpl.java)/**
 * Implementation of the OzoneManager Service provider.
 */
ReconContainerDBProvider (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/spi/impl/ReconContainerDBProvider.java)/**
 * Provider for the Recon container DB (Metadata store).
 */
OzoneManagerServiceProvider (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/spi/OzoneManagerServiceProvider.java)/**
 * Interface to access OM endpoints.
 */
StorageContainerServiceProvider (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/spi/StorageContainerServiceProvider.java)/**
 * Interface to access SCM endpoints.
 */
ContainerKeyMapperTask (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/tasks/ContainerKeyMapperTask.java)/**
 * Class to iterate over the OM DB and populate the Recon container DB with
 * the container -> Key reverse mapping.
 */
FileSizeCountTask (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/tasks/FileSizeCountTask.java)/**
 * Class to iterate over the OM DB and store the counts of existing/new
 * files binned into ranges (1KB, 2Kb..,4MB,.., 1TB,..1PB) to the Recon
 * fileSize DB.
 */
OMUpdateEventBuilder (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/tasks/OMDBUpdateEvent.java)/**
   * Builder used to construct an OM DB Update event.
   * @param <KEY> Key type.
   * @param <VALUE> Value type.
   */
OMDBUpdateEvent (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/tasks/OMDBUpdateEvent.java)/**
 * A class used to encapsulate a single OM DB update event.
 * Currently only PUT and DELETE are supported.
 * @param <KEY> Type of Key.
 * @param <VALUE> Type of Value.
 */
OMDBUpdatesHandler (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/tasks/OMDBUpdatesHandler.java)/**
 * Class used to listen on OM RocksDB updates.
 */
OMUpdateEventBatch (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/tasks/OMUpdateEventBatch.java)/**
 * Wrapper class to hold multiple OM DB update events.
 */
ReconDBUpdateTask (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/tasks/ReconDBUpdateTask.java)/**
 * Interface used to denote a Recon task that needs to act on OM DB events.
 */
ReconTaskController (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/tasks/ReconTaskController.java)/**
 * Controller used by Recon to manage Tasks that are waiting on Recon events.
 */
ReconTaskControllerImpl (/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/tasks/ReconTaskControllerImpl.java)/**
 * Implementation of ReconTaskController.
 */
AbstractOMMetadataManagerTest (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/AbstractOMMetadataManagerTest.java)/**
 * Utility methods for test classes.
 */
TestContainerKeyService (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/api/TestContainerKeyService.java)/**
 * Test for container key service.
 */
TestUtilizationService (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/api/TestUtilizationService.java)/**
 * Test for File size count service.
 */
GuiceInjectorUtilsForTestsImpl (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/GuiceInjectorUtilsForTestsImpl.java)/**
 * Implementation for GuiceInjectorUtilsForTests.
 */
DataSourceConfigurationProvider (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/persistence/AbstractSqlDatabaseTest.java)/**
   * Local Sqlite datasource provider.
   */
AbstractSqlDatabaseTest (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/persistence/AbstractSqlDatabaseTest.java)/**
 * Create an injector for tests that need to access the SQl database.
 */
TestReconInternalSchemaDefinition (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/persistence/TestReconInternalSchemaDefinition.java)/**
 * Class used to test ReconInternalSchemaDefinition.
 */
TestStatsSchemaDefinition (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/persistence/TestStatsSchemaDefinition.java)/**
 * Class used to test StatsSchemaDefinition.
 */
TestUtilizationSchemaDefinition (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/persistence/TestUtilizationSchemaDefinition.java)/**
 * Test persistence module provides connection and transaction awareness.
 */
TestReconOmMetadataManagerImpl (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/recovery/TestReconOmMetadataManagerImpl.java)/**
 * Test Recon OM Metadata Manager implementation.
 */
TestContainerDBServiceProviderImpl (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/spi/impl/TestContainerDBServiceProviderImpl.java)/**
 * Unit Tests for ContainerDBServiceProviderImpl.
 */
TestOzoneManagerServiceProviderImpl (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/spi/impl/TestOzoneManagerServiceProviderImpl.java)/**
 * Class to test Ozone Manager Service Provider Implementation.
 */
MockOzoneServiceProvider (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/spi/impl/TestOzoneManagerServiceProviderImpl.java)/**
 * Mock OzoneManagerServiceProviderImpl which overrides
 * updateReconOmDBWithNewSnapshot.
 */
TestReconContainerDBProvider (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/spi/impl/TestReconContainerDBProvider.java)/**
 * Tests the class that provides the instance of the DB Store used by Recon to
 * store its container - key data.
 */
DummyReconDBTask (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/tasks/DummyReconDBTask.java)/**
 * Dummy Recon task that has 3 modes of operations.
 * ALWAYS_FAIL / FAIL_ONCE / ALWAYS_PASS
 */
TestContainerKeyMapperTask (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/tasks/TestContainerKeyMapperTask.java)/**
 * Unit test for Container Key mapper task.
 */
TestFileSizeCountTask (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/tasks/TestFileSizeCountTask.java)/**
 * Unit test for File Size Count Task.
 */
TestOMDBUpdatesHandler (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/tasks/TestOMDBUpdatesHandler.java)/**
 * Class used to test OMDBUpdatesHandler.
 */
TestReconTaskControllerImpl (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/tasks/TestReconTaskControllerImpl.java)/**
 * Class used to test ReconTaskControllerImpl.
 */
TestReconCodecs (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/TestReconCodecs.java)/**
 * Unit Tests for Codecs used in Recon.
 */
TestReconUtils (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/TestReconUtils.java)/**
 * Test Recon Utility methods.
 */
GuiceInjectorUtilsForTests (/hadoop-ozone/recon/src/test/java/org/apache/hadoop/ozone/recon/types/GuiceInjectorUtilsForTests.java)/**
 * Utility methods to get guice injector and ozone configuration.
 */
LocalDataSourceProvider (/hadoop-ozone/recon-codegen/src/main/java/org/hadoop/ozone/recon/codegen/JooqCodeGenerator.java)/**
   * Provider for embedded datasource.
   */
JooqCodeGenerator (/hadoop-ozone/recon-codegen/src/main/java/org/hadoop/ozone/recon/codegen/JooqCodeGenerator.java)/**
 * Utility class that generates the Dao and Pojos for Recon schema. The
 * implementations of {@link ReconSchemaDefinition} are discovered through
 * Guice bindings in order to avoid ugly reflection code, and invoked to
 * generate the schema over an embedded database. The jooq code generator then
 * runs over the embedded database to generate classes for recon.
 */
ReconSchemaGenerationModule (/hadoop-ozone/recon-codegen/src/main/java/org/hadoop/ozone/recon/codegen/ReconSchemaGenerationModule.java)/**
 * Bindings for DDL generation and used by
 * {@link org.hadoop.ozone.recon.codegen.JooqCodeGenerator}.
 */
TableNamingStrategy (/hadoop-ozone/recon-codegen/src/main/java/org/hadoop/ozone/recon/codegen/TableNamingStrategy.java)/**
 * Generate Table classes with a different name from POJOS to improve
 * readability, loaded at runtime.
 */
ReconInternalSchemaDefinition (/hadoop-ozone/recon-codegen/src/main/java/org/hadoop/ozone/recon/schema/ReconInternalSchemaDefinition.java)/**
 * Class used to create tables that are required for Recon's internal
 * management.
 */
ReconSchemaDefinition (/hadoop-ozone/recon-codegen/src/main/java/org/hadoop/ozone/recon/schema/ReconSchemaDefinition.java)/**
 * Classes meant to initialize the SQL schema for Recon. The implementations of
 * this class will be used to create the SQL schema programmatically.
 * Note: Make sure add a binding for your implementation to the Guice module,
 * otherwise code-generator will not pick up the schema changes.
 */
StatsSchemaDefinition (/hadoop-ozone/recon-codegen/src/main/java/org/hadoop/ozone/recon/schema/StatsSchemaDefinition.java)/**
 * Class used to create tables that are required for storing Ozone statistics.
 */
UtilizationSchemaDefinition (/hadoop-ozone/recon-codegen/src/main/java/org/hadoop/ozone/recon/schema/UtilizationSchemaDefinition.java)/**
 * Programmatic definition of Recon DDL.
 */
AWSV4AuthParser (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/AWSV4AuthParser.java)/**
 * Parser to process AWS v4 auth request. Creates string to sign and auth
 * header. For more details refer to AWS documentation https://docs.aws
 * .amazon.com/general/latest/gr/sigv4-create-canonical-request.html.
 **/
CommonHeadersContainerResponseFilter (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/CommonHeadersContainerResponseFilter.java)/**
 * This class adds common header responses for all the requests.
 */
BucketMetadata (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/commontypes/BucketMetadata.java)/**
 * Metadata object represents one bucket.
 */
CommonPrefix (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/commontypes/CommonPrefix.java)/**
 * Directory name ("key prefix") in case of listing.
 */
IsoDateAdapter (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/commontypes/IsoDateAdapter.java)/**
 * A converter to convert Instant to standard date string.
 */
KeyMetadata (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/commontypes/KeyMetadata.java)/**
 * Metadata object represents one key in the object store.
 */
BucketEndpoint (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/BucketEndpoint.java)/**
 * Bucket level rest endpoints.
 */
Part (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/CompleteMultipartUploadRequest.java)/**
   * JAXB entity for child element.
   */
CompleteMultipartUploadRequest (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/CompleteMultipartUploadRequest.java)/**
 * Request for Complete Multipart Upload request.
 */
CopyObjectResponse (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/CopyObjectResponse.java)/**
 * Copy object Response.
 */
CopyPartResult (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/CopyPartResult.java)/**
 * Copy object Response.
 */
EndpointBase (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/EndpointBase.java)/**
 * Basic helpers for all the REST endpoints.
 */
ListBucketResponse (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ListBucketResponse.java)/**
 * Response from the ListBucket RPC Call.
 */
Upload (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ListMultipartUploadsResult.java)/**
   * Upload information.
   */
Owner (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ListMultipartUploadsResult.java)/**
   * Upload information.
   */
ListMultipartUploadsResult (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ListMultipartUploadsResult.java)/**
 * AWS compatible REST response for list multipart upload.
 */
ListObjectResponse (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ListObjectResponse.java)/**
 * Response from the ListObject RPC Call.
 */
Part (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ListPartsResponse.java)/**
   * Part information.
   */
ListPartsResponse (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ListPartsResponse.java)/**
 * Request for list parts of a multipart upload request.
 */
DeleteObject (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/MultiDeleteRequest.java)/**
   * JAXB entity for child element.
   */
MultiDeleteRequestUnmarshaller (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/MultiDeleteRequestUnmarshaller.java)/**
 * Custom unmarshaller to read MultiDeleteRequest w/wo namespace.
 */
DeletedObject (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/MultiDeleteResponse.java)/**
   * JAXB entity for child element.
   */
Error (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/MultiDeleteResponse.java)/**
   * JAXB entity for child element.
   */
MultiDeleteResponse (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/MultiDeleteResponse.java)/**
 * Response for multi object delete request.
 */
MultipartUploadInitiateResponse (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/MultipartUploadInitiateResponse.java)/**
 * Response for Initiate Multipart Upload request.
 */
ObjectEndpoint (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ObjectEndpoint.java)/**
 * Key level rest endpoints.
 */
PlainTextMultipartUploadReader (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/PlainTextMultipartUploadReader.java)/**
 * Body reader to accept plain text MPU.
 * <p>
 * Aws s3 api sends a multipartupload request with the content type
 * 'text/plain' in case of using 'aws s3 cp' (instead of aws s3api).
 * <p>
 * Our generic ObjectEndpoint.multipartUpload has a
 * CompleteMultipartUploadRequest parameter, which is required only for the
 * completion request.
 * <p>
 * But JaxRS tries to parse it from the body for the requests and in case of
 * text/plain requests this parsing is failed. This simple BodyReader enables
 * to parse an empty text/plain message and return with an empty completion
 * request.
 */
RootEndpoint (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/RootEndpoint.java)/**
 * Top level rest endpoint.
 */
XmlNamespaceFilter (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/XmlNamespaceFilter.java)/**
 * SAX filter to force namespace usage.
 * <p>
 * This filter will read the XML content as namespace qualified content
 * independent from the current namespace usage.
 */
OS3Exception (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/exception/OS3Exception.java)/**
 * This class represents exceptions raised from Ozone S3 service.
 *
 * Ref:https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html
 */
OS3ExceptionMapper (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/exception/OS3ExceptionMapper.java)/**
 *  Class the represents various errors returned by the
 *  Ozone S3 service.
 */
S3ErrorTable (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/exception/S3ErrorTable.java)/**
 * This class represents errors from Ozone S3 service.
 * This class needs to be updated to add new errors when required.
 */
Gateway (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/Gateway.java)/**
 * This class is used to start/stop S3 compatible rest server.
 */
GatewayApplication (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/GatewayApplication.java)/**
 * JaxRS resource definition.
 */
AuthenticationHeaderParser (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/header/AuthenticationHeaderParser.java)/**
 * Authentication Header parser to parse HttpHeader Authentication.
 */
AuthorizationHeaderV2 (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/header/AuthorizationHeaderV2.java)/**
 * Authorization Header v2.
 */
AuthorizationHeaderV4 (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/header/AuthorizationHeaderV4.java)/**
 * S3 Authorization header.
 * Ref: https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-auth-using
 * -authorization-header.html
 */
Credential (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/header/Credential.java)/**
 * Credential in the AWS authorization header.
 * Ref: https://docs.aws.amazon.com/AmazonS3/latest/API/
 * sigv4-auth-using-authorization-header.html
 *
 */
HeaderPreprocessor (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/HeaderPreprocessor.java)/**
 * Filter to adjust request headers for compatible reasons.
 *
 * It should be executed AFTER signature check (VirtualHostStyleFilter) as the
 * original Content-Type could be part of the base of the signature.
 */
S3WrapperInputStream (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/io/S3WrapperInputStream.java)/**
 * S3Wrapper Input Stream which encapsulates KeyInputStream from ozone.
 */
OzoneClientProducer (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/OzoneClientProducer.java)/**
 * This class creates the OzoneClient for the Rest endpoints.
 */
OzoneConfigurationHolder (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/OzoneConfigurationHolder.java)/**
 * Ozone Configuration factory.
 * <p>
 * As the OzoneConfiguration is created by the CLI application here we inject
 * it via a singleton instance to the Jax-RS/CDI instances.
 */
OzoneServiceProvider (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/OzoneServiceProvider.java)/**
 * This class creates the OM service .
 */
RequestIdentifier (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/RequestIdentifier.java)/**
 * Request specific identifiers.
 */
S3GatewayConfigKeys (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/S3GatewayConfigKeys.java)/**
 * This class contains constants for configuration keys used in S3G.
 */
S3GatewayHttpServer (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/S3GatewayHttpServer.java)/**
 * S3 Gateway specific configuration keys.
 */
SignedChunksInputStream (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/SignedChunksInputStream.java)/**
 * Input stream implementation to read body with chunked signatures.
 * <p>
 * see: https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-streaming.html
 */
ContinueToken (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/util/ContinueToken.java)/**
 * Token which holds enough information to continue the key iteration.
 */
OzoneS3Util (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/util/OzoneS3Util.java)/**
 * Ozone util for S3 related operations.
 */
RangeHeader (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/util/RangeHeader.java)/**
 * Ranger Header class which hold startoffset, endoffset of the Range header
 * value provided as part of get object.
 *
 */
RangeHeaderParserUtil (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/util/RangeHeaderParserUtil.java)/**
 * Utility class for S3.
 */
RFC1123Util (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/util/RFC1123Util.java)/**
 * Stricter RFC1123 data format.
 * <p>
 * This format always use two digits for the days to make it compatible with
 * golang clients.
 */
S3Consts (/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/util/S3Consts.java)/**
 * Set of constants used for S3 implementation.
 */
ObjectStoreStub (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/ObjectStoreStub.java)/**
 * ObjectStore implementation with in-memory state.
 */
Part (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/OzoneBucketStub.java)/**
   * Class used to hold part information in a upload part request.
   */
OzoneBucketStub (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/OzoneBucketStub.java)/**
 * In-memory ozone bucket for testing.
 */
OzoneClientStub (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/OzoneClientStub.java)/**
 * In-memory OzoneClient for testing.
 */
OzoneOutputStreamStub (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/OzoneOutputStreamStub.java)/**
 * OzoneOutputStream stub for testing.
 */
OzoneVolumeStub (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/OzoneVolumeStub.java)/**
 * Ozone volume with in-memory state for testing.
 */
TestAbortMultipartUpload (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestAbortMultipartUpload.java)/**
 * This class tests abort multipart upload request.
 */
TestBucketDelete (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketDelete.java)/**
 * This class tests delete bucket functionality.
 */
TestBucketGet (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketGet.java)/**
 * Testing basic object list browsing.
 */
TestBucketHead (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketHead.java)/**
 * This class test HeadBucket functionality.
 */
TestBucketResponse (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketResponse.java)/**
 * Testing JAXB serialization.
 */
TestInitiateMultipartUpload (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestInitiateMultipartUpload.java)/**
 * This class tests Initiate Multipart Upload request.
 */
TestListParts (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestListParts.java)/**
 * This class test list parts request.
 */
TestMultiDeleteRequestUnmarshaller (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestMultiDeleteRequestUnmarshaller.java)/**
 * Test custom marshalling of MultiDeleteRequest.
 */
TestObjectDelete (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestObjectDelete.java)/**
 * Test delete object.
 */
TestObjectEndpoint (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestObjectEndpoint.java)/**
 * Test static utility methods of the ObjectEndpoint.
 */
TestObjectGet (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestObjectGet.java)/**
 * Test get object.
 */
TestObjectHead (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestObjectHead.java)/**
 * Test head object.
 */
TestObjectMultiDelete (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestObjectMultiDelete.java)/**
 * Test object multi delete.
 */
TestObjectPut (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestObjectPut.java)/**
 * Test put object.
 */
TestPartUpload (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestPartUpload.java)/**
 * This class tests Upload part request.
 */
TestRootList (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestRootList.java)/**
 * This class test HeadBucket functionality.
 */
TestOS3Exception (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/exception/TestOS3Exception.java)/**
 * This class tests OS3Exception class.
 */
TestAuthorizationHeaderV2 (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/header/TestAuthorizationHeaderV2.java)/**
 * This class tests Authorization header format v2.
 */
TestOzoneClientProducer (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/TestOzoneClientProducer.java)/**
 * Test class for @{@link OzoneClientProducer}.
 * */
TestSignedChunksInputStream (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/TestSignedChunksInputStream.java)/**
 * Test input stream parsing with signatures.
 */
TestVirtualHostStyleFilter (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/TestVirtualHostStyleFilter.java)/**
 * This class test virtual host style mapping conversion to path style.
 */
TestContinueToken (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/util/TestContinueToken.java)/**
 * Test encode/decode of the continue token.
 */
TestOzoneS3Util (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/util/TestOzoneS3Util.java)/**
 * Class used to test OzoneS3Util.
 */
TestRangeHeaderParserUtil (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/util/TestRangeHeaderParserUtil.java)/**
 * Test class to test RangeHeaderParserUtil.
 */
TestRFC1123Util (/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/util/TestRFC1123Util.java)/**
 * Test for RFC1123 util.
 */
AuditParser (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/audit/parser/AuditParser.java)/**
 * Ozone audit parser tool.
 */
DatabaseHelper (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/audit/parser/common/DatabaseHelper.java)/**
 * Database helper for ozone audit parser tool.
 */
ParserConsts (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/audit/parser/common/ParserConsts.java)/**
 * Constants used for ozone audit parser.
 */
LoadCommandHandler (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/audit/parser/handler/LoadCommandHandler.java)/**
 * Load command handler for ozone audit parser.
 */
QueryCommandHandler (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/audit/parser/handler/QueryCommandHandler.java)/**
 * Custom query command handler for ozone audit parser.
 * The query must be enclosed within double quotes.
 */
TemplateCommandHandler (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/audit/parser/handler/TemplateCommandHandler.java)/**
 * Template command handler for ozone audit parser.
 */
Builder (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/audit/parser/model/AuditEntry.java)/**
   * Builder for AuditEntry.
   */
AuditEntry (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/audit/parser/model/AuditEntry.java)/**
 * POJO used for ozone audit parser tool.
 */
TaskProvider (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/BaseFreonGenerator.java)/**
   * Simple contract to execute a new step during a freon test.
   */
BaseFreonGenerator (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/BaseFreonGenerator.java)/**
 * Base class for simplified performance tests.
 */
ContentGenerator (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/ContentGenerator.java)/**
 * Utility class to write random keys from a limited buffer.
 */
Freon (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/Freon.java)/**
 * Ozone data generator and performance test tool.
 */
FreonHttpServer (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/FreonHttpServer.java)/**
 * Http server to provide metrics + profile endpoint.
 */
HadoopFsGenerator (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/HadoopFsGenerator.java)/**
 * Data generator tool test om performance.
 */
HadoopFsValidator (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/HadoopFsValidator.java)/**
 * Data generator tool test om performance.
 */
OmBucketGenerator (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/OmBucketGenerator.java)/**
 * Data generator tool test om performance.
 */
OmKeyGenerator (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/OmKeyGenerator.java)/**
 * Data generator tool test om performance.
 */
OzoneClientKeyGenerator (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/OzoneClientKeyGenerator.java)/**
 * Data generator tool test om performance.
 */
OzoneClientKeyValidator (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/OzoneClientKeyValidator.java)/**
 * Data generator tool test om performance.
 */
PathSchema (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/PathSchema.java)/**
 * Class to generate the path based on a counter.
 */
ProgressBar (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/ProgressBar.java)/**
 * Creates and runs a ProgressBar in new Thread which gets printed on
 * the provided PrintStream.
 */
KeyValidate (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/RandomKeyGenerator.java)/**
   * Wrapper to hold ozone keyValidate entry.
   */
Validator (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/RandomKeyGenerator.java)/**
   * Validates the write done in ozone cluster.
   */
RandomKeyGenerator (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/RandomKeyGenerator.java)/**
 * Data generator tool to generate as much keys as possible.
 */
S3KeyGenerator (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/S3KeyGenerator.java)/**
 * Generate random keys via the s3 interface.
 */
SameKeyReader (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/SameKeyReader.java)/**
 * Data generator tool test om performance.
 */
GenerateOzoneRequiredConfigurations (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genconf/GenerateOzoneRequiredConfigurations.java)/**
 * GenerateOzoneRequiredConfigurations - A tool to generate ozone-site.xml<br>
 * This tool generates an ozone-site.xml with minimally required configs.
 * This tool can be invoked as follows:<br>
 * <ul>
 * <li>ozone genconf {@literal <Path to output file>}</li>
 * <li>ozone genconf --help</li>
 * <li>ozone genconf -h</li>
 * </ul>
 */
BenchMarkContainerStateMap (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkContainerStateMap.java)/**
 * Benchmarks ContainerStateMap class.
 */
BenchMarkDatanodeDispatcher (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkDatanodeDispatcher.java)/**
 * Benchmarks DatanodeDispatcher class.
 */
BenchMarkMetadataStoreReads (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkMetadataStoreReads.java)/**
 * Measure metadatastore read performance.
 */
BenchMarkMetadataStoreWrites (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkMetadataStoreWrites.java)/**
 * Measure default metadatastore put performance.
 */
BenchMarkOMClient (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkOMClient.java)/**
 * Benchmarks OM Client.
 */
BenchMarkOMKeyAllocation (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkOMKeyAllocation.java)/**
 * Benchmark key creation in a bucket in OM.
 */
BenchMarkOzoneManager (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkOzoneManager.java)/**
 * Benchmarks OzoneManager.
 */
BenchMarkRocksDbStore (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkRocksDbStore.java)/**
 * Benchmark rocksdb store.
 */
BenchMarkSCM (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkSCM.java)/**
 * Benchmarks BlockManager class.
 */
Genesis (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/Genesis.java)/**
 * Main class that executes a set of HDDS/Ozone benchmarks.
 * We purposefully don't use the runner and tools classes from Hadoop.
 * There are some name collisions with OpenJDK JMH package.
 * <p>
 * Hence, these classes do not use the Tool/Runner pattern of standard Hadoop
 * CLI.
 */
GenesisMemoryProfiler (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/GenesisMemoryProfiler.java)/**
 * Max memory profiler.
 */
GenesisUtil (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/GenesisUtil.java)/**
 * Utility class for benchmark test cases.
 */
SQLCLI (/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/scm/cli/SQLCLI.java)/**
 * This is the CLI that can be use to convert an ozone metadata DB into
 * a sqlite DB file.
 *
 * NOTE: user should use this CLI in an offline fashion. Namely, this should not
 * be used to convert a DB that is currently being used by Ozone. Instead,
 * this should be used to debug and diagnosis closed DB instances.
 *
 */
TestAuditParser (/hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/audit/parser/TestAuditParser.java)/**
 * Tests AuditParser.
 */
TestDataValidate (/hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/freon/TestDataValidate.java)/**
 * Tests Freon, with MiniOzoneCluster and validate data.
 */
TestDataValidateWithDummyContainers (/hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/freon/TestDataValidateWithDummyContainers.java)/**
 * Tests Freon with MiniOzoneCluster and ChunkManagerDummyImpl.
 * Data validation is disabled in RandomKeyGenerator.
 */
TestDataValidateWithSafeByteOperations (/hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/freon/TestDataValidateWithSafeByteOperations.java)/**
 * Tests Freon, with MiniOzoneCluster and validate data.
 */
TestDataValidateWithUnsafeByteOperations (/hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/freon/TestDataValidateWithUnsafeByteOperations.java)/**
 * Tests Freon, with MiniOzoneCluster and validate data.
 */
TestFreonWithDatanodeFastRestart (/hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/freon/TestFreonWithDatanodeFastRestart.java)/**
 * Tests Freon with Datanode restarts without waiting for pipeline to close.
 */
TestFreonWithDatanodeRestart (/hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/freon/TestFreonWithDatanodeRestart.java)/**
 * Tests Freon with Datanode restarts.
 */
TestFreonWithPipelineDestroy (/hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/freon/TestFreonWithPipelineDestroy.java)/**
 * Tests Freon with Pipeline destroy.
 */
TestProgressBar (/hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/freon/TestProgressBar.java)/**
 * Using Mockito runner.
 */
TestRandomKeyGenerator (/hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/freon/TestRandomKeyGenerator.java)/**
 * Tests Freon, with MiniOzoneCluster.
 */
TestGenerateOzoneRequiredConfigurations (/hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/genconf/TestGenerateOzoneRequiredConfigurations.java)/**
 * Tests GenerateOzoneRequiredConfigurations.
 */
TestOmSQLCli (/hadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/om/TestOmSQLCli.java)/**
 * This class tests the CLI that transforms om.db into SQLite DB files.
 */
OzoneTestDriver (/hadoop-ozone/tools/src/test/java/org/apache/hadoop/test/OzoneTestDriver.java)/**
 * Driver for Ozone tests.
 */
Balance (/hadoop-ozone/upgrade/src/main/java/org/apache/hadoop/ozone/upgrade/Balance.java)/**
 * Command to move blocks between HDFS datanodes.
 */
Execute (/hadoop-ozone/upgrade/src/main/java/org/apache/hadoop/ozone/upgrade/Execute.java)/**
 * Execute Ozone specific HDFS ballanced..
 */
InPlaceUpgrade (/hadoop-ozone/upgrade/src/main/java/org/apache/hadoop/ozone/upgrade/InPlaceUpgrade.java)/**
 * Command  line interface for the In-Place upgrade utility.
 * <p>
 * In-Place upgrade can convert HDFS cluster data to Ozone data without
 * (or minimal) data moving.
 */
Plan (/hadoop-ozone/upgrade/src/main/java/org/apache/hadoop/ozone/upgrade/Plan.java)/**
 * Command to calculate statistics and estimate the upgrade.
 */
BaseParameters (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/BaseParameters.java)/**
 * Base class of all parameters.
 */
Localization (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/Localization.java)/**
 * Localization parameter.
 * */
ParametersHolder (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/ParametersHolder.java)/**
 * This class acts as a wrapper of {@code CommandLine} values along with
 * YAML configuration values.
 * YAML configuration is only stored if the -f &lt;filename&gt;
 * option is specified along the CLI arguments.
 * Using this wrapper class makes easy to deal with
 * any form of configuration source potentially added into Submarine,
 * in the future.
 * If both YAML and CLI value is found for a config, this is an error case.
 */
Quicklink (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/Quicklink.java)/**
 * A class represents quick links to a web page.
 */
PyTorchRunJobParameters (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/PyTorchRunJobParameters.java)/**
 * Parameters for PyTorch job.
 */
RunJobParameters (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/RunJobParameters.java)/**
 * Parameters used to run a job
 */
TensorFlowRunJobParameters (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/runjob/TensorFlowRunJobParameters.java)/**
 * Parameters for TensorFlow job.
 */
RunParameters (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/RunParameters.java)/**
 * Parameters required to run anything on cluster. Such as run job / serve model
 */
Configs (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Configs.java)/**
 * Class that holds values found in 'configs' section of YAML configuration.
 */
PsRole (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/PsRole.java)/**
 * Holds configuration values for PS (parameter server).
 * 'ps' is a section underneath the 'roles' section of the YAML
 * configuration file.
 */
Role (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Role.java)/**
 * Base class for Roles. 'roles' is a section of the YAML configuration file.
 */
Roles (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Roles.java)/**
 * This class represents a section of the YAML configuration file.
 */
Scheduling (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Scheduling.java)/**
 * Class that holds values found in 'scheduling' section of YAML configuration.
 */
Security (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Security.java)/**
 * Class that holds values found in 'security' section of YAML configuration.
 */
Spec (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/Spec.java)/**
 * Class that holds values found in 'spec' section of YAML configuration.
 */
TensorBoard (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/TensorBoard.java)/**
 * Class that holds values found in 'tensorboard' section of YAML configuration.
 */
WorkerRole (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/WorkerRole.java)/**
 * Holds configuration values for the worker role.
 * 'worker' is a section underneath the 'roles' section of the YAML
 * configuration file.
 */
YamlConfigFile (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/YamlConfigFile.java)/**
 * Root class of YAML configuration.
 */
YamlParseException (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/param/yaml/YamlParseException.java)/**
 * This exception is thrown if any issue arises while parsing the
 * YAML configuration.
 */
RoleParameters (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/RoleParameters.java)/**
 * This class encapsulates data related to a particular Role.
 * Some examples: TF Worker process, TF PS process or a PyTorch worker process.
 */
RunJobCli (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/RunJobCli.java)/**
 * This purpose of this class is to handle / parse CLI arguments related to
 * the run job Submarine command.
 */
JobComponentStatus (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/JobComponentStatus.java)/**
 * Status of component of training job
 */
JobStatus (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/JobStatus.java)/**
 * Status of training job.
 */
Role (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/api/Role.java)/**
 * Interface for a Role.
 */
DefaultRemoteDirectoryManager (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/fs/DefaultRemoteDirectoryManager.java)/**
 * Manages remote directories for staging, log, etc.
 * TODO, need to properly handle permission / name validation, etc.
 */
ResourceUtils (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/resource/ResourceUtils.java)/**
 * This class implements some methods with the almost the same logic as
 * org.apache.hadoop.yarn.util.resource.ResourceUtils of hadoop 3.3.
 * If the hadoop dependencies are upgraded to 3.3, this class can be refactored
 * with org.apache.hadoop.yarn.util.resource.ResourceUtils.
 */
Converter (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/resource/UnitsConversionUtil.java)/**
   * Helper class for encapsulating conversion values.
   */
UnitsConversionUtil (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/common/resource/UnitsConversionUtil.java)/**
 * Almost the same logic as UnitsConversionUtil[YARN-4081]. If the dependencies
 * are upgraded to hadoop 3.*, this class can be replaced.
 */
FSBasedSubmarineStorageImpl (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/FSBasedSubmarineStorageImpl.java)/**
 * A super naive FS-based storage.
 */
JobMonitor (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/JobMonitor.java)/**
 * Monitor status of job(s)
 */
JobSubmitter (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/JobSubmitter.java)/**
 * Submit job to cluster master.
 */
SubmarineStorage (/hadoop-submarine/hadoop-submarine-core/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/common/SubmarineStorage.java)/**
 * Persistent job/model, etc.
 */
TestRunJobCliParsingPyTorch (/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/pytorch/TestRunJobCliParsingPyTorch.java)/**
 * Test class that verifies the correctness of PyTorch
 * CLI configuration parsing.
 */
TestRunJobCliParsingPyTorchYaml (/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/pytorch/TestRunJobCliParsingPyTorchYaml.java)/**
 * Test class that verifies the correctness of PyTorch
 * YAML configuration parsing.
 */
TestRunJobCliParsingTensorFlow (/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/tensorflow/TestRunJobCliParsingTensorFlow.java)/**
 * Test class that verifies the correctness of TensorFlow
 * CLI configuration parsing.
 */
TestRunJobCliParsingTensorFlowYaml (/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/tensorflow/TestRunJobCliParsingTensorFlowYaml.java)/**
 * Test class that verifies the correctness of TF YAML configuration parsing.
 */
TestRunJobCliParsingTensorFlowYamlStandalone (/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/tensorflow/TestRunJobCliParsingTensorFlowYamlStandalone.java)/**
 * Test class that verifies the correctness of YAML configuration parsing.
 * Please note that this class just tests YAML parsing,
 * but only in an isolated fashion.
 */
TestRunJobCliParsingCommon (/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/TestRunJobCliParsingCommon.java)/**
 * This class contains some test methods to test common functionality
 * (including TF / PyTorch) of the run job Submarine command.
 */
TestRunJobCliParsingCommonYaml (/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/TestRunJobCliParsingCommonYaml.java)/**
 * This class contains some test methods to test common YAML parsing
 * functionality (including TF / PyTorch) of the run job Submarine command.
 */
TestRunJobCliParsingParameterized (/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/runjob/TestRunJobCliParsingParameterized.java)/**
 * This class contains some test methods to test common CLI parsing
 * functionality (including TF / PyTorch) of the run job Submarine command.
 */
YamlConfigTestUtils (/hadoop-submarine/hadoop-submarine-core/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/YamlConfigTestUtils.java)/**
 * Test utility class for test code that deals with YAML configuration parsing.
 */
JobStatusBuilder (/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/buider/JobStatusBuilder.java)/**
 * JobStatusBuilder builds the job status from a set of TaskInfos.
 */
TonyJobMonitor (/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyJobMonitor.java)/**
 * An implementation of JobMonitor with TonY library.
 */
TonyJobSubmitter (/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyJobSubmitter.java)/**
 * Implementation of JobSumitter with TonY runtime.
 */
TonyRuntimeFactory (/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyRuntimeFactory.java)/**
 * Implementation of RuntimeFactory with Tony Runtime
 */
TonyUtils (/hadoop-submarine/hadoop-submarine-tony-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/tony/TonyUtils.java)/**
 * Utilities for Tony Runtime.
 */
AbstractComponent (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/AbstractComponent.java)/**
 * Abstract base class for Component classes.
 * The implementations of this class are act like factories for
 * {@link Component} instances.
 * All dependencies are passed to the constructor so that child classes
 * are obliged to provide matching constructors.
 */
AbstractServiceSpec (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/AbstractServiceSpec.java)/**
 * Abstract base class that supports creating service specs for Native Service.
 */
AbstractLaunchCommand (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/AbstractLaunchCommand.java)/**
 * Abstract base class for Launch command implementations for Services.
 * Currently we have launch command implementations
 * for TensorFlow PS, worker and Tensorboard instances.
 */
LaunchCommandFactory (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/LaunchCommandFactory.java)/**
 * Interface for creating launch commands.
 */
LaunchScriptBuilder (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/LaunchScriptBuilder.java)/**
 * This class is a builder to conveniently create launch scripts.
 * All dependencies are provided with the constructor except
 * the launch command.
 */
PyTorchLaunchCommandFactory (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/PyTorchLaunchCommandFactory.java)/**
 * Simple factory to create instances of {@link AbstractLaunchCommand}
 * based on the {@link Role}.
 * All dependencies are passed to this factory that could be required
 * by any implementor of {@link AbstractLaunchCommand}.
 */
TensorFlowLaunchCommandFactory (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/TensorFlowLaunchCommandFactory.java)/**
 * Simple factory to create instances of {@link AbstractLaunchCommand}
 * based on the {@link Role}.
 * All dependencies are passed to this factory that could be required
 * by any implementor of {@link AbstractLaunchCommand}.
 */
FileSystemOperations (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/FileSystemOperations.java)/**
 * Contains methods to perform file system operations. Almost all of the methods
 * are regular non-static methods as the operations are performed with the help
 * of a {@link RemoteDirectoryManager} instance passed in as a constructor
 * dependency. Please note that some operations require to read config settings
 * as well, so that we have Submarine and YARN config objects as dependencies as
 * well.
 */
HadoopEnvironmentSetup (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/HadoopEnvironmentSetup.java)/**
 * This class contains helper methods to fill HDFS and Java environment
 * variables into scripts.
 */
PyTorchWorkerLaunchCommand (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/command/PyTorchWorkerLaunchCommand.java)/**
 * Launch command implementation for PyTorch components.
 */
PyTorchWorkerComponent (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/component/PyTorchWorkerComponent.java)/**
 * Component implementation for Worker process of PyTorch.
 */
PyTorchServiceSpec (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/pytorch/PyTorchServiceSpec.java)/**
 * This class contains all the logic to create an instance
 * of a {@link Service} object for PyTorch.
 * Please note that currently, only single-node (non-distributed)
 * support is implemented for PyTorch.
 */
ServiceSpec (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/ServiceSpec.java)/**
 * This interface is to provide means of creating wrappers around
 * {@link org.apache.hadoop.yarn.service.api.records.Service} instances.
 */
ServiceSpecFileGenerator (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/ServiceSpecFileGenerator.java)/**
 * This class is merely responsible for creating Json representation of
 * {@link Service} instances.
 */
ServiceWrapper (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/ServiceWrapper.java)/**
 * This class is only existing because we need a component name to
 * local launch command mapping from the test code.
 * Once this is solved in more clean or different way, we can delete this class.
 */
TensorBoardLaunchCommand (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorBoardLaunchCommand.java)/**
 * Launch command implementation for Tensorboard.
 */
TensorFlowLaunchCommand (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorFlowLaunchCommand.java)/**
 * Launch command implementation for
 * TensorFlow PS and Worker Service components.
 */
TensorFlowPsLaunchCommand (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorFlowPsLaunchCommand.java)/**
 * Launch command implementation for Tensorboard's PS component.
 */
TensorFlowWorkerLaunchCommand (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TensorFlowWorkerLaunchCommand.java)/**
 * Launch command implementation for Tensorboard's Worker component.
 */
TensorBoardComponent (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TensorBoardComponent.java)/**
 * Component implementation for Tensorboard's Tensorboard.
 */
TensorFlowPsComponent (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TensorFlowPsComponent.java)/**
 * Component implementation for TensorFlow's PS process.
 */
TensorFlowWorkerComponent (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TensorFlowWorkerComponent.java)/**
 * Component implementation for TensorFlow's Worker process.
 */
TensorFlowCommons (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/TensorFlowCommons.java)/**
 * This class has common helper methods for TensorFlow.
 */
TensorFlowServiceSpec (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/TensorFlowServiceSpec.java)/**
 * This class contains all the logic to create an instance
 * of a {@link Service} object for TensorFlow.
 * Worker,PS and Tensorboard components are added to the Service
 * based on the value of the received {@link RunJobParameters}.
 */
WorkerComponentFactory (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/WorkerComponentFactory.java)/**
 * Factory class that helps creating Native Service components.
 */
YarnServiceJobSubmitter (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceJobSubmitter.java)/**
 * Submit a job to cluster.
 */
YarnServiceUtils (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/YarnServiceUtils.java)/**
 * This class contains some static helper methods to query DNS data
 * based on the provided parameters.
 */
ClassPathUtilities (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/ClassPathUtilities.java)/**
 * Utilities for classpath operations.
 */
DockerUtilities (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/DockerUtilities.java)/**
 * Utilities for Docker-related operations.
 */
EnvironmentUtilities (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/EnvironmentUtilities.java)/**
 * Utilities for environment variable related operations
 * for {@link Service} objects.
 */
KerberosPrincipalFactory (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/KerberosPrincipalFactory.java)/**
 * Simple factory that creates a {@link KerberosPrincipal}.
 */
Localizer (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/Localizer.java)/**
 * This class holds all dependencies in order to localize dependencies
 * for containers.
 */
SubmarineResourceUtils (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/SubmarineResourceUtils.java)/**
 * Resource utilities for Submarine.
 */
ZipUtilities (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/main/java/org/apache/hadoop/yarn/submarine/utils/ZipUtilities.java)/**
 * Utilities for zipping directories and adding existing directories to zips.
 */
TestYarnServiceRunJobCli (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/TestYarnServiceRunJobCli.java)/**
 * Class to test YarnService with the Run job CLI action.
 */
TestYarnServiceRunJobCliCommons (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/TestYarnServiceRunJobCliCommons.java)/**
 * Common operations shared with test classes using Run job-related actions.
 */
TestYarnServiceRunJobCliLocalization (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/client/cli/yarnservice/TestYarnServiceRunJobCliLocalization.java)/**
 * Class to test YarnService localization feature with the Run job CLI action.
 */
FileUtilitiesForTests (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/FileUtilitiesForTests.java)/**
 * File utilities for tests.
 * Provides methods that can create, delete files or directories
 * in a temp directory, or any specified directory.
 */
AbstractTFLaunchCommandTestHelper (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/AbstractTFLaunchCommandTestHelper.java)/**
 * This class is an abstract base class for testing Tensorboard and TensorFlow
 * launch commands.
 */
TestLaunchCommandFactory (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/command/TestLaunchCommandFactory.java)/**
 * This class is to test the {@link TensorFlowLaunchCommandFactory}.
 */
TestTensorBoardLaunchCommand (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TestTensorBoardLaunchCommand.java)/**
 * This class is to test the {@link TensorBoardLaunchCommand}.
 */
TestTensorFlowLaunchCommand (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/command/TestTensorFlowLaunchCommand.java)/**
 * This class is to test the implementors of {@link TensorFlowLaunchCommand}.
 */
ComponentTestCommons (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/ComponentTestCommons.java)/**
 * This class has some helper methods and fields
 * in order to test TensorFlow-related Components easier.
 */
TestTensorBoardComponent (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TestTensorBoardComponent.java)/**
 * This class is to test {@link TensorBoardComponent}.
 */
TestTensorFlowPsComponent (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TestTensorFlowPsComponent.java)/**
 * This class is to test {@link TensorFlowPsComponent}.
 */
TestTensorFlowWorkerComponent (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/tensorflow/component/TestTensorFlowWorkerComponent.java)/**
 * This class is to test {@link TensorFlowWorkerComponent}.
 */
TestServiceWrapper (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/TestServiceWrapper.java)/**
 * Class to test the {@link ServiceWrapper}.
 */
TestTFConfigGenerator (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/runtimes/yarnservice/TestTFConfigGenerator.java)/**
 * Class to test some functionality of {@link TensorFlowCommons}.
 */
TestClassPathUtilities (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestClassPathUtilities.java)/**
 * This class is to test {@link ClassPathUtilities}.
 */
TestEnvironmentUtilities (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestEnvironmentUtilities.java)/**
 * This class is to test {@link EnvironmentUtilities}.
 */
TestKerberosPrincipalFactory (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestKerberosPrincipalFactory.java)/**
 * This class is to test {@link KerberosPrincipalFactory}.
 */
CustomResourceTypesConfigurationProvider (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestSubmarineResourceUtils.java)/**
   * With the dependencies of hadoop 3.2.0, Need to create a
   * CustomResourceTypesConfigurationProvider implementations. If the
   * dependencies are upgraded to hadoop 3.3.0. It can be replaced by
   * org.apache.hadoop.yarn.util.resource.CustomResourceTypesConfigurationProvi-
   * der
   */
TestSubmarineResourceUtils (/hadoop-submarine/hadoop-submarine-yarnservice-runtime/src/test/java/org/apache/hadoop/yarn/submarine/utils/TestSubmarineResourceUtils.java)/**
 * This class is to test {@link SubmarineResourceUtils}.
 */
AliyunCredentialsProvider (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunCredentialsProvider.java)/**
 * Support session credentials for authenticating with Aliyun.
 */
AliyunOSSBlockOutputStream (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSBlockOutputStream.java)/**
 * Asynchronous multi-part based uploading mechanism to support huge file
 * which is larger than 5GB. Data will be buffered on local disk, then uploaded
 * to OSS in {@link #close()} method.
 */
AliyunOSSCopyFileContext (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSCopyFileContext.java)/**
 * Used by {@link AliyunOSSFileSystem} and {@link AliyunOSSCopyFileTask}
 * as copy context. It contains some variables used in copy process.
 */
AliyunOSSCopyFileTask (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSCopyFileTask.java)/**
 * Used by {@link AliyunOSSFileSystem} as an task that submitted
 * to the thread pool to accelerate the copy progress.
 * Each AliyunOSSCopyFileTask copies one file from src path to dst path
 */
AliyunOSSFileReaderTask (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSFileReaderTask.java)/**
 * Used by {@link AliyunOSSInputStream} as an task that submitted
 * to the thread pool.
 * Each AliyunOSSFileReaderTask reads one part of the file so that
 * we can accelerate the sequential read.
 */
AliyunOSSFileSystem (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSFileSystem.java)/**
 * Implementation of {@link FileSystem} for <a href="https://oss.aliyun.com">
 * Aliyun OSS</a>, used to access OSS blob system in a filesystem style.
 */
AliyunOSSFileSystemStore (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSFileSystemStore.java)/**
 * Core implementation of Aliyun OSS Filesystem for Hadoop.
 * Provides the bridging logic between Hadoop's abstract filesystem and
 * Aliyun OSS.
 */
AliyunOSSInputStream (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSInputStream.java)/**
 * The input stream for OSS blob system.
 * The class uses multi-part downloading to read data from the object content
 * stream.
 */
AliyunOSSUtils (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSUtils.java)/**
 * Utility methods for Aliyun OSS code.
 */
Constants (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/Constants.java)/**
 * ALL configuration constants for OSS filesystem.
 */
AcceptFilesOnly (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/FileStatusAcceptor.java)/**
   * Accept all entries except the base path.
   */
AcceptAllButSelf (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/FileStatusAcceptor.java)/**
   * Accept all entries except the base path.
   */
FileStatusAcceptor (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/FileStatusAcceptor.java)/**
 * Interface to implement by the logic deciding whether to accept a summary
 * entry or path as a valid file or directory.
 */
OSS (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/OSS.java)/**
 * OSS implementation of AbstractFileSystem.
 * This impl delegates to the AliyunOSSFileSystem.
 */
OSSFileStatus (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/OSSFileStatus.java)/**
 * This class is used by listStatus for oss files.
 */
ReadBuffer (/hadoop-tools/hadoop-aliyun/src/main/java/org/apache/hadoop/fs/aliyun/oss/ReadBuffer.java)/**
 * This class is used by {@link AliyunOSSInputStream}
 * and {@link AliyunOSSFileReaderTask} to buffer data that read from oss.
 */
AliyunOSSTestUtils (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/AliyunOSSTestUtils.java)/**
 * Utility class for Aliyun OSS Tests.
 */
AliyunOSSContract (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/contract/AliyunOSSContract.java)/**
 * The contract of Aliyun OSS: only enabled if the test bucket is provided.
 */
TestAliyunOSSContractCreate (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/contract/TestAliyunOSSContractCreate.java)/**
 * Aliyun OSS contract creating tests.
 */
TestAliyunOSSContractDelete (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/contract/TestAliyunOSSContractDelete.java)/**
 * Aliyun OSS contract deleting tests.
 */
TestAliyunOSSContractDistCp (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/contract/TestAliyunOSSContractDistCp.java)/**
 * Contract test suite covering Aliyun OSS integration with DistCp.
 */
TestAliyunOSSContractGetFileStatus (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/contract/TestAliyunOSSContractGetFileStatus.java)/**
 * Test getFileStatus and related listing operations.
 */
TestAliyunOSSContractMkdir (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/contract/TestAliyunOSSContractMkdir.java)/**
 * Aliyun OSS contract directory tests.
 */
TestAliyunOSSContractOpen (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/contract/TestAliyunOSSContractOpen.java)/**
 * Aliyun OSS contract opening file tests.
 */
TestAliyunOSSContractRename (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/contract/TestAliyunOSSContractRename.java)/**
 * Aliyun OSS contract renaming tests.
 */
TestAliyunOSSContractRootDir (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/contract/TestAliyunOSSContractRootDir.java)/**
 * Root dir operations against an Aliyun OSS bucket.
 */
TestAliyunOSSContractSeek (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/contract/TestAliyunOSSContractSeek.java)/**
 * Aliyun OSS contract seeking tests.
 */
TestOSSFileContext (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/fileContext/TestOSSFileContext.java)/**
 * Implementation of TestFileContext for OSS.
 */
TestOSSFileContextCreateMkdir (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/fileContext/TestOSSFileContextCreateMkdir.java)/**
 * OSS implementation of FileContextCreateMkdirBaseTest.
 */
TestOSSFileContextMainOperations (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/fileContext/TestOSSFileContextMainOperations.java)/**
 * OSS implementation of FileContextMainOperationsBaseTest.
 */
TestOSSFileContextStatistics (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/fileContext/TestOSSFileContextStatistics.java)/**
 * OSS implementation of FCStatisticsBaseTest.
 */
TestOSSFileContextURI (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/fileContext/TestOSSFileContextURI.java)/**
 * OSS implementation of FileContextURIBase.
 */
TestOSSFileContextUtil (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/fileContext/TestOSSFileContextUtil.java)/**
 * OSS implementation of FileContextUtilBase.
 */
TestAliyunCredentials (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/TestAliyunCredentials.java)/**
 * Tests use of temporary credentials (for example, Aliyun STS & Aliyun OSS).
 * This test extends a class that "does things to the root directory", and
 * should only be used against transient filesystems where you don't care about
 * the data.
 */
TestAliyunOSSBlockOutputStream (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/TestAliyunOSSBlockOutputStream.java)/**
 * Tests regular and multi-part upload functionality for
 * AliyunOSSBlockOutputStream.
 */
TestAliyunOSSFileSystemContract (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/TestAliyunOSSFileSystemContract.java)/**
 * Tests a live Aliyun OSS system.
 */
TestAliyunOSSFileSystemStore (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/TestAliyunOSSFileSystemStore.java)/**
 * Test the bridging logic between Hadoop's abstract filesystem and
 * Aliyun OSS.
 */
TestAliyunOSSInputStream (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/TestAliyunOSSInputStream.java)/**
 * Tests basic functionality for AliyunOSSInputStream, including seeking and
 * reading files.
 */
TestOSS (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/yarn/TestOSS.java)/**
 * OSS tests through the {@link FileContext} API.
 */
TestOSSMiniYarnCluster (/hadoop-tools/hadoop-aliyun/src/test/java/org/apache/hadoop/fs/aliyun/oss/yarn/TestOSSMiniYarnCluster.java)/**
 * Tests that OSS is usable through a YARN application.
 */
HadoopArchiveLogs (/hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogs.java)/**
 * This tool moves Aggregated Log files into HAR archives using the
 * {@link HadoopArchives} tool and the Distributed Shell via the
 * {@link HadoopArchiveLogsRunner}.
 */
HadoopArchiveLogsRunner (/hadoop-tools/hadoop-archive-logs/src/main/java/org/apache/hadoop/tools/HadoopArchiveLogsRunner.java)/**
 * This is a child program designed to be used by the {@link HadoopArchiveLogs}
 * tool via the Distributed Shell.  It's not meant to be run directly.
 */
HarEntry (/hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java)/** HarEntry is used in the {@link HArchivesMapper} as the input value. */
HArchiveInputFormat (/hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java)/**
   * Input format of a hadoop archive job responsible for 
   * generating splits of the file list
   */
FileStatusDir (/hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java)/**
   * A static class that keeps
   * track of status of a path 
   * and there children if path is a dir
   */
HArchivesReducer (/hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java)/** the reduce for creating the index and the master index 
   * 
   */
HadoopArchives (/hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java)/**
 * a archive creation utility.
 * This class provides methods that can be used 
 * to create hadoop archives. For understanding of 
 * Hadoop archives look at {@link HarFileSystem}.
 */
TestHadoopArchives (/hadoop-tools/hadoop-archives/src/test/java/org/apache/hadoop/tools/TestHadoopArchives.java)/**
 * test {@link HadoopArchives}
 */
AnonymousAWSCredentialsProvider (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/AnonymousAWSCredentialsProvider.java)/**
 * AnonymousAWSCredentialsProvider supports anonymous access to AWS services
 * through the AWS SDK.  AWS requests will not be signed.  This is not suitable
 * for most cases, because allowing anonymous access to an S3 bucket compromises
 * security.  This can be useful for accessing public data sets without
 * requiring AWS credentials.
 *
 * Please note that users may reference this class name from configuration
 * property fs.s3a.aws.credentials.provider.  Therefore, changing the class name
 * would be a backward-incompatible change.
 */
AbstractAWSCredentialProvider (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/AbstractAWSCredentialProvider.java)/**
 * Base class for AWS credential providers which
 * take a URI and config in their constructor.
 */
NoCredentials (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/AbstractSessionCredentialsProvider.java)/**
   * A special set of null credentials which are not the anonymous class.
   * This will be interpreted as "this provider has no credentials to offer",
   * rather than an explicit error or anonymous access.
   */
AbstractSessionCredentialsProvider (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/AbstractSessionCredentialsProvider.java)/**
 * Base class for session credential support.
 */
AssumedRoleCredentialProvider (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/AssumedRoleCredentialProvider.java)/**
 * Support IAM Assumed roles by instantiating an instance of
 * {@code STSAssumeRoleSessionCredentialsProvider} from configuration
 * properties, including wiring up the inner authenticator, and,
 * unless overridden, creating a session name from the current user.
 *
 * Classname is used in configuration files; do not move.
 */
AwsSignerInitializer (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/AwsSignerInitializer.java)/**
 * Interface which can be implemented to allow initialization of any custom
 * signers which may be used by the {@link S3AFileSystem}.
 */
TokenSecretManager (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding.java)/**
   * The secret manager always uses the same secret; the
   * factory for new identifiers is that of the token manager.
   */
AbstractDelegationTokenBinding (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationTokenBinding.java)/**
 *  An AbstractDelegationTokenBinding implementation is a class which
 *  handles the binding of its underlying authentication mechanism to the
 *  Hadoop Delegation token mechanism.
 *
 *  See also {@code org.apache.hadoop.fs.azure.security.WasbDelegationTokenManager}
 *  but note that it assumes Kerberos tokens for which the renewal mechanism
 *  is the sole plugin point.
 *  This class is designed to be more generic.
 *
 *  <b>Lifecycle</b>
 *
 *  It is a Hadoop Service, so has a standard lifecycle: once started
 *  its lifecycle will follow that of the {@link S3ADelegationTokens}
 *  instance which created it --which itself follows the lifecycle of the FS.
 *
 *  One big difference is that
 *  {@link #bindToFileSystem(URI, S3AFileSystem)} will be called
 *  before the {@link #init(Configuration)} operation, this is where
 *  the owning FS is passed in.
 *
 *  Implementations are free to start background operations in their
 *  {@code serviceStart()} method, provided they are safely stopped in
 *  {@code serviceStop()}.
 *
 *  <b>When to check for the ability to issue tokens</b>
 *  Implementations MUST start up without actually holding the secrets
 *  needed to issue tokens (config options, credentials to talk to STS etc)
 *  as in server-side deployments they are not expected to have these.
 *
 *  <b>Retry Policy</b>
 *
 *  All methods which talk to AWS services are expected to do translation,
 *  with retries as they see fit.
 */
AbstractDTService (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/AbstractDTService.java)/**
 * This is the base class for both the delegation binding
 * code and the back end service created; allows for
 * shared methods across both.
 *
 * The lifecycle sequence is as follows
 * <pre>
 *   - create
 *   - bindToFileSystem(uri, ownerFS)
 *   - init
 *   - start
 *   ...api calls...
 *   - stop
 * </pre>
 *
 * As the S3ADelegation mechanism is all configured during the filesystem
 * initalize() operation, it is not ready for use through all the start process.
 */
AbstractS3ATokenIdentifier (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/AbstractS3ATokenIdentifier.java)/**
 * An S3A Delegation Token Identifier: contains the information needed
 * to talk to S3A.
 *
 * These are loaded via the service loader API an used in a map of
 * Kind to class, which is then looked up to deserialize token
 * identifiers of a given class.
 *
 * Every non-abstract class must provide
 * <ol>
 *   <li>Their unique token kind.</li>
 *   <li>An empty constructor.</li>
 *   <li>An entry in the resource file
 *   {@code /META-INF/services/org.apache.hadoop.security.token.TokenIdentifier}
 *   </li>
 * </ol>
 *
 * The base implementation contains
 * <ol>
 *   <li>The URI of the FS.</li>
 *   <li>Encryption secrets for use in the destination FS.</li>
 * </ol>
 * Subclasses are required to add whatever information is needed to authenticate
 * the user with the credential provider which their binding class will
 * provide.
 *
 * <i>Important: Add no references to any AWS SDK class, to
 * ensure it can be safely deserialized whenever the relevant token
 * identifier of a token type declared in this JAR is examined.</i>
 */
AWSPolicyProvider (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/AWSPolicyProvider.java)/**
 * Interface for providers of AWS policy for accessing data.
 * This is used when building up the role permissions for a delegation
 * token.
 *
 * The permissions requested are from the perspective of
 * S3A filesystem operations on the data, <i>not</i> the simpler
 * model of "permissions on the the remote service".
 * As an example, to use S3Guard effectively, the client needs full CRUD
 * access to the table, even for {@link AccessLevel#READ}.
 */
DelegationConstants (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/DelegationConstants.java)/**
 * All the constants related to delegation tokens.
 * Not in the normal S3 constants while unstable.
 *
 * Where possible, the existing assumed role properties are used to configure
 * STS binding, default ARN, etc. This makes documenting everything that
 * much easier and avoids trying to debug precisely which sts endpoint
 * property should be set.
 *
 * Most settings here are replicated in {@code core-default.xml}; the
 * values MUST be kept in sync.
 */
DelegationTokenIOException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/DelegationTokenIOException.java)/**
 * General IOException for Delegation Token issues.
 * Includes recommended error strings, which can be used in tests when
 * looking for specific errors.
 */
DelegationTokenProvider (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/DelegationTokenProvider.java)/**
 * Interface for S3A Delegation Token access.
 */
EncryptionSecretOperations (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/EncryptionSecretOperations.java)/**
 * These support operations on {@link EncryptionSecrets} which use the AWS SDK
 * operations. Isolating them here ensures that that class is not required on
 * the classpath.
 */
EncryptionSecrets (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/EncryptionSecrets.java)/**
 * Encryption options in a form which can serialized or marshalled as a hadoop
 * Writeable.
 *
 * Maintainers: For security reasons, don't print any of this.
 *
 * Note this design marshalls/unmarshalls its serialVersionUID
 * in its writable, which is used to compare versions.
 *
 * <i>Important.</i>
 * If the wire format is ever changed incompatibly,
 * update the serial version UID to ensure that older clients get safely
 * rejected.
 *
 * <i>Important</i>
 * Do not import any AWS SDK classes, directly or indirectly.
 * This is to ensure that S3A Token identifiers can be unmarshalled even
 * without that SDK.
 */
FullCredentialsTokenBinding (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/FullCredentialsTokenBinding.java)/**
 * Full credentials: they are simply passed as-is, rather than
 * converted to a session.
 * These aren't as secure; this class exists to (a) support deployments
 * where there is not STS service and (b) validate the design of
 * S3A DT support to support different managers.
 */
FullCredentialsTokenIdentifier (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/FullCredentialsTokenIdentifier.java)/**
 * The full credentials payload is the same of that for a session token, but
 * a different token kind is used.
 *
 * Token kind is {@link DelegationConstants#FULL_TOKEN_KIND}.
 */
RoleTokenBinding (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/RoleTokenBinding.java)/**
 * Role Token support requests an explicit role and automatically restricts
 * that role to the given policy of the binding.
 * The session is locked down as much as possible.
 */
RoleTokenIdentifier (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/RoleTokenIdentifier.java)/**
 * Role token identifier.
 * Token kind is {@link DelegationConstants#ROLE_TOKEN_KIND}
 */
S3ADelegationTokens (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/S3ADelegationTokens.java)/**
 * Support for creating a DT from a filesystem.
 *
 * Isolated from S3A for control and testability.
 *
 * The S3A Delegation Tokens are special in that the tokens are not directly
 * used to authenticate with the AWS services.
 * Instead they can session/role  credentials requested off AWS on demand.
 *
 * The design is extensible in that different back-end bindings can be used
 * to switch to different session creation mechanisms, or indeed, to any
 * other authentication mechanism supported by an S3 service, provided it
 * ultimately accepts some form of AWS credentials for authentication through
 * the AWS SDK. That is, if someone wants to wire this up to Kerberos, or
 * OAuth2, this design should support them.
 *
 * URIs processed must be the canonical URIs for the service.
 */
S3ADtFetcher (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/S3ADtFetcher.java)/**
 * A DT fetcher for S3A.
 * This is a copy-and-paste of
 * {@code org.apache.hadoop.hdfs.HdfsDtFetcher}.
 *
 * It is only needed for the `hadoop dtutil` command.
 */
SessionTokenBinding (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/SessionTokenBinding.java)/**
 * The session token DT binding: creates an AWS session token
 * for the DT, extracts and serves it up afterwards.
 */
SessionTokenIdentifier (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/delegation/SessionTokenIdentifier.java)/**
 * A token identifier which contains a set of AWS session credentials,
 * credentials which will be valid until they expire.
 *
 * <b>Note 1:</b>
 * There's a risk here that the reference to {@link MarshalledCredentials}
 * may trigger a transitive load of AWS classes, a load which will
 * fail if the aws SDK isn't on the classpath.
 *
 * <b>Note 2:</b>
 * This class does support subclassing, but every subclass MUST declare itself
 * to be of a different token kind.
 * Otherwise the process for decoding tokens breaks.
 */
IAMInstanceCredentialsProvider (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/IAMInstanceCredentialsProvider.java)/**
 * This is an IAM credential provider which wraps
 * an {@code EC2ContainerCredentialsProviderWrapper}
 * to provide credentials when the S3A connector is instantiated on AWS EC2
 * or the AWS container services.
 * <p>
 * When it fails to authenticate, it raises a
 * {@link NoAwsCredentialsException} which can be recognized by retry handlers
 * as a non-recoverable failure.
 * <p>
 * It is implicitly public; marked evolving as we can change its semantics.
 */
MarshalledCredentialBinding (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/MarshalledCredentialBinding.java)/**
 * Class to bridge from the serializable/marshallabled
 * {@link MarshalledCredentialBinding} class to/from AWS classes.
 * This is to keep that class isolated and not dependent on aws-sdk JARs
 * to load.
 */
MarshalledCredentialProvider (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/MarshalledCredentialProvider.java)/**
 * AWS credential provider driven from marshalled session/full credentials
 * (full, simple session or role).
 * This is <i>not</i> intended for explicit use in job/app configurations,
 * instead it is returned by Delegation Token Bindings, as needed.
 * The constructor implicitly prevents explicit use.
 */
MarshalledCredentials (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/MarshalledCredentials.java)/**
 * Stores the credentials for a session or for a full login.
 * This structure is {@link Writable}, so can be marshalled inside a
 * delegation token.
 *
 * The class is designed so that keys inside are kept non-null; to be
 * unset just set them to the empty string. This is to simplify marshalling.
 *
 * <i>Important: Add no references to any AWS SDK class, to
 * ensure it can be safely deserialized whenever the relevant token
 * identifier of a token type declared in this JAR is examined.</i>
 */
NoAuthWithAWSException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/NoAuthWithAWSException.java)/**
 * A specific subclass of {@code AmazonClientException} which is
 * used in the S3A retry policy to fail fast when there is any
 * authentication problem.
 */
NoAwsCredentialsException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/NoAwsCredentialsException.java)/**
 * A special exception which declares that no credentials were found;
 * this can be treated specially in logging, handling, etc.
 * As it subclasses {@link NoAuthWithAWSException}, the S3A retry handler
 * knows not to attempt to ask for the credentials again.
 */
RoleElt (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/RoleModel.java)/**
   * Any element in a role.
   */
Statement (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/RoleModel.java)/**
   * A single statement.
   */
Policy (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/RoleModel.java)/**
   * A policy is one or more statements.
   */
RoleModel (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/RoleModel.java)/**
 * Jackson Role Model for Role Properties, for API clients and tests.
 *
 * Doesn't have complete coverage of the entire AWS IAM policy model;
 * don't expect to be able to parse everything.
 * It can generate simple models.
 * @see <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-arn-format.html">Example S3 Policies</a>
 * @see <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/api-permissions-reference.html">Dynamno DB Permissions</a>
 */
RolePolicies (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/RolePolicies.java)/**
 * Operations, statements and policies covering the operations
 * needed to work with S3 and S3Guard.
 */
SignerManager (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/SignerManager.java)/**
 * Class to handle custom signers.
 */
STSClient (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/STSClientFactory.java)/**
   * STS client connection with retries.
   */
STSClientFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/auth/STSClientFactory.java)/**
 * Factory for creating STS Clients.
 */
AWSBadRequestException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/AWSBadRequestException.java)/**
 * A 400 "Bad Request" exception was received.
 * This is the general "bad parameters, headers, whatever" failure.
 */
AWSClientIOException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/AWSClientIOException.java)/**
 * IOException equivalent of an {@link AmazonClientException}.
 */
AWSCredentialProviderList (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/AWSCredentialProviderList.java)/**
 * A list of providers.
 *
 * This is similar to the AWS SDK {@code AWSCredentialsProviderChain},
 * except that:
 * <ol>
 *   <li>Allows extra providers to be added dynamically.</li>
 *   <li>If any provider in the chain throws an exception other than
 *   an {@link AmazonClientException}, that is rethrown, rather than
 *   swallowed.</li>
 *   <li>Has some more diagnostics.</li>
 *   <li>On failure, the last "relevant" AmazonClientException raised is
 *   rethrown; exceptions other than 'no credentials' have priority.</li>
 *   <li>Special handling of {@link AnonymousAWSCredentials}.</li>
 * </ol>
 */
AWSNoResponseException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/AWSNoResponseException.java)/**
 * Status code 443, no response from server. This is considered idempotent.
 */
AWSRedirectException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/AWSRedirectException.java)/**
 * Request is redirected.
 * If this gets as far as the user, it's unrecoverable
 */
AWSS3IOException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/AWSS3IOException.java)/**
 * Wrap a {@link AmazonS3Exception} as an IOE, relaying all
 * getters.
 */
AWSServiceIOException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/AWSServiceIOException.java)/**
 * A specific exception from AWS operations.
 * The exception must always be created with an {@link AmazonServiceException}.
 * The attributes of this exception can all be directly accessed.
 */
AWSServiceThrottledException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/AWSServiceThrottledException.java)/**
 * Exception raised when a service was throttled.
 */
AWSStatus500Exception (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/AWSStatus500Exception.java)/**
 * A 500 response came back from a service.
 * This is considered <i>probably</i> retriable, That is, we assume
 * <ol>
 *   <li>whatever error happened in the service itself to have happened
 *    before the infrastructure committed the operation.</li>
 *    <li>Nothing else got through either.</li>
 * </ol>
 */
ActiveCommit (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/AbstractS3ACommitter.java)/**
   * State of the active commit operation.
   *
   * It contains a list of all pendingset files to load as the source
   * of outstanding commits to complete/abort,
   * and tracks the files uploaded.
   *
   * To avoid running out of heap by loading all the source files
   * simultaneously:
   * <ol>
   *   <li>
   *     The list of files to load is passed round but
   *     the contents are only loaded on demand.
   *   </li>
   *   <li>
   *     The number of written files tracked for logging in
   *     the _SUCCESS file are limited to a small amount -enough
   *     for testing only.
   *   </li>
   * </ol>
   */
AbstractS3ACommitter (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/AbstractS3ACommitter.java)/**
 * Abstract base class for S3A committers; allows for any commonality
 * between different architectures.
 *
 * Although the committer APIs allow for a committer to be created without
 * an output path, this is not supported in this class or its subclasses:
 * a destination must be supplied. It is left to the committer factory
 * to handle the creation of a committer when the destination is unknown.
 *
 * Requiring an output directory simplifies coding and testing.
 *
 * The original implementation loaded all .pendingset files
 * before attempting any commit/abort operations.
 * While straightforward and guaranteeing that no changes were made to the
 * destination until all files had successfully been loaded -it didn't scale;
 * the list grew until it exceeded heap size.
 *
 * The second iteration builds up an {@link ActiveCommit} class with the
 * list of .pendingset files to load and then commit; that can be done
 * incrementally and in parallel.
 * As a side effect of this change, unless/until changed,
 * the commit/abort/revert of all files uploaded by a single task will be
 * serialized. This may slow down these operations if there are many files
 * created by a few tasks, <i>and</i> the HTTP connection pool in the S3A
 * committer was large enough for more all the parallel POST requests.
 */
AbstractS3ACommitterFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/AbstractS3ACommitterFactory.java)/**
 * Dynamically create the output committer based on subclass type and settings.
 */
CommitConstants (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/CommitConstants.java)/**
 * Constants for working with committers.
 */
CommitContext (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/CommitOperations.java)/**
   * Commit context.
   *
   * It is used to manage the final commit sequence where files become
   * visible. It contains a {@link BulkOperationState} field, which, if
   * there is a metastore, will be requested from the store so that it
   * can track multiple creation operations within the same overall operation.
   * This will be null if there is no metastore, or the store chooses not
   * to provide one.
   *
   * This can only be created through {@link #initiateCommitOperation(Path)}.
   *
   * Once the commit operation has completed, it must be closed.
   * It must not be reused.
   */
MaybeIOE (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/CommitOperations.java)/**
   * A holder for a possible IOException; the call {@link #maybeRethrow()}
   * will throw any exception passed into the constructor, and be a no-op
   * if none was.
   *
   * Why isn't a Java 8 optional used here? The main benefit would be that
   * {@link #maybeRethrow()} could be done as a map(), but because Java doesn't
   * allow checked exceptions in a map, the following code is invalid
   * <pre>
   *   exception.map((e) -&gt; {throw e;}
   * </pre>
   * As a result, the code to work with exceptions would be almost as convoluted
   * as the original.
   */
CommitOperations (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/CommitOperations.java)/**
 * The implementation of the various actions a committer needs.
 * This doesn't implement the protocol/binding to a specific execution engine,
 * just the operations needed to to build one.
 *
 * When invoking FS operations, it assumes that the underlying FS is
 * handling retries and exception translation: it does not attempt to
 * duplicate that work.
 *
 */
CommitUtils (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/CommitUtils.java)/**
 * Static utility methods related to S3A commitment processing, both
 * staging and magic.
 *
 * <b>Do not use in any codepath intended to be used from the S3AFS
 * except in the committers themselves.</b>
 */
CommitUtilsWithMR (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/CommitUtilsWithMR.java)/**
 * These are commit utility methods which import classes from
 * hadoop-mapreduce, and so only work when that module is on the
 * classpath.
 *
 * <b>Do not use in any codepath intended to be used from the S3AFS
 * except in the committers themselves.</b>
 */
PendingSet (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/files/PendingSet.java)/**
 * Persistent format for multiple pending commits.
 * Contains 0 or more {@link SinglePendingCommit} entries; validation logic
 * checks those values on load.
 */
PersistentCommitData (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/files/PersistentCommitData.java)/**
 * Class for single/multiple commit data structures.
 */
SinglePendingCommit (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/files/SinglePendingCommit.java)/**
 * This is the serialization format for uploads yet to be committerd.
 *
 * It's marked as {@link Serializable} so that it can be passed in RPC
 * calls; for this to work it relies on the fact that java.io ArrayList
 * and LinkedList are serializable. If any other list type is used for etags,
 * it must also be serialized. Jackson expects lists, and it is used
 * to persist to disk.
 *
 */
SuccessData (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/files/SuccessData.java)/**
 * Summary data saved into a {@code _SUCCESS} marker file.
 *
 * This provides an easy way to determine which committer was used
 * to commit work.
 * <ol>
 *   <li>File length == 0: classic {@code FileOutputCommitter}.</li>
 *   <li>Loadable as {@link SuccessData}:
 *   A s3guard committer with name in in {@link #committer} field.</li>
 *   <li>Not loadable? Something else.</li>
 * </ol>
 *
 * This is an unstable structure intended for diagnostics and testing.
 * Applications reading this data should use/check the {@link #name} field
 * to differentiate from any other JSON-based manifest and to identify
 * changes in the output format.
 *
 * Note: to deal with scale issues, the S3A committers do not include any
 * more than the number of objects listed in
 * {@link org.apache.hadoop.fs.s3a.commit.CommitConstants#SUCCESS_MARKER_FILE_LIMIT}.
 * This is intended to suffice for basic integration tests.
 * Larger tests should examine the generated files themselves.
 */
InternalCommitterConstants (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/InternalCommitterConstants.java)/**
 * These are internal constants not intended for public use.
 */
LocalTempDir (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/LocalTempDir.java)/**
 * A class which manages access to a temporary directory store, uses the
 * directories listed in {@link Constants#BUFFER_DIR} for this.
 */
MagicCommitTracker (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/magic/MagicCommitTracker.java)/**
 * Put tracker for Magic commits.
 * <p>Important</p>: must not directly or indirectly import a class which
 * uses any datatype in hadoop-mapreduce.
 */
MagicS3GuardCommitter (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/magic/MagicS3GuardCommitter.java)/**
 * This is a dedicated committer which requires the "magic" directory feature
 * of the S3A Filesystem to be enabled; it then uses paths for task and job
 * attempts in magic paths, so as to ensure that the final output goes direct
 * to the destination directory.
 */
MagicS3GuardCommitterFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/magic/MagicS3GuardCommitterFactory.java)/**
 * Factory for the Magic committer.
 */
MagicCommitIntegration (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/MagicCommitIntegration.java)/**
 * Adds the code needed for S3A to support magic committers.
 * It's pulled out to keep S3A FS class slightly less complex.
 * This class can be instantiated even when magic commit is disabled;
 * in this case:
 * <ol>
 *   <li>{@link #isMagicCommitPath(Path)} will always return false.</li>
 *   <li>{@link #createTracker(Path, String)} will always return an instance
 *   of {@link PutTracker}.</li>
 * </ol>
 *
 * <p>Important</p>: must not directly or indirectly import a class which
 * uses any datatype in hadoop-mapreduce.
 */
MagicCommitPaths (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/MagicCommitPaths.java)/**
 * Operations on (magic) paths.
 */
PathCommitException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/PathCommitException.java)/**
 * Path exception to use for various commit issues.
 */
PutTracker (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/PutTracker.java)/**
 * Multipart put tracker.
 * Base class does nothing except declare that any
 * MPU must complete in the {@code close()} operation.
 *
 */
S3ACommitterFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/S3ACommitterFactory.java)/**
 * The S3A committer factory which chooses the committer based on the
 * specific option chosen in a per-bucket basis from the property
 * {@link CommitConstants#FS_S3A_COMMITTER_NAME}.
 *
 * This should be instantiated by using the property value {@link #CLASSNAME}
 * as the committer for the job, then set the filesystem property
 * {@link CommitConstants#FS_S3A_COMMITTER_NAME} to one of
 * <ul>
 *   <li>{@link CommitConstants#COMMITTER_NAME_FILE}: File committer.</li>
 *   <li>{@link CommitConstants#COMMITTER_NAME_DIRECTORY}:
 *   Staging directory committer.</li>
 *   <li>{@link CommitConstants#COMMITTER_NAME_PARTITIONED}:
 *   Staging partitioned committer.</li>
 *   <li>{@link CommitConstants#COMMITTER_NAME_MAGIC}:
 *   the "Magic" committer</li>
 *   <li>{@link InternalCommitterConstants#COMMITTER_NAME_STAGING}:
 *   the "staging" committer, which isn't intended for use outside tests.</li>
 * </ul>
 * There are no checks to verify that the filesystem is compatible with
 * the committer.
 */
DirectoryStagingCommitter (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/DirectoryStagingCommitter.java)/**
 * This commits to a directory.
 * The conflict policy is
 * <ul>
 *   <li>FAIL: fail the commit</li>
 *   <li>APPEND: add extra data to the destination.</li>
 *   <li>REPLACE: delete the destination directory in the job commit
 *   (i.e. after and only if all tasks have succeeded.</li>
 * </ul>
 */
DirectoryStagingCommitterFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/DirectoryStagingCommitterFactory.java)/**
 * Factory for the Directory committer.
 */
PartitionedStagingCommitter (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/PartitionedStagingCommitter.java)/**
 * Partitioned committer.
 * This writes data to specific "partition" subdirectories, applying
 * conflict resolution on a partition-by-partition basis. The existence
 * and state of any parallel partitions for which there is no are output
 * files are not considered in the conflict resolution.
 *
 * The conflict policy is
 * <ul>
 *   <li>FAIL: fail the commit if any of the partitions have data.</li>
 *   <li>APPEND: add extra data to the destination partitions.</li>
 *   <li>REPLACE: delete the destination partition in the job commit
 *   (i.e. after and only if all tasks have succeeded.</li>
 * </ul>
 * To determine the paths, the precommit process actually has to read
 * in all source files, independently of the final commit phase.
 * This is inefficient, though some parallelization here helps.
 */
PartitionedStagingCommitterFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/PartitionedStagingCommitterFactory.java)/**
 * Factory for the {@link PartitionedStagingCommitter}.
 */
Paths (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/Paths.java)/**
 * Path operations for the staging committers.
 */
StagingCommitter (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/StagingCommitter.java)/**
 * Committer based on the contributed work of the
 * <a href="https://github.com/rdblue/s3committer">Netflix multipart committers.</a>
 * <ol>
 *   <li>
 *   The working directory of each task is actually under a temporary
 *   path in the local filesystem; jobs write directly into it.
 *   </li>
 *   <li>
 *     Task Commit: list all files under the task working dir, upload
 *     each of them but do not commit the final operation.
 *     Persist the information for each pending commit into the cluster
 *     for enumeration and commit by the job committer.
 *   </li>
 *   <li>Task Abort: recursive delete of task working dir.</li>
 *   <li>Job Commit: list all pending PUTs to commit; commit them.</li>
 *   <li>
 *     Job Abort: list all pending PUTs to commit; abort them.
 *     Delete all task attempt directories.
 *   </li>
 * </ol>
 *
 * This is the base class of the Partitioned and Directory committers.
 * It does not do any conflict resolution, and is made non-abstract
 * primarily for test purposes. It is not expected to be used in production.
 */
StagingCommitterConstants (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/StagingCommitterConstants.java)/**
 * Internal staging committer constants.
 */
StagingCommitterFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/staging/StagingCommitterFactory.java)/**
 * Factory for the staging committer.
 * This is for internal test use, rather than the public directory and
 * partitioned committers.
 */
Task (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/Tasks.java)/**
   * Callback invoked to process an item.
   * @param <I> item type being processed
   * @param <E> exception class which may be raised
   */
FailureTask (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/Tasks.java)/**
   * Callback invoked on a failure.
   * @param <I> item type being processed
   * @param <E> exception class which may be raised
   */
Builder (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/Tasks.java)/**
   * Builder for task execution.
   * @param <I> item type
   */
Tasks (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/Tasks.java)/**
 * Utility class for parallel execution, takes closures for the various
 * actions.
 * There is no retry logic: it is expected to be handled by the closures.
 */
ValidationFailure (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/ValidationFailure.java)/**
 * Exception raised on validation failures; kept as an IOException
 * for consistency with other failures.
 */
Constants (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java)/**
 * All the constants used with the {@link S3AFileSystem}.
 *
 * Some of the strings are marked as {@code Unstable}. This means
 * that they may be unsupported in future; at which point they will be marked
 * as deprecated and simply ignored.
 */
CredentialInitializationException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/CredentialInitializationException.java)/**
 * Exception which Hadoop's AWSCredentialsProvider implementations should
 * throw when there is a problem with the credential setup. This
 * is a subclass of {@link AmazonClientException} which sets
 * {@link #isRetryable()} to false, so as to fail fast.
 */
DefaultS3ClientFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/DefaultS3ClientFactory.java)/**
 * The default {@link S3ClientFactory} implementation.
 * This which calls the AWS SDK to configure and create an
 * {@link AmazonS3Client} that communicates with the S3 service.
 */
FailureInjectionPolicy (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/FailureInjectionPolicy.java)/**
 * Simple object which stores current failure injection settings.
 * "Delaying a key" can mean:
 *    - Removing it from the S3 client's listings while delay is in effect.
 *    - Causing input stream reads to fail.
 *    - Causing the S3 side of getFileStatus(), i.e.
 *      AmazonS3#getObjectMetadata(), to throw FileNotFound.
 */
AbstractStoreOperation (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/AbstractStoreOperation.java)/**
 * Base class of operations in the store.
 * An operation is something which executes against the context to
 * perform a single function.
 * It is expected to have a limited lifespan.
 */
CallableSupplier (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/CallableSupplier.java)/**
 * A bridge from Callable to Supplier; catching exceptions
 * raised by the callable and wrapping them as appropriate.
 * @param <T> return type.
 */
ETagChangeDetectionPolicy (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/ChangeDetectionPolicy.java)/**
   * Change detection policy based on {@link ObjectMetadata#getETag() eTag}.
   */
VersionIdChangeDetectionPolicy (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/ChangeDetectionPolicy.java)/**
   * Change detection policy based on
   * {@link ObjectMetadata#getVersionId() versionId}.
   */
NoChangeDetection (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/ChangeDetectionPolicy.java)/**
   * Don't check for changes.
   */
ChangeDetectionPolicy (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/ChangeDetectionPolicy.java)/**
 * Object change detection policy.
 * Determines which attribute is used to detect change and what to do when
 * change is detected.
 */
ChangeTracker (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/ChangeTracker.java)/**
 * Change tracking for input streams: the version ID or etag of the object is
 * tracked and compared on open/re-open.  An initial version ID or etag may or
 * may not be available, depending on usage (e.g. if S3Guard is utilized).
 *
 * Self-contained for testing and use in different streams.
 */
ContextAccessors (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/ContextAccessors.java)/**
 * An interface to implement for providing accessors to
 * S3AFileSystem-level API calls.
 * <p>
 * This is used to avoid giving any explicit reference to the owning
 * FS in the store context; there are enough calls that using lambda-expressions
 * gets over-complex.
 * <ol>
 *   <li>Test suites are free to provide their own implementation, using
 *  * the S3AFileSystem methods as the normative reference.</li>
 *  <li>All implementations <i>MUST</i> translate exceptions.</li>
 * </ol>
 */
CopyOutcome (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/CopyOutcome.java)/**
 * Extracts the outcome of a TransferManager-executed copy operation.
 */
DeleteOperation (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DeleteOperation.java)/**
 * Implementation of the delete() operation.
 * <p>
 * How S3Guard/Store inconsistency is handled:
 * <ol>
 *   <li>
 *     The list operation does not ask for tombstone markers; objects
 *     under tombstones will be found and deleted.
 *     The {@code extraFilesDeleted} counter will be incremented here.
 *   </li>
 *   <li>
 *     That may result in recently deleted files being found and
 *     duplicate delete requests issued. This is mostly harmless.
 *   </li>
 *   <li>
 *     If a path is considered authoritative on the client, so only S3Guard
 *     is used for listings, we wrap up the delete with a scan of raw S3.
 *     This will find and eliminate OOB additions.
 *   </li>
 *   <li>
 *     Exception 1: simple directory markers of the form PATH + "/".
 *     These are treated as a signal that there are no children; no
 *     listing is made.
 *   </li>
 *   <li>
 *     Exception 2: delete(path, true) where path has a tombstone in S3Guard.
 *     Here the delete is downgraded to a no-op even before this operation
 *     is created. Thus: no listings of S3.
 *   </li>
 * </ol>
 * If this class is logged at debug, requests will be audited:
 * the response to a bulk delete call will be reviewed to see if there
 * were fewer files deleted than requested; that will be printed
 * at WARN level. This is independent of handling rejected delete
 * requests which raise exceptions -those are processed lower down.
 * <p>
 * Performance tuning:
 * <p>
 * The operation to POST a delete request (or issue many individual
 * DELETE calls) then update the S3Guard table is done in an async
 * operation so that it can overlap with the LIST calls for data.
 * However, only one single operation is queued at a time.
 * <p>
 * Executing more than one batch delete is possible, it just
 * adds complexity in terms of error handling as well as in
 * the datastructures used to track outstanding operations.
 * If this is done, then it may be good to experiment with different
 * page sizes. The default value is
 * {@link InternalConstants#MAX_ENTRIES_TO_DELETE}, the maximum a single
 * POST permits.
 * <p>
 * 1. Smaller pages executed in parallel may have different
 * performance characteristics when deleting very large directories,
 * because it will be the DynamoDB calls which will come to dominate.
 * Any exploration of options here MUST be done with performance
 * measurements taken from test runs in EC2 against local DDB and S3 stores,
 * so as to ensure network latencies do not skew the results.
 * <p>
 * 2. Note that as the DDB thread/connection pools will be shared across
 * all active delete operations, speedups will be minimal unless
 * those pools are large enough to cope the extra load.
 * <p>
 * There are also some opportunities to explore in
 * {@code DynamoDBMetadataStore} with batching delete requests
 * in the DDB APIs.
 */
ExecutingStoreOperation (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/ExecutingStoreOperation.java)/**
 * A subclass of {@link AbstractStoreOperation} which
 * provides a method {@link #execute()} that may be invoked
 * exactly once.
 * @param <T> return type of executed operation.
 */
InternalConstants (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/InternalConstants.java)/**
 * Internal constants private only to the S3A codebase.
 * Please don't refer to these outside of this module &amp; its tests.
 * If you find you need to then either the code is doing something it
 * should not, or these constants need to be uprated to being
 * public and stable entries.
 */
LogExactlyOnce (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/LogExactlyOnce.java)/**
 * Log exactly once, even across threads.
 */
MultiObjectDeleteSupport (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/MultiObjectDeleteSupport.java)/**
 * Support for Multi Object Deletion.
 */
NetworkBinding (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/NetworkBinding.java)/**
 * Configures network settings when communicating with AWS services.
 */
OperationCallbacks (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/OperationCallbacks.java)/**
 * These are all the callbacks which the {@link RenameOperation}
 * and {@link DeleteOperation } operations need,
 * derived from the appropriate S3AFileSystem methods.
 */
RenameOperation (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/RenameOperation.java)/**
 * A parallelized rename operation which updates the metastore in the
 * process, through whichever {@link RenameTracker} the store provides.
 * The parallel execution is in groups of size
 * {@link InternalConstants#RENAME_PARALLEL_LIMIT}; it is only
 * after one group completes that the next group is initiated.
 * Once enough files have been copied that they meet the
 * {@link InternalConstants#MAX_ENTRIES_TO_DELETE} threshold, a delete
 * is initiated.
 * If it succeeds, the rename continues with the next group of files.
 *
 * The RenameTracker has the task of keeping the metastore up to date
 * as the rename proceeds.
 *
 * The rename operation implements the classic HDFS rename policy of
 * rename(file, dir) renames the file under the directory.
 *
 * There is <i>no</i> validation of input and output paths.
 * Callers are required to themselves verify that destination is not under
 * the source, above the source, the source itself, etc, etc.
 */
StoreContext (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/StoreContext.java)/**
 * This class provides the core context of the S3A filesystem to subsidiary
 * components, without exposing the entire parent class.
 * This is eliminate explicit recursive coupling.
 *
 * Where methods on the FS are to be invoked, they are referenced
 * via the {@link ContextAccessors} interface, so tests can implement
 * their own.
 *
 * <i>Warning:</i> this really is private and unstable. Do not use
 * outside the org.apache.hadoop.fs.s3a package.
 */
Delete (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/InconsistentAmazonS3Client.java)/**
   * Composite of data we need to track about recently deleted objects:
   * when it was deleted (same was with recently put objects) and the object
   * summary (since we should keep returning it for sometime after its
   * deletion).
   */
CustomObjectListing (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/InconsistentAmazonS3Client.java)/** Since ObjectListing is immutable, we just override it with wrapper. */
InconsistentAmazonS3Client (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/InconsistentAmazonS3Client.java)/**
 * A wrapper around {@link com.amazonaws.services.s3.AmazonS3} that injects
 * inconsistency and/or errors.  Used for testing S3Guard.
 * Currently only delays listing visibility, not affecting GET.
 */
InconsistentS3ClientFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/InconsistentS3ClientFactory.java)/**
 * S3 Client factory used for testing with eventual consistency fault injection.
 * This client is for testing <i>only</i>; it is in the production
 * {@code hadoop-aws} module to enable integration tests to use this
 * just by editing the Hadoop configuration used to bring up the client.
 */
InconsistentS3InputStream (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/InconsistentS3Object.java)/**
   * Wraps S3ObjectInputStream and implements failure injection.
   */
InconsistentS3Object (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/InconsistentS3Object.java)/**
 * Wrapper around S3Object so we can do failure injection on
 * getObjectContent() and S3ObjectInputStream.
 * See also {@link InconsistentAmazonS3Client}.
 */
Operation (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Invoker.java)/**
   * Arbitrary operation throwing an IOException.
   * @param <T> return type
   */
VoidOperation (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Invoker.java)/**
   * Void operation which may raise an IOException.
   */
Retried (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Invoker.java)/**
   * Callback for retry and notification operations.
   * Even if the interface is throwing up "raw" exceptions, this handler
   * gets the translated one.
   */
Invoker (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Invoker.java)/**
 * Class to provide lambda expression invocation of AWS operations.
 *
 * The core retry logic is in
 * {@link #retryUntranslated(String, boolean, Retried, Operation)};
 * the other {@code retry() and retryUntranslated()} calls are wrappers.
 *
 * The static {@link #once(String, String, Operation)} and
 * {@link #once(String, String, VoidOperation)} calls take an operation and
 * return it with AWS exceptions translated to IOEs of some form.
 *
 * The retry logic on a failure is defined by the retry policy passed in
 * the constructor; the standard retry policy is {@link S3ARetryPolicy},
 * though others may be used.
 *
 * The constructor also takes two {@link Retried} callbacks.
 * The {@code caughtCallback} is called whenever an exception (IOE or AWS)
 * is caught, before the retry processing looks at it.
 * The {@code retryCallback} is invoked after a retry is scheduled
 * but before the sleep.
 * These callbacks can be used for reporting and incrementing statistics.
 *
 * The static {@link #quietly(String, String, VoidOperation)} and
 * {@link #quietlyEval(String, String, Operation)} calls exist to take any
 * operation and quietly catch and log at debug. The return value of
 * {@link #quietlyEval(String, String, Operation)} is a java 8 optional,
 * which can then be used in java8-expressions.
 */
FileStatusAcceptor (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java)/**
   * Interface to implement by the logic deciding whether to accept a summary
   * entry or path as a valid file or directory.
   */
SingleStatusRemoteIterator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java)/**
   * A remote iterator which only iterates over a single `LocatedFileStatus`
   * value.
   *
   * If the status value is null, the iterator declares that it has no data.
   * This iterator is used to handle {@link S3AFileSystem#listStatus} calls
   * where the path handed in refers to a file, not a directory: this is the
   * iterator returned.
   */
ProvidedFileStatusIterator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java)/**
   * This wraps up a provided non-null list of file status as a remote iterator.
   *
   * It firstly filters the provided list and later {@link #next} call will get
   * from the filtered list. This suffers from scalability issues if the
   * provided list is too large.
   *
   * There is no remote data to fetch.
   */
FileStatusListingIterator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java)/**
   * Wraps up object listing into a remote iterator which will ask for more
   * listing data if needed.
   *
   * This is a complex operation, especially the process to determine
   * if there are more entries remaining. If there are no more results
   * remaining in the (filtered) results of the current listing request, then
   * another request is made <i>and those results filtered</i> before the
   * iterator can declare that there is more data available.
   *
   * The need to filter the results precludes the iterator from simply
   * declaring that if the {@link ObjectListingIterator#hasNext()}
   * is true then there are more results. Instead the next batch of results must
   * be retrieved and filtered.
   *
   * What does this mean? It means that remote requests to retrieve new
   * batches of object listings are made in the {@link #hasNext()} call;
   * the {@link #next()} call simply returns the filtered results of the last
   * listing processed. However, do note that {@link #next()} calls
   * {@link #hasNext()} during its operation. This is critical to ensure
   * that a listing obtained through a sequence of {@link #next()} will
   * complete with the same set of results as a classic
   * {@code while(it.hasNext()} loop.
   *
   * Thread safety: None.
   */
ObjectListingIterator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java)/**
   * Wraps up AWS `ListObjects` requests in a remote iterator
   * which will ask for more listing data if needed.
   *
   * That is:
   *
   * 1. The first invocation of the {@link #next()} call will return the results
   * of the first request, the one created during the construction of the
   * instance.
   *
   * 2. Second and later invocations will continue the ongoing listing,
   * calling {@link S3AFileSystem#continueListObjects} to request the next
   * batch of results.
   *
   * 3. The {@link #hasNext()} predicate returns true for the initial call,
   * where {@link #next()} will return the initial results. It declares
   * that it has future results iff the last executed request was truncated.
   *
   * Thread safety: none.
   */
AcceptFilesOnly (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java)/**
   * Accept all entries except the base path and those which map to S3N
   * pseudo directory markers.
   */
LocatedFileStatusIterator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java)/**
   * Take a remote iterator over a set of {@link FileStatus} instances and
   * return a remote iterator of {@link LocatedFileStatus} instances.
   */
TombstoneReconcilingIterator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java)/**
   * Wraps another iterator and filters out files that appear in the provided
   * set of tombstones.  Will read ahead in the iterator when necessary to
   * ensure that emptiness is detected early enough if only deleted objects
   * remain in the source iterator.
   */
AcceptAllButS3nDirs (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java)/**
   * Accept all entries except those which map to S3N pseudo directory markers.
   */
AcceptAllButSelfAndS3nDirs (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java)/**
   * Accept all entries except the base path and those which map to S3N
   * pseudo directory markers.
   */
Listing (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Listing.java)/**
 * Place for the S3A listing classes; keeps all the small classes under control.
 */
MetadataPersistenceException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/MetadataPersistenceException.java)/**
 * Indicates the metadata associated with the given Path could not be persisted
 * to the metadata store (e.g. S3Guard / DynamoDB).  When this occurs, the
 * file itself has been successfully written to S3, but the metadata may be out
 * of sync.  The metadata can be corrected with the "s3guard import" command
 * provided by {@link org.apache.hadoop.fs.s3a.s3guard.S3GuardTool}.
 */
ListingIterator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/MultipartUtils.java)/**
   * Simple RemoteIterator wrapper for AWS `listMultipartUpload` API.
   * Iterates over batches of multipart upload metadata listings.
   */
UploadIterator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/MultipartUtils.java)/**
   * Iterator over multipart uploads. Similar to
   * {@link org.apache.hadoop.fs.s3a.Listing.FileStatusListingIterator}, but
   * iterates over pending uploads instead of existing objects.
   */
MultipartUtils (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/MultipartUtils.java)/**
 * MultipartUtils upload-specific functions for use by S3AFileSystem and Hadoop
 * CLI.
 */
NoVersionAttributeException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/NoVersionAttributeException.java)/**
 * Indicates the S3 object does not provide the versioning attribute required
 * by the configured change detection policy.
 */
ProgressableProgressListener (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/ProgressableProgressListener.java)/**
 * Listener to progress from AWS regarding transfers.
 */
RemoteFileChangedException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/RemoteFileChangedException.java)/**
 * Indicates the S3 object is out of sync with the expected version.  Thrown in
 * cases such as when the object is updated while an {@link S3AInputStream} is
 * open, or when a file expected was never found.
 */
RenameFailedException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/RenameFailedException.java)/**
 * Error to indicate that a specific rename failed.
 * The exit code defines the exit code to be returned in the {@code rename()}
 * call.
 * Target path is set to destination.
 */
Retries (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Retries.java)/**
 * <p>
 *   Annotations to inform the caller of an annotated method whether
 *   the method performs retries and/or exception translation internally.
 *   Callers should use this information to inform their own decisions about
 *   performing retries or exception translation when calling the method. For
 *   example, if a method is annotated {@code RetryTranslated}, the caller
 *   MUST NOT perform another layer of retries.  Similarly, the caller shouldn't
 *   perform another layer of exception translation.
 * </p>
 * <p>
 *   Declaration for documentation only.
 *   This is purely for visibility in source and is currently package-scoped.
 *   Compare with {@link org.apache.hadoop.io.retry.AtMostOnce}
 *   and {@link org.apache.hadoop.io.retry.Idempotent}; these are real
 *   markers used by Hadoop RPC.
 * </p>
 */
S3A (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3A.java)/**
 * S3A implementation of AbstractFileSystem.
 * This impl delegates to the S3AFileSystem
 */
MultiPartUpload (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java)/**
   * Multiple partition upload.
   */
BlockUploadProgress (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java)/**
   * The upload progress listener registered for events returned
   * during the upload of a single block.
   * It updates statistics and handles the end of the upload.
   * Transfer failures are logged at WARN.
   */
ProgressableListener (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java)/**
   * Bridge from AWS {@code ProgressListener} to Hadoop {@link Progressable}.
   */
S3ABlockOutputStream (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java)/**
 * Upload files/parts directly via different buffering mechanisms:
 * including memory and disk.
 *
 * If the stream is closed and no update has started, then the upload
 * is instead done as a single PUT operation.
 *
 * Unstable: statistics and error handling might evolve.
 */
BlockUploadData (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java)/**
   * The output information for an upload.
   * It can be one of a file or an input stream.
   * When closed, any stream is closed. Any source file is untouched.
   */
BlockFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java)/**
   * Base class for block factories.
   */
DataBlock (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java)/**
   * This represents a block being uploaded.
   */
ArrayBlockFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java)/**
   * Use byte arrays on the heap for storage.
   */
ByteBufferInputStream (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java)/**
       * Provide an input stream from a byte buffer; supporting
       * {@link #mark(int)}, which is required to enable replay of failed
       * PUT attempts.
       */
ByteBufferBlock (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java)/**
     * A DataBlock which requests a buffer from pool on creation; returns
     * it when it is closed.
     */
DiskBlockFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java)/**
   * Buffer blocks to disk.
   */
DiskBlock (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java)/**
   * Stream to a file.
   * This will stop at the limit; the caller is expected to create a new block.
   */
S3ADataBlocks (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ADataBlocks.java)/**
 * Set of classes to support output streaming into blocks which are then
 * uploaded as to S3 as a single PUT, or as part of a multipart request.
 */
S3AFileStatus (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileStatus.java)/**
 * File status for an S3A "file".
 * Modification time is trouble, see {@link #getModificationTime()}.
 *
 * The subclass is private as it should not be created directly.
 */
OperationCallbacksImpl (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java)/**
   * The callbacks made by the rename and delete operations.
   * This separation allows the operation to be factored out and
   * still avoid knowledge of the S3AFilesystem implementation.
   */
ContextAccessorsImpl (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java)/**
   * The implementation of context accessors.
   */
S3AFileSystem (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java)/**
 * The core S3A Filesystem implementation.
 *
 * This subclass is marked as private as code should not be creating it
 * directly; use {@link FileSystem#get(Configuration)} and variants to
 * create one.
 *
 * If cast to {@code S3AFileSystem}, extra methods and features may be accessed.
 * Consider those private and unstable.
 *
 * Because it prints some of the state of the instrumentation,
 * the output of {@link #toString()} must also be considered unstable.
 */
S3AInputStream (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java)/**
 * The input stream for an S3A object.
 *
 * As this stream seeks withing an object, it may close then re-open the stream.
 * When this happens, any updated stream data may be retrieved, and, given
 * the consistency model of Amazon S3, outdated data may in fact be picked up.
 *
 * As a result, the outcome of reading from a stream of an object which is
 * actively manipulated during the read process is "undefined".
 *
 * The class is marked as private as code should not be creating instances
 * themselves. Any extra feature (e.g instrumentation) should be considered
 * unstable.
 *
 * Because it prints some of the state of the instrumentation,
 * the output of {@link #toString()} must also be considered unstable.
 */
InputStreamStatistics (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInstrumentation.java)/**
   * Statistics updated by an input stream during its actual operation.
   * These counters not thread-safe and are for use in a single instance
   * of a stream.
   */
OutputStreamStatistics (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInstrumentation.java)/**
   * Statistics updated by an output stream during its actual operation.
   * Some of these stats may be relayed. However, as block upload is
   * spans multiple
   */
S3GuardInstrumentation (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInstrumentation.java)/**
   * Instrumentation exported to S3Guard.
   */
CommitterStatistics (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInstrumentation.java)/**
   * Instrumentation exported to S3Guard Committers.
   */
DelegationTokenStatistics (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInstrumentation.java)/**
   * Instrumentation exported to S3A Delegation Token support.
   */
MetricsToMap (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInstrumentation.java)/**
   * Convert all metrics to a map.
   */
S3AInstrumentation (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInstrumentation.java)/**
 * Instrumentation of S3a.
 * Derived from the {@code AzureFileSystemInstrumentation}.
 *
 * Counters and metrics are generally addressed in code by their name or
 * {@link Statistic} key. There <i>may</i> be some Statistics which do
 * not have an entry here. To avoid attempts to access such counters failing,
 * the operations to increment/query metric values are designed to handle
 * lookup failures.
 */
S3ALocatedFileStatus (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ALocatedFileStatus.java)/**
 * {@link LocatedFileStatus} extended to also carry ETag and object version ID.
 */
Factory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AMultipartUploader.java)/**
   * Factory for creating MultipartUploader objects for s3a:// FileSystems.
   */
S3AMultipartUploader (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AMultipartUploader.java)/**
 * MultipartUploader for S3AFileSystem. This uses the S3 multipart
 * upload mechanism.
 */
S3AOpContext (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AOpContext.java)/**
 * Base class for operation context struct passed through codepaths for main
 * S3AFileSystem operations.
 * Anything op-specific should be moved to a subclass of this.
 */
S3AReadOpContext (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AReadOpContext.java)/**
 * Read-specific operation context struct.
 */
IdempotencyRetryFilter (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ARetryPolicy.java)/**
   * Policy which fails fast any non-idempotent call; hands off
   * all idempotent calls to the next retry policy.
   */
FailNonIOEs (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ARetryPolicy.java)/**
   * All non-IOE exceptions are failed.
   */
S3ARetryPolicy (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ARetryPolicy.java)/**
 * The S3A request retry policy.
 *
 * This uses the retry options in the configuration file to determine retry
 * count and delays for "normal" retries and separately, for throttling;
 * the latter is best handled for longer with an exponential back-off.
 *
 * <ol>
 * <li> Those exceptions considered unrecoverable (networking) are
 *    failed fast.</li>
 * <li>All non-IOEs are failed immediately. Assumed: bugs in code,
 *    unrecoverable errors, etc</li>
 * </ol>
 *
 * For non-idempotent operations, only failures due to throttling or
 * from failures which are known to only arise prior to talking to S3
 * are retried.
 *
 * The retry policy is all built around that of the normal IO exceptions,
 * particularly those extracted from
 * {@link S3AUtils#translateException(String, Path, AmazonClientException)}.
 * Because the {@link #shouldRetry(Exception, int, int, boolean)} method
 * does this translation if an {@code AmazonClientException} is processed,
 * the policy defined for the IOEs also applies to the original exceptions.
 *
 * Put differently: this retry policy aims to work for handlers of the
 * untranslated exceptions, as well as the translated ones.
 * @see <a href="http://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html">S3 Error responses</a>
 * @see <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/ErrorBestPractices.html">Amazon S3 Error Best Practices</a>
 * @see <a href="http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/CommonErrors.html">Dynamo DB Commmon errors</a>
 */
S3AStorageStatistics (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AStorageStatistics.java)/**
 * Storage statistics for S3A.
 */
CallOnLocatedFileStatus (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java)/**
   * An interface for use in lambda-expressions working with
   * directory tree listings.
   */
LocatedFileStatusMap (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java)/**
   * An interface for use in lambda-expressions working with
   * directory tree listings.
   */
S3AUtils (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java)/**
 * Utility methods for S3A code.
 */
S3ClientFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ClientFactory.java)/**
 * Factory for creation of {@link AmazonS3} client instances.
 */
AbstractS3GuardDynamoDBDiagnostic (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/AbstractS3GuardDynamoDBDiagnostic.java)/**
 * Entry point for S3Guard diagnostics operations against DynamoDB tables.
 */
BulkOperationState (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/BulkOperationState.java)/**
 * This represents state which may be passed to bulk IO operations
 * to enable them to store information about the state of the ongoing
 * operation across invocations.
 * <p>
 * A bulk operation state <i>MUST</i> only be be used for the single store
 * from which it was created, and <i>MUST</i>only for the duration of a single
 * bulk update operation.
 * <p>
 * Passing in the state is to allow the stores to maintain state about
 * updates they have already made to their store during this single operation:
 * a cache of what has happened. It is not a list of operations to be applied.
 * If a list of operations to perform is built up (e.g. during rename)
 * that is the duty of the caller, not this state.
 * <p>
 * After the operation has completed, it <i>MUST</i> be closed so
 * as to guarantee that all state is released.
 */
DDBPathMetadata (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DDBPathMetadata.java)/**
 * {@code DDBPathMetadata} wraps {@link PathMetadata} and adds the
 * isAuthoritativeDir flag to provide support for authoritative directory
 * listings in {@link DynamoDBMetadataStore}.
 */
DelayedUpdateRenameTracker (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DelayedUpdateRenameTracker.java)/**
 * This is the rename updating strategy originally used:
 * a collection of source paths and a list of destinations are created,
 * then updated at the end (possibly slow).
 * <p>
 * It is not currently instantiated by any of the active trackers,
 * but is preserved to show that the original rename strategy
 * can be implemented via the tracker model.
 */
DescendantsIterator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DescendantsIterator.java)/**
 * {@code DescendantsIterator} is a {@link RemoteIterator} that implements
 * pre-ordering breadth-first traversal (BFS) of a path and all of its
 * descendants recursively.  After visiting each path, that path's direct
 * children are discovered by calling {@link MetadataStore#listChildren(Path)}.
 * Each iteration returns the next direct child, and if that child is a
 * directory, also pushes it onto a queue to discover its children later.
 *
 * For example, assume the consistent store contains metadata representing this
 * file system structure:
 *
 * <pre>
 * /dir1
 * |-- dir2
 * |   |-- file1
 * |   `-- file2
 * `-- dir3
 *     |-- dir4
 *     |   `-- file3
 *     |-- dir5
 *     |   `-- file4
 *     `-- dir6
 * </pre>
 *
 * Consider this code sample:
 * <pre>
 * final PathMetadata dir1 = get(new Path("/dir1"));
 * for (DescendantsIterator descendants = new DescendantsIterator(dir1);
 *     descendants.hasNext(); ) {
 *   final FileStatus status = descendants.next().getFileStatus();
 *   System.out.printf("%s %s%n", status.isDirectory() ? 'D' : 'F',
 *       status.getPath());
 * }
 * </pre>
 *
 * The output is:
 * <pre>
 * D /dir1
 * D /dir1/dir2
 * D /dir1/dir3
 * F /dir1/dir2/file1
 * F /dir1/dir2/file2
 * D /dir1/dir3/dir4
 * D /dir1/dir3/dir5
 * F /dir1/dir3/dir4/file3
 * F /dir1/dir3/dir5/file4
 * D /dir1/dir3/dir6
 * </pre>
 */
DirListingMetadata (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DirListingMetadata.java)/**
 * {@code DirListingMetadata} models a directory listing stored in a
 * {@link MetadataStore}.  Instances of this class are mutable and thread-safe.
 */
CsvFile (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DumpS3GuardDynamoTable.java)/**
   * Writer for generating test CSV files.
   *
   * Quotes are manged by passing in a long whose specific bits control
   * whether or not a row is quoted, bit 0 for column 0, etc.
   *
   * There is no escaping of values here.
   */
DumpS3GuardDynamoTable (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DumpS3GuardDynamoTable.java)/**
 * This is a low-level diagnostics entry point which does a CVE/TSV dump of
 * the DDB state.
 * As it also lists the filesystem, it actually changes the state of the store
 * during the operation.
 */
DefaultDynamoDBClientFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBClientFactory.java)/**
   * The default implementation for creating an AmazonDynamoDB.
   */
DynamoDBClientFactory (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBClientFactory.java)/**
 * Interface to create a DynamoDB client.
 *
 * Implementation should be configured for setting and getting configuration.
 */
AncestorState (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java)/**
   * This tracks all the ancestors created,
   * across multiple move/write operations.
   * This is to avoid duplicate creation of ancestors during bulk commits
   * and rename operations managed by a rename tracker.
   */
DynamoDBMetadataStore (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java)/**
 * DynamoDBMetadataStore is a {@link MetadataStore} that persists
 * file system metadata to DynamoDB.
 *
 * The current implementation uses a schema consisting of a single table.  The
 * name of the table can be configured by config key
 * {@link org.apache.hadoop.fs.s3a.Constants#S3GUARD_DDB_TABLE_NAME_KEY}.
 * By default, it matches the name of the S3 bucket.  Each item in the table
 * represents a single directory or file.  Its path is split into separate table
 * attributes:
 * <ul>
 * <li> parent (absolute path of the parent, with bucket name inserted as
 * first path component). </li>
 * <li> child (path of that specific child, relative to parent). </li>
 * <li> optional boolean attribute tracking whether the path is a directory.
 *      Absence or a false value indicates the path is a file. </li>
 * <li> optional long attribute revealing modification time of file.
 *      This attribute is meaningful only to file items.</li>
 * <li> optional long attribute revealing file length.
 *      This attribute is meaningful only to file items.</li>
 * <li> optional long attribute revealing block size of the file.
 *      This attribute is meaningful only to file items.</li>
 * <li> optional string attribute tracking the s3 eTag of the file.
 *      May be absent if the metadata was entered with a version of S3Guard
 *      before this was tracked.
 *      This attribute is meaningful only to file items.</li>
  * <li> optional string attribute tracking the s3 versionId of the file.
 *      May be absent if the metadata was entered with a version of S3Guard
 *      before this was tracked.
 *      This attribute is meaningful only to file items.</li>
 * </ul>
 *
 * The DynamoDB partition key is the parent, and the range key is the child.
 *
 * To allow multiple buckets to share the same DynamoDB table, the bucket
 * name is treated as the root directory.
 *
 * For example, assume the consistent store contains metadata representing this
 * file system structure:
 *
 * <pre>
 * s3a://bucket/dir1
 * |-- dir2
 * |   |-- file1
 * |   `-- file2
 * `-- dir3
 *     |-- dir4
 *     |   `-- file3
 *     |-- dir5
 *     |   `-- file4
 *     `-- dir6
 * </pre>
 *
 * This is persisted to a single DynamoDB table as:
 *
 * <pre>
 * ====================================================================================
 * | parent                 | child | is_dir | mod_time | len | etag | ver_id |  ...  |
 * ====================================================================================
 * | /bucket                | dir1  | true   |          |     |      |        |       |
 * | /bucket/dir1           | dir2  | true   |          |     |      |        |       |
 * | /bucket/dir1           | dir3  | true   |          |     |      |        |       |
 * | /bucket/dir1/dir2      | file1 |        |   100    | 111 | abc  |  mno   |       |
 * | /bucket/dir1/dir2      | file2 |        |   200    | 222 | def  |  pqr   |       |
 * | /bucket/dir1/dir3      | dir4  | true   |          |     |      |        |       |
 * | /bucket/dir1/dir3      | dir5  | true   |          |     |      |        |       |
 * | /bucket/dir1/dir3/dir4 | file3 |        |   300    | 333 | ghi  |  stu   |       |
 * | /bucket/dir1/dir3/dir5 | file4 |        |   400    | 444 | jkl  |  vwx   |       |
 * | /bucket/dir1/dir3      | dir6  | true   |          |     |      |        |       |
 * ====================================================================================
 * </pre>
 *
 * This choice of schema is efficient for read access patterns.
 * {@link #get(Path)} can be served from a single item lookup.
 * {@link #listChildren(Path)} can be served from a query against all rows
 * matching the parent (the partition key) and the returned list is guaranteed
 * to be sorted by child (the range key).  Tracking whether or not a path is a
 * directory helps prevent unnecessary queries during traversal of an entire
 * sub-tree.
 *
 * Some mutating operations, notably
 * {@link MetadataStore#deleteSubtree(Path, BulkOperationState)} and
 * {@link MetadataStore#move(Collection, Collection, BulkOperationState)}
 * are less efficient with this schema.
 * They require mutating multiple items in the DynamoDB table.
 *
 * By default, DynamoDB access is performed within the same AWS region as
 * the S3 bucket that hosts the S3A instance.  During initialization, it checks
 * the location of the S3 bucket and creates a DynamoDB client connected to the
 * same region. The region may also be set explicitly by setting the config
 * parameter {@code fs.s3a.s3guard.ddb.region} to the corresponding region.
 */
DynamoDBMetadataStoreTableManager (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStoreTableManager.java)/**
 * Managing dynamo tables for S3Guard dynamodb based metadatastore.
 * Factored out from DynamoDBMetadataStore.
 */
ExpirableMetadata (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/ExpirableMetadata.java)/**
 * Expirable Metadata abstract class is for storing the field needed for
 * metadata classes in S3Guard that could be expired with TTL.
 */
PathFromRemoteStatusIterator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/InternalIterators.java)/**
   * From a remote status iterator, build a path iterator.
   */
RemoteIteratorFromIterator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/InternalIterators.java)/**
   * From a classic java.util.Iterator, build a Hadoop remote iterator.
   * @param <T> type of iterated value.
   */
InternalIterators (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/InternalIterators.java)/**
 * Internal iterators.
 */
ITtlTimeProvider (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/ITtlTimeProvider.java)/**
 * This interface is defined for handling TTL expiry of metadata in S3Guard.
 *
 * TTL can be tested by implementing this interface and setting is as
 * {@code S3Guard.ttlTimeProvider}. By doing this, getNow() can return any
 * value preferred and flaky tests could be avoided. By default getNow()
 * returns the EPOCH in runtime.
 *
 * Time is measured in milliseconds,
 */
LocalMetadataEntry (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/LocalMetadataEntry.java)/**
 * LocalMetadataEntry is used to store entries in the cache of
 * LocalMetadataStore. PathMetadata or dirListingMetadata can be null. The
 * entry is not immutable.
 */
LocalMetadataStore (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/LocalMetadataStore.java)/**
 * This is a local, in-memory implementation of MetadataStore.
 * This is <i>not</i> a coherent cache across processes.  It is only
 * locally-coherent.
 *
 * The purpose of this is for unit and integration testing.
 * It could also be used to accelerate local-only operations where only one
 * process is operating on a given object store, or multiple processes are
 * accessing a read-only storage bucket.
 *
 * This MetadataStore does not enforce filesystem rules such as disallowing
 * non-recursive removal of non-empty directories.  It is assumed the caller
 * already has to perform these sorts of checks.
 *
 * Contains one cache internally with time based eviction.
 */
MetadataStore (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/MetadataStore.java)/**
 * {@code MetadataStore} defines the set of operations that any metadata store
 * implementation must provide.  Note that all {@link Path} objects provided
 * to methods must be absolute, not relative paths.
 * Implementations must implement any retries needed internally, such that
 * transient errors are generally recovered from without throwing exceptions
 * from this API.
 */
MetadataStoreCapabilities (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/MetadataStoreCapabilities.java)/**
 * All the capability constants used for the
 * {@link MetadataStore} implementations.
 */
MetadataStoreListFilesIterator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/MetadataStoreListFilesIterator.java)/**
 * {@code MetadataStoreListFilesIterator} is a {@link RemoteIterator} that
 * is similar to {@code DescendantsIterator} but does not return directories
 * that have (or may have) children, and will also provide access to the set of
 * tombstones to allow recently deleted S3 objects to be filtered out from a
 * corresponding request.  In other words, it returns tombstones and the same
 * set of objects that should exist in S3: empty directories, and files, and not
 * other directories whose existence is inferred therefrom.
 *
 * For example, assume the consistent store contains metadata representing this
 * file system structure:
 *
 * <pre>
 * /dir1
 * |-- dir2
 * |   |-- file1
 * |   `-- file2
 * `-- dir3
 *     |-- dir4
 *     |   `-- file3
 *     |-- dir5
 *     |   `-- file4
 *     `-- dir6
 * </pre>
 *
 * Consider this code sample:
 * <pre>
 * final PathMetadata dir1 = get(new Path("/dir1"));
 * for (MetadataStoreListFilesIterator files =
 *     new MetadataStoreListFilesIterator(dir1); files.hasNext(); ) {
 *   final FileStatus status = files.next().getFileStatus();
 *   System.out.printf("%s %s%n", status.isDirectory() ? 'D' : 'F',
 *       status.getPath());
 * }
 * </pre>
 *
 * The output is:
 * <pre>
 * F /dir1/dir2/file1
 * F /dir1/dir2/file2
 * F /dir1/dir3/dir4/file3
 * F /dir1/dir3/dir5/file4
 * D /dir1/dir3/dir6
 * </pre>
 */
NullMetadataStore (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/NullMetadataStore.java)/**
 * A no-op implementation of MetadataStore.  Clients that use this
 * implementation should behave the same as they would without any
 * MetadataStore.
 */
PathMetadata (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/PathMetadata.java)/**
 * {@code PathMetadata} models path metadata stored in the
 * {@link MetadataStore}. The lastUpdated field is implicitly set to 0 in the
 * constructors without that parameter to show that it will be initialized
 * with 0 if not set otherwise.
 */
PathMetadataDynamoDBTranslation (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/PathMetadataDynamoDBTranslation.java)/**
 * Defines methods for translating between domain model objects and their
 * representations in the DynamoDB schema.
 */
TopmostLast (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/PathOrderComparators.java)/**
   * Compare the topmost last.
   * For some reason the .reverse() option wasn't giving the
   * correct outcome.
   */
PathMetadataComparator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/PathOrderComparators.java)/**
   * Compare on path status.
   */
PathOrderComparators (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/PathOrderComparators.java)/**
 * Comparator of path ordering for sorting collections.
 *
 * The definition of "topmost" is:
 * <ol>
 *   <li>The depth of a path is the primary comparator.</li>
 *   <li>Root is topmost, "0"</li>
 *   <li>If two paths are of equal depth, {@link Path#compareTo(Path)}</li>
 *   is used. This delegates to URI compareTo.
 *   <li>repeated sorts do not change the order</li>
 * </ol>
 */
ProgressiveRenameTracker (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/ProgressiveRenameTracker.java)/**
 * This rename tracker progressively updates the metadata store
 * as it proceeds, during the parallelized copy operation.
 * <p>
 * Algorithm
 * <ol>
 *   <li>
 *     As {@code RenameTracker.fileCopied()} callbacks
 *     are raised, the metastore is updated with the new file entry.
 *   </li>
 *   <li>
 *     Including parent entries, as appropriate.
 *   </li>
 *   <li>
 *     All directories which have been created are tracked locally,
 *     to avoid needing to read the store; this is a thread-safe structure.
 *   </li>
 *   <li>
 *    The actual update is performed out of any synchronized block.
 *   </li>
 *   <li>
 *     When deletes are executed, the store is also updated.
 *   </li>
 *   <li>
 *     And at the completion of a successful rename, the source directory
 *     is also removed.
 *   </li>
 * </ol>
 * <pre>
 *
 * </pre>
 */
PurgeS3GuardDynamoTable (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/PurgeS3GuardDynamoTable.java)/**
 * Purge the S3Guard table of a FileSystem from all entries related to
 * that table.
 * Will fail if there is no table, or the store is in auth mode.
 * <pre>
 *   hadoop org.apache.hadoop.fs.s3a.s3guard.PurgeS3GuardDynamoTable \
 *   -force s3a://example-bucket/
 * </pre>
 *
 */
RenameTracker (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/RenameTracker.java)/**
 * A class which manages updating the metastore with the rename process
 * as initiated in the S3AFilesystem rename.
 * <p>
 * Subclasses must provide an implementation and return it in
 * {@code MetadataStore.initiateRenameOperation()}.
 * <p>
 * The {@link #operationState} field/constructor argument is an opaque state to
 * be passed down to the metastore in its move operations; this allows the
 * stores to manage ongoing state -while still being able to share
 * rename tracker implementations.
 * <p>
 * This is to avoid performance problems wherein the progressive rename
 * tracker causes the store to repeatedly create and write duplicate
 * ancestor entries for every file added.
 */
TtlTimeProvider (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3Guard.java)/**
   * Runtime implementation for TTL Time Provider interface.
   */
S3Guard (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3Guard.java)/**
 * Logic for integrating MetadataStore with S3A.
 */
S3GuardDataAccessRetryPolicy (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardDataAccessRetryPolicy.java)/**
 * A Retry policy whose throttling comes from the S3Guard config options.
 */
ComparePair (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsck.java)/**
   * A compare pair with the pair of metadata and the list of violations.
   */
S3GuardFsck (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsck.java)/**
 * Main class for the FSCK factored out from S3GuardTool
 * The implementation uses fixed DynamoDBMetadataStore as the backing store
 * for metadata.
 *
 * Functions:
 * <ul>
 *   <li>Checking metadata consistency between S3 and metadatastore</li>
 * </ul>
 */
ViolationHandler (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * Violation handler abstract class.
   * This class should be extended for violation handlers.
   */
NoMetadataEntry (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * The violation handler when there's no matching metadata entry in the MS.
   */
NoParentEntry (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * The violation handler when there's no parent entry.
   */
ParentIsAFile (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * The violation handler when the parent of an entry is a file.
   */
ParentTombstoned (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * The violation handler when the parent of an entry is tombstoned.
   */
DirInS3FileInMs (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * The violation handler when there's a directory is a file metadata in MS.
   */
FileInS3DirInMs (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * The violation handler when a file metadata is a directory in MS.
   */
AuthDirContentMismatch (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * The violation handler when there's a directory listing content mismatch.
   */
LengthMismatch (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * The violation handler when there's a length mismatch.
   */
ModTimeMismatch (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * The violation handler when there's a modtime mismatch.
   */
VersionIdMismatch (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * The violation handler when there's a version id mismatch.
   */
EtagMismatch (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * The violation handler when there's an etag mismatch.
   */
NoEtag (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * The violation handler when there's no etag.
   */
TombstonedInMsNotDeletedInS3 (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
   * The violation handler when there's a tombstoned entry in the ms is
   * present, but the object is not deleted in S3.
   */
S3GuardFsckViolationHandler (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardFsckViolationHandler.java)/**
 * Violation handler for the S3Guard's fsck.
 */
DDBPathMetadataCollection (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTableAccess.java)/**
   * A collection which wraps the result of a query or scan.
   * Important: iterate through this only once; the outcome
   * of repeating an iteration is "undefined"
   * @param <T> type of outcome.
   */
DDBPathMetadataIterator (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTableAccess.java)/**
   * An iterator which converts the iterated-over result of
   * a query or scan into a {@code DDBPathMetadataIterator} entry.
   * @param <T> type of source.
   */
VersionMarker (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTableAccess.java)/**
   * DDBPathMetadata subclass returned when a query returns
   * the version marker.
   * There is a FileStatus returned where the owner field contains
   * the table version; the path is always the unqualified path "/VERSION".
   * Because it is unqualified, operations which treat this as a normal
   * DDB metadata entry usually fail.
   */
S3GuardTableAccess (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTableAccess.java)/**
 * Package-scoped accessor to table state in S3Guard.
 * This is for maintenance, diagnostics and testing: it is <i>not</i> to
 * be used otherwise.
 * <ol>
 *   <li>
 *     Some of the operations here may dramatically alter the state of
 *     a table, so use carefully.
 *   </li>
 *   <li>
 *     Operations to assess consistency of a store are best executed
 *     against a table which is otherwise inactive.
 *   </li>
 *   <li>
 *     No retry/throttling or AWS to IOE logic here.
 *   </li>
 *   <li>
 *     If a scan or query includes the version marker in the result, it
 *     is converted to a {@link VersionMarker} instance.
 *   </li>
 * </ol>
 *
 */
Init (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java)/**
   * Create the metadata store.
   */
SetCapacity (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java)/**
   * Change the capacity of the metadata store.
   */
Destroy (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java)/**
   * Destroy a metadata store.
   */
Import (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java)/**
   * Import s3 metadata to the metadata store.
   */
Diff (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java)/**
   * Show diffs between the s3 and metadata store.
   */
Prune (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java)/**
   * Prune metadata that has not been modified recently.
   */
BucketInfo (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java)/**
   * Get info about a bucket and its S3Guard integration status.
   */
Uploads (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java)/**
   * Command to list / abort pending multipart uploads.
   */
Fsck (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java)/**
   * Fsck - check for consistency between S3 and the metadatastore.
   */
S3GuardTool (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardTool.java)/**
 * CLI to manage S3Guard Metadata Store.
 */
TableDeleteTimeoutException (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/TableDeleteTimeoutException.java)/**
 * An exception raised when a table being deleted is still present after
 * the wait time is exceeded.
 */
S3GuardExistsRetryPolicy (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3GuardExistsRetryPolicy.java)/**
 * Slightly-modified retry policy for cases when the file is present in the
 * MetadataStore, but may be still throwing FileNotFoundException from S3.
 */
S3ListRequest (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ListRequest.java)/**
 * API version-independent container for S3 List requests.
 */
S3ListResult (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ListResult.java)/**
 * API version-independent container for S3 List responses.
 */
S3ObjectAttributes (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ObjectAttributes.java)/**
 * This class holds attributed of an object independent of the
 * file status type.
 * It is used in {@link S3AInputStream} and the select equivalent.
 * as a way to reduce parameters being passed
 * to the constructor of such class,
 * and elsewhere to be a source-neutral representation of a file status.
 */
InternalSelectConstants (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/select/InternalSelectConstants.java)/**
 * Constants for internal use in the org.apache.hadoop.fs.s3a module itself.
 * Please don't refer to these outside of this module &amp; its tests.
 * If you find you need to then either the code is doing something it
 * should not, or these constants need to be uprated to being
 * public and stable entries.
 */
SelectBinding (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/select/SelectBinding.java)/**
 * Class to do the S3 select binding and build a select request from the
 * supplied arguments/configuration.
 *
 * This class is intended to be instantiated by the owning S3AFileSystem
 * instance to handle the construction of requests: IO is still done exclusively
 * in the filesystem.
 */
SelectConstants (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/select/SelectConstants.java)/**
 * Options related to S3 Select.
 *
 * These options are set for the entire filesystem unless overridden
 * as an option in the URI
 */
SelectInputStream (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/select/SelectInputStream.java)/**
 * An input stream for S3 Select return values.
 * This is simply an end-to-end GET request, without any
 * form of seek or recovery from connectivity failures.
 *
 * Currently only seek and positioned read operations on the current
 * location are supported.
 *
 * The normal S3 input counters are updated by this stream.
 */
SelectTool (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/select/SelectTool.java)/**
 * This is a CLI tool for the select operation, which is available
 * through the S3Guard command.
 *
 * Usage:
 * <pre>
 *   hadoop s3guard select [options] Path Statement
 * </pre>
 */
SharedInstanceCredentialProvider (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/SharedInstanceCredentialProvider.java)/**
 * This credential provider has jittered between existing and non-existing,
 * but it turns up in documentation enough that it has been restored.
 * It extends {@link IAMInstanceCredentialsProvider} to pick up its
 * bindings, which are currently to use the
 * {@code EC2ContainerCredentialsProviderWrapper} class for IAM and container
 * authentication.
 * <p>
 * When it fails to authenticate, it raises a
 * {@link NoAwsCredentialsException} which can be recognized by retry handlers
 * as a non-recoverable failure.
 * <p>
 * It is implicitly public; marked evolving as we can change its semantics.
 */
SimpleAWSCredentialsProvider (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/SimpleAWSCredentialsProvider.java)/**
 * Support simple credentials for authenticating with AWS.
 *
 * Please note that users may reference this class name from configuration
 * property fs.s3a.aws.credentials.provider.  Therefore, changing the class name
 * would be a backward-incompatible change.
 */
TemporaryAWSCredentialsProvider (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/TemporaryAWSCredentialsProvider.java)/**
 * Support session credentials for authenticating with AWS.
 *
 * Please note that users may reference this class name from configuration
 * property fs.s3a.aws.credentials.provider.  Therefore, changing the class name
 * would be a backward-incompatible change.
 *
 * This credential provider must not fail in creation because that will
 * break a chain of credential providers.
 */
UploadInfo (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/UploadInfo.java)/**
 * Simple struct that contains information about a S3 upload.
 */
WriteOperationHelper (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java)/**
 * Helper for low-level operations against an S3 Bucket for writing data,
 * creating and committing pending writes, and other S3-layer operations.
 * <p>
 * It hides direct access to the S3 API
 * and is a location where the object operations can be evolved/enhanced.
 * <p>
 * Features
 * <ul>
 *   <li>Methods to create and submit requests to S3, so avoiding
 *   all direct interaction with the AWS APIs.</li>
 *   <li>Some extra preflight checks of arguments, so failing fast on
 *   errors.</li>
 *   <li>Callbacks to let the FS know of events in the output stream
 *   upload process.</li>
 *   <li>Other low-level access to S3 functions, for private use.</li>
 *   <li>Failure handling, including converting exceptions to IOEs.</li>
 *   <li>Integration with instrumentation and S3Guard.</li>
 *   <li>Evolution to add more low-level operations, such as S3 select.</li>
 * </ul>
 *
 * This API is for internal use only.
 */
NativeS3FileSystem (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java)/**
 * This is a stub filesystem purely present to fail meaningfully when
 * someone who explicitly declares
 * {@code fs.s3n.impl=org.apache.hadoop.fs.s3native.NativeS3FileSystem}
 * and then tries to create a filesystem off an s3n:// URL.
 *
 * The {@link #initialize(URI, Configuration)} method will throw
 * an IOException informing the user of their need to migrate.
 * @deprecated Replaced by the S3A client.
 */
Login (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3native/S3xLoginHelper.java)/**
   * Simple tuple of login details.
   */
S3xLoginHelper (/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3native/S3xLoginHelper.java)/**
 * Class to aid logging in to S3 endpoints.
 * It is in S3N so that it can be used across all S3 filesystems.
 *
 * The core function of this class was the extraction and decoding of user:secret
 * information from filesystems URIs. As this is no longer supported,
 * its role has been reduced to checking for secrets in the URI and rejecting
 * them where found.
 */
ITestS3AContractCreate (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/contract/s3a/ITestS3AContractCreate.java)/**
 * S3A contract tests creating files.
 */
ITestS3AContractDelete (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/contract/s3a/ITestS3AContractDelete.java)/**
 * S3A contract tests covering deletes.
 */
ITestS3AContractDistCp (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/contract/s3a/ITestS3AContractDistCp.java)/**
 * Contract test suite covering S3A integration with DistCp.
 * Uses the block output stream, buffered to disk. This is the
 * recommended output mechanism for DistCP due to its scalability.
 */
ITestS3AContractGetFileStatus (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/contract/s3a/ITestS3AContractGetFileStatus.java)/**
 * S3A contract tests covering getFileStatus.
 * Some of the tests can take too long when the fault injection rate is high,
 * so the test timeout is extended.
 */
ITestS3AContractMkdir (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/contract/s3a/ITestS3AContractMkdir.java)/**
 * Test dir operations on S3A.
 */
ITestS3AContractMultipartUploader (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/contract/s3a/ITestS3AContractMultipartUploader.java)/**
 * Test MultipartUploader with S3A.
 * Although not an S3A Scale test subclass, it uses the -Dscale option
 * to enable it, and partition size option to control the size of
 * parts uploaded.
 */
ITestS3AContractOpen (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/contract/s3a/ITestS3AContractOpen.java)/**
 * S3A contract tests opening files.
 */
ITestS3AContractRename (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/contract/s3a/ITestS3AContractRename.java)/**
 * S3A contract tests covering rename.
 */
ITestS3AContractRootDir (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/contract/s3a/ITestS3AContractRootDir.java)/**
 * root dir operations against an S3 bucket.
 */
ITestS3AContractSeek (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/contract/s3a/ITestS3AContractSeek.java)/**
 * S3A contract tests covering file seek.
 */
S3AContract (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/contract/s3a/S3AContract.java)/**
 * The contract of S3A: only enabled if the test bucket is provided.
 */
AbstractS3AMockTest (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/AbstractS3AMockTest.java)/**
 * Abstract base class for S3A unit tests using a mock S3 client and a null
 * metadata store.
 */
AbstractS3ATestBase (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/AbstractS3ATestBase.java)/**
 * An extension of the contract test base set up for S3A tests.
 */
AbstractTestS3AEncryption (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/AbstractTestS3AEncryption.java)/**
 * Test whether or not encryption works by turning it on. Some checks
 * are made for different file sizes as there have been reports that the
 * file length may be rounded up to match word boundaries.
 */
AbstractDelegationIT (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/AbstractDelegationIT.java)/**
 * superclass class for DT tests.
 */
CountInvocationsProvider (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/CountInvocationsProvider.java)/**
 * Simple AWS credential provider which counts how often it is invoked.
 */
Csvout (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/Csvout.java)/**
 * This is a small utility class to write out rows to a CSV/TSV file.
 * It does not do any escaping of written text, so don't write entries
 * containing separators.
 * Quoting must be done external to this class.
 */
ILoadTestRoleCredentials (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/ILoadTestRoleCredentials.java)/**
 * This looks at the cost of assume role, to see if it is more expensive
 * than creating simple session credentials.
 */
Outcome (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/ILoadTestSessionCredentials.java)/**
   * Outcome of one of the load operations.
   */
ILoadTestSessionCredentials (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/ILoadTestSessionCredentials.java)/**
 * This test has a unique name as it is designed to do something special:
 * generate enough load on the AWS STS service to get some
 * statistics on its throttling.
 * This isn't documented anywhere, and for DT support it's
 * important to know how much effort it takes to overload the service.
 *
 * <b>Important</b>
 *
 * If this test does trigger STS throttling, then all users in the same
 * AWS account will experience throttling. This may be observable,
 * in delays and, if the applications in use are not resilient to
 * throttling events in STS, from application failures.
 *
 * Use with caution.
 * <ol>
 *   <li>Don't run it on an AWS endpoint which other users in a
 *   shared AWS account are actively using. </li>
 *   <li>Don't run it on the same AWS account which is being used for
 *   any production service.</li>
 *   <li>And choose a time (weekend, etc) where the account is under-used.</li>
 *   <li>Warn your fellow users.</li>
 * </ol>
 *
 * In experiments, the throttling recovers fast and appears restricted
 * to the single STS service which the test overloads.
 *
 * @see <a href="https://github.com/steveloughran/datasets/releases/tag/tag_2018-09-17-aws">
 *   AWS STS login throttling statistics</a>
 */
ITestDelegatedMRJob (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/ITestDelegatedMRJob.java)/**
 * Submit a job with S3 delegation tokens.
 *
 * YARN will not collect DTs unless it is running secure, and turning
 * security on complicates test setup "significantly".
 * Specifically: buts of MR refuse to work on a local FS unless the
 * native libraries are loaded and it can use lower level POSIX APIs
 * for creating files and directories with specific permissions.
 * In production, this is a good thing. In tests, this is not.
 *
 * To address this, Job to YARN communications are mocked.
 * The client-side job submission is as normal, but the implementation
 * of org.apache.hadoop.mapreduce.protocol.ClientProtocol is mock.
 *
 * It's still an ITest though, as it does use S3A as the source and
 * dest so as to collect delegation tokens.
 *
 * It also uses the open street map open bucket, so that there's an extra
 * S3 URL in job submission which can be added as a job resource.
 * This is needed to verify that job resources have their tokens extracted
 * too.
 */
ITestRoleDelegationInFileystem (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/ITestRoleDelegationInFileystem.java)/**
 * Subclass of the session test which checks roles; only works if
 * a role ARN has been declared.
 */
ITestRoleDelegationTokens (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/ITestRoleDelegationTokens.java)/**
 * Rerun the session token tests with a role binding.
 * Some tests will fail as role bindings prevent certain operations.
 */
ITestSessionDelegationInFileystem (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/ITestSessionDelegationInFileystem.java)/**
 * Tests use of Hadoop delegation tokens within the FS itself.
 * This instantiates a MiniKDC as some of the operations tested require
 * UGI to be initialized with security enabled.
 */
ITestSessionDelegationTokens (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/ITestSessionDelegationTokens.java)/**
 * Tests use of Hadoop delegation tokens to marshall S3 credentials.
 */
MiniKerberizedHadoopCluster (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/MiniKerberizedHadoopCluster.java)/**
 * This is intended to support setting up an mini-secure Hadoop + YARN + MR
 * cluster.
 * It does not do this, yet, for the following reason: things don't work.
 * It is designed to be started/stopped at the class level.
 * however, users should be logged in in test cases, so that their local state
 * (credentials etc) are reset in every test.
 */
SessionSecretManager (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/TestS3ADelegationTokenSupport.java)/**
   * The secret manager always uses the same secret; the
   * factory for new identifiers is that of the token manager.
   */
TestS3ADelegationTokenSupport (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/delegation/TestS3ADelegationTokenSupport.java)/**
 * Unit tests related to S3A DT support.
 */
ITestAssumedRoleCommitOperations (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestAssumedRoleCommitOperations.java)/**
 * Verify that the commit operations work with a restricted set of operations.
 * The superclass, {@link ITestCommitOperations} turns on an inconsistent client
 * to see how things work in the presence of inconsistency.
 * These tests disable it, to remove that as a factor in these tests, which are
 * verifying that the policy settings to enabled MPU list/commit/abort are all
 * enabled properly.
 */
ITestAssumeRole (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestAssumeRole.java)/**
 * Tests use of assumed roles.
 * Only run if an assumed role is provided.
 */
ITestCustomSigner (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestCustomSigner.java)/**
 * Tests for custom Signers and SignerInitializers.
 */
ITestRestrictedReadAccess (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/ITestRestrictedReadAccess.java)/**
 * This test creates a client with no read access to the underlying
 * filesystem and then tries to perform various read operations on it.
 * S3Guard in non-auth mode always goes to the FS, so we parameterize the
 * test for S3Guard + Auth to see how failures move around.
 * <ol>
 *   <li>Tests only run if an assumed role is provided.</li>
 *   <li>And the S3Guard tests require DynamoDB.</li>
 * </ol>
 * The tests are all bundled into one big test case.
 * From a purist unit test perspective, this is utterly wrong as it goes
 * against the
 * <i>"Each test case tests exactly one thing"</i>
 * philosophy of JUnit.
 * <p>
 * However is significantly reduces setup costs on the parameterized test runs,
 * as it means that the filesystems and directories only need to be
 * created and destroyed once per parameterized suite, rather than
 * once per individual test.
 * <p>
 * All the test probes have informative messages so when a test failure
 * does occur, its cause should be discoverable. It main weaknesses are
 * therefore:
 * <ol>
 *   <li>A failure of an assertion blocks all subsequent assertions from
 *   being checked.</li>
 *   <li>Maintenance is potentially harder.</li>
 * </ol>
 * To simplify maintenance, the operations tested are broken up into
 * their own methods, with fields used to share the restricted role and
 * created paths.
 *
 */
RoleTestUtils (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/RoleTestUtils.java)/**
 * Helper class for testing roles.
 */
TestMarshalledCredentials (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/TestMarshalledCredentials.java)/**
 * Unit test of marshalled credential support.
 */
SignerForTest1 (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/TestSignerManager.java)/**
   * SignerForTest1.
   */
SignerForTest2 (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/TestSignerManager.java)/**
   * SignerForTest2.
   */
SignerInitializerForTest (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/TestSignerManager.java)/**
   * SignerInitializerForTest.
   */
SignerForInitializerTest (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/TestSignerManager.java)/**
   * To be used in conjunction with {@link SignerInitializerForTest}.
   */
DelegationTokenProviderForTest (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/TestSignerManager.java)/**
   * DelegationTokenProviderForTest.
   */
SignerInitializer2ForTest (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/TestSignerManager.java)/**
   * SignerInitializer2ForTest.
   */
TestSignerManager (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/auth/TestSignerManager.java)/**
 * Tests for the SignerManager.
 */
CloseWriter (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/AbstractCommitITest.java)/**
   * Closeable which can be used to safely close writers in
   * a try-with-resources block..
   */
AbstractCommitITest (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/AbstractCommitITest.java)/**
 * Base test suite for committer operations.
 *
 * By default, these tests enable the inconsistent committer, with
 * a delay of {@link #CONSISTENCY_DELAY}; they may also have throttling
 * enabled/disabled.
 *
 * <b>Important:</b> all filesystem probes will have to wait for
 * the FS inconsistency delays and handle things like throttle exceptions,
 * or disable throttling and fault injection before the probe.
 *
 */
CommitterFactory (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/AbstractITCommitProtocol.java)/**
   * Functional interface for creating committers, designed to allow
   * different factories to be used to create different failure modes.
   */
StandardCommitterFactory (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/AbstractITCommitProtocol.java)/**
   * The normal committer creation factory, uses the abstract methods
   * in the class.
   */
JobData (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/AbstractITCommitProtocol.java)/**
   * Details on a job for use in {@code startJob} and elsewhere.
   */
ActionToTest (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/AbstractITCommitProtocol.java)/**
   * A functional interface which an action to test must implement.
   */
FailingCommitterFactory (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/AbstractITCommitProtocol.java)/**
   * Factory for failing committers.
   */
AbstractITCommitProtocol (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/AbstractITCommitProtocol.java)/**
 * Test the job/task commit actions of an S3A Committer, including trying to
 * simulate some failure and retry conditions.
 * Derived from
 * {@code org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter}.
 *
 * This is a complex test suite as it tries to explore the full lifecycle
 * of committers, and is designed for subclassing.
 */
ClusterBinding (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/AbstractYarnClusterITest.java)/**
   * This is the cluster binding which every subclass must create.
   */
AbstractYarnClusterITest (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/AbstractYarnClusterITest.java)/**
 * Full integration test MR jobs.
 *
 * This is all done on shared static mini YARN and (optionally) HDFS clusters,
 * set up before any of the tests methods run.
 *
 * To isolate tests properly for parallel test runs, that static state
 * needs to be stored in the final classes implementing the tests, and
 * exposed to the base class, with the setup clusters in the
 * specific test suites creating the clusters with unique names.
 *
 * This is "hard" to do in Java, unlike, say, Scala.
 *
 * Note: this turns out not to be the root cause of ordering problems
 * with the Terasort tests (that is hard coded use of a file in the local FS),
 * but this design here does make it clear that the before and after class
 * operations are explicitly called in the subclasses.
 * If two subclasses of this class are instantiated in the same JVM, in order,
 * they are guaranteed to be isolated.
 *
 */
CommitterFaultInjection (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/CommitterFaultInjection.java)/**
 * Support for adding fault injection: all the failing committers in the IT
 * tests must implement this.
 */
Failure (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/CommitterFaultInjectionImpl.java)/**
   * The exception raised on failure.
   */
CommitterFaultInjectionImpl (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/CommitterFaultInjectionImpl.java)/**
 * Implementation of the fault injection lifecycle.
 * Can reset a fault on failure or always raise it.
 */
MapClass (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/integration/ITestS3ACommitterMRJob.java)/**
   *  Test Mapper.
   *  This is executed in separate process, and must not make any assumptions
   *  about external state.
   */
CommitterTestBinding (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/integration/ITestS3ACommitterMRJob.java)/**
   * A binding class for committer tests.
   * Subclasses of this will be instantiated and drive the parameterized
   * test suite.
   *
   * These classes will be instantiated in a static array of the suite, and
   * not bound to a cluster binding or filesystem.
   *
   * The per-method test {@link #setup()} method will call
   * {@link #setup(ClusterBinding, S3AFileSystem)}, to link the instance
   * to the specific test cluster <i>and test filesystem</i> in use
   * in that test.
   */
DirectoryCommitterTestBinding (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/integration/ITestS3ACommitterMRJob.java)/**
   * The directory staging committer.
   */
PartitionCommitterTestBinding (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/integration/ITestS3ACommitterMRJob.java)/**
   * The partition committer test binding.
   */
MagicCommitterTestBinding (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/integration/ITestS3ACommitterMRJob.java)/**
   * The magic committer test binding.
   * This includes extra result validation.
   */
ITestS3ACommitterMRJob (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/integration/ITestS3ACommitterMRJob.java)/**
 * Test an MR Job with all the different committers.
 * <p>
 * This is a fairly complex parameterization: it is designed to
 * avoid the overhead of starting a Yarn cluster for
 * individual committer types, so speed up operations.
 * <p>
 * It also implicitly guarantees that there is never more than one of these
 * MR jobs active at a time, so avoids overloading the test machine with too
 * many processes.
 * How the binding works:
 * <ol>
 *   <li>
 *     Each parameterized suite is configured through its own
 *     {@link CommitterTestBinding} subclass.
 *   </li>
 *   <li>
 *     JUnit runs these test suites one parameterized binding at a time.
 *   </li>
 *   <li>
 *     The test suites are declared to be executed in ascending order, so
 *     that for a specific binding, the order is {@link #test_000()},
 *     {@link #test_100()} {@link #test_200_execute()} and finally
 *     {@link #test_500()}.
 *   </li>
 *   <li>
 *     {@link #test_000()} calls {@link CommitterTestBinding#validate()} to
 *     as to validate the state of the committer. This is primarily to
 *     verify that the binding setup mechanism is working.
 *   </li>
 *   <li>
 *     {@link #test_100()} is relayed to
 *     {@link CommitterTestBinding#test_100()},
 *     for any preflight tests.
 *   </li>
 *   <li>
 *     The {@link #test_200_execute()} test runs the MR job for that
 *     particular binding with standard reporting and verification of the
 *     outcome.
 *   </li>
 *   <li>
 *     {@link #test_500()} test is relayed to
 *     {@link CommitterTestBinding#test_500()}, for any post-MR-job tests.
 * </ol>
 *
 * A new S3A FileSystem instance is created for each test_ method, so the
 * pre-execute and post-execute validators cannot inspect the state of the
 * FS as part of their tests.
 * However, as the MR workers and AM all run in their own processes, there's
 * generally no useful information about the job in the local S3AFileSystem
 * instance.
 */
ITestCommitOperations (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/ITestCommitOperations.java)/**
 * Test the low-level binding of the S3A FS to the magic commit mechanism,
 * and handling of the commit operations.
 * This is done with an inconsistent client.
 */
ITestS3ACommitterFactory (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/ITestS3ACommitterFactory.java)/**
 * Tests for some aspects of the committer factory.
 * All tests are grouped into one single test so that only one
 * S3A FS client is set up and used for the entire run.
 * Saves time and money.
 */
LoggingLineRecordWriter (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/LoggingTextOutputFormat.java)/**
   * Write a line; counts the number of lines written and logs @ debug in the
   * {@code close()} call.
   * @param <K> key
   * @param <V> value
   */
LoggingTextOutputFormat (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/LoggingTextOutputFormat.java)/**
 * A subclass of {@link TextOutputFormat} which logs what is happening, and
 * returns a {@link LoggingLineRecordWriter} which allows the caller
 * to get the destination path.
 * @param <K> key
 * @param <V> value
 */
ITestMagicCommitProtocol (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/magic/ITestMagicCommitProtocol.java)/**
 * Test the magic committer's commit protocol.
 */
ITestS3AHugeMagicCommits (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/magic/ITestS3AHugeMagicCommits.java)/**
 * Write a huge file via the magic commit mechanism,
 * commit it and verify that it is there. This is needed to
 * verify that the pending-upload mechanism works with multipart files
 * of more than one part.
 *
 * This is a scale test.
 */
MiniDFSClusterService (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/MiniDFSClusterService.java)/**
 * MiniDFS Cluster, encapsulated for use in different test suites.
 */
CommitterWithFailedThenSucceed (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/integration/ITestDirectoryCommitProtocol.java)/**
   * The class provides a overridden implementation of commitJobInternal which
   * causes the commit failed for the first time then succeed.
   */
ITestDirectoryCommitProtocol (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/integration/ITestDirectoryCommitProtocol.java)/** ITest of the low level protocol methods. */
CommitterWithFailedThenSucceed (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/integration/ITestPartitionedCommitProtocol.java)/**
   * The class provides a overridden implementation of commitJobInternal which
   * causes the commit failed for the first time then succeed.
   */
ITestPartitionedCommitProtocol (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/integration/ITestPartitionedCommitProtocol.java)/** ITest of the low level protocol methods. */
CommitterWithFailedThenSucceed (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/integration/ITestStagingCommitProtocol.java)/**
   * The class provides a overridden implementation of commitJobInternal which
   * causes the commit failed for the first time then succeed.
   */
ITestStagingCommitProtocol (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/integration/ITestStagingCommitProtocol.java)/** Test the staging committer's handling of the base protocol operations. */
MockedStagingCommitter (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/MockedStagingCommitter.java)/**
 * Committer subclass that uses a mocked S3A connection for testing.
 */
PartitionedCommitterForTesting (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/PartitionedCommitterForTesting.java)/**
 * Partitioned committer overridden for better testing.
 */
MiniDFSTest (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/StagingTestBase.java)/**
   * Provides setup/teardown of a MiniDFSCluster for tests that need one.
   */
JobCommitterTest (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/StagingTestBase.java)/**
   * Base class for job committer tests.
   * @param <C> committer
   */
TaskCommitterTest (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/StagingTestBase.java)/** Abstract test of task commits. */
ClientResults (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/StagingTestBase.java)/**
   * Results accrued during mock runs.
   * This data is serialized in MR Tests and read back in in the test runner
   */
ClientErrors (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/StagingTestBase.java)/** Control errors to raise in mock S3 client. */
StagingTestBase (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/StagingTestBase.java)/**
 * Test base for mock tests of staging committers:
 * core constants and static methods, inner classes
 * for specific test types.
 *
 * Some of the verification methods here are unused...they are being left
 * in place in case changes on the implementation make the verifications
 * relevant again.
 */
DirectoryCommitterForTesting (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/TestDirectoryCommitterScale.java)/**
   * Committer overridden for better testing.
   */
TestDirectoryCommitterScale (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/TestDirectoryCommitterScale.java)/**
 * Scale test of the directory committer: if there are many, many files
 * does job commit overload.
 * This is a mock test as to avoid the overhead of going near S3;
 * it does use a lot of local filesystem files though so as to
 * simulate real large scale deployment better.
 */
TestPaths (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/TestPaths.java)/**
 * Test {@link org.apache.hadoop.fs.s3a.commit.staging.Paths}.
 */
TestStagingCommitter (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/TestStagingCommitter.java)/**
 * The main unit test suite of the staging committer.
 * Parameterized on thread count and unique filename policy.
 */
TestStagingDirectoryOutputCommitter (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/TestStagingDirectoryOutputCommitter.java)/** Mocking test of directory committer. */
TestStagingPartitionedFileListing (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/TestStagingPartitionedFileListing.java)/**
 * Test partitioned staging committer's logic for putting data in the right
 * place.
 */
PartitionedStagingCommitterForTesting (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/TestStagingPartitionedJobCommit.java)/**
   * Subclass of the Partitioned Staging committer used in the test cases.
   */
TestStagingPartitionedJobCommit (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/TestStagingPartitionedJobCommit.java)/** Mocking test of partitioned committer. */
TestStagingPartitionedTaskCommit (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/staging/TestStagingPartitionedTaskCommit.java)/** Mocking test of the partitioned committer. */
ITestTerasortOnS3A (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/terasort/ITestTerasortOnS3A.java)/**
 * Runs Terasort against S3A.
 *
 * Parameterized by committer name, using a YARN cluster
 * shared across all test runs.
 * The tests run in sequence, so each operation is isolated.
 * This also means that the test paths are deleted in test
 * teardown; shared variables must all be static.
 *
 * The test is a scale test; for each parameter it takes a few minutes to
 * run the full suite.
 * Before anyone calls that out as slow: try running the test with the file
 * committer.
 */
TestMagicCommitPaths (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/TestMagicCommitPaths.java)/**
 * Tests for {@link MagicCommitPaths} path operations.
 */
Item (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/TestTasks.java)/**
   * The Item which tasks process.
   */
BaseCounter (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/TestTasks.java)/**
   * Class which can count invocations and, if limit > 0, will raise
   * an exception on the specific invocation of {@link #note(Object)}
   * whose count == limit.
   */
TestTasks (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/commit/TestTasks.java)/**
 * Test Tasks class.
 */
ITestS3AFileContext (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/fileContext/ITestS3AFileContext.java)/**
 * Implementation of TestFileContext for S3a.
 */
ITestS3AFileContextCreateMkdir (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/fileContext/ITestS3AFileContextCreateMkdir.java)/**
 * Extends FileContextCreateMkdirBaseTest for a S3a FileContext.
 */
ITestS3AFileContextMainOperations (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/fileContext/ITestS3AFileContextMainOperations.java)/**
 * S3A implementation of FileContextMainOperationsBaseTest.
 */
ITestS3AFileContextStatistics (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/fileContext/ITestS3AFileContextStatistics.java)/**
 * S3a implementation of FCStatisticsBaseTest.
 */
ITestS3AFileContextURI (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/fileContext/ITestS3AFileContextURI.java)/**
 * S3a implementation of FileContextURIBase.
 */
ITestS3AFileContextUtil (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/fileContext/ITestS3AFileContextUtil.java)/**
 * S3A implementation of FileContextUtilBase.
 */
ITestPartialRenamesDeletes (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestPartialRenamesDeletes.java)/**
 * Test partial failures of delete and rename operations, especially
 * that the S3Guard tables are consistent with the state of
 * the filesystem.
 *
 * All these test have a unique path for each run, with a roleFS having
 * full RW access to part of it, and R/O access to a restricted subdirectory
 *
 * <ol>
 *   <li>
 *     The tests are parameterized to single/multi delete, which control which
 *     of the two delete mechanisms are used.
 *   </li>
 *   <li>
 *     In multi delete, in a scale test run, a significantly larger set of files
 *     is created and then deleted.
 *   </li>
 *   <li>
 *     This isn't done in the single delete as it is much slower and it is not
 *     the situation we are trying to create.
 *   </li>
 * </ol>
 *
 * This test manages to create lots of load on the s3guard prune command
 * when that is tested in a separate test suite;
 * too many tombstone files for the test to complete.
 * An attempt is made in {@link #deleteTestDirInTeardown()} to prune these test
 * files.
 */
TestNeworkBinding (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/TestNeworkBinding.java)/**
 * Unit tests related to the {@link NetworkBinding} class.
 */
OperationTrackingStore (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/TestPartialDeleteFailures.java)/**
   * MetadataStore which tracks what is deleted and added.
   */
TestPartialDeleteFailures (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/TestPartialDeleteFailures.java)/**
 * Unit test suite covering translation of AWS SDK exceptions to S3A exceptions,
 * and retry/recovery policies.
 */
ITestBlockingThreadPoolExecutorService (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestBlockingThreadPoolExecutorService.java)/**
 * Basic test for S3A's blocking executor service.
 */
ITestLocatedFileStatusFetcher (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestLocatedFileStatusFetcher.java)/**
 * Test the LocatedFileStatusFetcher can do.
 * This is related to HADOOP-16458.
 * There's basic tests in ITestS3AFSMainOperations; this
 * is see if we can create better corner cases.
 */
BadCredentialsProviderConstructor (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAWSCredentialsProvider.java)/**
   * A bad CredentialsProvider which has no suitable constructor.
   *
   * This class does not provide a public constructor accepting Configuration,
   * or a public factory method named getInstance that accepts no arguments,
   * or a public default constructor.
   */
ITestS3AAWSCredentialsProvider (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AAWSCredentialsProvider.java)/**
 * Integration tests for {@link Constants#AWS_CREDENTIALS_PROVIDER} logic.
 */
ITestS3ABlockOutputArray (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABlockOutputArray.java)/**
 * Tests small file upload functionality for
 * {@link S3ABlockOutputStream} with the blocks buffered in byte arrays.
 *
 * File sizes are kept small to reduce test duration on slow connections;
 * multipart tests are kept in scale tests.
 */
ITestS3ABlockOutputByteBuffer (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABlockOutputByteBuffer.java)/**
 * Use {@link Constants#FAST_UPLOAD_BYTEBUFFER} for buffering.
 */
ITestS3ABlockOutputDisk (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABlockOutputDisk.java)/**
 * Use {@link Constants#FAST_UPLOAD_BUFFER_DISK} for buffering.
 */
ITestS3ABlocksize (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABlocksize.java)/**
 * S3A tests for configuring block size.
 */
ITestS3AClosedFS (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AClosedFS.java)/**
 * Tests of the S3A FileSystem which is closed.
 */
ITestS3AConfiguration (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AConfiguration.java)/**
 * S3A tests for configuration, especially credentials.
 */
ITestS3AContractGetFileStatusV1List (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AContractGetFileStatusV1List.java)/**
 * S3A contract tests for getFileStatus, using the v1 List Objects API.
 */
ITestS3ACopyFromLocalFile (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ACopyFromLocalFile.java)/**
 * Test {@link S3AFileSystem#copyFromLocalFile(boolean, boolean, Path, Path)}.
 * Some of the tests have been disabled pending a fix for HADOOP-15932 and
 * recursive directory copying; the test cases themselves may be obsolete.
 */
ITestS3ADelayedFNF (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ADelayedFNF.java)/**
 * Tests behavior of a FileNotFound error that happens after open(), i.e. on
 * the first read.
 */
ITestS3AEmptyDirectory (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEmptyDirectory.java)/**
 * Tests which exercise treatment of empty/non-empty directories.
 */
ITestS3AEncryptionAlgorithmValidation (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionAlgorithmValidation.java)/**
 * Test whether or not encryption settings propagate by choosing an invalid
 * one. We expect the S3AFileSystem to fail to initialize.
 */
ITestS3AEncryptionSSEC (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionSSEC.java)/**
 * Concrete class that extends {@link AbstractTestS3AEncryption}
 * and tests SSE-C encryption.
 */
ITestS3AEncryptionSSEKMSDefaultKey (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionSSEKMSDefaultKey.java)/**
 * Concrete class that extends {@link AbstractTestS3AEncryption}
 * and tests SSE-KMS encryption when no KMS encryption key is provided and AWS
 * uses the default.  Since this resource changes for every account and region,
 * there is no good way to explicitly set this value to do a equality check
 * in the response.
 */
ITestS3AEncryptionSSEKMSUserDefinedKey (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionSSEKMSUserDefinedKey.java)/**
 * Concrete class that extends {@link AbstractTestS3AEncryption}
 * and tests SSE-KMS encryption.  This requires the SERVER_SIDE_ENCRYPTION_KEY
 * to be set in auth-keys.xml for it to run.
 */
ITestS3AEncryptionSSES3 (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AEncryptionSSES3.java)/**
 * Concrete class that extends {@link AbstractTestS3AEncryption}
 * and tests SSE-S3 encryption.
 */
ITestS3AFailureHandling (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFailureHandling.java)/**
 * ITest for failure handling, primarily multipart deletion.
 */
ITestS3AFileOperationCost (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileOperationCost.java)/**
 * Use metrics to assert about the cost of file status queries.
 * {@link S3AFileSystem#getFileStatus(Path)}.
 * Parameterized on guarded vs raw.
 */
ITestS3AFileSystemContract (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFileSystemContract.java)/**
 *  Tests a live S3 system. If your keys and bucket aren't specified, all tests
 *  are marked as passed.
 *
 *  This uses BlockJUnit4ClassRunner because FileSystemContractBaseTest from
 *  TestCase which uses the old Junit3 runner that doesn't ignore assumptions
 *  properly making it impossible to skip the tests if we don't have a valid
 *  bucket.
 **/
ITestS3AFSMainOperations (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AFSMainOperations.java)/**
 * S3A Test suite for the FSMainOperationsBaseTest tests.
 */
ITestS3AInconsistency (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AInconsistency.java)/**
 * Tests S3A behavior under forced inconsistency via {@link
 * InconsistentAmazonS3Client}.
 *
 * These tests are for validating expected behavior *without* S3Guard, but
 * may also run with S3Guard enabled.  For tests that validate S3Guard's
 * consistency features, see {@link ITestS3GuardListConsistency}.
 */
ITestS3AMetadataPersistenceException (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AMetadataPersistenceException.java)/**
 * Tests failed writes to metadata store generate the expected
 * MetadataPersistenceException.
 */
ITestS3AMetrics (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AMetrics.java)/**
 * Test s3a performance metrics register and output.
 */
ITestS3AMiscOperations (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AMiscOperations.java)/**
 * Tests of the S3A FileSystem which don't have a specific home and can share
 * a filesystem instance with others.
 * Checksums are turned on unless explicitly disabled for a test case.
 */
ITestS3AMultipartUtils (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AMultipartUtils.java)/**
 * Tests for {@link MultipartUtils}.
 */
ITestS3ARemoteFileChanged (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ARemoteFileChanged.java)/**
 * Test S3A remote file change detection.
 * This is a very parameterized test; the first three parameters
 * define configuration options for the tests, while the final one
 * declares the expected outcomes given those options.
 *
 * This test uses mocking to insert transient failures into the S3 client,
 * underneath the S3A Filesystem instance.
 *
 * This is used to simulate eventual consistency, so force the change policy
 * failure modes to be encountered.
 *
 * If changes are made to the filesystem such that the number of calls to
 * operations such as {@link S3AFileSystem#getObjectMetadata(Path)} are
 * changed, the number of failures which the mock layer must generate may
 * change.
 *
 * As the S3Guard auth mode flag does control whether or not a HEAD is issued
 * in a call to {@code getFileStatus()}; the test parameter {@link #authMode}
 * is used to help predict this count.
 *
 * <i>Important:</i> if you are seeing failures in this test after changing
 * one of the rename/copy/open operations, it may be that an increase,
 * decrease or change in the number of low-level S3 HEAD/GET operations is
 * triggering the failures.
 * Please review the changes to see that you haven't unintentionally done this.
 * If it is intentional, please update the parameters here.
 *
 * If you are seeing failures without such a change, and nobody else is,
 * it is likely that you have a different bucket configuration option which
 * is somehow triggering a regression. If you can work out which option
 * this is, then extend {@link #createConfiguration()} to reset that parameter
 * too.
 *
 * Note: to help debug these issues, set the log for this to DEBUG:
 * <pre>
 *   log4j.logger.org.apache.hadoop.fs.s3a.ITestS3ARemoteFileChanged=DEBUG
 * </pre>
 * The debug information printed will include a trace of where operations
 * are being called from, to help understand why the test is failing.
 */
ITestS3ATemporaryCredentials (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ATemporaryCredentials.java)/**
 * Tests use of temporary credentials (for example, AWS STS & S3).
 *
 * The property {@link Constants#ASSUMED_ROLE_STS_ENDPOINT} can be set to
 * point this at different STS endpoints.
 * This test will use the AWS credentials (if provided) for
 * S3A tests to request temporary credentials, then attempt to use those
 * credentials instead.
 */
ITestS3ATestUtils (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ATestUtils.java)/**
 * Test the test utils. Why an integration test? it's needed to
 * verify property pushdown.
 */
ITestS3AUnbuffer (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3AUnbuffer.java)/**
 * Integration test for calling
 * {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer} on {@link S3AInputStream}.
 * Validates that the object has been closed using the
 * {@link S3AInputStream#isObjectStreamOpen()} method. Unlike the
 * {@link org.apache.hadoop.fs.contract.s3a.ITestS3AContractUnbuffer} tests,
 * these tests leverage the fact that isObjectStreamOpen exposes if the
 * underlying stream has been closed or not.
 */
ITestS3GuardCreate (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardCreate.java)/**
 * Home for testing the creation of new files and directories with S3Guard
 * enabled.
 */
ITestS3GuardEmptyDirs (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardEmptyDirs.java)/**
 * Test logic around whether or not a directory is empty, with S3Guard enabled.
 * The fact that S3AFileStatus has an isEmptyDirectory flag in it makes caching
 * S3AFileStatus's really tricky, as the flag can change as a side effect of
 * changes to other paths.
 * After S3Guard is merged to trunk, we should try to remove the
 * isEmptyDirectory flag from S3AFileStatus, or maintain it outside
 * of the MetadataStore.
 */
ITestS3GuardListConsistency (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardListConsistency.java)/**
 * Test S3Guard list consistency feature by injecting delayed listObjects()
 * visibility via {@link InconsistentAmazonS3Client}.
 *
 * Tests here generally:
 * 1. Use the inconsistency injection mentioned above.
 * 2. Only run when S3Guard is enabled.
 */
ITestS3GuardOutOfBandOperations (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardOutOfBandOperations.java)/**
 *
 * This integration test is for documenting and defining how S3Guard should
 * behave in case of out-of-band (OOB) operations.
 * <pre>
 * The behavior is the following in case of S3AFileSystem.getFileStatus:
 * A client with S3Guard
 * B client without S3Guard (Directly to S3)
 *
 * * OOB OVERWRITE, authoritative mode:
 * ** A client creates F1 file
 * ** B client overwrites F1 file with F2 (Same, or different file size)
 * ** A client's getFileStatus returns F1 metadata
 *
 * * OOB OVERWRITE, NOT authoritative mode:
 * ** A client creates F1 file
 * ** B client overwrites F1 file with F2 (Same, or different file size)
 * ** A client's getFileStatus returns F2 metadata. In not authoritative
 * mode we check S3 for the file. If the modification time of the file in S3
 * is greater than in S3Guard, we can safely return the S3 file metadata and
 * update the cache.
 *
 * * OOB DELETE, authoritative mode:
 * ** A client creates F file
 * ** B client deletes F file
 * ** A client's getFileStatus returns that the file is still there
 *
 * * OOB DELETE, NOT authoritative mode:
 * ** A client creates F file
 * ** B client deletes F file
 * ** A client's getFileStatus returns that the file is still there
 *
 * As you can see, authoritative and NOT authoritative mode behaves the same
 * at OOB DELETE case.
 *
 * The behavior is the following in case of S3AFileSystem.listStatus:
 * * File status in metadata store gets updated during the listing (in
 * S3Guard.dirListingUnion) the same way as in getFileStatus.
 * </pre>
 */
ITestS3GuardTtl (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardTtl.java)/**
 * These tests are testing the S3Guard TTL (time to live) features.
 */
ITestS3GuardWriteBack (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3GuardWriteBack.java)/**
 * Test cases that validate S3Guard's behavior for writing things like
 * directory listings back to the MetadataStore.
 */
MockS3AFileSystem (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/MockS3AFileSystem.java)/**
 * Relays FS calls to the mocked FS, allows for some extra logging with
 * stack traces to be included, stubbing out other methods
 * where needed to avoid failures.
 *
 * The logging is useful for tracking
 * down why there are extra calls to a method than a test would expect:
 * changes in implementation details often trigger such false-positive
 * test failures.
 *
 * This class is in the s3a package so that it has access to methods
 */
MockS3ClientFactory (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/MockS3ClientFactory.java)/**
 * An {@link S3ClientFactory} that returns Mockito mocks of the {@link AmazonS3}
 * interface suitable for unit testing.
 */
IdKey (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/MultipartTestUtils.java)/** Struct of object key, upload ID. */
MultipartTestUtils (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/MultipartTestUtils.java)/**
 * Utilities for S3A multipart upload tests.
 */
S3ATestConstants (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/S3ATestConstants.java)/**
 * Constants for S3A Testing.
 */
MetricDiff (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/S3ATestUtils.java)/**
   * Helper class to do diffs of metrics.
   */
S3ATestUtils (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/S3ATestUtils.java)/**
 * Utilities for the S3A tests.
 */
AbstractMSContract (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/AbstractMSContract.java)/**
 * Test specification for MetadataStore contract tests. Supplies configuration
 * and MetadataStore instance.
 */
AbstractS3GuardToolTestBase (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/AbstractS3GuardToolTestBase.java)/**
 * Common functionality for S3GuardTool test cases.
 */
DDBCapacities (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/DDBCapacities.java)/**
 * Tuple of read and write capacity of a DDB table.
 */
DynamoDBMSContract (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestDynamoDBMetadataStore.java)/**
   * Each contract has its own S3AFileSystem and DynamoDBMetadataStore objects.
   */
ITestDynamoDBMetadataStore (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestDynamoDBMetadataStore.java)/**
 * Test that {@link DynamoDBMetadataStore} implements {@link MetadataStore}.
 *
 * In this integration test, we use a real AWS DynamoDB. A
 * {@link DynamoDBMetadataStore} object is created in the @BeforeClass method,
 * and shared for all test in the @BeforeClass method. You will be charged
 * bills for AWS S3 or DynamoDB when you run these tests.
 *
 * According to the base class, every test case will have independent contract
 * to create a new {@link S3AFileSystem} instance and initializes it.
 * A table will be created and shared between the tests; some tests also
 * create their own.
 *
 * Important: Any new test which creates a table must do the following
 * <ol>
 *   <li>Enable on-demand pricing.</li>
 *   <li>Always destroy the table, even if an assertion fails.</li>
 * </ol>
 * This is needed to avoid "leaking" DDB tables and running up bills.
 */
ExecutionOutcome (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestDynamoDBMetadataStoreScale.java)/**
   * Outcome of a thread's execution operation.
   */
ITestDynamoDBMetadataStoreScale (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestDynamoDBMetadataStoreScale.java)/**
 * Scale test for DynamoDBMetadataStore.
 *
 * The throttle tests aren't quite trying to verify that throttling can
 * be recovered from, because that makes for very slow tests: you have
 * to overload the system and them have them back of until they finally complete.
 * <p>
 * With DDB on demand, throttling is very unlikely.
 * Here the tests simply run to completion, so act as regression tests of
 * parallel invocations on the metastore APIs
 */
ITestS3GuardConcurrentOps (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardConcurrentOps.java)/**
 * Tests concurrent operations on S3Guard.
 */
ITestS3GuardDDBRootOperations (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardDDBRootOperations.java)/**
 * This test run against the root of the FS, and operations which span the DDB
 * table and the filesystem.
 * For this reason, these tests are executed in the sequential phase of the
 * integration tests.
 * <p>
 * The tests only run if DynamoDB is the metastore.
 */
ITestS3GuardFsck (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardFsck.java)/**
 * Integration tests for the S3Guard Fsck against a dyamodb backed metadata
 * store.
 */
ITestS3GuardToolDynamoDB (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardToolDynamoDB.java)/**
 * Test S3Guard related CLI commands against DynamoDB.
 */
ITestS3GuardToolLocal (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardToolLocal.java)/**
 * Test S3Guard related CLI commands against a LocalMetadataStore.
 * Also responsible for testing the non s3guard-specific commands that, for
 * now, live under the s3guard CLI command.
 */
MetadataStoreTestBase (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/MetadataStoreTestBase.java)/**
 * Main test class for MetadataStore implementations.
 * Implementations should each create a test by subclassing this and
 * overriding {@link #createContract()}.
 * If your implementation may return missing results for recently set paths,
 * override {@link MetadataStoreTestBase#allowMissing()}.
 */
S3GuardToolTestHelper (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/S3GuardToolTestHelper.java)/**
 * Helper class for tests which make CLI invocations of the S3Guard tools.
 * That's {@link AbstractS3GuardToolTestBase} and others.
 */
TestDirListingMetadata (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestDirListingMetadata.java)/**
 * Unit tests of {@link DirListingMetadata}.
 */
TestDynamoDBMiscOperations (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestDynamoDBMiscOperations.java)/**
 * Unit test suite for misc dynamoDB metastore operations.
 */
TestLocalMetadataStore (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestLocalMetadataStore.java)/**
 * MetadataStore unit test for {@link LocalMetadataStore}.
 */
TestNullMetadataStore (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestNullMetadataStore.java)/**
 * Run MetadataStore unit tests on the NullMetadataStore implementation.
 */
TestObjectChangeDetectionAttributes (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestObjectChangeDetectionAttributes.java)/**
 * Unit tests to ensure object eTag and versionId are captured on S3 PUT and
 * used on GET.
 * Further (integration) testing is performed in
 * {@link org.apache.hadoop.fs.s3a.ITestS3ARemoteFileChanged}.
 */
TestPathMetadataDynamoDBTranslation (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestPathMetadataDynamoDBTranslation.java)/**
 * Test the PathMetadataDynamoDBTranslation is able to translate between domain
 * model objects and DynamoDB items.
 */
TestPathOrderComparators (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestPathOrderComparators.java)/**
 * Test ordering of paths with the comparator matches requirements.
 */
TestS3Guard (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestS3Guard.java)/**
 * Tests for the {@link S3Guard} utility class.
 */
TestS3GuardCLI (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestS3GuardCLI.java)/**
 * Test the S3Guard CLI entry point.
 */
ThrottleTracker (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ThrottleTracker.java)/**
 * Something to track throttles in DynamoDB metastores.
 * The constructor sets the counters to the current count in the
 * DDB table; a call to {@link #reset()} will set it to the latest values.
 * The {@link #probe()} will pick up the latest values to compare them with
 * the original counts.
 * <p>
 * The toString value logs the state.
 * <p>
 * This class was originally part of ITestDynamoDBMetadataStoreScale;
 * it was converted to a toplevel class for broader use.
 */
AbstractITestS3AMetadataStoreScale (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/AbstractITestS3AMetadataStoreScale.java)/**
 * Test the performance of a MetadataStore.  Useful for load testing.
 * Could be separated from S3A code, but we're using the S3A scale test
 * framework for convenience.
 */
ProgressCallback (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/AbstractSTestS3AHugeFiles.java)/**
   * Progress callback from AWS. Likely to come in on a different thread.
   */
AbstractSTestS3AHugeFiles (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/AbstractSTestS3AHugeFiles.java)/**
 * Scale test which creates a huge file.
 *
 * <b>Important:</b> the order in which these tests execute is fixed to
 * alphabetical order. Test cases are numbered {@code test_123_} to impose
 * an ordering based on the numbers.
 *
 * Having this ordering allows the tests to assume that the huge file
 * exists. Even so: they should all have a {@link #assumeHugeFileExists()}
 * check at the start, in case an individual test is executed.
 */
ITestLocalMetadataStoreScale (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestLocalMetadataStoreScale.java)/**
 * Scale test for LocalMetadataStore.
 */
ITestS3AConcurrentOps (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AConcurrentOps.java)/**
 * Tests concurrent operations on a single S3AFileSystem instance.
 */
ITestS3ACreatePerformance (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3ACreatePerformance.java)/**
 * Tests for create(): performance and/or load testing.
 */
ITestS3ADeleteFilesOneByOne (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3ADeleteFilesOneByOne.java)/**
 * Tests file deletion with multi-delete disabled.
 */
ITestS3ADeleteManyFiles (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3ADeleteManyFiles.java)/**
 * Test some scalable operations related to file renaming and deletion.
 */
ITestS3ADirectoryPerformance (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3ADirectoryPerformance.java)/**
 * Test the performance of listing files/directories.
 */
ITestS3AHugeFilesArrayBlocks (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFilesArrayBlocks.java)/**
 * Use {@link Constants#FAST_UPLOAD_BUFFER_ARRAY} for buffering.
 */
ITestS3AHugeFilesByteBufferBlocks (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFilesByteBufferBlocks.java)/**
 * Use {@link Constants#FAST_UPLOAD_BYTEBUFFER} for buffering.
 */
ITestS3AHugeFilesDiskBlocks (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFilesDiskBlocks.java)/**
 * Use {@link Constants#FAST_UPLOAD_BUFFER_DISK} for buffering.
 */
ITestS3AHugeFilesSSECDiskBlocks (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AHugeFilesSSECDiskBlocks.java)/**
 * Concrete class that extends {@link ITestS3AHugeFilesDiskBlocks}
 * and tests huge files operations with SSE-C encryption enabled.
 * Skipped if the SSE tests are disabled.
 */
ITestS3AInputStreamPerformance (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AInputStreamPerformance.java)/**
 * Look at the performance of S3a operations.
 */
NanoTimerStats (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/NanoTimerStats.java)/**
 * Collect statistics from duration data from
 * {@link ContractTestUtils.NanoTimer} values.
 *
 * The mean and standard deviation is built up as the stats are collected,
 * using "Welford's Online algorithm" for the variance.
 * Trends in statistics (e.g. slowing down) are not tracked.
 * Not synchronized.
 */
S3AScaleTestBase (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/S3AScaleTestBase.java)/**
 * Base class for scale tests; here is where the common scale configuration
 * keys are defined.
 * <p>
 * Configuration setup is a bit more complex than in the parent classes,
 * as the test timeout is desired prior to the {@link #getTestTimeoutMillis()}
 * being called to set the test timeout rule; this happens before any of
 * the methods tagged with {@code @Before} are invoked.
 * <p>
 * The algorithm is:
 * <ol>
 *   <li>Create a configuration on demand, via
 *   {@link #demandCreateConfiguration()}</li>
 *   <li>Have that return the value of {@link #conf} or create a new one
 *   if that field is null (and set the field to the created value).</li>
 *   <li>Override the superclasses {@link #createConfiguration()}
 *   to return the demand created value; make that method final so that
 *   subclasses don't break things by overridding it.</li>
 *   <li>Add a new override point {@link #createScaleConfiguration()}
 *   to create the config, one which subclasses can (and do) override.</li>
 * </ol>
 * Bear in mind that this process also takes place during initialization
 * of the superclass; the overridden methods are being invoked before
 * their instances are fully configured. This is considered
 * <i>very bad form</i> in Java code (indeed, in C++ it is actually permitted;
 * the base class implementations get invoked instead).
 */
AbstractS3SelectTest (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/select/AbstractS3SelectTest.java)/**
 * Superclass for S3 Select tests.
 * A lot of the work here goes into creating and querying a simple CSV test
 * format, with various datatypes which can be used in type-casting queries.
 * <pre>
 * 1  "ID": index of the row
 * 2  "date": date as ISO 8601
 * 3  "timestamp": timestamp in seconds of epoch
 * 4  "name", entry-$row
 * 5  "odd", odd/even as boolean. True means odd,
 * 6  "oddint", odd/even as int : 1 for odd, 0 for even
 * 7  "oddrange": odd/even as 1 for odd, -1 for even
 * </pre>
 */
CsvFile (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/select/CsvFile.java)/**
 * Writer for generating test CSV files.
 *
 * Quotes are manged by passing in a long whose specific bits control
 * whether or not a row is quoted, bit 0 for column 0, etc.
 */
ITestS3Select (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/select/ITestS3Select.java)/**
 * Test the S3 Select feature with some basic SQL Commands.
 * Executed if the destination store declares its support for the feature.
 */
ITestS3SelectCLI (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/select/ITestS3SelectCLI.java)/**
 * Test the S3 Select CLI through some operations against landsat
 * and files generated from it.
 */
ITestS3SelectLandsat (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/select/ITestS3SelectLandsat.java)/**
 * Test the S3 Select feature with the Landsat dataset.
 *
 * This helps explore larger datasets, compression and the like.
 *
 * This suite is only executed if the destination store declares its support for
 * the feature and the test CSV file configuration option points to the
 * standard landsat GZip file. That's because these tests require the specific
 * format of the landsat file.
 *
 * Normally working with the landsat file is a scale test.
 * Here, because of the select operations, there's a lot less data
 * to download.
 * For this to work: write aggressive select calls: filtering, using LIMIT
 * and projecting down to a few columns.
 *
 * For the structure, see
 * <a href="https://docs.opendata.aws/landsat-pds/readme.html">Landsat on AWS</a>
 *
 * <code>
 *   entityId: String         LC80101172015002LGN00
 *   acquisitionDate: String  2015-01-02 15:49:05.571384
 *   cloudCover: Float (possibly -ve) 80.81
 *   processingLevel: String  L1GT
 *   path: Int                10
 *   row:  Int                117
 *   min_lat: Float           -79.09923
 *   min_lon: Float           -139.66082
 *   max_lat: Float           -77.7544
 *   max_lon: Float           125.09297
 *   download_url: HTTPS URL https://s3-us-west-2.amazonaws.com/landsat-pds/L8/010/117/LC80101172015002LGN00/index.html
 * </code>
 * Ranges
 * <ol>
 *   <li>Latitude should range in -180 <= lat <= 180</li>
 *   <li>Longitude in 0 <= lon <= 360</li>
 *   <li>Standard Greenwich Meridian (not the french one which still surfaces)</li>
 *   <li>Cloud cover <i>Should</i> be 0-100, but there are some negative ones.</li>
 * </ol>
 *
 * Head of the file:
 * <code>
 entityId,acquisitionDate,cloudCover,processingLevel,path,row,min_lat,min_lon,max_lat,max_lon,download_url
 * LC80101172015002LGN00,2015-01-02 15:49:05.571384,80.81,L1GT,10,117,-79.09923,-139.66082,-77.7544,-125.09297,https://s3-us-west-2.amazonaws.com/landsat-pds/L8/010/117/LC80101172015002LGN00/index.html
 * LC80260392015002LGN00,2015-01-02 16:56:51.399666,90.84,L1GT,26,39,29.23106,-97.48576,31.36421,-95.16029,https://s3-us-west-2.amazonaws.com/landsat-pds/L8/026/039/LC80260392015002LGN00/index.html
 * LC82270742015002LGN00,2015-01-02 13:53:02.047000,83.44,L1GT,227,74,-21.28598,-59.27736,-19.17398,-57.07423,https://s3-us-west-2.amazonaws.com/landsat-pds/L8/227/074/LC82270742015002LGN00/index.html
 * LC82270732015002LGN00,2015-01-02 13:52:38.110317,52.29,L1T,227,73,-19.84365,-58.93258,-17.73324,-56.74692,https://s3-us-west-2.amazonaws.com/landsat-pds/L8/227/073/LC82270732015002LGN00/index.html
 * </code>
 *
 * For the Curious this is the Scala/Spark declaration of the schema.
 * <code>
 *   def addLandsatColumns(csv: DataFrame): DataFrame = {
 *     csv
 *       .withColumnRenamed("entityId", "id")
 *       .withColumn("acquisitionDate",
 *         csv.col("acquisitionDate").cast(TimestampType))
 *       .withColumn("cloudCover", csv.col("cloudCover").cast(DoubleType))
 *       .withColumn("path", csv.col("path").cast(IntegerType))
 *       .withColumn("row", csv.col("row").cast(IntegerType))
 *       .withColumn("min_lat", csv.col("min_lat").cast(DoubleType))
 *       .withColumn("min_lon", csv.col("min_lon").cast(DoubleType))
 *       .withColumn("max_lat", csv.col("max_lat").cast(DoubleType))
 *       .withColumn("max_lon", csv.col("max_lon").cast(DoubleType))
 *       .withColumn("year",
 *         year(col("acquisitionDate")))
 *       .withColumn("month",
 *         month(col("acquisitionDate")))
 *       .withColumn("day",
 *         month(col("acquisitionDate")))
 *   }
 * </code>
 */
ITestS3SelectMRJob (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/select/ITestS3SelectMRJob.java)/**
 * Run an MR job with a select query.
 * This is the effective end-to-end test which verifies:
 * <ol>
 *   <li>Passing of select parameters through an MR job conf.</li>
 *   <li>Automatic pick-up of these parameter through TextInputFormat's use
 *   of the mapreduce.lib.input.LineRecordReaderLineRecordReader.</li>
 *   <li>Issuing of S3 Select queries in mapper processes.</li>
 *   <li>Projection of columns in a select.</li>
 *   <li>Ability to switch to the Passthrough decompressor in an MR job.</li>
 *   <li>Saving of results through the S3A Staging committer.</li>
 *   <li>Basic validation of results.</li>
 * </ol>
 * This makes it the most complex of the MR jobs in the hadoop-aws test suite.
 *
 * The query used is
 * {@link ITestS3SelectLandsat#SELECT_PROCESSING_LEVEL_NO_LIMIT},
 * which lists the processing level of all records in the source file,
 * and counts the number in each one by way of the normal word-count
 * routines.
 * This works because the SQL is projecting only the processing level.
 *
 * The result becomes something like (with tabs between fields):
 * <pre>
 * L1GT   370231
 * L1T    689526
 * </pre>
 */
StatsIterator (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/StorageStatisticsTracker.java)/**
   * Provide an iterator to the stats.
   */
StorageStatisticsTracker (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/StorageStatisticsTracker.java)/**
 * Class to track storage statistics of a filesystem, generate diffs.
 */
ExtraAssertions (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/test/ExtraAssertions.java)/**
 * Some extra assertions for tests.
 */
TestDataBlocks (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestDataBlocks.java)/**
 * Unit tests for {@link S3ADataBlocks}.
 */
ConnectTimeoutException (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestInvoker.java)/**
     * A local exception with a name to match the expected one.
     */
NotAConnectTimeoutException (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestInvoker.java)/**
     * A local exception whose name should not match.
     */
Local (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestInvoker.java)/**
   * Container for the local exceptions, to help keep visible which
   * specific class of exception.
   */
CatchCallback (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestInvoker.java)/**
   * Catch the exception and preserve it for later queries.
   */
TestInvoker (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestInvoker.java)/**
 * Test the {@link Invoker} code and the associated {@link S3ARetryPolicy}.
 *
 * Some of the tests look at how Connection Timeout Exceptions are processed.
 * Because of how the AWS libraries shade the classes, there have been some
 * regressions here during development. These tests are intended to verify that
 * the current match process based on classname works.
 */
TestListing (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestListing.java)/**
 * Place for the S3A listing classes; keeps all the small classes under control.
 */
AbstractProvider (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AAWSCredentialsProvider.java)/**
   * A credential provider declared as abstract, so it cannot be instantiated.
   */
ConstructorSignatureErrorProvider (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AAWSCredentialsProvider.java)/**
   * A credential provider whose constructor signature doesn't match.
   */
ConstructorFailureProvider (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AAWSCredentialsProvider.java)/**
   * A credential provider whose constructor raises an NPE.
   */
IOERaisingProvider (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AAWSCredentialsProvider.java)/**
   * Credential provider which raises an IOE when constructed.
   */
TestS3AAWSCredentialsProvider (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AAWSCredentialsProvider.java)/**
 * Unit tests for {@link Constants#AWS_CREDENTIALS_PROVIDER} logic.
 */
TestS3ABlockOutputStream (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3ABlockOutputStream.java)/**
 * Unit tests for {@link S3ABlockOutputStream}.
 */
TestS3AExceptionTranslation (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AExceptionTranslation.java)/**
 * Unit test suite covering translation of AWS SDK exceptions to S3A exceptions,
 * and retry/recovery policies.
 */
TestS3AGetFileStatus (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AGetFileStatus.java)/**
 * S3A tests for getFileStatus using mock S3 client.
 */
TestS3AInputPolicies (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AInputPolicies.java)/**
 * Unit test of the input policy logic, without making any S3 calls.
 */
TestS3AMultipartUploaderSupport (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AMultipartUploaderSupport.java)/**
 * Test multipart upload support methods and classes.
 */
TestS3AUnbuffer (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AUnbuffer.java)/**
 * Uses mocks to check that the {@link S3ObjectInputStream} is closed when
 * {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer} is called. Unlike the
 * other unbuffer tests, this specifically tests that the underlying S3 object
 * stream is closed.
 */
TestSSEConfiguration (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestSSEConfiguration.java)/**
 * Test SSE setup operations and errors raised.
 * Tests related to secret providers and AWS credentials are also
 * included, as they share some common setup operations.
 */
TestStreamChangeTracker (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestStreamChangeTracker.java)/**
 * Test {@link ChangeTracker}.
 */
ITestS3A (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/yarn/ITestS3A.java)/**
 * S3A tests through the {@link FileContext} API.
 */
ITestS3AMiniYarnCluster (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/yarn/ITestS3AMiniYarnCluster.java)/**
 * Tests that S3A is usable through a YARN application.
 */
TestS3xLoginHelper (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3native/TestS3xLoginHelper.java)/**
 * Test how URIs and login details are extracted from URIs.
 */
TestS3AResourceScope (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/mapreduce/filecache/TestS3AResourceScope.java)/**
 * Test how S3A resources are scoped in YARN caching.
 * In this package to make use of package-private methods of
 * {@link ClientDistributedCacheManager}.
 */
MockJob (/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/mapreduce/MockJob.java)/**
 * This is a mock job which doesn't talk to YARN.
 * It's in this package as the JobSubmitter API is package-scoped.
 */
AzureException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureException.java)/**
 * Thrown if there is a problem communicating with Azure Storage service.
 */
AzureFileSystemThreadFactory (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureFileSystemThreadPoolExecutor.java)/**
   * A ThreadFactory for Azure File operation threads with meaningful names helpful
   * for debugging purposes.
   */
AzureFileSystemThreadTask (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureFileSystemThreadTask.java)/**
 * Interface for executing the file operation by a thread.
 */
TestHookOperationContext (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java)/**
   * A test hook interface that can modify the operation context we use for
   * Azure Storage operations, e.g. to inject errors.
   */
PermissionStatusJsonSerializer (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java)/**
   * A converter for PermissionStatus to/from JSON as we want it in the blob
   * metadata.
   */
AzureNativeFileSystemStore (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java)/**
 * Core implementation of Windows Azure Filesystem for Hadoop.
 * Provides the bridging logic between Hadoop's abstract filesystem and Azure Storage
 *
 */
BlobOperationDescriptor (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/BlobOperationDescriptor.java)/**
 * Determines the operation type (PutBlock, PutPage, GetBlob, etc) of Azure
 * Storage operations.  This is used by the handlers of the SendingRequestEvent
 * and ResponseReceivedEvent exposed by the Azure Storage SDK to identify
 * operation types (since the type of operation is not exposed by the SDK).
 */
UploadCommand (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/BlockBlobAppendStream.java)/**
   * Commands send from client calls to the background thread pool.
   */
UploaderThreadFactory (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/BlockBlobAppendStream.java)/**
   * A ThreadFactory that creates uploader thread with
   * meaningful names helpful for debugging purposes.
   */
UploadBlockCommand (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/BlockBlobAppendStream.java)/**
   * Upload block commands.
   */
ByteArrayOutputStreamInternal (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/BlockBlobAppendStream.java)/**
     * Internal output stream with read access to the internal buffer.
     */
UploadBlockListCommand (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/BlockBlobAppendStream.java)/**
   * Upload blob block list commands.
   */
WriteRequest (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/BlockBlobAppendStream.java)/**
   * Runnable instance that uploads the block of data to azure storage.
   */
MemoryOutputStream (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/BlockBlobInputStream.java)/**
   * An <code>OutputStream</code> backed by a user-supplied buffer.
   */
BlockBlobInputStream (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/BlockBlobInputStream.java)/**
 * Encapsulates the BlobInputStream used by block blobs and adds support for
 * random access and seek. Random access performance is improved by several
 * orders of magnitude.
 */
CachingAuthorizer (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/CachingAuthorizer.java)/**
 * Class that provides caching for Authorize and getSasUri calls
 * @param <K> -  The cache key type
 * @param <V> - The cached value type
 */
CachedAuthorizerEntry (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/CachingAuthorizer.java)/**
 * POJO representing the cache key for authorization calls
 */
CachedSASKeyEntry (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/CachingAuthorizer.java)/**
 * POJO representing the cache key for sas-key calls
 */
TimerTaskImpl (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ClientThrottlingAnalyzer.java)/**
   * Timer callback implementation for periodically analyzing metrics.
   */
BlobOperationMetrics (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ClientThrottlingAnalyzer.java)/**
   * Stores blob operation metrics during each analysis period.
   */
ClientThrottlingAnalyzer (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ClientThrottlingAnalyzer.java)/**
 * Throttles storage operations to minimize errors and maximum throughput. This
 * improves throughput by as much as 35% when the service throttles requests due
 * to exceeding account level ingress or egress limits.
 */
ErrorReceivingResponseEventHandler (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ClientThrottlingIntercept.java)/**
   * The ErrorReceivingResponseEvent is fired when the Azure Storage SDK
   * encounters a network error before the HTTP status and response headers are
   * received.
   */
SendingRequestEventHandler (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ClientThrottlingIntercept.java)/**
   * The SendingRequestEvent is fired before the Azure Storage SDK sends a
   * request.
   */
ResponseReceivedEventHandler (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ClientThrottlingIntercept.java)/**
   * The ResponseReceivedEvent is fired after the Azure Storage SDK receives a
   * response.
   */
ClientThrottlingIntercept (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ClientThrottlingIntercept.java)/**
 * Throttles Azure Storage read and write operations to achieve maximum
 * throughput by minimizing errors.  The errors occur when the account ingress
 * or egress limits are exceeded and the server-side throttles requests.
 * Server-side throttling causes the retry policy to be used, but the retry
 * policy sleeps for long periods of time causing the total ingress or egress
 * throughput to be as much as 35% lower than optimal.  The retry policy is also
 * after the fact, in that it applies after a request fails.  On the other hand,
 * the client-side throttling implemented here happens before requests are made
 * and sleeps just enough to minimize errors, allowing optimal ingress and/or
 * egress throughput.
 */
FileMetadata (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/FileMetadata.java)/**
 * <p>
 * Holds basic metadata for a file stored in a {@link NativeFileSystemStore}.
 * </p>
 */
KeyProvider (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/KeyProvider.java)/**
 * The interface that every Azure file system key provider must implement.
 */
KeyProviderException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/KeyProviderException.java)/**
 * Thrown if there is a problem instantiating a KeyProvider or retrieving a key
 * using a KeyProvider object.
 */
AzureFileSystemInstrumentation (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/AzureFileSystemInstrumentation.java)/**
 * A metrics source for the WASB file system to track all the metrics we care
 * about for getting a clear picture of the performance/reliability/interaction
 * of the Hadoop cluster with Azure Storage.
 */
AzureFileSystemMetricsSystem (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/AzureFileSystemMetricsSystem.java)/**
 * AzureFileSystemMetricsSystem
 */
BlockTransferWindow (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/BandwidthGaugeUpdater.java)/**
   * A single block transfer.
   */
UploadBandwidthUpdater (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/BandwidthGaugeUpdater.java)/**
   * The auto-update thread.
   */
BandwidthGaugeUpdater (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/BandwidthGaugeUpdater.java)/**
 * Internal implementation class to help calculate the current bytes
 * uploaded/downloaded and the maximum bandwidth gauges.
 */
ErrorMetricUpdater (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/ErrorMetricUpdater.java)/**
 * An event listener to the ResponseReceived event from Azure Storage that will
 * update error metrics appropriately when it gets that event.
 */
ResponseReceivedMetricUpdater (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/ResponseReceivedMetricUpdater.java)/**
 * An event listener to the ResponseReceived event from Azure Storage that will
 * update metrics appropriately when it gets that event.
 */
DataPoint (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/RollingWindowAverage.java)/**
   * A single data point.
   */
RollingWindowAverage (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/RollingWindowAverage.java)/**
 * Helper class to calculate rolling-window averages.
 * Used to calculate rolling-window metrics in AzureNativeFileSystem.
 */
Secure (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java)/**
   * <p>
   * A {@link FileSystem} for reading and writing files stored on <a
   * href="http://store.azure.com/">Windows Azure</a>. This implementation is
   * blob-based and stores files on Azure in their native form so they can be read
   * by other Azure tools. This implementation uses HTTPS for secure network communication.
   * </p>
   */
NativeAzureFsOutputStream (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java)/**
   * Azure output stream; wraps an inner stream of different types.
   */
DanglingFileHandler (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java)/**
   * A handler that defines what to do with blobs whose upload was
   * interrupted.
   */
DanglingFileDeleter (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java)/**
   * Handler implementation for just deleting dangling files and cleaning
   * them up.
   */
DanglingFileRecoverer (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java)/**
   * Handler implementation for just moving dangling files to recovery
   * location (/lost+found).
   */
NativeAzureFileSystem (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java)/**
 * A {@link FileSystem} for reading and writing files stored on <a
 * href="http://store.azure.com/">Windows Azure</a>. This implementation is
 * blob-based and stores files on Azure in their native form so they can be read
 * by other Azure tools.
 */
NativeFileSystemStore (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeFileSystemStore.java)/**
 * <p>
 * An abstraction for a key-based {@link File} store.
 * </p>
 */
PageBlobFormatHelpers (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobFormatHelpers.java)/**
 * Constants and helper methods for ASV's custom data format in page blobs.
 */
WriteRequest (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobOutputStream.java)/**
   * A single write request for data to write to Azure storage.
   */
PageBlobOutputStream (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobOutputStream.java)/**
 * An output stream that write file data to a page blob stored using ASV's
 * custom format.
 */
RemoteSASKeyGeneratorImpl (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/RemoteSASKeyGeneratorImpl.java)/**
 * Class implementing a RemoteSASKeyGenerator. This class
 * uses the url passed in via the Configuration to make a
 * rest call to generate the required SAS Key.
 */
RemoteSASKeyGenerationResponse (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/RemoteSASKeyGeneratorImpl.java)/**
 * POJO representing the response expected from a Remote
 * SAS Key generation service.
 * The remote SAS Key generation service is expected to
 * return SAS key in json format:
 * {
 *   "responseCode" : 0 or non-zero <int>,
 *   "responseMessage" : relavant message on failure <String>,
 *   "sasKey" : Requested SAS Key <String>
 * }
 */
RemoteWasbAuthorizerImpl (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/RemoteWasbAuthorizerImpl.java)/**
 * Class implementing WasbAuthorizerInterface using a remote
 * service that implements the authorization operation. This
 * class expects the url of the remote service to be passed
 * via config.
 */
RemoteWasbAuthorizerResponse (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/RemoteWasbAuthorizerImpl.java)/**
 * POJO representing the response expected from a remote
 * authorization service.
 * The remote service is expected to return the authorization
 * response in the following JSON format
 * {
 *   "responseCode" : 0 or non-zero <int>,
 *   "responseMessage" : relevant message of failure <String>
 *   "authorizationResult" : authorization result <boolean>
 *   true - if auhorization allowed
 *   false - otherwise.
 * }
 */
SASKeyGenerationException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SASKeyGenerationException.java)/**
 * Exception that gets thrown during generation of SAS Key.
 *
 */
SASKeyGeneratorImpl (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SASKeyGeneratorImpl.java)/**
 * Abstract base class for the SAS Key Generator implementation
 *
 */
SASKeyGeneratorInterface (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SASKeyGeneratorInterface.java)/**
 * Iterface used by AzureNativeFileSysteStore to retrieve SAS Keys for the
 * respective azure storage entity. This interface is expected to be
 * implemented in two modes:
 * 1) Local Mode: In this mode SAS Keys are generated
 *    in same address space as the WASB. This will be primarily used for
 *    testing purposes.
 * 2) Remote Mode: In this mode SAS Keys are generated in a sepearte process
 *    other than WASB and will be communicated via client.
 */
SecureModeException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SecureModeException.java)/**
 * Exception that is thrown when any error is encountered
 * is SAS Mode operation of WASB.
 */
SASWrappingIterator (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SecureStorageInterfaceImpl.java)/**
   * This iterator wraps every ListBlobItem as they come from the listBlobs()
   * calls to their proper wrapping objects.
   */
SecureWasbRemoteCallHelper (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SecureWasbRemoteCallHelper.java)/**
 * Helper class the has constants and helper methods
 * used in WASB when integrating with a remote http cred
 * service which uses Kerberos and delegation tokens.
 * Currently, remote service will be used to generate
 * SAS keys, authorization and delegation token operations.
 */
Constants (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/security/Constants.java)/**
 * Constants for used with WASB security implementation.
 */
JsonUtils (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/security/JsonUtils.java)/**
 * Utility class to parse JSON.
 */
RemoteWasbDelegationTokenManager (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/security/RemoteWasbDelegationTokenManager.java)/**
 * Class to manage delegation token operations by making rest call to remote service.
 */
SpnegoToken (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/security/SpnegoToken.java)/**
 * Class to represent SPNEGO token.
 */
TokenUtils (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/security/TokenUtils.java)/**
 * Utility methods common for token management
 */
WasbDelegationTokenIdentifier (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/security/WasbDelegationTokenIdentifier.java)/**
 * Delegation token Identifier for WASB delegation tokens.
 */
WasbDelegationTokenManager (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/security/WasbDelegationTokenManager.java)/**
 * Interface for Managing the Delegation tokens.
 */
WasbTokenRenewer (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/security/WasbTokenRenewer.java)/**
 * Token Renewer for renewing WASB delegation tokens with remote service.
 */
SelfRenewingLease (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfRenewingLease.java)/**
 * An Azure blob lease that automatically renews itself indefinitely
 * using a background thread. Use it to synchronize distributed processes,
 * or to prevent writes to the blob by other processes that don't
 * have the lease.
 *
 * Creating a new Lease object blocks the caller until the Azure blob lease is
 * acquired.
 *
 * Attempting to get a lease on a non-existent blob throws StorageException.
 *
 * Call free() to release the Lease.
 *
 * You can use this Lease like a distributed lock. If the holder process
 * dies, the lease will time out since it won't be renewed.
 */
SelfThrottlingIntercept (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfThrottlingIntercept.java)/**
 * 
 * Introduces delays in our Azure traffic to prevent overrunning the server-side throttling limits.
 *
 */
SendRequestIntercept (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SendRequestIntercept.java)/**
 * Manages the lifetime of binding on the operation contexts to intercept send
 * request events to Azure storage and allow concurrent OOB I/Os.
 */
ShellDecryptionKeyProvider (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ShellDecryptionKeyProvider.java)/**
 * Shell decryption key provider which invokes an external script that will
 * perform the key decryption.
 */
SimpleKeyProvider (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SimpleKeyProvider.java)/**
 * Key provider that simply returns the storage account key from the
 * configuration as plaintext.
 */
CloudBlobDirectoryWrapper (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java)/**
   * A thin wrapper over the
   * {@link com.microsoft.azure.storage.blob.CloudBlobDirectory} class
   * that simply redirects calls to the real object except in unit tests.
   */
CloudBlobContainerWrapper (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java)/**
   * A thin wrapper over the
   * {@link com.microsoft.azure.storage.blob.CloudBlobContainer} class
   * that simply redirects calls to the real object except in unit tests.
   */
CloudBlobWrapper (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java)/**
   * A thin wrapper over the {@link CloudBlob} class that simply redirects calls
   * to the real object except in unit tests.
   */
CloudBlockBlobWrapper (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java)/**
   * A thin wrapper over the
   * {@link com.microsoft.azure.storage.blob.CloudBlockBlob} class
   * that simply redirects calls to the real object except in unit tests.
   */
CloudPageBlobWrapper (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java)/**
   * A thin wrapper over the
   * {@link com.microsoft.azure.storage.blob.CloudPageBlob}
   * class that simply redirects calls to the real object except in unit tests.
   */
StorageInterface (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java)/**
 * This is a very thin layer over the methods exposed by the Windows Azure
 * Storage SDK that we need for WASB implementation. This base class has a real
 * implementation that just simply redirects to the SDK, and a memory-backed one
 * that's used for unit tests.
 *
 * IMPORTANT: all the methods here must remain very simple redirects since code
 * written here can't be properly unit tested.
 */
WrappingIterator (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterfaceImpl.java)/**
   * This iterator wraps every ListBlobItem as they come from the listBlobs()
   * calls to their proper wrapping objects.
   */
StorageInterfaceImpl (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterfaceImpl.java)/**
 * A real implementation of the Azure interaction layer that just redirects
 * calls to the Windows Azure storage SDK.
 */
SyncableDataOutputStream (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SyncableDataOutputStream.java)/**
 * Support the Syncable interface on top of a DataOutputStream.
 * This allows passing the sync/hflush/hsync calls through to the
 * wrapped stream passed in to the constructor. This is required
 * for HBase when wrapping a PageBlobOutputStream used as a write-ahead log.
 */
Wasb (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/Wasb.java)/**
 * WASB implementation of AbstractFileSystem.
 * This impl delegates to the old FileSystem
 */
WasbAuthorizationException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/WasbAuthorizationException.java)/**
 *  Exception that gets thrown during the authorization failures
 *  in WASB.
 */
WasbAuthorizerInterface (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/WasbAuthorizerInterface.java)/**
 *  Interface to implement authorization support in WASB.
 *  API's of this interface will be implemented in the
 *  StorageInterface Layer before making calls to Azure
 *  Storage.
 */
WasbFsck (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/WasbFsck.java)/**
 * An fsck tool implementation for WASB that does various admin/cleanup/recovery
 * tasks on the WASB file system.
 */
WasbRemoteCallHelper (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/WasbRemoteCallHelper.java)/**
 * Helper class the has constants and helper methods
 * used in WASB when integrating with a remote http cred
 * service. Currently, remote service will be used to generate
 * SAS keys.
 */
Wasbs (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/Wasbs.java)/**
 * WASB implementation of AbstractFileSystem for wasbs scheme.
 * This impl delegates to the old FileSystem
 */
Abfs (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/Abfs.java)/**
 * Azure Blob File System implementation of AbstractFileSystem.
 * This impl delegates to the old FileSystem
 */
AbfsConfiguration (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java)/**
 * Configuration for Azure Blob FileSystem.
 */
Abfss (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/Abfss.java)/**
 * Azure Blob File System implementation of AbstractFileSystem.
 * This impl delegates to the old FileSystem
 */
AzureBlobFileSystem (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java)/**
 * A {@link org.apache.hadoop.fs.FileSystem} for reading and writing files stored on <a
 * href="http://store.azure.com/">Windows Azure</a>
 */
AzureBlobFileSystemStore (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java)/**
 * Provides the bridging logic between Hadoop's abstract filesystem and Azure Storage.
 */
AbfsHttpConstants (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AbfsHttpConstants.java)/**
 * Responsible to keep all constant keys used in abfs rest client here.
 */
AuthConfigurations (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/AuthConfigurations.java)/**
 * Responsible to keep all the Azure Blob File System auth related
 * configurations.
 */
ConfigurationKeys (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java)/**
 * Responsible to keep all the Azure Blob File System configurations keys in Hadoop configuration file.
 */
FileSystemConfigurations (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java)/**
 * Responsible to keep all the Azure Blob File System related configurations.
 */
FileSystemUriSchemes (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemUriSchemes.java)/**
 * Responsible to keep all Azure Blob File System valid URI schemes.
 */
HttpHeaderConfigurations (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/HttpHeaderConfigurations.java)/**
 * Responsible to keep all abfs http headers here.
 */
HttpQueryParams (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/HttpQueryParams.java)/**
 * Responsible to keep all Http Query params here.
 */
ConfigurationValidationAnnotations (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/annotations/ConfigurationValidationAnnotations.java)/**
 * Definitions of Annotations for all types of the validators.
 */
ConfigurationValidator (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/diagnostics/ConfigurationValidator.java)/**
 * ConfigurationValidator to validate the value of a configuration key
 * @param <T> the type of the validator and the validated value.
 */
AbfsRestOperationException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/AbfsRestOperationException.java)/**
 * Exception to wrap Azure service error responses.
 */
AzureBlobFileSystemException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/AzureBlobFileSystemException.java)/**
 * Base exception for any Azure Blob File System driver exceptions. All the exceptions must inherit this class.
 */
ConfigurationPropertyNotFoundException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/ConfigurationPropertyNotFoundException.java)/**
 * Thrown when a searched for element is not found
 */
FileSystemOperationUnhandledException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/FileSystemOperationUnhandledException.java)/**
 * Thrown when an unhandled exception is occurred during a file system operation.
 */
InvalidAbfsRestOperationException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/InvalidAbfsRestOperationException.java)/**
 * Exception to wrap invalid Azure service error responses.
 */
InvalidAclOperationException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/InvalidAclOperationException.java)/**
 * Thrown when there is an attempt to perform an invalid operation on an ACL.
 */
InvalidConfigurationValueException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/InvalidConfigurationValueException.java)/**
 * Thrown when a configuration value is invalid
 */
InvalidFileSystemPropertyException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/InvalidFileSystemPropertyException.java)/**
 * Thrown when a file system property is invalid.
 */
InvalidUriAuthorityException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/InvalidUriAuthorityException.java)/**
 * Thrown when URI authority is invalid.
 */
InvalidUriException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/InvalidUriException.java)/**
 * Thrown when URI is invalid.
 */
KeyProviderException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/KeyProviderException.java)/**
 * Thrown if there is a problem instantiating a KeyProvider or retrieving a key
 * using a KeyProvider object.
 */
TimeoutException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/TimeoutException.java)/**
 * Thrown when a timeout happens.
 */
TokenAccessProviderException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/exceptions/TokenAccessProviderException.java)/**
 * Thrown if there is a problem instantiating a TokenAccessProvider or retrieving a configuration
 * using a TokenAccessProvider object.
 */
ListResultEntrySchema (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/ListResultEntrySchema.java)/**
 * The ListResultEntrySchema model.
 */
ListResultSchema (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/contracts/services/ListResultSchema.java)/**
 * The ListResultSchema model.
 */
Base64StringConfigurationBasicValidator (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/diagnostics/Base64StringConfigurationBasicValidator.java)/**
* String Base64 configuration value Validator.
*/
BooleanConfigurationBasicValidator (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/diagnostics/BooleanConfigurationBasicValidator.java)/**
 * Boolean configuration value validator.
 */
ConfigurationBasicValidator (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/diagnostics/ConfigurationBasicValidator.java)/**
 * ConfigurationBasicValidator covers the base case of missing user defined configuration value
 * @param <T> the type of the validated value
 */
IntegerConfigurationBasicValidator (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/diagnostics/IntegerConfigurationBasicValidator.java)/**
 * Integer configuration value Validator.
 */
LongConfigurationBasicValidator (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/diagnostics/LongConfigurationBasicValidator.java)/**
 * Long configuration value Validator.
 */
StringConfigurationBasicValidator (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/diagnostics/StringConfigurationBasicValidator.java)/**
 * String configuration value Validator.
 */
AbfsAuthorizationException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/extensions/AbfsAuthorizationException.java)/**
 * Exception raised on ABFS Authorization failures.
 */
AbfsAuthorizer (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/extensions/AbfsAuthorizer.java)/**
 * Interface to support authorization in Azure Blob File System.
 */
BoundDTExtension (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/extensions/BoundDTExtension.java)/**
 * An optional extension for custom extensions, so as to support
 * tighter integration.
 *
 * This interface can be implemented by either of a
 * {@link CustomDelegationTokenManager} or a {@link CustomTokenProviderAdaptee}.
 *
 * In both cases, extra lifecycle operation will be invoked.
 *
 * <ol>
 *   <li>{@link #bind(URI, Configuration)} will
 *   be invoked after {@code initialize()}</li>
 *   <li>{@link Closeable#close()} will be invoked
 *   when the Filesystem instance is closed.</li>
 * </ol>
 *
 * The {@link #getCanonicalServiceName()} will be invoked on a Custom
 * DT provider when the filesystem is asked for a Canonical Service Name.
 *
 * The {@link #getUserAgentSuffix()} is invoked on a CustomTokenProviderAdaptee
 * as the filesystem is initialized; the User Agent Suffix which it returns
 * is included in the UA header used for the ABFS Client -and so logged
 * in the ABFS access logs.
 *
 * This allows for token providers to to provide extra information
 * about the caller for use in auditing requests.
 */
CustomDelegationTokenManager (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/extensions/CustomDelegationTokenManager.java)/**
 * Interface for Managing the Delegation tokens.
 *
 * Implementations which also implement BoundDTExtension will have
 * the its {@code bind()} called
 * after {@code initialize)} and before any calls to
 * {@link #getDelegationToken(String)}.
 * It will not be bound during token renew or cancel operations: there is
 * no Filesystem to bind to in those operations.
 */
CustomTokenProviderAdaptee (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/extensions/CustomTokenProviderAdaptee.java)/**
 * This interface provides an extensibility model for customizing the acquisition
 * of Azure Active Directory Access Tokens.   When "fs.azure.account.auth.type" is
 * set to "Custom", implementors may use the
 * "fs.azure.account.oauth.provider.type.{accountName}" configuration property
 * to specify a class with a custom implementation of CustomTokenProviderAdaptee.
 * This class will be dynamically loaded, initialized, and invoked to provide
 * AAD Access Tokens and their Expiry.
 */
ExtensionHelper (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/extensions/ExtensionHelper.java)/**
 * Classes to help with use of extensions, expecially those
 * implementing @{@link BoundDTExtension}.
 */
AccessTokenProvider (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AccessTokenProvider.java)/**
 * Returns an Azure Active Directory token when requested. The provider can
 * cache the token if it has already retrieved one. If it does, then the
 * provider is responsible for checking expiry and refreshing as needed.
 *
 * In other words, this is is a token cache that fetches tokens when
 * requested, if the cached token has expired.
 *
 */
HttpException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AzureADAuthenticator.java)/**
   * This exception class contains the http error code,
   * requestId and error message, it is thrown when AzureADAuthenticator
   * failed to get the Azure Active Directory token.
   */
UnexpectedResponseException (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AzureADAuthenticator.java)/**
   * An unexpected HTTP response was raised, such as text coming back
   * from what should be an OAuth endpoint.
   */
AzureADToken (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/AzureADToken.java)/**
 * Object representing the AAD access token to use when making HTTP requests to Azure Data Lake Storage.
 */
ClientCredsTokenProvider (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/ClientCredsTokenProvider.java)/**
 * Provides tokens based on client credentials.
 */
CustomTokenProviderAdapter (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/CustomTokenProviderAdapter.java)/**
 * Provides tokens based on custom implementation, following the Adapter Design
 * Pattern.
 */
IdentityTransformer (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/IdentityTransformer.java)/**
 * Perform transformation for Azure Active Directory identities used in owner, group and acls.
 */
MsiTokenProvider (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/MsiTokenProvider.java)/**
 * Provides tokens based on Azure VM's Managed Service Identity.
 */
QueryParams (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/QueryParams.java)/**
 * Utilities class http query parameters.
 */
RefreshTokenBasedTokenProvider (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/RefreshTokenBasedTokenProvider.java)/**
 * Provides tokens based on refresh token.
 */
UserPasswordTokenProvider (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/oauth2/UserPasswordTokenProvider.java)/**
 * Provides tokens based on username and password.
 */
SecureAzureBlobFileSystem (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/SecureAzureBlobFileSystem.java)/**
 * A secure {@link org.apache.hadoop.fs.FileSystem} for reading and writing files stored on <a
 * href="http://store.azure.com/">Windows Azure</a>
 */
AbfsDelegationTokenIdentifier (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/security/AbfsDelegationTokenIdentifier.java)/**
 * Delegation token Identifier for ABFS delegation tokens.
 * The token kind from {@link #getKind()} is {@link #TOKEN_KIND}, always.
 *
 * Subclasses have to very careful when looking up tokens (which will of
 * course be registered in the credentials as of this kind), in case the
 * incoming credentials are actually of a different subtype.
 */
AbfsDelegationTokenManager (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/security/AbfsDelegationTokenManager.java)/**
 * Class for delegation token Manager.
 *
 * Instantiates the class declared in
 * {@link ConfigurationKeys#FS_AZURE_DELEGATION_TOKEN_PROVIDER_TYPE} and
 * issues tokens from it.
 */
AbfsDtFetcher (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/security/AbfsDtFetcher.java)/**
 * A DT fetcher for Abfs.
 * This is a copy-and-paste of
 * {@code org.apache.hadoop.hdfs.HdfsDtFetcher}.
 *
 * It is needed for the `hadoop dtutil` command.
 */
AbfssDtFetcher (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/security/AbfssDtFetcher.java)/**
 * The DT Fetcher for abfss.
 */
AbfsTokenRenewer (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/security/AbfsTokenRenewer.java)/**
 * Token Renewer for renewing ABFS delegation tokens with remote service.
 *
 * Handles tokens of kind  {@link AbfsDelegationTokenIdentifier#TOKEN_KIND}.
 */
AbfsAclHelper (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsAclHelper.java)/**
 * AbfsAclHelper provides convenience methods to implement modifyAclEntries / removeAclEntries / removeAcl / removeDefaultAcl
 * from setAcl and getAcl.
 */
AbfsClient (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java)/**
 * AbfsClient.
 */
TimerTaskImpl (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientThrottlingAnalyzer.java)/**
   * Timer callback implementation for periodically analyzing metrics.
   */
AbfsOperationMetrics (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientThrottlingAnalyzer.java)/**
   * Stores Abfs operation metrics during each analysis period.
   */
AbfsClientThrottlingIntercept (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClientThrottlingIntercept.java)/**
 * Throttles Azure Blob File System read and write operations to achieve maximum
 * throughput by minimizing errors.  The errors occur when the account ingress
 * or egress limits are exceeded and the server-side throttles requests.
 * Server-side throttling causes the retry policy to be used, but the retry
 * policy sleeps for long periods of time causing the total ingress or egress
 * throughput to be as much as 35% lower than optimal.  The retry policy is also
 * after the fact, in that it applies after a request fails.  On the other hand,
 * the client-side throttling implemented here happens before requests are made
 * and sleeps just enough to minimize errors, allowing optimal ingress and/or
 * egress throughput.
 */
AbfsHttpHeader (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpHeader.java)/**
 * The Http Request / Response Headers for Rest AbfsClient.
 */
AbfsHttpOperation (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java)/**
 * Represents an HTTP operation.
 */
AbfsInputStream (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java)/**
 * The AbfsInputStream for AbfsClient.
 */
AbfsIoUtils (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsIoUtils.java)/**
 * Utility classes to work with the remote store.
 */
AbfsOutputStream (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java)/**
 * The BlobFsOutputStream for Rest AbfsClient.
 */
AbfsPermission (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsPermission.java)/**
 * The AbfsPermission for AbfsClient.
 */
AbfsRestOperation (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java)/**
 * The AbfsRestOperation for Rest AbfsClient.
 */
AbfsUriQueryBuilder (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsUriQueryBuilder.java)/**
 * The UrlQueryBuilder for Rest AbfsClient.
 */
ExponentialRetryPolicy (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ExponentialRetryPolicy.java)/**
 * Retry policy used by AbfsClient.
 * */
KeyProvider (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/KeyProvider.java)/**
 * The interface that every Azure file system key provider must implement.
 */
ReadBufferManager (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java)/**
 * The Read Buffer Manager for Rest AbfsClient.
 */
SharedKeyCredentials (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SharedKeyCredentials.java)/**
 * Represents the shared key credentials used to access an Azure Storage
 * account.
 */
ShellDecryptionKeyProvider (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ShellDecryptionKeyProvider.java)/**
 * Shell decryption key provider which invokes an external script that will
 * perform the key decryption.
 */
SimpleKeyProvider (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/SimpleKeyProvider.java)/**
 * Key provider that simply returns the storage account key from the
 * configuration as plaintext.
 */
Base64 (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/Base64.java)/**
 * Base64
 */
CRC64 (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/CRC64.java)/**
 * CRC64 implementation for AzureBlobFileSystem.
 */
UriUtils (/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/UriUtils.java)/**
 * Utility class to help with Abfs url transformation to blob urls.
 */
AbstractWasbTestBase (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/AbstractWasbTestBase.java)/**
 * Abstract test class that provides basic setup and teardown of testing Azure
 * Storage account.  Each subclass defines a different set of test cases to run
 * and overrides {@link #createTestAccount()} to set up the testing account used
 * to run those tests.  The returned account might integrate with Azure Storage
 * directly or it might be a mock implementation.
 */
AbstractWasbTestWithTimeout (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/AbstractWasbTestWithTimeout.java)/**
 * Base class for any Wasb test with timeouts & named threads.
 * This class does not attempt to bind to Azure.
 */
AzureBlobStorageTestAccount (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/AzureBlobStorageTestAccount.java)/**
 * Helper class to create WASB file systems backed by either a mock in-memory
 * implementation or a real Azure Storage account.
 */
ITestAzureNativeContractCreate (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/contract/ITestAzureNativeContractCreate.java)/**
 * Contract test.
 */
ITestAzureNativeContractDelete (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/contract/ITestAzureNativeContractDelete.java)/**
 * Contract test.
 */
ITestAzureNativeContractDistCp (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/contract/ITestAzureNativeContractDistCp.java)/**
 * Contract test suite covering WASB integration with DistCp.
 */
ITestAzureNativeContractGetFileStatus (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/contract/ITestAzureNativeContractGetFileStatus.java)/**
 * Contract test.
 */
ITestAzureNativeContractMkdir (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/contract/ITestAzureNativeContractMkdir.java)/**
 * Contract test.
 */
ITestAzureNativeContractOpen (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/contract/ITestAzureNativeContractOpen.java)/**
 * Contract test.
 */
ITestAzureNativeContractRename (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/contract/ITestAzureNativeContractRename.java)/**
 * Contract test.
 */
ITestAzureNativeContractSeek (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/contract/ITestAzureNativeContractSeek.java)/**
 * Contract test.
 */
NativeAzureFileSystemContract (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/contract/NativeAzureFileSystemContract.java)/**
 * Azure Contract. Test paths are created using any maven fork
 * identifier, if defined. This guarantees paths unique to tests
 * running in parallel.
 */
InMemoryBlockBlobStore (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/InMemoryBlockBlobStore.java)/**
 * A simple memory key-value store to help mock the Windows Azure Storage
 * implementation for unit testing.
 */
AbstractAzureScaleTest (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/integration/AbstractAzureScaleTest.java)/**
 * Scale tests are only executed if the scale profile
 * is set; the setup method will check this and skip
 * tests if not.
 *
 */
AzureTestConstants (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/integration/AzureTestConstants.java)/**
 * Constants for the Azure tests.
 */
AzureTestUtils (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/integration/AzureTestUtils.java)/**
 * Utilities for the Azure tests. Based on {@code S3ATestUtils}, so
 * (initially) has unused method.
 */
CleanupTestContainers (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/integration/CleanupTestContainers.java)/**
 * This looks like a test, but it is really a command to invoke to
 * clean up containers created in other test runs.
 *
 */
Sizes (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/integration/Sizes.java)/**
 * Sizes of data.
 * Checkstyle doesn't like the naming scheme or the fact its an interface.
 */
ITestAzureConcurrentOutOfBandIo (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestAzureConcurrentOutOfBandIo.java)/**
 * Handle OOB IO into a shared container.
 */
ITestAzureConcurrentOutOfBandIoWithSecureMode (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestAzureConcurrentOutOfBandIoWithSecureMode.java)/**
 * Extends ITestAzureConcurrentOutOfBandIo in order to run testReadOOBWrites with secure mode
 * (fs.azure.secure.mode) both enabled and disabled.
 */
ITestAzureFileSystemErrorConditions (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestAzureFileSystemErrorConditions.java)/**
 * Error handling.
 */
ContentMD5Checker (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestBlobDataValidation.java)/**
   * Connection inspector to check that MD5 fields for content is set/not set as
   * expected.
   */
ITestBlobDataValidation (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestBlobDataValidation.java)/**
 * Test that we do proper data integrity validation with MD5 checks as
 * configured.
 */
ITestBlobTypeSpeedDifference (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestBlobTypeSpeedDifference.java)/**
 * A simple benchmark to find out the difference in speed between block
 * and page blobs.
 */
ITestBlockBlobInputStream (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestBlockBlobInputStream.java)/**
 * Test semantics and performance of the original block blob input stream
 * (KEY_INPUT_STREAM_VERSION=1) and the new
 * <code>BlockBlobInputStream</code> (KEY_INPUT_STREAM_VERSION=2).
 */
ITestContainerChecks (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestContainerChecks.java)/**
 * Tests that WASB creates containers only if needed.
 */
ITestFileSystemOperationExceptionHandling (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestFileSystemOperationExceptionHandling.java)/**
 * Single threaded exception handling.
 */
ITestFileSystemOperationExceptionMessage (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestFileSystemOperationExceptionMessage.java)/**
 * Test for error messages coming from SDK.
 */
RenameThread (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestFileSystemOperationsExceptionHandlingMultiThreaded.java)/**
   * Helper thread that just renames the test file.
   */
ITestFileSystemOperationsExceptionHandlingMultiThreaded (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestFileSystemOperationsExceptionHandlingMultiThreaded.java)/**
 * Multithreaded operations on FS, verify failures are as expected.
 */
ITestFileSystemOperationsWithThreads (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestFileSystemOperationsWithThreads.java)/**
 * Tests the Native Azure file system (WASB) using parallel threads for rename and delete operations.
 */
ITestListPerformance (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestListPerformance.java)/**
 * Test list performance.
 */
ITestNativeAzureFileSystemAppend (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestNativeAzureFileSystemAppend.java)/**
 * Test append operations.
 */
ITestNativeAzureFileSystemAtomicRenameDirList (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestNativeAzureFileSystemAtomicRenameDirList.java)/**
 * Test atomic renaming.
 */
ITestNativeAzureFileSystemClientLogging (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestNativeAzureFileSystemClientLogging.java)/**
 * Test to validate Azure storage client side logging. Tests works only when
 * testing with Live Azure storage because Emulator does not have support for
 * client-side logging.
 *
 * <I>Important: </I> Do not attempt to move off commons-logging.
 * The tests will fail.
 */
ITestNativeAzureFileSystemConcurrencyLive (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestNativeAzureFileSystemConcurrencyLive.java)/***
 * Test class to hold all Live Azure storage concurrency tests.
 */
ITestNativeAzureFileSystemContractEmulator (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestNativeAzureFileSystemContractEmulator.java)/**
 * Run the {@code FileSystemContractBaseTest} tests against the emulator
 */
ITestNativeAzureFileSystemContractLive (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestNativeAzureFileSystemContractLive.java)/**
 * Run the {@link FileSystemContractBaseTest} test suite against azure storage.
 */
ITestNativeAzureFileSystemContractPageBlobLive (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestNativeAzureFileSystemContractPageBlobLive.java)/**
 * Run the {@link FileSystemContractBaseTest} test suite against azure
 * storage, after switching the FS using page blobs everywhere.
 */
ITestNativeAzureFileSystemLive (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestNativeAzureFileSystemLive.java)/**
 * Tests the Native Azure file system (WASB) against an actual blob store.
 */
ITestNativeAzureFSAuthorizationCaching (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestNativeAzureFSAuthorizationCaching.java)/**
 * Test class to hold all WASB authorization caching related tests.
 */
ITestNativeAzureFSAuthWithBlobSpecificKeys (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestNativeAzureFSAuthWithBlobSpecificKeys.java)/**
 * Test class to hold all WASB authorization tests that use blob-specific keys
 * to access storage.
 */
ITestNativeAzureFSPageBlobLive (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestNativeAzureFSPageBlobLive.java)/**
 * Run the base Azure file system tests strictly on page blobs to make sure fundamental
 * operations on page blob files and folders work as expected.
 * These operations include create, delete, rename, list, and so on.
 */
ITestOutOfBandAzureBlobOperationsLive (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestOutOfBandAzureBlobOperationsLive.java)/**
 * Live blob operations.
 */
ITestOutputStreamSemantics (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestOutputStreamSemantics.java)/**
 * Test semantics of functions flush, hflush, hsync, and close for block blobs,
 * block blobs with compaction, and page blobs.
 */
ITestPageBlobInputStream (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestPageBlobInputStream.java)/**
 * Test semantics of the page blob input stream
 */
ITestReadAndSeekPageBlobAfterWrite (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestReadAndSeekPageBlobAfterWrite.java)/**
 * Write data into a page blob and verify you can read back all of it
 * or just a part of it.
 */
ITestWasbRemoteCallHelper (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/ITestWasbRemoteCallHelper.java)/**
 * Test class to hold all WasbRemoteCallHelper tests.
 */
TagMatcher (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/ITestAzureFileSystemInstrumentation.java)/**
   * A matcher class for asserting that we got a tag with a given
   * value.
   */
TagExistsMatcher (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/ITestAzureFileSystemInstrumentation.java)/**
   * A matcher class for asserting that we got a tag with any value.
   */
ITestAzureFileSystemInstrumentation (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/ITestAzureFileSystemInstrumentation.java)/**
 * Instrumentation test, changing state of time and verifying metrics are
 * consistent.
 */
TestNativeAzureFileSystemMetricsSystem (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/TestNativeAzureFileSystemMetricsSystem.java)/**
 * Tests that the WASB-specific metrics system is working correctly.
 */
MockStorageInterface (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/MockStorageInterface.java)/**
 * A mock implementation of the Azure Storage interaction layer for unit tests.
 * Just does in-memory storage.
 */
TestBlobMetadata (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobMetadata.java)/**
 * Tests that we put the correct metadata on blobs created through WASB.
 */
ResponseReceivedEventHandler (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobOperationDescriptor.java)/**
   * The ResponseReceivedEvent is fired after the Azure Storage SDK receives a
   * response.
   */
SendingRequestEventHandler (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobOperationDescriptor.java)/**
   * The SendingRequestEvent is fired before the Azure Storage SDK sends a
   * request.
   */
TestBlobOperationDescriptor (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobOperationDescriptor.java)/**
 * Tests for <code>BlobOperationDescriptor</code>.
 */
TestClientThrottlingAnalyzer (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestClientThrottlingAnalyzer.java)/**
 * Tests for <code>ClientThrottlingAnalyzer</code>.
 */
TestKeyPageBlobDirectories (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestKeyPageBlobDirectories.java)/**
 * Test config property KEY_PAGE_BLOB_DIRECTORIES.
 */
TestNativeAzureFileSystemAuthorization (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemAuthorization.java)/**
 * Test class to hold all WASB authorization tests.
 */
TestNativeAzureFileSystemContractMocked (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemContractMocked.java)/**
 * Mocked testing of FileSystemContractBaseTest.
 */
TestNativeAzureFileSystemFileNameCheck (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemFileNameCheck.java)/**
 * Tests the scenario where a colon is included in the file/directory name.
 * 
 * NativeAzureFileSystem#create(), #mkdir(), and #rename() disallow the
 * creation/rename of files/directories through WASB that have colons in the
 * names.
 */
TestNativeAzureFileSystemMocked (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemMocked.java)/**
 * Run {@link NativeAzureFileSystemBaseTest} tests against a mocked store,
 * skipping tests of unsupported features
 */
TestNativeAzureFileSystemUploadLogic (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemUploadLogic.java)/**
 * Tests for the upload, buffering and flush logic in WASB.
 */
TestOutOfBandAzureBlobOperations (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestOutOfBandAzureBlobOperations.java)/**
 * Tests that WASB handles things gracefully when users add blobs to the Azure
 * Storage container from outside WASB's control.
 */
TestShellDecryptionKeyProvider (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestShellDecryptionKeyProvider.java)/**
 * Windows only tests of shell scripts to provide decryption keys.
 */
TestWasbFsck (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestWasbFsck.java)/**
 * Tests which look at fsck recovery.
 */
AbstractAbfsIntegrationTest (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsIntegrationTest.java)/**
 * Base for AzureBlobFileSystem Integration tests.
 *
 * <I>Important: This is for integration tests only.</I>
 */
AbstractAbfsScaleTest (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsScaleTest.java)/**
 * Integration tests at bigger scale; configurable as to
 * size, off by default.
 */
AbstractAbfsTestWithTimeout (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsTestWithTimeout.java)/**
 * Base class for any ABFS test with timeouts & named threads.
 * This class does not attempt to bind to Azure.
 */
TestConfigurationKeys (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/constants/TestConfigurationKeys.java)/**
 * Responsible to keep all the Azure Blob File System configurations keys in Hadoop configuration file.
 */
ABFSContractTestBinding (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ABFSContractTestBinding.java)/**
 * Bind ABFS contract tests to the Azure test setup/teardown.
 */
AbfsFileSystemContract (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/AbfsFileSystemContract.java)/**
 * Azure BlobFileSystem Contract. Test paths are created using any maven fork
 * identifier, if defined. This guarantees paths unique to tests
 * running in parallel.
 */
ITestAbfsFileSystemContractAppend (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAbfsFileSystemContractAppend.java)/**
 * Contract test for open operation.
 */
ITestAbfsFileSystemContractConcat (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAbfsFileSystemContractConcat.java)/**
 * Contract test for concat operation.
 */
ITestAbfsFileSystemContractCreate (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAbfsFileSystemContractCreate.java)/**
 * Contract test for create operation.
 */
ITestAbfsFileSystemContractDelete (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAbfsFileSystemContractDelete.java)/**
 * Contract test for delete operation.
 */
ITestAbfsFileSystemContractDistCp (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAbfsFileSystemContractDistCp.java)/**
 * Contract test for distCp operation.
 */
ITestAbfsFileSystemContractGetFileStatus (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAbfsFileSystemContractGetFileStatus.java)/**
 * Contract test for getFileStatus operation.
 */
ITestAbfsFileSystemContractMkdir (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAbfsFileSystemContractMkdir.java)/**
 * Contract test for mkdir operation.
 */
ITestAbfsFileSystemContractOpen (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAbfsFileSystemContractOpen.java)/**
 * Contract test for open operation.
 */
ITestAbfsFileSystemContractRename (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAbfsFileSystemContractRename.java)/**
 * Contract test for rename operation.
 */
ITestAbfsFileSystemContractRootDirectory (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAbfsFileSystemContractRootDirectory.java)/**
 * Contract test for root directory operation.
 */
ITestAbfsFileSystemContractSecureDistCp (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAbfsFileSystemContractSecureDistCp.java)/**
 * Contract test for secure distCP operation.
 */
ITestAbfsFileSystemContractSeek (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAbfsFileSystemContractSeek.java)/**
 * Contract test for seek operation.
 */
ITestAbfsFileSystemContractSetTimes (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAbfsFileSystemContractSetTimes.java)/**
 * Contract test for setTimes operation.
 */
ITestAzureBlobFileSystemBasics (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/contract/ITestAzureBlobFileSystemBasics.java)/**
 * Basic Contract test for Azure BlobFileSystem.
 */
TestConfigurationValidators (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/diagnostics/TestConfigurationValidators.java)/**
 * Test configuration validators.
 */
TokenSecretManager (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/ClassicDelegationTokenManager.java)/**
   * The secret manager always uses the same secret; the
   * factory for new identifiers is that of the token manager.
   */
ClassicDelegationTokenManager (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/ClassicDelegationTokenManager.java)/**
 * This is a Stub DT manager for testing, one which
 * implements the the {@link CustomDelegationTokenManager} API, but
 * not the extended one.
 *
 * Member variables are updated as operations are performed, so
 * test cases can make assertions about the state of the plugin.
 */
ITestAbfsDelegationTokens (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/ITestAbfsDelegationTokens.java)/**
 * Test custom DT support in ABFS.
 * This brings up a mini KDC in class setup/teardown, as the FS checks
 * for that when it enables security.
 *
 * Much of this code is copied from
 * {@code org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationIT}
 */
KerberizedAbfsCluster (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/KerberizedAbfsCluster.java)/**
 * composite service for adding kerberos login for ABFS
 * tests which require a logged in user.
 * Based on
 * {@code org.apache.hadoop.fs.s3a.auth.delegation.MiniKerberizedHadoopCluster}
 */
MockAbfsAuthorizer (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/MockAbfsAuthorizer.java)/**
 * A mock Azure Blob File System Authorization Implementation
 */
StubAbfsTokenIdentifier (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/StubAbfsTokenIdentifier.java)/**
 * Token identifier for testing ABFS DT support; matched with
 * a service declaration so it can be unmarshalled.
 */
StubDelegationTokenManager (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/StubDelegationTokenManager.java)/**
 * This is a Stub DT manager which adds support for {@link BoundDTExtension}
 * to {@link ClassicDelegationTokenManager}.
 */
TestCustomOauthTokenProvider (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/TestCustomOauthTokenProvider.java)/**
 * Test custom OAuth token providers.
 * This is a unit test not an E2E integration test because that would
 * require OAuth auth setup, always.
 * Instead this just checks that the creation works and that everything
 * is propagated.
 */
TestDTManagerLifecycle (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/TestDTManagerLifecycle.java)/**
 * Test the lifecycle of custom DT managers.
 */
WrappingTokenProvider (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/extensions/WrappingTokenProvider.java)/**
 * Implements a wrapper around ClientCredsTokenProvider.
 */
ITestAbfsClient (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsClient.java)/**
 * Test continuation token which has equal sign.
 */
ITestAbfsReadWriteAndSeek (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsReadWriteAndSeek.java)/**
 * Test read, write and seek.
 * Uses package-private methods in AbfsConfiguration, which is why it is in
 * this package.
 */
ITestAbfsRestOperationException (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsRestOperationException.java)/**
 * Verify the AbfsRestOperationException error message format.
 * */
ITestAzureBlobFilesystemAcl (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFilesystemAcl.java)/**
 * Test acl operations.
 */
ITestAzureBlobFileSystemAppend (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemAppend.java)/**
 * Test append operations.
 */
ITestAzureBlobFileSystemAuthorization (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemAuthorization.java)/**
 * Test Perform Authorization Check operation
 */
ITestAzureBlobFileSystemBackCompat (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemBackCompat.java)/**
 * Test AzureBlobFileSystem back compatibility with WASB.
 */
ITestAzureBlobFileSystemCLI (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCLI.java)/**
 * Tests for Azure Blob FileSystem CLI.
 */
ITestAzureBlobFileSystemCopy (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCopy.java)/**
 * Test copy operation.
 */
ITestAzureBlobFileSystemCreate (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCreate.java)/**
 * Test create operation.
 */
ITestAzureBlobFileSystemDelete (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemDelete.java)/**
 * Test delete operation.
 */
ITestAzureBlobFileSystemE2E (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemE2E.java)/**
 * Test end to end between ABFS client and ABFS server.
 */
ITestAzureBlobFileSystemE2EScale (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemE2EScale.java)/**
 * Test end to end between ABFS client and ABFS server with heavy traffic.
 */
ITestAzureBlobFileSystemFileStatus (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemFileStatus.java)/**
 * Test FileStatus.
 */
ITestAzureBlobFileSystemFinalize (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemFinalize.java)/**
 * Test finalize() method when "fs.abfs.impl.disable.cache" is enabled.
 */
ITestAzureBlobFileSystemFlush (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemFlush.java)/**
 * Test flush operation.
 * This class cannot be run in parallel test mode--check comments in
 * testWriteHeavyBytesToFileSyncFlush().
 */
ITestAzureBlobFileSystemInitAndCreate (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemInitAndCreate.java)/**
 * Test filesystem initialization and creation.
 */
ITestAzureBlobFileSystemListStatus (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemListStatus.java)/**
 * Test listStatus operation.
 */
ITestAzureBlobFileSystemMainOperation (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemMainOperation.java)/**
 * Test AzureBlobFileSystem main operations.
 * */
ITestAzureBlobFileSystemMkDir (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemMkDir.java)/**
 * Test mkdir operation.
 */
ITestAzureBlobFileSystemOauth (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemOauth.java)/**
 * Test Azure Oauth with Blob Data contributor role and Blob Data Reader role.
 * The Test AAD client need to be configured manually through Azure Portal, then save their properties in
 * configuration files.
 */
ITestAzureBlobFileSystemPermission (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemPermission.java)/**
 * Test permission operations.
 */
ITestAzureBlobFileSystemRandomRead (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRandomRead.java)/**
 * Test random read operation.
 */
ITestAzureBlobFileSystemRename (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRename.java)/**
 * Test rename operation.
 */
ITestAzureBlobFileSystemRenameUnicode (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRenameUnicode.java)/**
 * Parameterized test of rename operations of unicode paths.
 */
ITestAzureBlobFileSystemStoreListStatusWithRange (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemStoreListStatusWithRange.java)/**
 * Test AzureBlobFileSystemStore listStatus with startFrom.
 * */
ITestFileSystemInitialization (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestFileSystemInitialization.java)/**
 * Test AzureBlobFileSystem initialization.
 */
ITestFileSystemProperties (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestFileSystemProperties.java)/**
 * Test FileSystemProperties.
 */
ITestFileSystemRegistration (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestFileSystemRegistration.java)/**
 * Test AzureBlobFileSystem registration.
 * Use casts to have interesting stack traces on failures.
 */
ITestGetNameSpaceEnabled (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestGetNameSpaceEnabled.java)/**
 * Test getIsNamespaceEnabled call.
 */
ITestOauthOverAbfsScheme (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestOauthOverAbfsScheme.java)/**
 * Test Oauth fail fast when uri scheme is incorrect.
 */
ITestWasbAbfsCompatibility (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestWasbAbfsCompatibility.java)/**
 * Test compatibility between ABFS client and WASB client.
 */
TestAbfsClient (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsClient.java)/**
 * Test useragent of abfs client.
 *
 */
TestAbfsClientThrottlingAnalyzer (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsClientThrottlingAnalyzer.java)/**
 * Tests for <code>AbfsClientThrottlingAnalyzer</code>.
 */
TestQueryParams (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestQueryParams.java)/**
 * Test query params serialization.
 */
TestShellDecryptionKeyProvider (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestShellDecryptionKeyProvider.java)/**
 * Test ShellDecryptionKeyProvider.
 *
 */
TestAbfsConfigurationFieldsValidation (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAbfsConfigurationFieldsValidation.java)/**
 * Test ConfigurationServiceFieldsValidation.
 */
TestAbfsCrc64 (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAbfsCrc64.java)/**
 * Test for Crc64 in AzureBlobFileSystem, notice that ABFS CRC64 has its own polynomial.
 * */
GetClassInterface (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAccountConfiguration.java)/**
   * Dummy type used for testing handling of classes in configuration.
   */
GetClassImpl0 (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAccountConfiguration.java)/**
   * Dummy type used for testing handling of classes in configuration.
   */
GetClassImpl1 (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAccountConfiguration.java)/**
   * Dummy type used for testing handling of classes in configuration.
   */
TestAccountConfiguration (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/TestAccountConfiguration.java)/**
 * Tests correct precedence of various configurations that might be returned.
 * Configuration can be specified with the account name as a suffix to the
 * config key, or without one. Account-specific values should be returned
 * whenever they exist. Account-agnostic values are returned if they do not.
 * Default values are returned if neither exists.
 *
 * These tests are in 2 main groups: tests of methods that allow default values
 * (such as get and getPasswordString) are of one form, while tests of methods
 * that do allow default values (all others) follow another form.
 */
AbfsTestUtils (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/AbfsTestUtils.java)/**
 * Some Utils for ABFS tests.
 */
AclTestHelpers (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/AclTestHelpers.java)/**
 * Helper methods useful for writing ACL tests.
 */
Parallelized (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/Parallelized.java)/**
 * Provided for convenience to execute parametrized test cases concurrently.
 */
TestUriUtils (/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/utils/TestUriUtils.java)/**
 * Test ABFS UriUtils.
 */
Adl (/hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/Adl.java)/**
 * Expose adl:// scheme to access ADL file system.
 */
AdlConfKeys (/hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlConfKeys.java)/**
 * Constants.
 */
AdlFileStatus (/hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlFileStatus.java)/**
 * Shim class supporting linking against 2.x clients.
 */
AdlFileSystem (/hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlFileSystem.java)/**
 * A FileSystem to access Azure Data Lake Store.
 */
AdlFsInputStream (/hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlFsInputStream.java)/**
 * Wraps {@link ADLFileInputStream} implementation.
 */
AdlFsOutputStream (/hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlFsOutputStream.java)/**
 * Wraps {@link com.microsoft.azure.datalake.store.ADLFileOutputStream}
 * implementation.
 *
 * Flush semantics.
 * no-op, since some parts of hadoop ecosystem call flush(), expecting it to
 * have no perf impact. In hadoop filesystems, flush() itself guarantees no
 * durability: that is achieved by calling hflush() or hsync()
 */
AdlPermission (/hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlPermission.java)/**
 * Hadoop shell command -getfacl does not invoke getAclStatus if FsPermission
 * from getFileStatus has not set ACL bit to true. By default getAclBit returns
 * false.
 *
 * Provision to make additional call to invoke getAclStatus would be redundant
 * when adls is running as additional FS. To avoid this redundancy, provided
 * configuration to return true/false on getAclBit.
 */
AzureADTokenProvider (/hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/oauth2/AzureADTokenProvider.java)/**
 * Provide an Azure Active Directory supported
 * OAuth2 access token to be used to authenticate REST calls against Azure data
 * lake file system {@link org.apache.hadoop.fs.adl.AdlFileSystem}.
 */
CustomMockTokenProvider (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/common/CustomMockTokenProvider.java)/**
 * Custom token management without cache enabled.
 */
Parallelized (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/common/Parallelized.java)/**
 * Provided for convenience to execute parametrized test cases concurrently.
 */
AdlStorageConfiguration (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/AdlStorageConfiguration.java)/**
 * Configure Adl storage file system.
 */
TestAdlContractAppendLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlContractAppendLive.java)/**
 * Test Append on Adl file system.
 */
TestAdlContractConcatLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlContractConcatLive.java)/**
 * Test concat on Adl file system.
 */
TestAdlContractCreateLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlContractCreateLive.java)/**
 * Test creating files, overwrite options.
 */
TestAdlContractDeleteLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlContractDeleteLive.java)/**
 * Test delete contract test.
 */
TestAdlContractDistCpLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlContractDistCpLive.java)/**
 * Test DistCP operations.
 */
TestAdlContractGetFileStatusLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlContractGetFileStatusLive.java)/**
 * Test getFileStatus contract test.
 */
TestAdlContractMkdirLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlContractMkdirLive.java)/**
 * Test Mkdir contract on Adl storage file system.
 */
TestAdlContractOpenLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlContractOpenLive.java)/**
 * Test OPEN - read API.
 */
TestAdlContractRenameLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlContractRenameLive.java)/**
 * Test rename contract test cases on Adl file system.
 */
TestAdlContractRootDirLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlContractRootDirLive.java)/**
 * Test operation on root level.
 */
TestAdlContractSeekLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlContractSeekLive.java)/**
 * Test seek operation on Adl file system.
 */
TestAdlDifferentSizeWritesLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlDifferentSizeWritesLive.java)/**
 * Verify data integrity with different data sizes with buffer size.
 */
TestAdlFileContextCreateMkdirLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlFileContextCreateMkdirLive.java)/**
 * Test file context Create/Mkdir operation.
 */
TestAdlFileContextMainOperationsLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlFileContextMainOperationsLive.java)/**
 * Run collection of tests for the {@link FileContext}.
 */
TestAdlFileSystemContractLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlFileSystemContractLive.java)/**
 * Test Base contract tests on Adl file system.
 */
TestAdlInternalCreateNonRecursive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlInternalCreateNonRecursive.java)/**
 * Test createNonRecursive API.
 */
TestAdlPermissionLive (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlPermissionLive.java)/**
 * Test ACL permission on file/folder on Adl file system.
 */
TestAdlSdkConfiguration (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlSdkConfiguration.java)/**
 * Tests interactions with SDK and ensures configuration is having the desired
 * effect.
 */
TestAdlSupportedCharsetInPath (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestAdlSupportedCharsetInPath.java)/**
 * Test supported ASCII, UTF-8 character set supported by Adl storage file
 * system on file/folder operation.
 */
TestMetadata (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/live/TestMetadata.java)/**
 * This class is responsible for testing ContentSummary, ListStatus on
 * file/folder.
 */
TestADLResponseData (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/TestADLResponseData.java)/**
 * Mock up response data returned from Adl storage account.
 */
TestAzureADTokenProvider (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/TestAzureADTokenProvider.java)/**
 * Test appropriate token provider is loaded as per configuration.
 */
TestRelativePathFormation (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/TestRelativePathFormation.java)/**
 * This class verifies path conversion to SDK.
 */
TestValidateConfiguration (/hadoop-tools/hadoop-azure-datalake/src/test/java/org/apache/hadoop/fs/adl/TestValidateConfiguration.java)/**
 * Validate configuration keys defined for adl storage file system instance.
 */
ArrayListBackedIterator (/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/ArrayListBackedIterator.java)/**
 * This class provides an implementation of ResetableIterator. The
 * implementation will be based on ArrayList.
 * 
 * 
 */
DataJoinJob (/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinJob.java)/**
 * This class implements the main function for creating a map/reduce
 * job to join data of different sources. To create sucn a job, the 
 * user must implement a mapper class that extends DataJoinMapperBase class,
 * and a reducer class that extends DataJoinReducerBase. 
 * 
 */
DataJoinMapperBase (/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinMapperBase.java)/**
 * This abstract class serves as the base class for the mapper class of a data
 * join job. This class expects its subclasses to implement methods for the
 * following functionalities:
 * 
 * 1. Compute the source tag of input values 2. Compute the map output value
 * object 3. Compute the map output key object
 * 
 * The source tag will be used by the reducer to determine from which source
 * (which table in SQL terminology) a value comes. Computing the map output
 * value object amounts to performing projecting/filtering work in a SQL
 * statement (through the select/where clauses). Computing the map output key
 * amounts to choosing the join key. This class provides the appropriate plugin
 * points for the user defined subclasses to implement the appropriate logic.
 * 
 */
DataJoinReducerBase (/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/DataJoinReducerBase.java)/**
 * This abstract class serves as the base class for the reducer class of a data
 * join job. The reduce function will first group the values according to their
 * input tags, and then compute the cross product of over the groups. For each
 * tuple in the cross product, it calls the following method, which is expected
 * to be implemented in a subclass.
 * 
 * protected abstract TaggedMapOutput combine(Object[] tags, Object[] values);
 * 
 * The above method is expected to produce one output value from an array of
 * records of different sources. The user code can also perform filtering here.
 * It can return null if it decides to the records do not meet certain
 * conditions.
 * 
 */
JobBase (/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/JobBase.java)/**
 * A common base implementing some statics collecting mechanisms that are
 * commonly used in a typical map/reduce job.
 * 
 */
ResetableIterator (/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/ResetableIterator.java)/**
 * This defines an iterator interface that will help the reducer class
 * re-group its input by source tags. Once the values are re-grouped,
 * the reducer will receive the cross product of values from different groups.
 */
TaggedMapOutput (/hadoop-tools/hadoop-datajoin/src/main/java/org/apache/hadoop/contrib/utils/join/TaggedMapOutput.java)/**
 * This abstract class serves as the base class for the values that 
 * flow from the mappers to the reducers in a data join job. 
 * Typically, in such a job, the mappers will compute the source
 * tag of an input record based on its attributes or based on the 
 * file name of the input file. This tag will be used by the reducers
 * to re-group the values of a given key according to their source tags.
 * 
 */
SampleDataJoinMapper (/hadoop-tools/hadoop-datajoin/src/test/java/org/apache/hadoop/contrib/utils/join/SampleDataJoinMapper.java)/**
 * This is a subclass of DataJoinMapperBase that is used to
 * demonstrate the functionality of INNER JOIN between 2 data
 * sources (TAB separated text files) based on the first column.
 */
SampleDataJoinReducer (/hadoop-tools/hadoop-datajoin/src/test/java/org/apache/hadoop/contrib/utils/join/SampleDataJoinReducer.java)/**
 * This is a subclass of DataJoinReducerBase that is used to
 * demonstrate the functionality of INNER JOIN between 2 data
 * sources (TAB separated text files) based on the first column.
 */
SampleTaggedMapOutput (/hadoop-tools/hadoop-datajoin/src/test/java/org/apache/hadoop/contrib/utils/join/SampleTaggedMapOutput.java)/**
 * This is a subclass of TaggedMapOutput that is used to
 * demonstrate the functionality of INNER JOIN between 2 data
 * sources (TAB separated text files) based on the first column.
 */
TestDataJoin (/hadoop-tools/hadoop-datajoin/src/test/java/org/apache/hadoop/contrib/utils/join/TestDataJoin.java)/**
 * Class to test JOIN between 2 data
 * sources.
 */
CopyFilter (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyFilter.java)/**
 * Interface for excluding files from DistCp.
 *
 */
CopyListing (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListing.java)/**
 * The CopyListing abstraction is responsible for how the list of
 * sources and targets is constructed, for DistCp's copy function.
 * The copy-listing should be a
 * SequenceFile&lt;Text, CopyListingFileStatus&gt;, located at the path
 * specified to buildListing(), each entry being a pair of (Source relative
 * path, source file status), all the paths being fully qualified.
 */
CopyListingFileStatus (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/CopyListingFileStatus.java)/**
 * CopyListingFileStatus is a view of {@link FileStatus}, recording additional
 * data members useful to distcp.
 *
 * This is the datastructure persisted in the sequence files generated
 * in the CopyCommitter when deleting files.
 * Any tool working with these generated files needs to be aware of an
 * important stability guarantee: there is none; expect it to change
 * across minor Hadoop releases without any support for reading the files of
 * different versions.
 * Tools parsing the listings must be built and tested against the point
 * release of Hadoop which they intend to support.
 */
DiffInfo (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DiffInfo.java)/**
 * Information presenting a rename/delete op derived from a snapshot diff entry.
 * This includes the source file/dir of the rename/delete op, and the target
 * file/dir of a rename op.
 */
DistCp (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java)/**
 * DistCp is the main driver-class for DistCpV2.
 * For command-line use, DistCp::main() orchestrates the parsing of command-line
 * parameters and the launch of the DistCp job.
 * For programmatic use, a DistCp object can be constructed by specifying
 * options (in a DistCpOptions object), and DistCp::execute() may be used to
 * launch the copy-job. DistCp may alternatively be sub-classed to fine-tune
 * behaviour.
 */
DistCpConstants (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpConstants.java)/**
 * Utility class to hold commonly used constants.
 */
DistCpContext (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpContext.java)/**
 * This is the context of the distcp at runtime.
 *
 * It has the immutable {@link DistCpOptions} and mutable runtime status.
 */
Builder (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java)/**
   * The builder of the {@link DistCpOptions}.
   *
   * This is designed to be the only public interface to create a
   * {@link DistCpOptions} object for users. It follows a simple Builder design
   * pattern.
   */
DistCpOptions (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpOptions.java)/**
 * The Options class encapsulates all DistCp options.
 *
 * When you add a new option, please:
 *  - Add the field along with javadoc in DistCpOptions and its Builder
 *  - Add setter method in the {@link Builder} class
 *
 * This class is immutable.
 */
DistCpSync (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpSync.java)/**
 * This class provides the basic functionality to sync two FileSystems based on
 * the snapshot diff report. More specifically, we have the following settings:
 * 1. Both the source and target FileSystem must be DistributedFileSystem
 * 2. Two snapshots (e.g., s1 and s2) have been created on the source FS.
 * The diff between these two snapshots will be copied to the target FS.
 * 3. The target has the same snapshot s1. No changes have been made on the
 * target since s1. All the files/directories in the target are the same with
 * source.s1
 */
FileBasedCopyListing (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/FileBasedCopyListing.java)/**
 * FileBasedCopyListing implements the CopyListing interface,
 * to create the copy-listing for DistCp,
 * by iterating over all source paths mentioned in a specified input-file.
 */
GlobbedCopyListing (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/GlobbedCopyListing.java)/**
 * GlobbedCopyListing implements the CopyListing interface, to create the copy
 * listing-file by "globbing" all specified source paths (wild-cards and all.)
 */
CopyCommitter (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java)/**
 * The CopyCommitter class is DistCp's OutputCommitter implementation. It is
 * responsible for handling the completion/cleanup of the DistCp run.
 * Specifically, it does the following:
 *  1. Cleanup of the meta-folder (where DistCp maintains its file-list, etc.)
 *  2. Preservation of user/group/replication-factor on any directories that
 *     have been copied. (Files are taken care of in their map-tasks.)
 *  3. Atomic-move of data from the temporary work-folder to the final path
 *     (if atomic-commit was opted for).
 *  4. Deletion of files from the target that are missing at source (if opted for).
 *  5. Cleanup of any partially copied files, from previous, failed attempts.
 */
CopyMapper (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java)/**
 * Mapper class that executes the DistCp copy operation.
 * Implements the o.a.h.mapreduce.Mapper interface.
 */
CopyOutputFormat (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyOutputFormat.java)/**
 * The CopyOutputFormat is the Hadoop OutputFormat used in DistCp.
 * It sets up the Job's Configuration (in the Job-Context) with the settings
 * for the work-directory, final commit-directory, etc. It also sets the right
 * output-committer.
 * @param <K>
 * @param <V>
 */
DeletedDirTracker (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/DeletedDirTracker.java)/**
 * Track deleted directories and support queries to
 * check for add them.
 *
 * Assumptions.
 * <ol>
 *   <liA sorted list of deletions are processed, where directories come
 *   before their children/descendants.</li>
 *   <li>Deep directory trees are being deleted.</li>
 *   <li>The total number of directories deleted is very much
 *   less than the number of files.</li>
 *   <li>Most deleted files are in directories which have
 *   been deleted.</li>
 *   <li>The cost of issuing a delete() call is less than that that
 *   of creating Path entries for parent directories and looking them
 *   up in a hash table.</li>
 *   <li>That a modest cache is sufficient to identify whether or not
 *   a parent directory has been deleted./li>
 *   <li>And that if a path has been evicted from a path, the cost of
 *   the extra deletions incurred is not significant.</li>
 * </ol>
 *
 * The directory structure this algorithm is intended to optimize for is
 * the deletion of datasets partitioned/bucketed into a directory tree,
 * and deleted in bulk.
 *
 * The ordering of deletions comes from the merge sort of the copy listings;
 * we rely on this placing a path "/dir1" ahead of "/dir1/file1",
 * "/dir1/dir2/file2", and other descendants.
 * We do not rely on parent entries being added immediately before children,
 * as sorting may place "/dir12" between "/dir1" and its descendants.
 *
 * Algorithm
 *
 * <ol>
 *   <li>
 *     Before deleting a directory or file, a check is made to see if an
 *     ancestor is in the cache of deleted directories.
 *   </li>
 *   <li>
 *     If an ancestor is found is: skip the delete.
 *   </li>
 *   <li>
 *     If an ancestor is not foundI: delete the file/dir.
 *   </li>
 *   <li>
 *     When the entry probed is a directory, it is always added to the cache of
 *     directories, irrespective of the search for an ancestor.
 *     This is to speed up scans of files directly underneath the path.
 *   </li>
 * </ol>
 *
 *
 */
DynamicInputChunk (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputChunk.java)/**
 * The DynamicInputChunk represents a single chunk of work, when used in
 * conjunction with the DynamicInputFormat and the DynamicRecordReader.
 * The records in the DynamicInputFormat's input-file are split across various
 * DynamicInputChunks. Each one is claimed and processed in an iteration of
 * a dynamic-mapper. When a DynamicInputChunk has been exhausted, the faster
 * mapper may claim another and process it, until there are no more to be
 * consumed.
 */
DynamicInputChunkContext (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputChunkContext.java)/**
 * Class to initialize the DynamicInputChunk invariants.
 */
DynamicInputFormat (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicInputFormat.java)/**
 * DynamicInputFormat implements the "Worker pattern" for DistCp.
 * Rather than to split up the copy-list into a set of static splits,
 * the DynamicInputFormat does the following:
 * 1. Splits the copy-list into small chunks on the DFS.
 * 2. Creates a set of empty "dynamic" splits, that each consume as many chunks
 *    as it can.
 * This arrangement ensures that a single slow mapper won't slow down the entire
 * job (since the slack will be picked up by other mappers, who consume more
 * chunks.)
 * By varying the split-ratio, one can vary chunk sizes to achieve different
 * performance characteristics. 
 */
DynamicRecordReader (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/lib/DynamicRecordReader.java)/**
 * The DynamicRecordReader is used in conjunction with the DynamicInputFormat
 * to implement the "Worker pattern" for DistCp.
 * The DynamicRecordReader is responsible for:
 * 1. Presenting the contents of each chunk to DistCp's mapper.
 * 2. Acquiring a new chunk when the current chunk has been completely consumed,
 *    transparently.
 */
RetriableDirectoryCreateCommand (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableDirectoryCreateCommand.java)/**
 * This class extends Retriable command to implement the creation of directories
 * with retries on failure.
 */
CopyReadException (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java)/**
   * Special subclass of IOException. This is used to distinguish read-operation
   * failures from other kinds of IOExceptions.
   * The failure to read from source is dealt with specially, in the CopyMapper.
   * Such failures may be skipped if the DistCpOptions indicate so.
   * Write failures are intolerable, and amount to CopyMapper failure.
   */
RetriableFileCopyCommand (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/RetriableFileCopyCommand.java)/**
 * This class extends RetriableCommand to implement the copy of files,
 * with retries on failure.
 */
UniformSizeInputFormat (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/UniformSizeInputFormat.java)/**
 * UniformSizeInputFormat extends the InputFormat class, to produce
 * input-splits for DistCp.
 * It looks at the copy-listing and groups the contents into input-splits such
 * that the total-number of bytes to be copied for each input split is
 * uniform.
 */
OptionsParser (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/OptionsParser.java)/**
 * The OptionsParser parses out the command-line options passed to DistCp,
 * and interprets those specific to DistCp, to create an Options object.
 */
RegexCopyFilter (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/RegexCopyFilter.java)/**
 * A CopyFilter which compares Java Regex Patterns to each Path to determine
 * whether a file should be copied.
 */
SimpleCopyListing (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java)/**
 * The SimpleCopyListing is responsible for making the exhaustive list of
 * all files/directories under its specified list of input-paths.
 * These are written into the specified copy-listing file.
 * Note: The SimpleCopyListing doesn't handle wild-cards in the input-paths.
 */
TrueCopyFilter (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/TrueCopyFilter.java)/**
 * A CopyFilter which always returns true.
 *
 */
DistCpUtils (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/DistCpUtils.java)/**
 * Utility functions used in DistCp.
 */
Worker (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/ProducerConsumer.java)/**
   * Worker thread implementation.
   *
   */
ProducerConsumer (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/ProducerConsumer.java)/**
 * ProducerConsumer class encapsulates input and output queues and a
 * thread-pool of Workers that loop on WorkRequest{@literal <T>} inputQueue
 * and for each consumed WorkRequest Workers invoke
 * WorkRequestProcessor.processItem() and output resulting
 * WorkReport{@literal <R>} to the outputQueue.
 */
RetriableCommand (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/RetriableCommand.java)/**
 * This class represents commands that be retried on failure, in a configurable
 * manner.
 */
ThrottledInputStream (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/ThrottledInputStream.java)/**
 * The ThrottleInputStream provides bandwidth throttling on a specified
 * InputStream. It is implemented as a wrapper on top of another InputStream
 * instance.
 * The throttling works by examining the number of bytes read from the underlying
 * InputStream from the beginning, and sleep()ing for a time interval if
 * the byte-transfer is found exceed the specified tolerable maximum.
 * (Thus, while the read-rate might exceed the maximum for a given short interval,
 * the average tends towards the specified maximum, overall.)
 */
WorkReport (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/WorkReport.java)/**
 *  WorkReport{@literal <T>} is a simple container for items of class T and its
 *  corresponding retry counter that indicates how many times this item
 *  was previously attempted to be processed.
 */
WorkRequest (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/WorkRequest.java)/**
 *  WorkRequest{@literal <T>} is a simple container for items of class T and its
 *  corresponding retry counter that indicates how many times this item
 *  was previously attempted to be processed.
 */
WorkRequestProcessor (/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/util/WorkRequestProcessor.java)/**
 *  Interface for ProducerConsumer worker loop.
 *
 */
AbstractContractDistCpTest (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/contract/AbstractContractDistCpTest.java)/**
 * Contract test suite covering a file system's integration with DistCp.  The
 * tests coordinate two file system instances: one "local", which is the local
 * file system, and the other "remote", which is the file system implementation
 * under test.  The tests in the suite cover both copying from local to remote
 * (e.g. a backup use case) and copying from remote to local (e.g. a restore use
 * case).
 */
TestLocalContractDistCp (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/contract/TestLocalContractDistCp.java)/**
 * Verifies that the local FS passes all the tests in
 * {@link AbstractContractDistCpTest}.
 * As such, it acts as an in-module validation of this contract test itself.
 */
TestCopyMapperCompositeCrc (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestCopyMapperCompositeCrc.java)/**
 * End-to-end tests for COMPOSITE_CRC combine mode.
 */
TestDeletedDirTracker (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/mapred/TestDeletedDirTracker.java)/**
 * Unit tests of the deleted directory tracker.
 */
TestCopyListingFileStatus (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestCopyListingFileStatus.java)/**
 * Verify CopyListingFileStatus serialization and requirements for distcp.
 */
TestDistCpOptions (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpOptions.java)/**
 * This is to test constructing {@link DistCpOptions} manually with setters.
 *
 * The test cases in this class is very similar to the parser test, see
 * {@link TestOptionsParser}.
 */
TestDistCpSyncReverseBase (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSyncReverseBase.java)/**
 * Base class to test "-rdiff s2 s1".
 * Shared by "-rdiff s2 s1 src tgt" and "-rdiff s2 s1 tgt tgt"
 */
TestDistCpSyncReverseFromSource (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSyncReverseFromSource.java)/**
 * Test the case "-rdiff s2 s1 src tgt".
 */
TestDistCpSyncReverseFromTarget (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSyncReverseFromTarget.java)/**
 * Test the case "-rdiff s2 s1 tgt tgt".
 */
StubFileSystem (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpWithAcls.java)/**
   * Stub FileSystem implementation used for testing the case of attempting
   * distcp with ACLs preserved on a file system that does not support ACLs.
   * The base class implementation throws UnsupportedOperationException for the
   * ACL methods, so we don't need to override them.
   */
TestDistCpWithAcls (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpWithAcls.java)/**
 * Tests distcp in combination with HDFS ACLs.
 */
TestDistCpWithRawXAttrs (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpWithRawXAttrs.java)/**
 * Tests distcp in combination with HDFS raw.* XAttrs.
 */
StubFileSystem (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpWithXAttrs.java)/**
   * Stub FileSystem implementation used for testing the case of attempting
   * distcp with XAttrs preserved on a file system that does not support XAttrs. 
   * The base class implementation throws UnsupportedOperationException for 
   * the XAttr methods, so we don't need to override them.
   */
TestDistCpWithXAttrs (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpWithXAttrs.java)/**
 * Tests distcp in combination with HDFS XAttrs.
 */
DistCpTestUtils (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/DistCpTestUtils.java)/**
 * Utility class for DistCpTests
 */
TestDistCpUtilsWithCombineMode (/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/util/TestDistCpUtilsWithCombineMode.java)/**
 * Test length and checksums comparison with checksum combine mode.
 * When the combine mode is COMPOSITE_CRC, it should tolerate different file
 * systems and different block sizes.
 */
NoSplitTextInputFormat (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-blockgen/src/main/java/org/apache/hadoop/tools/dynamometer/blockgenerator/GenerateBlockImagesDriver.java)/** A simple text input format that doesn't allow splitting of files. */
GenerateDNBlockInfosReducer (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-blockgen/src/main/java/org/apache/hadoop/tools/dynamometer/blockgenerator/GenerateDNBlockInfosReducer.java)/**
 * This Reducer class generates a simple text file for each DN, listing the
 * blocks to be generated.
 *
 * Input: {@link BlockInfo}'s from {@link XMLParserMapper}
 *
 * Output: A text file named as dni-XXX, where i is the ID of the DN and XXX is
 * a reducer ID. Each line in the file is in format:
 * blockID,blockGenStamp,blockSize
 */
XMLParserMapper (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-blockgen/src/main/java/org/apache/hadoop/tools/dynamometer/blockgenerator/XMLParserMapper.java)/**
 * This Mapper class generates a list of {@link BlockInfo}'s from a given
 * fsimage.
 *
 * Input: fsimage in XML format. It should be generated using
 * {@code org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer}.
 *
 * Output: list of all {@link BlockInfo}'s
 */
TestBlockGen (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-blockgen/src/test/java/org/apache/hadoop/tools/dynamometer/blockgenerator/TestBlockGen.java)/** Tests for block generation via {@link GenerateBlockImagesDriver}. */
TestXMLParser (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-blockgen/src/test/java/org/apache/hadoop/tools/dynamometer/blockgenerator/TestXMLParser.java)/** Tests for {@link XMLParser}. */
AllowAllImpersonationProvider (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/AllowAllImpersonationProvider.java)/**
 * An {@link ImpersonationProvider} that indiscriminately allows all users to
 * proxy as any other user.
 */
AMOptions (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/AMOptions.java)/**
 * Options supplied to the Client which are then passed through to the
 * ApplicationMaster.
 */
LaunchContainerRunnable (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/ApplicationMaster.java)/**
   * Thread to connect to the {@link ContainerManagementProtocol} and launch the
   * container that will execute the shell command.
   */
ApplicationMaster (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/ApplicationMaster.java)/**
 * The ApplicationMaster for Dynamometer. This will launch DataNodes in YARN
 * containers. If the RPC address of a NameNode is specified, it will configure
 * the DataNodes to talk to that NameNode. Else, a NameNode will be launched as
 * part of this YARN application. This does not implement any retry/failure
 * handling.
 * TODO: Add proper retry/failure handling
 * <p>
 * The AM will persist until it has run for a period of time equal to the
 * timeout specified or until the application is killed.
 * <p>
 * If the NameNode is launched internally, it will upload some information
 * onto the remote HDFS instance (i.e., the default FileSystem) about its
 * hostname and ports. This is in the location determined by the
 * {@link DynoConstants#DYNAMOMETER_STORAGE_DIR} and
 * {@link DynoConstants#NN_INFO_FILE_NAME} constants and is in the
 * {@link Properties} file format. This is consumed by this AM as well as the
 * {@link Client} to determine how to contact the NameNode.
 * <p>
 * Information about the location of the DataNodes is logged by the AM.
 */
BlockPlacementPolicyAlwaysSatisfied (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/BlockPlacementPolicyAlwaysSatisfied.java)/**
 * A BlockPlacementPolicy which always considered itself satisfied. This avoids
 * the issue that the Dynamometer NameNode will complain about blocks being
 * under-replicated because they're not being put on distinct racks.
 */
Client (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/Client.java)/**
 * Client for submitting a Dynamometer YARN application, and optionally, a
 * workload MapReduce job. This client uploads resources to HDFS as necessary
 * for them to be accessed by the YARN app, then launches an
 * {@link ApplicationMaster}, which is responsible for managing the lifetime of
 * the application.
 * <p>
 * The Dynamometer YARN application starts up the DataNodes of an HDFS
 * cluster. If the namenode_servicerpc_addr option is specified, it should point
 * to the service RPC address of an existing namenode, which the datanodes will
 * talk to. Else, a namenode will be launched internal to this YARN application.
 * The ApplicationMaster's logs contain links to the NN / DN containers to be
 * able to access their logs. Some of this information is also printed by the
 * client.
 * <p>
 * The application will store files in the submitting user's home directory
 * under a `.dynamometer/applicationID/` folder. This is mostly for uses
 * internal to the application, but if the NameNode is launched through YARN,
 * the NameNode's metrics will also be uploaded to a file `namenode_metrics`
 * within this folder. This file is also accessible as part of the NameNode's
 * logs, but this centralized location is easier to access for subsequent
 * parsing.
 * <p>
 * If the NameNode is launched internally, this Client will monitor the
 * status of the NameNode, printing information about its availability as the
 * DataNodes register (e.g., outstanding under replicated blocks as block
 * reports arrive). If this is configured to launch the workload job, once the
 * NameNode has gathered information from all of its DataNodes, the client will
 * launch a workload job which is configured to act against the newly launched
 * NameNode. Once the workload job completes, the infrastructure application
 * will be shut down. At this time only the audit log replay
 * ({@link AuditReplayMapper}) workload is supported.
 * <p>
 * If there is no workload job configured, this application will, by
 * default, persist indefinitely until killed by YARN. You can specify the
 * timeout option to have it exit automatically after some time. This timeout
 * will enforced if there is a workload job configured as well.
 */
DynoConstants (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/DynoConstants.java)/**
 * Constants used in both Client and Application Master.
 */
DynoInfraUtils (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/DynoInfraUtils.java)/**
 * A collection of utilities used by the Dynamometer infrastructure application.
 */
SimulatedDataNodes (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/SimulatedDataNodes.java)/**
 * Starts up a number of DataNodes within the same JVM. These DataNodes all use
 * {@link org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset}, so they do
 * not store any actual data, and do not persist anything to disk; they maintain
 * all metadata in memory. This is useful for testing and simulation purposes.
 * <p>
 * The DataNodes will attempt to connect to a NameNode defined by the default
 * FileSystem. There will be one DataNode started for each block list file
 * passed as an argument. Each of these files should contain a list of blocks
 * that the corresponding DataNode should contain, as specified by a triplet of
 * block ID, block size, and generation stamp. Each line of the file is one
 * block, in the format:
 * <p>
 * {@code blockID,blockGenStamp,blockSize}
 * <p>
 * This class is loosely based off of
 * {@link org.apache.hadoop.hdfs.DataNodeCluster}.
 */
TestDynamometerInfra (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/test/java/org/apache/hadoop/tools/dynamometer/TestDynamometerInfra.java)/**
 * Start a Dynamometer cluster in a MiniYARNCluster. Ensure that the NameNode is
 * able to start correctly, exit safemode, and run some commands. Subsequently
 * the workload job is launched and it is verified that it completes
 * successfully and is able to replay commands as expected.
 *
 * To run this test JAVA_HOME must be set correctly, and the {@code tar} utility
 * must be available.
 *
 * You can optionally specify which version of HDFS should be started within the
 * Dynamometer cluster; the default is {@value HADOOP_BIN_VERSION_DEFAULT}. This
 * can be adjusted by setting the {@value HADOOP_BIN_VERSION_KEY} property. This
 * will automatically download the correct Hadoop tarball for the specified
 * version. It downloads from an Apache mirror (by default
 * {@value DynoInfraUtils#APACHE_DOWNLOAD_MIRROR_DEFAULT}); which mirror is used
 * can be controlled with the {@value DynoInfraUtils#APACHE_DOWNLOAD_MIRROR_KEY}
 * property. Note that mirrors normally contain only the latest releases on any
 * given release line; you may need to use
 * {@code http://archive.apache.org/dist/} for older releases. The downloaded
 * tarball will be stored in the test directory and can be reused between test
 * executions. Alternatively, you can specify the {@value HADOOP_BIN_PATH_KEY}
 * property to point directly to a Hadoop tarball which is present locally and
 * no download will occur.
 */
TestDynoInfraUtils (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/test/java/org/apache/hadoop/tools/dynamometer/TestDynoInfraUtils.java)/** Tests for {@link DynoInfraUtils}. */
AuditCommandParser (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/audit/AuditCommandParser.java)/**
 * This interface represents a pluggable command parser. It will accept in one
 * line of {@link Text} input at a time and return an {@link AuditReplayCommand}
 * which represents the input text. Each input line should produce exactly one
 * command.
 */
AuditLogDirectParser (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/audit/AuditLogDirectParser.java)/**
 * This {@link AuditCommandParser} is used to read commands from an audit log in
 * the original format audit logs are produced in with a standard configuration.
 * It requires setting the {@value AUDIT_START_TIMESTAMP_KEY} configuration to
 * specify what the start time of the audit log was to determine when events
 * occurred relative to this start time.
 * <p>
 * By default, this assumes that the audit log is in the default log format
 * set up by Hadoop, like:
 * <pre>{@code
 *   1970-01-01 00:00:00,000 INFO FSNamesystem.audit: allowed=true ...
 * }</pre>
 * You can adjust this parsing behavior using the various configurations
 * available.
 */
AuditLogHiveTableParser (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/audit/AuditLogHiveTableParser.java)/**
 * This {@link AuditCommandParser} is used to read commands assuming that the
 * input was generated by a Hive query storing uncompressed output files, in
 * which fields should be separated by the start-of-heading (U+0001) character.
 * The fields available should be, in order:
 *
 * <pre>
 *   relativeTimestampMs,ugi,command,src,dest,sourceIP
 * </pre>
 *
 * Where relativeTimestampMs represents the time elapsed between the start of
 * the audit log and the occurrence of the audit event. Assuming your audit logs
 * are available in Hive, this can be generated with a query looking like:
 *
 * <pre>
 *   INSERT OVERWRITE DIRECTORY '${outputPath}'
 *   SELECT (timestamp - ${startTime} AS relTime, ugi, cmd, src, dst, ip
 *   FROM '${auditLogTableLocation}'
 *   WHERE
 *     timestamp {@literal >=} ${startTime}
 *     AND timestamp {@literal <} ${endTime}
 *   DISTRIBUTE BY src
 *   SORT BY relTime ASC;
 * </pre>
 *
 * Note that the sorting step is important; events in each distinct file must be
 * in time-ascending order.
 */
PoisonPillCommand (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/audit/AuditReplayCommand.java)/**
   * A command representing a Poison Pill, indicating that the processing thread
   * should not process any further items and instead should terminate itself.
   * Always returns true for {@link #isPoison()}. It does not contain any other
   * information besides a timestamp; other getter methods wil return null.
   */
AuditReplayCommand (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/audit/AuditReplayCommand.java)/**
 * This class represents a single command to be replayed by the workload
 * generator. It implements the {@link Delayed} interface so that they can be
 * fetched in timestamp order from a {@link java.util.concurrent.DelayQueue}.
 * You can use the {@link #getPoisonPill(long)} method to retrieve "Poison Pill"
 * {@link AuditReplayCommand} which has {@link #isPoison()} as true,
 * representing to a consumer(s) of the {@link java.util.concurrent.DelayQueue}
 * that it should stop processing further items and instead terminate itself.
 */
AuditReplayMapper (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/audit/AuditReplayMapper.java)/**
 * AuditReplayMapper replays the given audit trace against the NameNode under
 * test. Each mapper spawns a number of threads equal to the
 * {@value NUM_THREADS_KEY} configuration (by default
 * {@value NUM_THREADS_DEFAULT}) to use for replaying. Each mapper reads a
 * single input file which will be consumed by all of the available threads. A
 * {@link FileInputFormat} with splitting disabled is used so any files present
 * in the input path directory (given by the {@value INPUT_PATH_KEY}
 * configuration) will be used as input; one file per mapper. The expected
 * format of these files is determined by the value of the
 * {@value COMMAND_PARSER_KEY} configuration, which defaults to
 * {@link AuditLogDirectParser}.
 * <p>
 * This generates a number of {@link org.apache.hadoop.mapreduce.Counter}
 * values which can be used to get information into the replay, including the
 * number of commands replayed, how many of them were "invalid" (threw an
 * exception), how many were "late" (replayed later than they should have been),
 * and the latency (from client perspective) of each command. If there are a
 * large number of "late" commands, you likely need to increase the number of
 * threads used and/or the number of mappers.
 * <p>
 * By default, commands will be replayed at the same rate as they were
 * originally performed. However a rate factor can be specified via the
 * {@value RATE_FACTOR_KEY} configuration; all of the (relative) timestamps will
 * be divided by this rate factor, effectively changing the rate at which they
 * are replayed. For example, a rate factor of 2 would make the replay occur
 * twice as fast, and a rate factor of 0.5 would make it occur half as fast.
 */
AuditReplayThread (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/audit/AuditReplayThread.java)/**
 * This class replays each audit log entry at a specified timestamp in the
 * future. Each of these threads maintains a {@link DelayQueue} into which items
 * are inserted by the {@link AuditReplayMapper}. Once an item is ready, this
 * thread will fetch the command from the queue and attempt to replay it.
 */
NoSplitTextInputFormat (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/audit/NoSplitTextInputFormat.java)/**
 * A simple {@link TextInputFormat} that disables splitting of files. This is
 * the {@link org.apache.hadoop.mapreduce.InputFormat} used by
 * {@link AuditReplayMapper}.
 */
CreateFileMapper (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/CreateFileMapper.java)/**
 * CreateFileMapper continuously creates 1 byte files for the specified duration
 * to increase the number of file objects on the NN.
 * <p>
 * Configuration options available:
 * <ul>
 *   <li>{@value NUM_MAPPERS_KEY} (required): Number of mappers to launch.</li>
 *   <li>{@value DURATION_MIN_KEY} (required): Number of minutes to induce
 *       workload for.</li>
 *   <li>{@value SHOULD_DELETE_KEY} (default: {@value SHOULD_DELETE_DEFAULT}):
 *       If true, delete the files after creating them. This can be useful for
 *       generating constant load without increasing the number of file
 *       objects.</li>
 *   <li>{@value FILE_PARENT_PATH_KEY} (default:
 *       {@value FILE_PARENT_PATH_DEFAULT}): The root directory in which to
 *       create files.</li>
 * </ul>
 */
VirtualInputFormat (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/VirtualInputFormat.java)/**
 * An input format which does not read any input, but rather starts a
 * configurable number of mappers and runs them for a configurable duration.
 */
VirtualInputSplit (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/VirtualInputSplit.java)/**
 * A fake input split.
 */
VirtualRecordReader (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/VirtualRecordReader.java)/**
 * A simple fake record reader which simply runs for some time duration.
 */
WorkloadDriver (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/WorkloadDriver.java)/**
 * This is the driver for generating generic workloads against a NameNode under
 * test. It launches a map-only job with a mapper class specified by the
 * {@value MAPPER_CLASS_NAME} argument. See the specific mappers (currently
 * {@link AuditReplayMapper} and {@link CreateFileMapper}) for information on
 * their specific behavior and parameters.
 */
WorkloadMapper (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/main/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/WorkloadMapper.java)/**
 * Represents the base class for a generic workload-generating mapper. By
 * default, it will expect to use {@link VirtualInputFormat} as its
 * {@link InputFormat}. Subclasses expecting a different {@link InputFormat}
 * should override the {@link #getInputFormat(Configuration)} method.
 */
TestAuditLogDirectParser (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/test/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/audit/TestAuditLogDirectParser.java)/** Tests for {@link AuditLogDirectParser}. */
AllowUserImpersonationProvider (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/test/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/TestWorkloadGenerator.java)/**
   * {@link ImpersonationProvider} that confirms the user doing the
   * impersonating is the same as the user running the MiniCluster.
   */
TestWorkloadGenerator (/hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-workload/src/test/java/org/apache/hadoop/tools/dynamometer/workloadgenerator/TestWorkloadGenerator.java)/** Tests for {@link WorkloadDriver} and related classes. */
GetGroups (/hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/mapred/tools/GetGroups.java)/**
 * MR implementation of a tool for getting the groups which a given user
 * belongs to.
 */
FileOperation (/hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/DistCh.java)/** File operations. */
ChangeInputFormat (/hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/DistCh.java)/** Responsible for generating splits of the src file list. */
ChangeFilesMapper (/hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/DistCh.java)/** The mapper for changing files. */
DistCh (/hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/DistCh.java)/**
 * A Map-reduce program to recursively change files properties
 * such as owner, group and permission.
 */
DuplicationException (/hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/DistTool.java)/** An exception class for duplicated source files. */
DistTool (/hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/DistTool.java)/**
 * An abstract class for distributed tool for file related operations.
 */
TestGetGroups (/hadoop-tools/hadoop-extras/src/test/java/org/apache/hadoop/mapred/tools/TestGetGroups.java)/**
 * Tests for the MR implementation of {@link GetGroups}
 */
BlockResolver (/hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/BlockResolver.java)/**
 * Given an external reference, create a sequence of blocks and associated
 * metadata.
 */
FileSystemImage (/hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileSystemImage.java)/**
 * Create FSImage from an external namespace.
 */
FixedBlockMultiReplicaResolver (/hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/FixedBlockMultiReplicaResolver.java)/**
 * Resolver mapping all files to a configurable, uniform blocksize
 * and replication.
 */
FixedBlockResolver (/hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/FixedBlockResolver.java)/**
 * Resolver mapping all files to a configurable, uniform blocksize.
 */
FSTreeWalk (/hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSTreeWalk.java)/**
 * Traversal of an external FileSystem.
 */
FsUGIResolver (/hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/FsUGIResolver.java)/**
 * Dynamically assign ids to users/groups as they appear in the external
 * filesystem.
 */
Options (/hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageWriter.java)/**
   * Configurable options for image generation mapping pluggable components.
   */
NullBlockAliasMap (/hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/NullBlockAliasMap.java)/**
 * Null sink for region information emitted from FSImage.
 */
SingleUGIResolver (/hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/SingleUGIResolver.java)/**
 * Map all owners/groups in external system to a single user in FSImage.
 */
TreePath (/hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/TreePath.java)/**
 * Traversal cursor in external filesystem.
 * TODO: generalize, move FS/FileRegion to FSTreePath
 */
TreeIterator (/hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/TreeWalk.java)/**
   * Enumerator class for hierarchies. Implementations SHOULD support a fork()
   * operation yielding a subtree of the current cursor.
   */
TreeWalk (/hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/TreeWalk.java)/**
 * Traversal yielding a hierarchical sequence of paths.
 */
UGIResolver (/hadoop-tools/hadoop-fs2img/src/main/java/org/apache/hadoop/hdfs/server/namenode/UGIResolver.java)/**
 * Pluggable class for mapping ownership and permissions from an external
 * store to an FSImage.
 */
MiniDFSClusterBuilderAliasMap (/hadoop-tools/hadoop-fs2img/src/test/java/org/apache/hadoop/hdfs/server/namenode/ITestProvidedImplementation.java)/**
   * Extends the {@link MiniDFSCluster.Builder} to create instances of
   * {@link MiniDFSClusterBuilderAliasMap}.
   */
MiniDFSClusterAliasMap (/hadoop-tools/hadoop-fs2img/src/test/java/org/apache/hadoop/hdfs/server/namenode/ITestProvidedImplementation.java)/**
   * Extends {@link MiniDFSCluster} to correctly configure the InMemoryAliasMap.
   */
ITestProvidedImplementation (/hadoop-tools/hadoop-fs2img/src/test/java/org/apache/hadoop/hdfs/server/namenode/ITestProvidedImplementation.java)/**
 * Integration tests for the Provided implementation.
 */
RandomTreeWalk (/hadoop-tools/hadoop-fs2img/src/test/java/org/apache/hadoop/hdfs/server/namenode/RandomTreeWalk.java)/**
 * Random, repeatable hierarchy generator.
 */
TestFixedBlockResolver (/hadoop-tools/hadoop-fs2img/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFixedBlockResolver.java)/**
 * Validate fixed-size block partitioning.
 */
TestFSTreeWalk (/hadoop-tools/hadoop-fs2img/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSTreeWalk.java)/**
 * Validate FSTreeWalk specific behavior.
 */
TestRandomTreeWalk (/hadoop-tools/hadoop-fs2img/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestRandomTreeWalk.java)/**
 * Validate randomly generated hierarchies, including fork() support in
 * base class.
 */
TestSingleUGIResolver (/hadoop-tools/hadoop-fs2img/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSingleUGIResolver.java)/**
 * Validate resolver assigning all paths to a single owner/group.
 */
AvgRecordFactory (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java)/**
 * Given byte and record targets, emit roughly equal-sized records satisfying
 * the contract.
 */
ClusterSummarizer (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java)/**
 * Summarizes the Hadoop cluster used in this {@link Gridmix} run. 
 * Statistics that are reported are
 * <ul>
 *   <li>Total number of active trackers in the cluster</li>
 *   <li>Total number of blacklisted trackers in the cluster</li>
 *   <li>Max map task capacity of the cluster</li>
 *   <li>Max reduce task capacity of the cluster</li>
 * </ul>
 * 
 * Apart from these statistics, {@link JobTracker} and {@link FileSystem} 
 * addresses are also recorded in the summary.
 */
RandomTextDataMapper (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java)/**
   * This is a {@link Mapper} implementation for generating random text data.
   * It uses {@link RandomTextDataGenerator} for generating text data and the
   * output files are compressed.
   */
CompressionRatioLookupTable (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java)/**
   * This is the lookup table for mapping compression ratio to the size of the 
   * word in the {@link RandomTextDataGenerator}'s dictionary. 
   * 
   * Note that this table is computed (empirically) using a dictionary of 
   * default length i.e {@value RandomTextDataGenerator#DEFAULT_LIST_SIZE}.
   */
CompressionEmulationUtil (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java)/**
 * This is a utility class for all the compression related modules.
 */
DistributedCacheEmulator (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java)/**
 * Emulation of Distributed Cache Usage in gridmix.
 * <br> Emulation of Distributed Cache Load in gridmix will put load on
 * TaskTrackers and affects execution time of tasks because of localization of
 * distributed cache files by TaskTrackers.
 * <br> Gridmix creates distributed cache files for simulated jobs by launching
 * a MapReduce job {@link GenerateDistCacheData} in advance i.e. before
 * launching simulated jobs.
 * <br> The distributed cache file paths used in the original cluster are mapped
 * to unique file names in the simulated cluster.
 * <br> All HDFS-based distributed cache files generated by gridmix are
 * public distributed cache files. But Gridmix makes sure that load incurred due
 * to localization of private distributed cache files on the original cluster
 * is also faithfully simulated. Gridmix emulates the load due to private
 * distributed cache files by mapping private distributed cache files of
 * different users in the original cluster to different public distributed cache
 * files in the simulated cluster.
 *
 * <br> The configuration properties like
 * {@link MRJobConfig#CACHE_FILES}, {@link MRJobConfig#CACHE_FILE_VISIBILITIES},
 * {@link MRJobConfig#CACHE_FILES_SIZES} and
 * {@link MRJobConfig#CACHE_FILE_TIMESTAMPS} obtained from trace are used to
 *  decide
 * <li> file size of each distributed cache file to be generated
 * <li> whether a distributed cache file is already seen in this trace file
 * <li> whether a distributed cache file was considered public or private.
 * <br>
 * <br> Gridmix configures these generated files as distributed cache files for
 * the simulated jobs.
 */
EchoUserResolver (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/EchoUserResolver.java)/**
 * Echos the UGI offered.
 */
CpuUsageEmulatorCore (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java)/**
   * This is the core CPU usage emulation algorithm. This is the core engine
   * which actually performs some CPU intensive operations to consume some
   * amount of CPU. Multiple calls of {@link #compute()} should help the 
   * plugin emulate the desired level of CPU usage. This core engine can be
   * calibrated using the {@link #calibrate(ResourceCalculatorPlugin, long)}
   * API to suit the underlying hardware better. It also can be used to optimize
   * the emulation cycle.
   */
DefaultCpuUsageEmulator (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java)/**
   * This is the core engine to emulate the CPU usage. The only responsibility 
   * of this class is to perform certain math intensive operations to make sure 
   * that some desired value of CPU is used.
   */
CumulativeCpuUsageEmulatorPlugin (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java)/**
 * <p>A {@link ResourceUsageEmulatorPlugin} that emulates the cumulative CPU 
 * usage by performing certain CPU intensive operations. Performing such CPU 
 * intensive operations essentially uses up some CPU. Every 
 * {@link ResourceUsageEmulatorPlugin} is configured with a feedback module i.e 
 * a {@link ResourceCalculatorPlugin}, to monitor the resource usage.</p>
 * 
 * <p>{@link CumulativeCpuUsageEmulatorPlugin} emulates the CPU usage in steps. 
 * The frequency of emulation can be configured via 
 * {@link #CPU_EMULATION_PROGRESS_INTERVAL}.
 * CPU usage values are matched via emulation only on the interval boundaries.
 * </p>
 *  
 * {@link CumulativeCpuUsageEmulatorPlugin} is a wrapper program for managing 
 * the CPU usage emulation feature. It internally uses an emulation algorithm 
 * (called as core and described using {@link CpuUsageEmulatorCore}) for 
 * performing the actual emulation. Multiple calls to this core engine should 
 * use up some amount of CPU.<br>
 * 
 * <p>{@link CumulativeCpuUsageEmulatorPlugin} provides a calibration feature 
 * via {@link #initialize(Configuration, ResourceUsageMetrics, 
 *                        ResourceCalculatorPlugin, Progressive)} to calibrate 
 *  the plugin and its core for the underlying hardware. As a result of 
 *  calibration, every call to the emulation engine's core should roughly use up
 *  1% of the total usage value to be emulated. This makes sure that the 
 *  underlying hardware is profiled before use and that the plugin doesn't 
 *  accidently overuse the CPU. With 1% as the unit emulation target value for 
 *  the core engine, there will be roughly 100 calls to the engine resulting in 
 *  roughly 100 calls to the feedback (resource usage monitor) module. 
 *  Excessive usage of the feedback module is discouraged as 
 *  it might result into excess CPU usage resulting into no real CPU emulation.
 *  </p>
 */
ResourceUsageEmulatorPlugin (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageEmulatorPlugin.java)/**
 * <p>Each resource to be emulated should have a corresponding implementation 
 * class that implements {@link ResourceUsageEmulatorPlugin}.</p>
 * <br><br>
 * {@link ResourceUsageEmulatorPlugin} will be configured using the 
 * {@link #initialize(Configuration, ResourceUsageMetrics, 
 *                    ResourceCalculatorPlugin, Progressive)} call.
 * Every 
 * {@link ResourceUsageEmulatorPlugin} is also configured with a feedback module
 * i.e a {@link ResourceCalculatorPlugin}, to monitor the current resource 
 * usage. {@link ResourceUsageMetrics} decides the final resource usage value to
 * emulate. {@link Progressive} keeps track of the task's progress.
 * 
 * <br><br>
 * 
 * For configuring GridMix to load and and use a resource usage emulator, 
 * see {@link ResourceUsageMatcher}. 
 */
ResourceUsageMatcher (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageMatcher.java)/**
 * <p>This is the driver class for managing all the resource usage emulators.
 * {@link ResourceUsageMatcher} expects a comma separated list of 
 * {@link ResourceUsageEmulatorPlugin} implementations specified using 
 * {@link #RESOURCE_USAGE_EMULATION_PLUGINS} as the configuration parameter.</p>
 * 
 * <p>Note that the order in which the emulators are invoked is same as the 
 * order in which they are configured.
 */
HeapUsageEmulatorCore (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java)/**
   * Defines the core heap usage emulation algorithm. This engine is expected
   * to perform certain memory intensive operations to consume some
   * amount of heap. {@link #load(long)} should load the current heap and 
   * increase the heap usage by the specified value. This core engine can be 
   * initialized using the {@link #initialize(ResourceCalculatorPlugin, long)} 
   * API to suit the underlying hardware better.
   */
DefaultHeapUsageEmulator (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java)/**
   * This is the core engine to emulate the heap usage. The only responsibility 
   * of this class is to perform certain memory intensive operations to make 
   * sure that some desired value of heap is used.
   */
TotalHeapUsageEmulatorPlugin (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java)/**
 * <p>A {@link ResourceUsageEmulatorPlugin} that emulates the total heap 
 * usage by loading the JVM heap memory. Adding smaller chunks of data to the 
 * heap will essentially use up some heap space thus forcing the JVM to expand 
 * its heap and thus resulting into increase in the heap usage.</p>
 * 
 * <p>{@link TotalHeapUsageEmulatorPlugin} emulates the heap usage in steps. 
 * The frequency of emulation can be configured via 
 * {@link #HEAP_EMULATION_PROGRESS_INTERVAL}.
 * Heap usage values are matched via emulation only at specific interval 
 * boundaries.
 * </p>
 *  
 * {@link TotalHeapUsageEmulatorPlugin} is a wrapper program for managing 
 * the heap usage emulation feature. It internally uses an emulation algorithm 
 * (called as core and described using {@link HeapUsageEmulatorCore}) for 
 * performing the actual emulation. Multiple calls to this core engine should 
 * use up some amount of heap.
 */
ExecutionSummarizer (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java)/**
 * Summarizes a {@link Gridmix} run. Statistics that are reported are
 * <ul>
 *   <li>Total number of jobs in the input trace</li>
 *   <li>Trace signature</li>
 *   <li>Total number of jobs processed from the input trace</li>
 *   <li>Total number of jobs submitted</li>
 *   <li>Total number of successful and failed jobs</li>
 *   <li>Total number of map/reduce tasks launched</li>
 *   <li>Gridmix start & end time</li>
 *   <li>Total time for the Gridmix run (data-generation and simulation)</li>
 *   <li>Gridmix Configuration (i.e job-type, submission-type, resolver)</li>
 * </ul>
 */
LeafDesc (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FilePool.java)/**
   * Files in current directory of this Node.
   */
InnerDesc (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FilePool.java)/**
   * A subdirectory of the current Node.
   */
MinFileFilter (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FilePool.java)/**
   * Filter enforcing the minFile/maxTotal parameters of the scan.
   */
FilePool (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FilePool.java)/**
 * Class for caching a pool of input data to be used by synthetic jobs for
 * simulating read traffic.
 */
FileQueue (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FileQueue.java)/**
 * Given a {@link org.apache.hadoop.mapreduce.lib.input.CombineFileSplit},
 * circularly read through each input source.
 */
DataStatistics (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateData.java)/**
   * Represents the input data characteristics.
   */
GenDCDataFormat (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java)/**
   * InputFormat for GenerateDistCacheData.
   * Input to GenerateDistCacheData is the special file(in SequenceFile format)
   * that contains the list of distributed cache files to be generated along
   * with their file sizes.
   */
GenerateDistCacheData (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java)/**
 * GridmixJob that generates distributed cache files.
 * {@link GenerateDistCacheData} expects a list of distributed cache files to be
 * generated as input. This list is expected to be stored as a sequence file
 * and the filename is expected to be configured using
 * {@code gridmix.distcache.file.list}.
 * This input file contains the list of distributed cache files and their sizes.
 * For each record (i.e. file size and file path) in this input file,
 * a file with the specific file size at the specific path is created.
 */
Shutdown (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Gridmix.java)/**
   * Handles orderly shutdown by requesting that each component in the
   * pipeline abort its progress, waiting for each to exit and killing
   * any jobs still running on the cluster.
   */
Component (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Gridmix.java)/**
   * Components in the pipeline must support the following operations for
   * orderly startup and shutdown.
   */
Gridmix (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Gridmix.java)/**
 * Driver class for the Gridmix3 benchmark. Gridmix accepts a timestamped
 * stream (trace) of job/task descriptions. For each job in the trace, the
 * client will submit a corresponding, synthetic job to the target cluster at
 * the rate in the original trace. The intent is to provide a benchmark that
 * can be configured and extended to closely match the measured resource
 * profile of actual, production loads.
 */
GridmixJob (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java)/**
 * Synthetic job generated from a trace description.
 */
InputStriper (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/InputStriper.java)/**
 * Given a {@link #FilePool}, obtain a set of files capable of satisfying
 * a full set of splits, then iterate over each source to fill the request.
 */
IntermediateRecordFactory (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/IntermediateRecordFactory.java)/**
 * Factory passing reduce specification as its last record.
 */
JobFactory (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobFactory.java)/**
 * Component reading job traces generated by Rumen. Each job in the trace is
 * assigned a sequence number and given a submission time relative to the
 * job that preceded it. Jobs are enqueued in the JobSubmitter provided at
 * construction.
 * @see org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer
 */
MonitorThread (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java)/**
   * Monitoring thread pulling running jobs from the component and into
   * a queue to be polled for status.
   */
JobMonitor (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java)/**
 * Component accepting submitted, running {@link Statistics.JobStats} and 
 * responsible for monitoring jobs for success and failure. Once a job is 
 * submitted, it is polled for status until complete. If a job is complete, 
 * then the monitor thread returns immediately to the queue. If not, the monitor
 * will sleep for some duration.
 * 
 * {@link JobMonitor} can be configured to use multiple threads for polling
 * the job statuses. Use {@link Gridmix#GRIDMIX_JOBMONITOR_THREADS} to specify
 * the total number of monitoring threads. 
 * 
 * The duration for which a monitoring thread sleeps if the first job in the 
 * queue is running can also be configured. Use 
 * {@link Gridmix#GRIDMIX_JOBMONITOR_SLEEPTIME_MILLIS} to specify a custom 
 * value.
 */
SubmitTask (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java)/**
   * Runnable wrapping a job to be submitted to the cluster.
   */
JobSubmitter (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java)/**
 * Component accepting deserialized job traces, computing split data, and
 * submitting to the cluster on deadline. Each job added from an upstream
 * factory must be submitted to the cluster by the deadline recorded on it.
 * Once submitted, jobs must be added to a downstream component for
 * monitoring.
 */
LoadSortComparator (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java)/**
   * This is a load matching key comparator which will make sure that the
   * resource usage load is matched even when the framework is in control.
   */
BoostingProgress (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java)/**
     * This is a progress bar that can be boosted for weaker use-cases.
     */
ResourceUsageMatcherRunner (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java)/**
   * This is a progress based resource usage matcher.
   */
LoadJob (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java)/**
 * Synthetic job generated from a trace description.
 */
Progressive (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Progressive.java)/**
 * Used to track progress of tasks.
 */
RandomInputStream (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java)/**
   * Input Stream that generates specified number of random bytes.
   */
PseudoLocalFs (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java)/**
 * Pseudo local file system that generates random data for any file on the fly
 * instead of storing files on disk. So opening same file multiple times will
 * not give same file content. There are no directories in this file system
 * other than the root and all the files are under root i.e. "/". All file URIs
 * on pseudo local file system should be of the format <code>
 * pseudo:///&lt;name&gt;.&lt;fileSize&gt;</code> where name is a unique name
 * and &lt;fileSize&gt; is a number representing the size of the file in bytes.
 */
SparseIndexMapper (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java)/**
   * A sparse index mapping table - useful when we want to
   * non-destructively permute a small fraction of a large array.
   */
DenseIndexMapper (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java)/**
   * A dense index mapping table - useful when we want to
   * non-destructively permute a large fraction of an array.
   */
Selector (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java)/**
   * Iteratively pick random numbers from pool 0..n-1. Each number can only be
   * picked once.
   */
RandomAlgorithms (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java)/**
 * Random algorithms.
 */
RandomTextDataGenerator (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java)/**
 * A random text generator. The words are simply sequences of alphabets.
 */
ReadRecordFactory (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReadRecordFactory.java)/**
 * For every record consumed, read key + val bytes from the stream provided.
 */
RecordFactory (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RecordFactory.java)/**
 * Interface for producing records as inputs and outputs to tasks.
 */
JobStats (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Statistics.java)/**
   * Class to encapsulate the JobStats information.
   * Current we just need information about completedJob.
   * TODO: In future we need to extend this to send more information.
   */
Statistics (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Statistics.java)/**
 * Component collecting the stats required by other components
 * to make decisions.
 * Single thread collector tries to collect the stats (currently cluster stats)
 * and caches it internally.
 * Components interested in these stats need to register themselves and will get
 * notified either on every job completion event or some fixed time interval.
 */
StatListener (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StatListener.java)/**
 * Stat listener.
 * @param <T>
 */
SubmitterUserResolver (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SubmitterUserResolver.java)/**
 * Resolves all UGIs to the submitting user.
 */
Summarizer (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Summarizer.java)/**
 * Summarizes various aspects of a {@link Gridmix} run.
 */
UserResolver (/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/UserResolver.java)/**
 * Maps users in the trace to a set of valid target users on the test cluster.
 */
DebugJobFactory (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobFactory.java)/**
 * Component generating random job traces for testing on a single node.
 */
MockJob (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java)/**
   * Generate random task data for a synthetic job.
   */
DummyResourceCalculatorPlugin (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DummyResourceCalculatorPlugin.java)/**
 * Plugin class to test resource information reported by NM. Use configuration
 * items {@link #MAXVMEM_TESTING_PROPERTY} and {@link #MAXPMEM_TESTING_PROPERTY}
 * to tell NM the total vmem and the total pmem. Use configuration items
 * {@link #NUM_PROCESSORS}, {@link #CPU_FREQUENCY}, {@link #CUMULATIVE_CPU_TIME}
 * and {@link #CPU_USAGE} to tell TT the CPU information.
 */
GridmixTestUtils (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/GridmixTestUtils.java)/**
 * This is a test class.
 */
TestCompressionEmulationUtils (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java)/**
 * Test {@link CompressionEmulationUtil}
 */
TestDistCacheEmulation (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestDistCacheEmulation.java)/**
 * Validate emulation of distributed cache load in gridmix simulated jobs.
 * 
 */
FakeHeapUsageEmulatorCore (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java)/**
   * This is a dummy class that fakes heap usage.
   */
FakeHeapUsageEmulatorPlugin (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java)/**
   * This is a dummy class that fakes the heap usage emulator plugin.
   */
TestGridmixMemoryEmulation (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java)/**
 * Test Gridmix memory emulation.
 */
FakeJobStoryProducer (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java)/**
     * A fake {@link JobStoryProducer} for {@link FakeJobFactory}.
     */
FakeJobFactory (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java)/**
   * A fake {@link JobFactory}.
   */
TestGridmixSummary (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java)/**
 * Test {@link ExecutionSummarizer} and {@link ClusterSummarizer}.
 */
DummyGridmixJob (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java)/**
   * A dummy {@link GridmixJob} that opens up the simulated job for testing.
   */
TestHighRamJob (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java)/**
 * Test if Gridmix correctly configures the simulated job's configuration for
 * high ram job properties.
 */
TestPseudoLocalFs (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java)/**
 * Test the basic functionality of PseudoLocalFs
 */
TestRandomTextDataGenerator (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java)/**
 * Test {@link RandomTextDataGenerator}.
 */
TestResourceUsageEmulatorPlugin (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java)/**
   * A {@link ResourceUsageEmulatorPlugin} implementation for testing purpose.
   * It essentially creates a file named 'test' in the test directory.
   */
TestOthers (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java)/**
   * Test implementation of {@link ResourceUsageEmulatorPlugin} which creates
   * a file named 'others' in the test directory.
   */
TestCpu (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java)/**
   * Test implementation of {@link ResourceUsageEmulatorPlugin} which creates
   * a file named 'cpu' in the test directory.
   */
FakeResourceUsageMonitor (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java)/**
   * Fakes the cumulative usage using {@link FakeCpuUsageEmulatorCore}.
   */
FakeProgressive (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java)/**
   * A dummy {@link Progressive} implementation that allows users to set the
   * progress for testing. The {@link Progressive#getProgress()} call will 
   * return the last progress value set using 
   * {@link FakeProgressive#setProgress(float)}.
   */
DummyReporter (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java)/**
   * A dummy reporter for {@link LoadJob.ResourceUsageMatcherRunner}.
   */
FakeCpuUsageEmulatorCore (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java)/**
   * This is a dummy class that fakes CPU usage.
   */
TestResourceUsageEmulators (/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java)/**
 * Test Gridmix's resource emulator framework and supported plugins.
 */
KafkaSink (/hadoop-tools/hadoop-kafka/src/main/java/org/apache/hadoop/metrics2/sink/KafkaSink.java)/**
 * A metrics sink that writes to a Kafka broker. This requires you to configure
 * a broker_list and a topic in the metrics2 configuration file. The broker_list
 * must contain a comma-separated list of kafka broker host and ports. The topic
 * will contain only one topic.
 */
TestKafkaMetrics (/hadoop-tools/hadoop-kafka/src/test/java/org/apache/hadoop/metrics2/impl/TestKafkaMetrics.java)/**
 * This tests that the KafkaSink properly formats the Kafka message.
 */
ApiKeyAuthenticationRequest (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/ApiKeyAuthenticationRequest.java)/**
 * Class that represents authentication request to Openstack Keystone.
 * Contains basic authentication information.
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS
 */
ApiKeyCredentials (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/ApiKeyCredentials.java)/**
 * Describes credentials to log in Swift using Keystone authentication.
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
AuthenticationRequest (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/AuthenticationRequest.java)/**
 * Class that represents authentication request to Openstack Keystone.
 * Contains basic authentication information.
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
AuthenticationRequestWrapper (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/AuthenticationRequestWrapper.java)/**
 * This class is used for correct hierarchy mapping of
 * Keystone authentication model and java code.
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
AuthenticationResponse (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/AuthenticationResponse.java)/**
 * Response from KeyStone deserialized into AuthenticationResponse class.
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
AuthenticationWrapper (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/AuthenticationWrapper.java)/**
 * This class is used for correct hierarchy mapping of
 * Keystone authentication model and java code
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
AccessToken (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/entities/AccessToken.java)/**
 * Access token representation of Openstack Keystone authentication.
 * Class holds token id, tenant and expiration time.
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 *
 * Example:
 * <pre>
 * "token" : {
 *   "RAX-AUTH:authenticatedBy" : [ "APIKEY" ],
 *   "expires" : "2013-07-12T05:19:24.685-05:00",
 *   "id" : "8bbea4215113abdab9d4c8fb0d37",
 *   "tenant" : { "id" : "01011970",
 *   "name" : "77777"
 *   }
 *  }
 * </pre>
 */
Catalog (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/entities/Catalog.java)/**
 * Describes Openstack Swift REST endpoints.
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
Endpoint (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/entities/Endpoint.java)/**
 * Openstack Swift endpoint description.
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
Tenant (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/entities/Tenant.java)/**
 * Tenant is abstraction in Openstack which describes all account
 * information and user privileges in system.
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
User (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/entities/User.java)/**
 * Describes user entity in Keystone
 * In different Swift installations User is represented differently.
 * To avoid any JSON deserialization failures this entity is ignored.
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
KeystoneApiKeyCredentials (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/KeystoneApiKeyCredentials.java)/**
 * Class for Keystone authentication.
 * Used when {@link ApiKeyCredentials} is not applicable
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
KeyStoneAuthRequest (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/KeyStoneAuthRequest.java)/**
 * Class that represents authentication to OpenStack Keystone.
 * Contains basic authentication information.
 * Used when {@link ApiKeyAuthenticationRequest} is not applicable.
 * (problem with different Keystone installations/versions/modifications)
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
PasswordAuthenticationRequest (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/PasswordAuthenticationRequest.java)/**
 * Class that represents authentication request to Openstack Keystone.
 * Contains basic authentication information.
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
PasswordCredentials (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/PasswordCredentials.java)/**
 * Describes credentials to log in Swift using Keystone authentication.
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
Roles (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/auth/Roles.java)/**
 * Describes user roles in Openstack system.
 * THIS FILE IS MAPPED BY JACKSON TO AND FROM JSON.
 * DO NOT RENAME OR MODIFY FIELDS AND THEIR ACCESSORS.
 */
SwiftAuthenticationFailedException (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/exceptions/SwiftAuthenticationFailedException.java)/**
 * An exception raised when an authentication request was rejected
 */
SwiftBadRequestException (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/exceptions/SwiftBadRequestException.java)/**
 * Thrown to indicate that data locality can't be calculated or requested path is incorrect.
 * Data locality can't be calculated if Openstack Swift version is old.
 */
SwiftConfigurationException (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/exceptions/SwiftConfigurationException.java)/**
 * Exception raised to indicate there is some problem with how the Swift FS
 * is configured
 */
SwiftConnectionClosedException (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/exceptions/SwiftConnectionClosedException.java)/**
 * Exception raised when an attempt is made to use a closed stream
 */
SwiftConnectionException (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/exceptions/SwiftConnectionException.java)/**
 * Thrown to indicate that connection is lost or failed to be made
 */
SwiftException (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/exceptions/SwiftException.java)/**
 * A Swift-specific exception -subclasses exist
 * for various specific problems.
 */
SwiftInternalStateException (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/exceptions/SwiftInternalStateException.java)/**
 * The internal state of the Swift client is wrong -presumably a sign
 * of some bug
 */
SwiftInvalidResponseException (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/exceptions/SwiftInvalidResponseException.java)/**
 * Exception raised when the HTTP code is invalid. The status code,
 * method name and operation URI are all in the response.
 */
SwiftJsonMarshallingException (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/exceptions/SwiftJsonMarshallingException.java)/**
 * Exception raised when the J/O mapping fails.
 */
SwiftOperationFailedException (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/exceptions/SwiftOperationFailedException.java)/**
 * Used to relay exceptions upstream from the inner implementation
 * to the public API, where it is downgraded to a log+failure.
 * Making it visible internally aids testing
 */
SwiftThrottledRequestException (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/exceptions/SwiftThrottledRequestException.java)/**
 * Exception raised if a Swift endpoint returned a HTTP response indicating
 * the caller is being throttled.
 */
SwiftUnsupportedFeatureException (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/exceptions/SwiftUnsupportedFeatureException.java)/**
 * Exception raised on an unsupported feature in the FS API -such as
 * <code>append()</code>
 */
CopyRequest (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/CopyRequest.java)/**
 * Implementation for SwiftRestClient to make copy requests.
 * COPY is a method that came with WebDAV (RFC2518), and is not something that
 * can be handled by all proxies en-route to a filesystem.
 */
ExceptionDiags (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/ExceptionDiags.java)/**
 * Variant of Hadoop NetUtils exception wrapping with URI awareness and
 * available in branch-1 too.
 */
HttpBodyContent (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/HttpBodyContent.java)/**
 * Response tuple from GET operations; combines the input stream with the content length
 */
HttpInputStreamWithRelease (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/HttpInputStreamWithRelease.java)/**
 * This replaces the input stream release class from JetS3t and AWS;
 * # Failures in the constructor are relayed up instead of simply logged.
 * # it is set up to be more robust at teardown
 * # release logic is thread safe
 * Note that the thread safety of the inner stream contains no thread
 * safety guarantees -this stream is not to be read across streams.
 * The thread safety logic here is to ensure that even if somebody ignores
 * that rule, the release code does not get entered twice -and that
 * any release in one thread is picked up by read operations in all others.
 */
RestClientBindings (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/RestClientBindings.java)/**
 * This class implements the binding logic between Hadoop configurations
 * and the swift rest client.
 * <p>
 * The swift rest client takes a Properties instance containing
 * the string values it uses to bind to a swift endpoint.
 * <p>
 * This class extracts the values for a specific filesystem endpoint
 * and then builds an appropriate Properties file.
 */
SwiftProtocolConstants (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/SwiftProtocolConstants.java)/**
 * Constants used in the Swift REST protocol,
 * and in the properties used to configure the {@link SwiftRestClient}.
 */
HttpRequestProcessor (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/SwiftRestClient.java)/**
   * Base class for all Swift REST operations.
   *
   * @param <M> request
   * @param <R> result
   */
AuthPostRequest (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/SwiftRestClient.java)/**
   * There's a special type for auth messages, so that low-level
   * message handlers can react to auth failures differently from everything
   * else.
   */
AuthRequestProcessor (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/SwiftRestClient.java)/**
   * Generate an auth message.
   * @param <R> response
   */
CopyRequestProcessor (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/SwiftRestClient.java)/**
   * Create operation.
   *
   * @param <R> result type
   */
DeleteRequestProcessor (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/SwiftRestClient.java)/**
   * Delete operation.
   *
   * @param <R>
   */
SwiftRestClient (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/SwiftRestClient.java)/**
 * This implements the client-side of the Swift REST API.
 *
 * The core actions put, get and query data in the Swift object store,
 * after authenticating the client.
 *
 * <b>Logging:</b>
 *
 * Logging at DEBUG level displays detail about the actions of this
 * client, including HTTP requests and responses -excluding authentication
 * details.
 */
StrictBufferedFSInputStream (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/snative/StrictBufferedFSInputStream.java)/**
 * Add stricter compliance with the evolving FS specifications
 */
SwiftFileStatus (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/snative/SwiftFileStatus.java)/**
 * A subclass of {@link FileStatus} that contains the
 * Swift-specific rules of when a file is considered to be a directory.
 */
SwiftNativeFileSystem (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/snative/SwiftNativeFileSystem.java)/**
 * Swift file system implementation. Extends Hadoop FileSystem
 */
SwiftNativeFileSystemStore (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/snative/SwiftNativeFileSystemStore.java)/**
 * File system store implementation.
 * Makes REST requests, parses data from responses
 */
SwiftNativeInputStream (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/snative/SwiftNativeInputStream.java)/**
 * The input stream from remote Swift blobs.
 * The class attempts to be buffer aware, and react to a forward seek operation
 * by trying to scan ahead through the current block of data to find it.
 * This accelerates some operations that do a lot of seek()/read() actions,
 * including work (such as in the MR engine) that do a seek() immediately after
 * an open().
 */
SwiftNativeOutputStream (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/snative/SwiftNativeOutputStream.java)/**
 * Output stream, buffers data on local disk.
 * Writes to Swift on the close() method, unless the
 * file is significantly large that it is being written as partitions.
 * In this case, the first partition is written on the first write that puts
 * data over the partition, as may later writes. The close() then causes
 * the final partition to be written, along with a partition manifest.
 */
DurationStats (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/util/DurationStats.java)/**
 * Build ongoing statistics from duration data
 */
DurationStatsTable (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/util/DurationStatsTable.java)/**
 * Build a duration stats table to which you can add statistics.
 * Designed to be multithreaded
 */
HttpResponseUtils (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/util/HttpResponseUtils.java)/**
 * Utility class for parsing HttpResponse. This class is implemented like
 * {@code org.apache.commons.httpclient.HttpMethodBase.java} in httpclient 3.x.
 */
SwiftObjectPath (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/util/SwiftObjectPath.java)/**
 * Swift hierarchy mapping of (container, path)
 */
SwiftTestUtils (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/util/SwiftTestUtils.java)/**
 * Utilities used across test cases
 */
SwiftUtils (/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/util/SwiftUtils.java)/**
 * Various utility classes for SwiftFS support
 */
AcceptAllFilter (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/AcceptAllFilter.java)/**
 * A path filter that accepts everything
 */
SwiftContract (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/contract/SwiftContract.java)/**
 * The contract of OpenStack Swift: only enabled if the test binding data is provided
 */
TestSwiftContractMkdir (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/contract/TestSwiftContractMkdir.java)/**
 * Test dir operations on S3
 */
TestSwiftContractRootDir (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/contract/TestSwiftContractRootDir.java)/**
 * root dir operations against an S3 bucket
 */
TestSwiftFileSystemDirectoriesHdfs2 (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/hdfs2/TestSwiftFileSystemDirectoriesHdfs2.java)/**
 * Add some HDFS-2 only assertions to {@link TestSwiftFileSystemDirectories}
 */
SwiftFileSystemBaseTest (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/SwiftFileSystemBaseTest.java)/**
 * This is the base class for most of the Swift tests
 */
SwiftTestConstants (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/SwiftTestConstants.java)/**
 * Hard coded constants for the test timeouts
 */
TestLogResources (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestLogResources.java)/**
 * This test just debugs which log resources are being picked up
 */
TestReadPastBuffer (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestReadPastBuffer.java)/**
 * Seek tests verify that
 * <ol>
 *   <li>When you seek on a 0 byte file to byte (0), it's not an error.</li>
 *   <li>When you seek past the end of a file, it's an error that should
 *   raise -what- EOFException?</li>
 *   <li>when you seek forwards, you get new data</li>
 *   <li>when you seek backwards, you get the previous data</li>
 *   <li>That this works for big multi-MB files as well as small ones.</li>
 * </ol>
 * These may seem "obvious", but the more the input streams try to be clever
 * about offsets and buffering, the more likely it is that seek() will start
 * to get confused.
 */
TestSeek (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestSeek.java)/**
 * Seek tests verify that
 * <ol>
 *   <li>When you seek on a 0 byte file to byte (0), it's not an error.</li>
 *   <li>When you seek past the end of a file, it's an error that should
 *   raise -what- EOFException?</li>
 *   <li>when you seek forwards, you get new data</li>
 *   <li>when you seek backwards, you get the previous data</li>
 *   <li>That this works for big multi-MB files as well as small ones.</li>
 * </ol>
 * These may seem "obvious", but the more the input streams try to be clever
 * about offsets and buffering, the more likely it is that seek() will start
 * to get confused.
 */
TestSwiftConfig (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestSwiftConfig.java)/**
 * Test the swift service-specific configuration binding features
 */
TestSwiftFileSystemBasicOps (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestSwiftFileSystemBasicOps.java)/**
 * Test basic filesystem operations.
 * Many of these are similar to those in {@link TestSwiftFileSystemContract}
 * -this is a JUnit4 test suite used to initially test the Swift
 * component. Once written, there's no reason not to retain these tests.
 */
TestSwiftFileSystemBlockLocation (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestSwiftFileSystemBlockLocation.java)/**
 * Test block location logic.
 * The endpoint may or may not be location-aware
 */
TestSwiftFileSystemBlocksize (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestSwiftFileSystemBlocksize.java)/**
 * Tests that blocksize is never zero for a file, either in the FS default
 * or the FileStatus value of a queried file 
 */
TestSwiftFileSystemConcurrency (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestSwiftFileSystemConcurrency.java)/**
 * Test Swift FS concurrency logic. This isn't a very accurate test,
 * because it is hard to consistently generate race conditions.
 * Consider it "best effort"
 */
TestSwiftFileSystemContract (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestSwiftFileSystemContract.java)/**
 * This is the full filesystem contract test -which requires the
 * Default config set up to point to a filesystem.
 *
 * Some of the tests override the base class tests -these
 * are where SwiftFS does not implement those features, or
 * when the behavior of SwiftFS does not match the normal
 * contract -which normally means that directories and equal files
 * are being treated as equal.
 */
TestSwiftFileSystemDelete (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestSwiftFileSystemDelete.java)/**
 * Test deletion operations
 */
TestSwiftFileSystemDirectories (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestSwiftFileSystemDirectories.java)/**
 * Test swift-specific directory logic.
 * This class is HDFS-1 compatible; its designed to be subclasses by something
 * with HDFS2 extensions
 */
TestSwiftFileSystemLsOperations (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestSwiftFileSystemLsOperations.java)/**
 * Test the FileSystem#listStatus() operations
 */
TestSwiftFileSystemPartitionedUploads (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestSwiftFileSystemPartitionedUploads.java)/**
 * Test partitioned uploads.
 * This is done by forcing a very small partition size and verifying that it
 * is picked up.
 */
TestSwiftFileSystemRead (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestSwiftFileSystemRead.java)/**
 * Test filesystem read operations
 */
TestSwiftObjectPath (/hadoop-tools/hadoop-openstack/src/test/java/org/apache/hadoop/fs/swift/TestSwiftObjectPath.java)/**
 * Unit tests for SwiftObjectPath class.
 */
RecurrenceId (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/common/api/RecurrenceId.java)/**
 * RecurrenceId is the id for the recurring pipeline jobs.
 * <p> We assume that the pipeline job can be uniquely identified with
 * {pipelineId, runId}.
 */
ResourceSkyline (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/common/api/ResourceSkyline.java)/**
 * ResourceSkyline records the job identification information as well as job's
 * requested {@code
 * container}s information during its lifespan.
 */
ResourceEstimatorConfiguration (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/common/config/ResourceEstimatorConfiguration.java)/**
 * Defines configuration keys for ResourceEstimatorServer.
 */
ResourceEstimatorUtil (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/common/config/ResourceEstimatorUtil.java)/**
 * General resourceestimator utils.
 */
ResourceEstimatorException (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/common/exception/ResourceEstimatorException.java)/**
 * Exception thrown by ResourceEstimatorServer utility classes.
 */
ResourceSerDe (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/common/serialization/ResourceSerDe.java)/**
 * Serialize/deserialize Resource object to/from JSON.
 */
RLESparseResourceAllocationSerDe (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/common/serialization/RLESparseResourceAllocationSerDe.java)/**
 * Serialize/deserialize RLESparseResourceAllocation object to/from JSON.
 */
ResourceEstimatorServer (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/service/ResourceEstimatorServer.java)/**
 * A simple embedded Hadoop HTTP server.
 */
ResourceEstimatorService (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/service/ResourceEstimatorService.java)/**
 * Resource Estimator Service which provides a set of REST APIs for users to
 * use the estimation service.
 */
ShutdownHook (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/service/ShutdownHook.java)/**
 * Simple shutdown hook for {@link ResourceEstimatorServer}.
 */
HistorySkylineStore (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/skylinestore/api/HistorySkylineStore.java)/**
 * HistorySkylineStore stores pipeline job's {@link ResourceSkyline}s in all
 * runs. {@code Estimator} will query the {@link ResourceSkyline}s for pipeline
 * jobs. {@code Parser} will parse various types of job logs, construct
 * {@link ResourceSkyline}s out of the logs and store them in the SkylineStore.
 */
PredictionSkylineStore (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/skylinestore/api/PredictionSkylineStore.java)/**
 * PredictionSkylineStore stores the predicted
 * {@code RLESparseResourceAllocation} of a job as computed by the
 * {@code Estimator} based on the {@code ResourceSkyline}s of past executions in
 * the {@code HistorySkylineStore}.
 */
SkylineStore (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/skylinestore/api/SkylineStore.java)/**
 * SkylineStore is composable interface for storing the history
 * {@code ResourceSkyline}s of past job runs and the predicted
 * {@code RLESparseResourceAllocation} for future execution.
 */
DuplicateRecurrenceIdException (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/skylinestore/exceptions/DuplicateRecurrenceIdException.java)/**
 * Exception thrown the {@code RecurrenceId} already exists in the
 * {@code SkylineStore}.
 */
EmptyResourceSkylineException (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/skylinestore/exceptions/EmptyResourceSkylineException.java)/**
 * Exception thrown if the @link{ResourceSkyline}s to be added to the
 * {@code SkylineStore} is empty.
 */
NullPipelineIdException (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/skylinestore/exceptions/NullPipelineIdException.java)/**
 * Exception thrown when pipelineId to be added is <em>null</em>.
 */
NullRecurrenceIdException (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/skylinestore/exceptions/NullRecurrenceIdException.java)/**
 * Exception thrown the {@code RecurrenceId} to be added is <em>null</em>.
 */
NullResourceSkylineException (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/skylinestore/exceptions/NullResourceSkylineException.java)/**
 * Exception thrown if the {@code ResourceSkyline} to be added is <em>null</em>.
 */
NullRLESparseResourceAllocationException (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/skylinestore/exceptions/NullRLESparseResourceAllocationException.java)/**
 * Exception thrown if the {@code ResourceSkyline} to be added is <em>null</em>.
 */
RecurrenceIdNotFoundException (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/skylinestore/exceptions/RecurrenceIdNotFoundException.java)/**
 * Exception thrown if {@code RecurrenceId} is not found in the
 * {@code SkylineStore}.
 */
SkylineStoreException (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/skylinestore/exceptions/SkylineStoreException.java)/**
 * Exception thrown the @link{SkylineStore} or the {@code Estimator} tries to
 * addHistory or query pipeline job's resource skylines.
 */
InMemoryStore (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/skylinestore/impl/InMemoryStore.java)/**
 * An in-memory implementation of {@link SkylineStore}.
 */
SkylineStoreValidator (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/skylinestore/validator/SkylineStoreValidator.java)/**
 * SkylineStoreValidator validates input parameters for {@link SkylineStore}.
 */
Solver (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/solver/api/Solver.java)/**
 * Solver takes recurring pipeline's {@link ResourceSkyline} history as input,
 * predicts its {@link Resource} requirement at each time t for the next run,
 * and translate them into {@link ResourceSkyline} which will be used to make
 * recurring resource reservations.
 */
InvalidInputException (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/solver/exceptions/InvalidInputException.java)/**
 * Exception thrown the {@code SkylineStore} or the {@code Estimator} tries to
 * addHistory or query pipeline job's resource skylines.
 */
InvalidSolverException (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/solver/exceptions/InvalidSolverException.java)/**
 * Exception thrown the @link{SkylineStore} or the {@code Estimator} tries to
 * addHistory or query pipeline job's resource skylines.
 */
SolverException (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/solver/exceptions/SolverException.java)/**
 * Exception thrown the @link{SkylineStore} or the {@code Estimator} tries to
 * addHistory or query pipeline job's resource skylines.
 */
BaseSolver (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/solver/impl/BaseSolver.java)/**
 * Common functions shared by {@code Solver} (translate predicted resource
 * allocation into Hadoop's {@link ReservationSubmissionRequest}.
 */
LpSolver (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/solver/impl/LpSolver.java)/**
 * A LP(Linear Programming) solution to predict recurring pipeline's
 * {@link Resource} requirements, and generate Hadoop {@code RDL} requests which
 * will be used to make recurring resource reservation.
 */
SolverPreprocessor (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/solver/preprocess/SolverPreprocessor.java)/**
 * Common preprocessing functions for {@link Solver}.
 */
JobMetaData (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/translator/api/JobMetaData.java)/**
 * Job metadata collected when parsing the log file.
 */
LogParser (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/translator/api/LogParser.java)/**
 * LogParser iterates over a stream of logs, uses {@link SingleLineParser} to
 * parse each line, and adds extracted {@code ResourceSkyline}s to the
 * {@code SkylineStore}.
 */
SingleLineParser (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/translator/api/SingleLineParser.java)/**
 * SingleLineParser parses one line in the log file, extracts the
 * {@link ResourceSkyline}s and stores them.
 */
DataFieldNotFoundException (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/translator/exceptions/DataFieldNotFoundException.java)/**
 * Exception thrown when job attributes are not found.
 */
BaseLogParser (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/translator/impl/BaseLogParser.java)/**
 * Base class to implement {@link LogParser}. It wraps a
 * {@link SingleLineParser} from the {@link Configuration} to parse a log
 * dir/file.
 */
LogParserUtil (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/translator/impl/LogParserUtil.java)/**
 * Common utility functions for {@link LogParser}.
 */
NativeSingleLineParser (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/translator/impl/NativeSingleLineParser.java)/**
 * This sample parser will parse the sample log and extract the resource
 * skyline.
 * <p> The expected log format is: NormalizedJobName NumInstances SubmitTime
 * StartTime EndTime JobInstanceName memUsage coreUsage
 */
RmSingleLineParser (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/translator/impl/RmSingleLineParser.java)/**
 * {@link SingleLineParser} for Hadoop Resource Manager logs.
 */
ParserValidator (/hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/translator/validator/ParserValidator.java)/**
 * Validates the input parameters for the {@link LogParser}.
 */
TestResourceSkyline (/hadoop-tools/hadoop-resourceestimator/src/test/java/org/apache/hadoop/resourceestimator/common/api/TestResourceSkyline.java)/**
 * Test {@link ResourceSkyline} class.
 */
TestHistorySkylineSerDe (/hadoop-tools/hadoop-resourceestimator/src/test/java/org/apache/hadoop/resourceestimator/common/serialization/TestHistorySkylineSerDe.java)/**
 * Test HistorySkylineSerDe.
 */
TestResourceSerDe (/hadoop-tools/hadoop-resourceestimator/src/test/java/org/apache/hadoop/resourceestimator/common/serialization/TestResourceSerDe.java)/**
 * Test ResourceSerDe.
 */
TestResourceSkylineSerDe (/hadoop-tools/hadoop-resourceestimator/src/test/java/org/apache/hadoop/resourceestimator/common/serialization/TestResourceSkylineSerDe.java)/**
 * Test ResourceSkylineSerDe.
 */
GuiceServletConfig (/hadoop-tools/hadoop-resourceestimator/src/test/java/org/apache/hadoop/resourceestimator/service/GuiceServletConfig.java)/**
 * GuiceServletConfig is a wrapper class to have a static Injector instance
 * instead of having the instance inside test classes. This allow us to use
 * Jersey test framework after 1.13.
 * Please check test cases to know how to use this class:
 * e.g. TestRMWithCSRFFilter.java
 */
TestResourceEstimatorService (/hadoop-tools/hadoop-resourceestimator/src/test/java/org/apache/hadoop/resourceestimator/service/TestResourceEstimatorService.java)/**
 * Test ResourceEstimatorService.
 */
TestInMemoryStore (/hadoop-tools/hadoop-resourceestimator/src/test/java/org/apache/hadoop/resourceestimator/skylinestore/impl/TestInMemoryStore.java)/**
 * Test {@link InMemoryStore} class.
 */
TestSkylineStore (/hadoop-tools/hadoop-resourceestimator/src/test/java/org/apache/hadoop/resourceestimator/skylinestore/impl/TestSkylineStore.java)/**
 * Test {@link SkylineStore} class.
 */
TestLpSolver (/hadoop-tools/hadoop-resourceestimator/src/test/java/org/apache/hadoop/resourceestimator/solver/impl/TestLpSolver.java)/**
 * This LPSolver class will make resource estimation using Linear Programming
 * model. We use ojAlgo solver to solve the model.
 */
TestSolver (/hadoop-tools/hadoop-resourceestimator/src/test/java/org/apache/hadoop/resourceestimator/solver/impl/TestSolver.java)/**
 * This LPSolver class will make resource estimation using Linear Programming
 * model. We use Google Or Tool to solve the model.
 */
TestJobMetaData (/hadoop-tools/hadoop-resourceestimator/src/test/java/org/apache/hadoop/resourceestimator/translator/api/TestJobMetaData.java)/**
 * Test JobMetaData.
 */
TestNativeParser (/hadoop-tools/hadoop-resourceestimator/src/test/java/org/apache/hadoop/resourceestimator/translator/impl/TestNativeParser.java)/**
 * This sample parser will parse the sample log and extract the resource
 * skyline.
 */
TestRmParser (/hadoop-tools/hadoop-resourceestimator/src/test/java/org/apache/hadoop/resourceestimator/translator/impl/TestRmParser.java)/**
 * This sample parser will parse the sample log and extract the resource
 * skyline.
 */
AbstractClusterStory (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/AbstractClusterStory.java)/**
 * {@link AbstractClusterStory} provides a partial implementation of
 * {@link ClusterStory} by parsing the topology tree.
 */
DataAnonymizer (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/anonymization/DataAnonymizer.java)/**
 * The data anonymizer interface.
 */
WordList (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/anonymization/WordList.java)/**
 * Represents the list of words used in list-backed anonymizers.
 */
WordListAnonymizerUtility (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/anonymization/WordListAnonymizerUtility.java)/**
 * Utility class to handle commonly performed tasks in a 
 * {@link org.apache.hadoop.tools.rumen.datatypes.DefaultAnonymizableDataType} 
 * using a {@link WordList} for anonymization.
 * //TODO There is no caching for saving memory.
 */
CDFRandomGenerator (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/CDFRandomGenerator.java)/**
 * An instance of this class generates random values that confirm to the
 * embedded {@link LoggedDiscreteCDF} . The discrete CDF is a pointwise
 * approximation of the "real" CDF. We therefore have a choice of interpolation
 * rules.
 * 
 * A concrete subclass of this abstract class will implement valueAt(double)
 * using a class-dependent interpolation rule.
 * 
 */
ClusterStory (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/ClusterStory.java)/**
 * {@link ClusterStory} represents all configurations of a MapReduce cluster,
 * including nodes, network topology, and slot configurations.
 */
ClusterTopologyReader (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/ClusterTopologyReader.java)/**
 * Reading JSON-encoded cluster topology and produce the parsed
 * {@link LoggedNetworkTopology} object.
 */
CurrentJHParser (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/CurrentJHParser.java)/**
 * {@link JobHistoryParser} that parses JobHistory files.
 */
AnonymizableDataType (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/AnonymizableDataType.java)/**
 * An interface for data-types that can be anonymized.
 */
ClassName (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/ClassName.java)/**
 * Represents a class name.
 */
DataType (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/DataType.java)/**
 * Represents a Rumen data-type.
 */
DefaultAnonymizableDataType (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/DefaultAnonymizableDataType.java)/**
 * Represents a default anonymizable Rumen data-type. It uses 
 * {@link WordListAnonymizerUtility} for anonymization.
 */
DefaultDataType (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/DefaultDataType.java)/**
 * This represents the default java data-types (like int, long, float etc).
 */
FileNameState (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/FileName.java)/**
   * A composite state for filename.
   */
FileName (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/FileName.java)/**
 * Represents a file's location.
 * 
 * Currently, only filenames that can be represented using {@link Path} are 
 * supported.
 */
JobName (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/JobName.java)/**
 * Represents a job's name.
 */
JobProperties (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/JobProperties.java)/**
 * This represents the job configuration properties.
 */
NodeNameState (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/NodeName.java)/**
   * A composite state for node-name.
   */
NodeName (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/NodeName.java)/**
 * Represents the cluster host.
 */
QueueName (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/QueueName.java)/**
 * Represents a queue name.
 */
UserName (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/UserName.java)/**
 * Represents a user's name.
 */
DefaultJobPropertiesParser (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/util/DefaultJobPropertiesParser.java)/**
 * A simple job property parser that acts like a pass-through filter.
 */
JobPropertyParser (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/util/JobPropertyParser.java)/**
 * A {@link JobProperties} parsing utility.
 */
MapReduceJobPropertiesParser (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/datatypes/util/MapReduceJobPropertiesParser.java)/**
 * A default parser for MapReduce job configuration properties.
 * MapReduce job configuration properties are represented as key-value pairs. 
 * Each key represents a configuration knob which controls or affects the 
 * behavior of a MapReduce job or a job's task. The value associated with the 
 * configuration key represents its value. Some of the keys are deprecated. As a
 * result of deprecation some keys change or are preferred over other keys, 
 * across versions. {@link MapReduceJobPropertiesParser} is a utility class that
 * parses MapReduce job configuration properties and converts the value into a 
 * well defined {@link DataType}. Users can use the
 * {@link #parseJobProperty(String, String)} API to process job 
 * configuration parameters. This API will parse a job property represented as a
 * key-value pair and return the value wrapped inside a {@link DataType}. 
 * Callers can then use the returned {@link DataType} for further processing.
 * 
 * {@link MapReduceJobPropertiesParser} thrives on the key name to decide which
 * {@link DataType} to wrap the value with. Values for keys representing 
 * job-name, queue-name, user-name etc are wrapped inside {@link JobName}, 
 * {@link QueueName}, {@link UserName} etc respectively. Keys ending with *dir* 
 * are considered as a directory and hence gets be wrapped inside 
 * {@link FileName}. Similarly key ending with *codec*, *log*, *class* etc are
 * also handled accordingly. Values representing basic java data-types like 
 * integer, float, double, boolean etc are wrapped inside 
 * {@link DefaultDataType}. If the key represents some jvm-level settings then 
 * only standard settings are extracted and gets wrapped inside 
 * {@link DefaultDataType}. Currently only '-Xmx' and '-Xms' settings are 
 * considered while the rest are ignored.
 * 
 * Note that the {@link #parseJobProperty(String, String)} API 
 * maps the keys to a configuration parameter listed in 
 * {@link MRJobConfig}. This not only filters non-framework specific keys thus 
 * ignoring user-specific and hard-to-parse keys but also provides a consistent
 * view for all possible inputs. So if users invoke the 
 * {@link #parseJobProperty(String, String)} API with either
 * &lt;"mapreduce.job.user.name", "bob"&gt; or &lt;"user.name", "bob"&gt;,
 * then the result would be a {@link UserName} {@link DataType} wrapping
 * the user-name "bob".
 */
DeepCompare (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/DeepCompare.java)/**
 * Classes that implement this interface can deep-compare [for equality only,
 * not order] with another instance. They do a deep compare. If there is any
 * semantically significant difference, an implementer throws an Exception to be
 * thrown with a chain of causes describing the chain of field references and
 * indices that get you to the miscompared point.
 * 
 */
DeepInequalityException (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/DeepInequalityException.java)/**
 * We use this exception class in the unit test, and we do a deep comparison
 * when we run the
 * 
 */
DefaultInputDemuxer (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/DefaultInputDemuxer.java)/**
 * {@link DefaultInputDemuxer} acts as a pass-through demuxer. It just opens
 * each file and returns back the input stream. If the input is compressed, it
 * would return a decompression stream.
 */
DefaultOutputter (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/DefaultOutputter.java)/**
 * The default {@link Outputter} that outputs to a plain file. Compression
 * will be applied if the path has the right suffix.
 */
Hadoop20JHParser (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/Hadoop20JHParser.java)/**
 * {@link JobHistoryParser} to parse job histories for hadoop 0.20 (META=1).
 */
HadoopLogsAnalyzer (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/HadoopLogsAnalyzer.java)/**
 * This is the main class for rumen log mining functionality.
 * 
 * It reads a directory of job tracker logs, and computes various information
 * about it. See {@code usage()}, below.
 * 
 */
Histogram (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/Histogram.java)/**
 * {@link Histogram} represents an ordered summary of a sequence of {@code long}
 * s which can be queried to produce a discrete approximation of its cumulative
 * distribution function
 * 
 */
InputDemuxer (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/InputDemuxer.java)/**
 * {@link InputDemuxer} dem-ultiplexes the input files into individual input
 * streams.
 */
JobBuilder (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/JobBuilder.java)/**
 * {@link JobBuilder} builds one job. It processes a sequence of
 * {@link HistoryEvent}s.
 */
JobConfigurationParser (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/JobConfigurationParser.java)/**
 * {@link JobConfigurationParser} parses the job configuration xml file, and
 * extracts configuration properties. It parses the file using a
 * stream-parser and thus is more memory efficient. [This optimization may be
 * postponed for a future release]
 */
JobHistoryParser (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/JobHistoryParser.java)/**
 * {@link JobHistoryParser} defines the interface of a Job History file parser.
 */
JobHistoryParserFactory (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/JobHistoryParserFactory.java)/**
 * {@link JobHistoryParserFactory} is a singleton class that attempts to
 * determine the version of job history and return a proper parser.
 */
JobHistoryUtils (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/JobHistoryUtils.java)/**
 * Job History related utils for handling multiple formats of history logs of
 * different hadoop versions like Pre21 history logs, current history logs.
 */
JobStory (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/JobStory.java)/**
 * {@link JobStory} represents the runtime information available for a
 * completed Map-Reduce job.
 */
JobStoryProducer (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/JobStoryProducer.java)/**
 * {@link JobStoryProducer} produces the sequence of {@link JobStory}'s.
 */
JobTraceReader (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/JobTraceReader.java)/**
 * Reading JSON-encoded job traces and produce {@link LoggedJob} instances.
 */
JsonObjectMapperParser (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/JsonObjectMapperParser.java)/**
 * A simple wrapper for parsing JSON-encoded data using ObjectMapper.
 * 
 * @param <T>
 *          The (base) type of the object(s) to be parsed by this parser.
 */
JsonObjectMapperWriter (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/JsonObjectMapperWriter.java)/**
 * Simple wrapper around {@link JsonGenerator} to write objects in JSON format.
 * @param <T> The type of the objects to be written.
 */
LoggedDiscreteCDF (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/LoggedDiscreteCDF.java)/**
 * A {@link LoggedDiscreteCDF} is a discrete approximation of a cumulative
 * distribution function, with this class set up to meet the requirements of the
 * Jackson JSON parser/generator.
 * 
 * All of the public methods are simply accessors for the instance variables we
 * want to write out in the JSON files.
 * 
 */
LoggedJob (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/LoggedJob.java)/**
 * A {@link LoggedDiscreteCDF} is a representation of an hadoop job, with the
 * details of this class set up to meet the requirements of the Jackson JSON
 * parser/generator.
 * 
 * All of the public methods are simply accessors for the instance variables we
 * want to write out in the JSON files.
 * 
 */
LoggedLocation (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/LoggedLocation.java)/**
 * A {@link LoggedLocation} is a representation of a point in an hierarchical
 * network, represented as a series of membership names, broadest first.
 * 
 * For example, if your network has <i>hosts</i> grouped into <i>racks</i>, then
 * in onecluster you might have a node {@code node1} on rack {@code rack1}. This
 * would be represented with a ArrayList of two layers, with two {@link String}
 * s being {@code "rack1"} and {@code "node1"}.
 * 
 * The details of this class are set up to meet the requirements of the Jackson
 * JSON parser/generator.
 * 
 * All of the public methods are simply accessors for the instance variables we
 * want to write out in the JSON files.
 * 
 */
TopoSort (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/LoggedNetworkTopology.java)/**
   * We need this because we have to sort the {@code children} field. That field
   * is set-valued, but if we sort these fields we ensure that comparisons won't
   * bogusly fail because the hash table happened to enumerate in a different
   * order.
   * 
   */
LoggedNetworkTopology (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/LoggedNetworkTopology.java)/**
 * A {@link LoggedNetworkTopology} represents a tree that in turn represents a
 * hierarchy of hosts. The current version requires the tree to have all leaves
 * at the same level.
 * 
 * All of the public methods are simply accessors for the instance variables we
 * want to write out in the JSON files.
 * 
 */
LoggedSingleRelativeRanking (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/LoggedSingleRelativeRanking.java)/**
 * A {@link LoggedSingleRelativeRanking} represents an X-Y coordinate of a
 * single point in a discrete CDF.
 * 
 * All of the public methods are simply accessors for the instance variables we
 * want to write out in the JSON files.
 * 
 */
LoggedTask (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/LoggedTask.java)/**
 * A {@link LoggedTask} represents a [hadoop] task that is part of a hadoop job.
 * It knows about the [pssibly empty] sequence of attempts, its I/O footprint,
 * and its runtime.
 * 
 * All of the public methods are simply accessors for the instance variables we
 * want to write out in the JSON files.
 * 
 */
LoggedTaskAttempt (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/LoggedTaskAttempt.java)/**
 * A {@link LoggedTaskAttempt} represents an attempt to run an hadoop task in a
 * hadoop job. Note that a task can have several attempts.
 * 
 * All of the public methods are simply accessors for the instance variables we
 * want to write out in the JSON files.
 * 
 */
Builder (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/MachineNode.java)/**
   * Builder for a NodeInfo object
   */
MachineNode (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/MachineNode.java)/**
 * {@link MachineNode} represents the configuration of a cluster node.
 * {@link MachineNode} should be constructed by {@link MachineNode.Builder}.
 */
MapTaskAttemptInfo (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/MapTaskAttemptInfo.java)/**
 * {@link MapTaskAttemptInfo} represents the information with regard to a
 * map task attempt.
 */
Node (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/Node.java)/**
 * {@link Node} represents a node in the cluster topology. A node can be a
 * {@link MachineNode}, or a {@link RackNode}, etc.
 */
Outputter (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/Outputter.java)/**
 * Interface to output a sequence of objects of type T.
 */
ParsedJob (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/ParsedJob.java)/**
 * This is a wrapper class around {@link LoggedJob}. This provides also the
 * extra information about the job obtained from job history which is not
 * written to the JSON trace file.
 */
ParsedTask (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/ParsedTask.java)/**
 * This is a wrapper class around {@link LoggedTask}. This provides also the
 * extra information about the task obtained from job history which is not
 * written to the JSON trace file.
 */
ParsedTaskAttempt (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/ParsedTaskAttempt.java)/**
 * This is a wrapper class around {@link LoggedTaskAttempt}. This provides
 * also the extra information about the task attempt obtained from
 * job history which is not written to the JSON trace file.
 */
Pre21JobHistoryConstants (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/Pre21JobHistoryConstants.java)/**
 * Job History related constants for Hadoop releases prior to 0.21
 */
RackNode (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/RackNode.java)/**
 * {@link RackNode} represents a rack node in the cluster topology.
 */
RandomSeedGenerator (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/RandomSeedGenerator.java)/**
 * The purpose of this class is to generate new random seeds from a master
 * seed. This is needed to make the Random().next*() calls in rumen and mumak
 * deterministic so that mumak simulations become deterministically replayable.
 *
 * In these tools we need many independent streams of random numbers, some of
 * which are created dynamically. We seed these streams with the sub-seeds 
 * returned by RandomSeedGenerator.
 * 
 * For a slightly more complicated approach to generating multiple streams of 
 * random numbers with better theoretical guarantees, see
 * P. L'Ecuyer, R. Simard, E. J. Chen, and W. D. Kelton, 
 * ``An Objected-Oriented Random-Number Package with Many Long Streams and 
 * Substreams'', Operations Research, 50, 6 (2002), 1073--1075
 * http://www.iro.umontreal.ca/~lecuyer/papers.html
 * http://www.iro.umontreal.ca/~lecuyer/myftp/streams00/
 */
ReduceTaskAttemptInfo (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/ReduceTaskAttemptInfo.java)/**
 * {@link ReduceTaskAttemptInfo} represents the information with regard to a
 * reduce task attempt.
 */
ResourceUsageMetrics (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/ResourceUsageMetrics.java)/**
 * Captures the resource usage metrics.
 */
RewindableInputStream (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/RewindableInputStream.java)/**
 * A simple wrapper class to make any input stream "rewindable". It could be
 * made more memory efficient by grow the internal buffer adaptively.
 */
BlockingSerializer (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/serializers/BlockingSerializer.java)/**
 * A JSON serializer for Strings.
 */
DefaultAnonymizingRumenSerializer (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/serializers/DefaultAnonymizingRumenSerializer.java)/**
 * Default Rumen JSON serializer.
 */
DefaultRumenSerializer (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/serializers/DefaultRumenSerializer.java)/**
 * Default Rumen JSON serializer.
 */
ObjectStringSerializer (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/serializers/ObjectStringSerializer.java)/**
 * Rumen JSON serializer for serializing object using toSring() API.
 */
State (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/state/State.java)/**
 * Represents a state. This state is managed by {@link StatePool}.
 * 
 * Note that a {@link State} objects should be persistable. Currently, the 
 * {@link State} objects are persisted using the Jackson JSON library. Hence the
 * implementors of the {@link State} interface should be careful while defining 
 * their public setter and getter APIs.  
 */
StateDeserializer (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/state/StateDeserializer.java)/**
 * Rumen JSON deserializer for deserializing the {@link State} object.
 */
StatePair (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/state/StatePool.java)/**
   * A wrapper class that binds the state implementation to its implementing 
   * class name.
   */
StatePool (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/state/StatePool.java)/**
 * A pool of states. States used by {@link DataType}'s can be managed the 
 * {@link StatePool}. {@link StatePool} also supports persistence. Persistence
 * is key to share states across multiple {@link Anonymizer} runs.
 */
TaskAttemptInfo (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/TaskAttemptInfo.java)/**
 * {@link TaskAttemptInfo} is a collection of statistics about a particular
 * task-attempt gleaned from job-history of the job.
 */
TopologyBuilder (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/TopologyBuilder.java)/**
 * Building the cluster topology.
 */
HistoryLogsComparator (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/TraceBuilder.java)/**
     * Compare the history file names, not the full paths.
     * Job history file name format is such that doing lexicographic sort on the
     * history file names should result in the order of jobs' submission times.
     */
TraceBuilder (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/TraceBuilder.java)/**
 * The main driver of the Rumen Parser.
 */
TreePath (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/TreePath.java)/**
 * This describes a path from a node to the root. We use it when we compare two
 * trees during rumen unit tests. If the trees are not identical, this chain
 * will be converted to a string which describes the path from the root to the
 * fields that did not compare.
 * 
 */
ZombieCluster (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/ZombieCluster.java)/**
 * {@link ZombieCluster} rebuilds the cluster topology using the information
 * obtained from job history logs.
 */
ZombieJob (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/ZombieJob.java)/**
 * {@link ZombieJob} is a layer above {@link LoggedJob} raw JSON objects.
 * 
 * Each {@link ZombieJob} object represents a job in job history. For everything
 * that exists in job history, contents are returned unchanged faithfully. To
 * get input splits of a non-exist task, a non-exist task attempt, or an
 * ill-formed task attempt, proper objects are made up from statistical
 * sketches.
 */
ZombieJobProducer (/hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/ZombieJobProducer.java)/**
 * Producing {@link JobStory}s from job trace.
 */
DelimitedInputStream (/hadoop-tools/hadoop-rumen/src/test/java/org/apache/hadoop/tools/rumen/ConcatenatedInputFilesDemuxer.java)/**
   * A simple wrapper class to make any input stream delimited. It has an extra
   * method, getName.
   * 
   * The input stream should have lines that look like
   * <marker><filename><endmarker> . The text <marker> should not occur
   * elsewhere in the file. The text <endmarker> should not occur in a file
   * name.
   */
ReservationClientUtil (/hadoop-tools/hadoop-sls/src/main/java/org/apache/hadoop/yarn/sls/ReservationClientUtil.java)/**
 * Simple support class, used to create reservation requests.
 */
NodeDetails (/hadoop-tools/hadoop-sls/src/main/java/org/apache/hadoop/yarn/sls/SLSRunner.java)/**
   * Class to encapsulate all details about the node.
   */
SynthTask (/hadoop-tools/hadoop-sls/src/main/java/org/apache/hadoop/yarn/sls/synthetic/SynthJob.java)/**
   * Nested class used to represent a task instance in a job. Each task
   * corresponds to one container allocation for the job.
   */
SynthJob (/hadoop-tools/hadoop-sls/src/main/java/org/apache/hadoop/yarn/sls/synthetic/SynthJob.java)/**
 * Generates random task data for a synthetic job.
 */
Trace (/hadoop-tools/hadoop-sls/src/main/java/org/apache/hadoop/yarn/sls/synthetic/SynthTraceJobProducer.java)/**
   * Class used to parse a trace configuration file.
   */
Workload (/hadoop-tools/hadoop-sls/src/main/java/org/apache/hadoop/yarn/sls/synthetic/SynthTraceJobProducer.java)/**
   * Class used to parse a workload from file.
   */
JobDefinition (/hadoop-tools/hadoop-sls/src/main/java/org/apache/hadoop/yarn/sls/synthetic/SynthTraceJobProducer.java)/**
   * Class used to parse a job class from file.
   */
TaskDefinition (/hadoop-tools/hadoop-sls/src/main/java/org/apache/hadoop/yarn/sls/synthetic/SynthTraceJobProducer.java)/**
   * A task representing a type of container - e.g. "map" in mapreduce
   */
Sample (/hadoop-tools/hadoop-sls/src/main/java/org/apache/hadoop/yarn/sls/synthetic/SynthTraceJobProducer.java)/**
   * Class used to parse value sample information.
   */
TimeSample (/hadoop-tools/hadoop-sls/src/main/java/org/apache/hadoop/yarn/sls/synthetic/SynthTraceJobProducer.java)/**
   * This is used to define time-varying probability of a job start-time (e.g.,
   * to simulate daily patterns).
   */
SynthTraceJobProducer (/hadoop-tools/hadoop-sls/src/main/java/org/apache/hadoop/yarn/sls/synthetic/SynthTraceJobProducer.java)/**
 * This is a JobStoryProducer that operates from distribution of different
 * workloads. The .json input file is used to determine how many weight, which
 * size, number of maps/reducers and their duration, as well as the temporal
 * distributed of submissions. For each parameter we control avg and stdev, and
 * generate values via normal or log-normal distributions.
 */
SynthUtils (/hadoop-tools/hadoop-sls/src/main/java/org/apache/hadoop/yarn/sls/synthetic/SynthUtils.java)/**
 * Utils for the Synthetic generator.
 */
BaseSLSRunnerTest (/hadoop-tools/hadoop-sls/src/test/java/org/apache/hadoop/yarn/sls/BaseSLSRunnerTest.java)/**
 * This is a base class to ease the implementation of SLS-based tests.
 */
TestDagAMSimulator (/hadoop-tools/hadoop-sls/src/test/java/org/apache/hadoop/yarn/sls/TestDagAMSimulator.java)/**
 * Tests for DagAMSimulator.
 */
TestReservationSystemInvariants (/hadoop-tools/hadoop-sls/src/test/java/org/apache/hadoop/yarn/sls/TestReservationSystemInvariants.java)/**
 * This test performs an SLS run enabling a
 * {@code ReservationInvariantsChecker}.
 */
TestSLSDagAMSimulator (/hadoop-tools/hadoop-sls/src/test/java/org/apache/hadoop/yarn/sls/TestSLSDagAMSimulator.java)/**
 * This test performs simple runs of the SLS with the generic syn json format.
 */
TestSLSGenericSynth (/hadoop-tools/hadoop-sls/src/test/java/org/apache/hadoop/yarn/sls/TestSLSGenericSynth.java)/**
 * This test performs simple runs of the SLS with the generic syn json format.
 */
TestSLSRunner (/hadoop-tools/hadoop-sls/src/test/java/org/apache/hadoop/yarn/sls/TestSLSRunner.java)/**
 * This test performs simple runs of the SLS with different trace types and
 * schedulers.
 */
TestSLSStreamAMSynth (/hadoop-tools/hadoop-sls/src/test/java/org/apache/hadoop/yarn/sls/TestSLSStreamAMSynth.java)/**
 * This test performs simple runs of the SLS with the generic syn json format.
 */
TestSynthJobGeneration (/hadoop-tools/hadoop-sls/src/test/java/org/apache/hadoop/yarn/sls/TestSynthJobGeneration.java)/**
 * Simple test class driving the {@code SynthTraceJobProducer}, and validating
 * jobs produce are within expected range.
 */
BinaryRecordInput (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/BinaryRecordInput.java)/**
 * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
 */
BinaryRecordOutput (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/BinaryRecordOutput.java)/**
 * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
 */
Buffer (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Buffer.java)/**
 * A byte sequence that is used as a Java native type for buffer.
 * It is resizable and distinguishes between the count of the sequence and
 * the current capacity.
 * 
 * @deprecated Replaced by <a href="http://avro.apache.org/">Avro</a>.
 */
CsvRecordOutput (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/CsvRecordOutput.java)/**
 * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
 */
Index (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Index.java)/**
 * Interface that acts as an iterator for deserializing maps.
 * The deserializer returns an instance that the record uses to
 * read vectors and maps. An example of usage is as follows:
 *
 * <code>
 * Index idx = startVector(...);
 * while (!idx.done()) {
 *   .... // read element of a vector
 *   idx.incr();
 * }
 * </code>
 * 
 * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
 */
Record (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Record.java)/**
 * Abstract class that is extended by generated classes.
 * 
 * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
 */
RecordComparator (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordComparator.java)/**
 * A raw record comparator base class
 * 
 * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
 */
RecordInput (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordInput.java)/**
 * Interface that all the Deserializers have to implement.
 * 
 * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
 */
RecordOutput (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordOutput.java)/**
 * Interface that all the serializers have to implement.
 * 
 * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
 */
Utils (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Utils.java)/**
 * Various utility functions for Hadoop record I/O runtime.
 * 
 * @deprecated Replaced by <a href="http://avro.apache.org/">Avro</a>.
 */
AutoInputFormat (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/AutoInputFormat.java)/**
 * An {@link InputFormat} that tries to deduce the types of the input files
 * automatically. It can currently handle text and sequence files.
 */
DumpTypedBytes (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/DumpTypedBytes.java)/**
 * Utility program that fetches all files that match a given pattern and dumps
 * their content to stdout as typed bytes. This works for all files that can be
 * handled by {@link org.apache.hadoop.streaming.AutoInputFormat}.
 */
Environment (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/Environment.java)/**
 * This is a class used to get the current environment
 * on the host machines running the map/reduce. This class
 * assumes that setting the environment in streaming is 
 * allowed on windows/ix/linuz/freebsd/sunos/solaris/hp-ux
 */
HadoopStreaming (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/HadoopStreaming.java)/** The main entry point. Usually invoked with the script 
 *  bin/hadoop jar hadoop-streaming.jar args.
 */
IdentifierResolver (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/IdentifierResolver.java)/**
 * This class is used to resolve a string identifier into the required IO
 * classes. By extending this class and pointing the property
 * <tt>stream.io.identifier.resolver.class</tt> to this extension, additional
 * IO classes can be added by external code.
 */
InputWriter (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/InputWriter.java)/**
 * Abstract base for classes that write the client's input.
 */
KeyOnlyTextOutputReader (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/KeyOnlyTextOutputReader.java)/**
 * OutputReader that reads the client's output as text, interpreting each line
 * as a key and outputting NullWritables for values.
 */
OutputReader (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/OutputReader.java)/**
 * Abstract base for classes that read the client's output.
 */
RawBytesInputWriter (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/RawBytesInputWriter.java)/**
 * InputWriter that writes the client's input as raw bytes.
 */
RawBytesOutputReader (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/RawBytesOutputReader.java)/**
 * OutputReader that reads the client's output as raw bytes.
 */
TextInputWriter (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TextInputWriter.java)/**
 * InputWriter that writes the client's input as text.
 */
TextOutputReader (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TextOutputReader.java)/**
 * OutputReader that reads the client's output as text.
 */
TypedBytesInputWriter (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TypedBytesInputWriter.java)/**
 * InputWriter that writes the client's input as typed bytes.
 */
TypedBytesOutputReader (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TypedBytesOutputReader.java)/**
 * OutputReader that reads the client's output as typed bytes.
 */
JarBuilder (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/JarBuilder.java)/**
 * This class is the main class for generating job.jar
 * for Hadoop Streaming jobs. It includes the files specified 
 * with the -file option and includes them in the jar. Also,
 * hadoop-streaming is a user level appplication, so all the classes
 * with hadoop-streaming that are needed in the job are also included
 * in the job.jar.
 */
LoadTypedBytes (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/LoadTypedBytes.java)/**
 * Utility program that reads typed bytes from standard input and stores them in
 * a sequence file for which the path is given as an argument.
 */
StreamBaseRecordReader (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamBaseRecordReader.java)/**
 * Shared functionality for hadoopStreaming formats. A custom reader can be
 * defined to be a RecordReader with the constructor below and is selected with
 * the option bin/hadoopStreaming -inputreader ...
 * 
 * @see StreamXmlRecordReader
 */
StreamInputFormat (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamInputFormat.java)/**
 * An input format that selects a RecordReader based on a JobConf property. This
 * should be used only for non-standard record reader such as
 * StreamXmlRecordReader. For all other standard record readers, the appropriate
 * input format classes should be used.
 */
StreamXmlRecordReader (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamXmlRecordReader.java)/**
 * A way to interpret XML fragments as Mapper input records. Values are XML
 * subtrees delimited by configurable tags. Keys could be the value of a certain
 * attribute in the XML subtree, but this is left to the stream processor
 * application.
 * 
 * The name-value properties that StreamXmlRecordReader understands are: String
 * begin (chars marking beginning of record) String end (chars marking end of
 * record) int maxrec (maximum record size) int lookahead(maximum lookahead to
 * sync CDATA) boolean slowmatch
 */
PathFinder (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PathFinder.java)/**
 * Maps a relative pathname to an absolute pathname using the PATH environment.
 */
PipeMapper (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapper.java)/** A generic Mapper bridge.
 *  It delegates operations to an external program via stdin and stdout.
 */
PipeMapRed (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRed.java)/** Shared functionality for PipeMapper, PipeReducer.
 */
PipeReducer (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeReducer.java)/** A generic Reducer bridge.
 *  It delegates operations to an external program via stdin and stdout.
 */
StreamBaseRecordReader (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamBaseRecordReader.java)/** 
 * Shared functionality for hadoopStreaming formats.
 * A custom reader can be defined to be a RecordReader with the constructor below
 * and is selected with the option bin/hadoopStreaming -inputreader ...
 * @see StreamXmlRecordReader 
 */
StreamInputFormat (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamInputFormat.java)/** An input format that selects a RecordReader based on a JobConf property.
 *  This should be used only for non-standard record reader such as 
 *  StreamXmlRecordReader. For all other standard 
 *  record readers, the appropriate input format classes should be used.
 */
StreamJob (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java)/** All the client-side work happens here.
 * (Jar packaging, MapRed job submission and monitoring)
 */
StreamUtil (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamUtil.java)/** 
 * Utilities used in streaming
 */
StreamXmlRecordReader (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamXmlRecordReader.java)/** A way to interpret XML fragments as Mapper input records.
 *  Values are XML subtrees delimited by configurable tags.
 *  Keys could be the value of a certain attribute in the XML subtree, 
 *  but this is left to the stream processor application.
 *
 *  The name-value properties that StreamXmlRecordReader understands are:
 *    String begin (chars marking beginning of record)
 *    String end   (chars marking end of record)
 *    int maxrec   (maximum record size)
 *    int lookahead(maximum lookahead to sync CDATA)
 *    boolean slowmatch
 */
TypedBytesInput (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesInput.java)/**
 * Provides functionality for reading typed bytes.
 */
TypedBytesOutput (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesOutput.java)/**
 * Provides functionality for writing typed bytes.
 */
TypedBytesRecordInput (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesRecordInput.java)/**
 * Serializer for records that writes typed bytes.
 */
TypedBytesRecordOutput (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesRecordOutput.java)/**
 * Deserialized for records that reads typed bytes.
 */
TypedBytesWritable (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritable.java)/**
 * Writable for typed bytes.
 */
TypedBytesWritableInput (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritableInput.java)/**
 * Provides functionality for reading typed bytes as Writable objects.
 * 
 * @see TypedBytesInput
 */
TypedBytesWritableOutput (/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritableOutput.java)/**
 * Provides functionality for writing Writable objects as typed bytes.
 * 
 * @see TypedBytesOutput
 */
DelayEchoApp (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/DelayEchoApp.java)/**
 * A simple Java app that will consume all input from stdin, wait a few seconds
 * and echoing it to stdout.
 */
FailApp (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/FailApp.java)/**
 * A simple Java app that will consume all input from stdin, echoing
 * it to stdout, and then optionally throw an exception (which should
 * cause a non-zero exit status for the process).
 */
TestStreamXmlRecordReader (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/mapreduce/TestStreamXmlRecordReader.java)/**
 * This class tests StreamXmlRecordReader The test creates an XML file, uses
 * StreamXmlRecordReader and compares the expected output against the generated
 * output
 */
OutputOnlyApp (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/OutputOnlyApp.java)/**
 * An application that outputs a specified number of lines
 * without consuming any input.
 */
StderrApp (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/StderrApp.java)/**
 * Output an arbitrary number of stderr lines before or after
 * consuming the keys/values from stdin.
 */
StreamAggregate (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/StreamAggregate.java)/** 
    Used to test the usage of external applications without adding
    platform-specific dependencies.
 */
TestClassWithNoPackage (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestClassWithNoPackage.java)/**
 * Test Hadoop StreamUtil successfully returns a class loaded by the job conf
 * but has no package name.
 */
TestFileArgs (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestFileArgs.java)/**
 * This class tests that the '-file' argument to streaming results
 * in files being unpacked in the job working directory.
 */
TestGzipInput (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestGzipInput.java)/**
 * This class tests gzip input streaming in MapReduce local mode.
 */
TestMultipleArchiveFiles (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMultipleArchiveFiles.java)/**
 * This class tests cacheArchive option of streaming
 * The test case creates 2 archive files, ships it with hadoop
 * streaming and compares the output with expected output
 */
TestMultipleCachefiles (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMultipleCachefiles.java)/**
 * This test case tests the symlink creation
 * utility provided by distributed caching 
 */
TestStreamAggregate (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamAggregate.java)/**
 * This class tests hadoopStreaming in MapReduce local mode.
 * It uses Hadoop Aggregate to count the numbers of word occurrences 
 * in the input.
 */
TestStreamDataProtocol (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamDataProtocol.java)/**
 * This class tests hadoopStreaming in MapReduce local mode.
 */
TestStreaming (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreaming.java)/**
 * This class tests hadoopStreaming in MapReduce local mode.
 */
TestStreamingBackground (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingBackground.java)/**
 * This class tests if hadoopStreaming background works fine. A DelayEchoApp
 * with 10 seconds delay is submited. 
 */
TestStreamingCounters (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingCounters.java)/**
 * This class tests streaming counters in MapReduce local mode.
 */
TestStreamingExitStatus (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingExitStatus.java)/**
 * This class tests if hadoopStreaming fails a job when the mapper or
 * reducers have non-zero exit status and the
 * stream.non.zero.exit.status.is.failure jobconf is set.
 */
TestStreamingFailure (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingFailure.java)/**
 * This class tests if hadoopStreaming returns Exception 
 * on failure when submitted an invalid/failed job
 * The test case provides an invalid input file for map/reduce job as
 * a unit test case
 */
TestStreamingKeyValue (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingKeyValue.java)/**
 * This class tests hadoopStreaming in MapReduce local mode.
 * This testcase looks at different cases of tab position in input. 
 */
TestStreamingOutputKeyValueTypes (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingOutputKeyValueTypes.java)/**
 * Tests stream job with java tasks, commands in MapReduce local mode.
 * Validates if user-set config properties
 * {@link MRJobConfig#MAP_OUTPUT_KEY_CLASS} and
 * {@link MRJobConfig#OUTPUT_KEY_CLASS} are honored by streaming jobs.
 */
TestStreamingSeparator (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingSeparator.java)/**
 * This class tests hadoopStreaming with customized separator in MapReduce local mode.
 */
TestStreamingStatus (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingStatus.java)/**
 * Tests if mapper/reducer with empty/nonempty input works properly if
 * reporting is done using lines like "reporter:status:" and
 * "reporter:counter:" before map()/reduce() method is called.
 * Validates the task's log of STDERR if messages are written to stderr before
 * map()/reduce() is called.
 * Also validates job output.
 * Uses MiniMR since the local jobtracker doesn't track task status. 
 */
TestStreamingStderr (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingStderr.java)/**
 * Test that streaming consumes stderr from the streaming process
 * (before, during, and after the main processing of mapred input),
 * and that stderr messages count as task progress.
 */
TestStreamJob (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamJob.java)/**
 * This class tests hadoop Streaming's StreamJob class.
 */
TestStreamReduceNone (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamReduceNone.java)/**
 * This class tests hadoopStreaming in MapReduce local mode.
 * It tests the case where number of reducers is set to 0.
   In this case, the mappers are expected to write out outputs directly.
   No reducer/combiner will be activated.
 */
TestStreamXmlMultipleRecords (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamXmlMultipleRecords.java)/**
 * Tests if StreamXmlRecordReader will read the next record, _after_ the
 * end of a split if the split falls before the end of end-tag of a record.
 * Also tests if StreamXmlRecordReader will read a record twice if end of a
 * split is after few characters after the end-tag of a record but before the
 * begin-tag of next record.
 */
TestStreamXmlRecordReader (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamXmlRecordReader.java)/**
 * This class tests StreamXmlRecordReader
 * The test creates an XML file, uses StreamXmlRecordReader and compares
 * the expected output against the generated output
 */
TestSymLink (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestSymLink.java)/**
 * This test case tests the symlink creation
 * utility provided by distributed caching 
 */
TrApp (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TrApp.java)/** A minimal Java implementation of /usr/bin/tr.
 *  Used to test the usage of external applications without adding
 *  platform-specific dependencies.
 *  Use TrApp as mapper only. For reducer, use TrAppReduce.
 */
TrAppReduce (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TrAppReduce.java)/** A minimal Java implementation of /usr/bin/tr.
 *  Used to test the usage of external applications without adding
 *  platform-specific dependencies.
 *  Use TrAppReduce as reducer only. For mapper, use TrApp.
 */
UniqApp (/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/UniqApp.java)/** A minimal Java implementation of /usr/bin/uniq
    Used to test the usage of external applications without adding
    platform-specific dependencies.
    Uniques lines and prepends a header on the line.
 */
Null (/hadoop-yarn-project/hadoop-yarn/dev-support/jdiff/Null.java)/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
ApplicationMasterServiceContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/ams/ApplicationMasterServiceContext.java)/**
 * This is a marker interface for a context object that is injected into
 * the ApplicationMasterService processor. The processor implementation
 * is free to type cast this based on the availability of the context's
 * implementation in the classpath.
 */
ApplicationMasterServiceProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/ams/ApplicationMasterServiceProcessor.java)/**
 * Interface to abstract out the the actual processing logic of the
 * Application Master Service.
 */
ApplicationMasterServiceUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/ams/ApplicationMasterServiceUtils.java)/**
 * Utility methods to be used by {@link ApplicationMasterServiceProcessor}.
 */
ApplicationBaseProtocol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/ApplicationBaseProtocol.java)/**
 * <p>
 * The protocol between clients and the <code>ResourceManager</code> or
 * <code>ApplicationHistoryServer</code> to get information on applications,
 * application attempts and containers.
 * </p>
 *
 */
ApplicationClientProtocol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/ApplicationClientProtocol.java)/**
 * <p>The protocol between clients and the <code>ResourceManager</code>
 * to submit/abort jobs and to get information on applications, cluster metrics,
 * nodes, queues and ACLs.</p> 
 */
ApplicationConstants (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/ApplicationConstants.java)/**
 * This is the API for the applications comprising of constants that YARN sets
 * up for the applications and the containers.
 *
 * TODO: Investigate the semantics and security of each cross-boundary refs.
 */
ApplicationHistoryProtocol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/ApplicationHistoryProtocol.java)/**
 * <p>
 * The protocol between clients and the <code>ApplicationHistoryServer</code> to
 * get the information of completed applications etc.
 * </p>
 */
ApplicationMasterProtocol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/ApplicationMasterProtocol.java)/**
 * <p>The protocol between a live instance of <code>ApplicationMaster</code> 
 * and the <code>ResourceManager</code>.</p>
 * 
 * <p>This is used by the <code>ApplicationMaster</code> to register/unregister
 * and to request and obtain resources in the cluster from the
 * <code>ResourceManager</code>.</p>
 */
ClientSCMProtocol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/ClientSCMProtocol.java)/**
 * <p>
 * The protocol between clients and the <code>SharedCacheManager</code> to claim
 * and release resources in the shared cache.
 * </p>
 */
ContainerManagementProtocol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/ContainerManagementProtocol.java)/**
 * <p>The protocol between an <code>ApplicationMaster</code> and a 
 * <code>NodeManager</code> to start/stop and increase resource of containers
 * and to get status of running containers.</p>
 *
 * <p>If security is enabled the <code>NodeManager</code> verifies that the
 * <code>ApplicationMaster</code> has truly been allocated the container
 * by the <code>ResourceManager</code> and also verifies all interactions such 
 * as stopping the container or obtaining status information for the container.
 * </p>
 */
CsiAdaptorPB (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/CsiAdaptorPB.java)/**
 * Interface for the CSI adaptor protocol.
 */
CsiAdaptorPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/CsiAdaptorPlugin.java)/**
 * csi-adaptor is a plugin, user can provide customized implementation
 * according to this interface. NM will init and load this into a NM aux
 * service, and it can run multiple csi-adaptor servers.
 *
 * User needs to implement all the methods defined in
 * {@link CsiAdaptorProtocol}, and plus the methods in this interface.
 */
CsiAdaptorProtocol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/CsiAdaptorProtocol.java)/**
 * CSI adaptor delegates all the calls from YARN to a CSI driver.
 */
AllocateRequestBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/AllocateRequest.java)/**
   * Class to construct instances of {@link AllocateRequest} with specific
   * options.
   */
AllocateRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/AllocateRequest.java)/**
 * <p>The core request sent by the <code>ApplicationMaster</code> to the 
 * <code>ResourceManager</code> to obtain resources in the cluster.</p> 
 *
 * <p>The request includes:
 * <ul>
 *   <li>A response id to track duplicate responses.</li>
 *   <li>Progress information.</li>
 *   <li>
 *     A list of {@link ResourceRequest} to inform the
 *     <code>ResourceManager</code> about the application's
 *     resource requirements.
 *   </li>
 *   <li>
 *     A list of unused {@link Container} which are being returned.
 *   </li>
 *   <li>
 *     A list of {@link UpdateContainerRequest} to inform
 *     the <code>ResourceManager</code> about the change in
 *     requirements of running containers.
 *   </li>
 * </ul>
 * 
 * @see ApplicationMasterProtocol#allocate(AllocateRequest)
 */
AllocateResponseBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/AllocateResponse.java)/**
   * Class to construct instances of {@link AllocateResponse} with specific
   * options.
   */
AllocateResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/AllocateResponse.java)/**
 * The response sent by the <code>ResourceManager</code> the
 * <code>ApplicationMaster</code> during resource negotiation.
 * <p>
 * The response, includes:
 * <ul>
 *   <li>Response ID to track duplicate responses.</li>
 *   <li>
 *     An AMCommand sent by ResourceManager to let the
 *     {@code ApplicationMaster} take some actions (resync, shutdown etc.).
 *   </li>
 *   <li>A list of newly allocated {@link Container}.</li>
 *   <li>A list of completed {@link Container}s' statuses.</li>
 *   <li>
 *     The available headroom for resources in the cluster for the
 *     application.
 *   </li>
 *   <li>A list of nodes whose status has been updated.</li>
 *   <li>The number of available nodes in a cluster.</li>
 *   <li>A description of resources requested back by the cluster</li>
 *   <li>AMRMToken, if AMRMToken has been rolled over</li>
 *   <li>
 *     A list of {@link Container} representing the containers
 *     whose resource has been increased.
 *   </li>
 *   <li>
 *     A list of {@link Container} representing the containers
 *     whose resource has been decreased.
 *   </li>
 * </ul>
 * 
 * @see ApplicationMasterProtocol#allocate(AllocateRequest)
 */
CancelDelegationTokenRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/CancelDelegationTokenRequest.java)/**
 * The request issued by the client to the {@code ResourceManager} to cancel a
 * delegation token.
 */
CancelDelegationTokenResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/CancelDelegationTokenResponse.java)/**
 * The response from the {@code ResourceManager} to a cancelDelegationToken
 * request.
 */
CommitResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/CommitResponse.java)/**
 * Response to Commit Container Request.
 */
ContainerUpdateRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ContainerUpdateRequest.java)/**
 * <p>The request sent by <code>Application Master</code> to the
 * <code>Node Manager</code> to change the resource quota of a container.</p>
 *
 * @see ContainerManagementProtocol#updateContainer(ContainerUpdateRequest)
 */
ContainerUpdateResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ContainerUpdateResponse.java)/**
 * <p>
 * The response sent by the <code>NodeManager</code> to the
 * <code>ApplicationMaster</code> when asked to update container resource.
 * </p>
 *
 * @see ContainerManagementProtocol#updateContainer(ContainerUpdateRequest)
 */
FailApplicationAttemptRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/FailApplicationAttemptRequest.java)/**
 * <p>The request sent by the client to the <code>ResourceManager</code>
 * to fail an application attempt.</p>
 *
 * <p>The request includes the {@link ApplicationAttemptId} of the attempt to
 * be failed.</p>
 *
 * @see ApplicationClientProtocol#failApplicationAttempt(FailApplicationAttemptRequest)
 */
FailApplicationAttemptResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/FailApplicationAttemptResponse.java)/**
 * <p>The response sent by the <code>ResourceManager</code> to the client
 * failing an application attempt.</p>
 *
 * <p>Currently it's empty.</p>
 *
 * @see ApplicationClientProtocol#failApplicationAttempt(FailApplicationAttemptRequest)
 */
FinishApplicationMasterRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/FinishApplicationMasterRequest.java)/**
 * The finalization request sent by the {@code ApplicationMaster} to
 * inform the {@code ResourceManager} about its completion.
 * <p>
 * The final request includes details such:
 * <ul>
 *   <li>Final state of the {@code ApplicationMaster}</li>
 *   <li>
 *     Diagnostic information in case of failure of the
 *     {@code ApplicationMaster}
 *   </li>
 *   <li>Tracking URL</li>
 * </ul>
 *
 * @see ApplicationMasterProtocol#finishApplicationMaster(FinishApplicationMasterRequest)
 */
FinishApplicationMasterResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/FinishApplicationMasterResponse.java)/**
 * The response sent by the <code>ResourceManager</code> to a
 * <code>ApplicationMaster</code> on it's completion.
 * <p>
 * The response, includes:
 * <ul>
 * <li>A flag which indicates that the application has successfully unregistered
 * with the RM and the application can safely stop.</li>
 * </ul>
 * <p>
 * Note: The flag indicates whether the application has successfully
 * unregistered and is safe to stop. The application may stop after the flag is
 * true. If the application stops before the flag is true then the RM may retry
 * the application.
 * 
 * @see ApplicationMasterProtocol#finishApplicationMaster(FinishApplicationMasterRequest)
 */
GetAllResourceProfilesRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetAllResourceProfilesRequest.java)/**
 * Request class for getting all the resource profiles from the RM.
 */
GetAllResourceProfilesResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetAllResourceProfilesResponse.java)/**
 * Response class for getting all the resource profiles from the RM.
 */
GetAllResourceTypeInfoRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetAllResourceTypeInfoRequest.java)/**
 * Request class for getting all the resource profiles from the RM.
 */
GetAllResourceTypeInfoResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetAllResourceTypeInfoResponse.java)/**
 * Response class for getting all the resource profiles from the RM.
 */
GetApplicationAttemptReportRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetApplicationAttemptReportRequest.java)/**
 * <p>
 * The request sent by a client to the <code>ResourceManager</code> to get an
 * {@link ApplicationAttemptReport} for an application attempt.
 * </p>
 * 
 * <p>
 * The request should include the {@link ApplicationAttemptId} of the
 * application attempt.
 * </p>
 * 
 * @see ApplicationAttemptReport
 * @see ApplicationHistoryProtocol#getApplicationAttemptReport(GetApplicationAttemptReportRequest)
 */
GetApplicationAttemptReportResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetApplicationAttemptReportResponse.java)/**
 * <p>
 * The response sent by the <code>ResourceManager</code> to a client requesting
 * an application attempt report.
 * </p>
 * 
 * <p>
 * The response includes an {@link ApplicationAttemptReport} which has the
 * details about the particular application attempt
 * </p>
 * 
 * @see ApplicationAttemptReport
 * @see ApplicationHistoryProtocol#getApplicationAttemptReport(GetApplicationAttemptReportRequest)
 */
GetApplicationAttemptsRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetApplicationAttemptsRequest.java)/**
 * <p>
 * The request from clients to get a list of application attempt reports of an
 * application from the <code>ResourceManager</code>.
 * </p>
 * 
 * @see ApplicationHistoryProtocol#getApplicationAttempts(GetApplicationAttemptsRequest)
 */
GetApplicationAttemptsResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetApplicationAttemptsResponse.java)/**
 * <p>
 * The response sent by the <code>ResourceManager</code> to a client requesting
 * a list of {@link ApplicationAttemptReport} for application attempts.
 * </p>
 * 
 * <p>
 * The <code>ApplicationAttemptReport</code> for each application includes the
 * details of an application attempt.
 * </p>
 * 
 * @see ApplicationAttemptReport
 * @see ApplicationHistoryProtocol#getApplicationAttempts(GetApplicationAttemptsRequest)
 */
GetApplicationReportRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetApplicationReportRequest.java)/**
 * <p>The request sent by a client to the <code>ResourceManager</code> to 
 * get an {@link ApplicationReport} for an application.</p>
 * 
 * <p>The request should include the {@link ApplicationId} of the 
 * application.</p>
 * 
 * @see ApplicationClientProtocol#getApplicationReport(GetApplicationReportRequest)
 * @see ApplicationReport
 */
GetApplicationReportResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetApplicationReportResponse.java)/**
 * <p>The response sent by the <code>ResourceManager</code> to a client
 * requesting an application report.</p>
 * 
 * <p>The response includes an {@link ApplicationReport} which has details such 
 * as user, queue, name, host on which the <code>ApplicationMaster</code> is 
 * running, RPC port, tracking URL, diagnostics, start time etc.</p>
 * 
 * @see ApplicationClientProtocol#getApplicationReport(GetApplicationReportRequest)
 */
GetApplicationsRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetApplicationsRequest.java)/**
 * <p>The request from clients to get a report of Applications
 * in the cluster from the <code>ResourceManager</code>.</p>
 *
 * @see ApplicationClientProtocol#getApplications(GetApplicationsRequest)
 */
GetApplicationsResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetApplicationsResponse.java)/**
 * <p>The response sent by the <code>ResourceManager</code> to a client
 * requesting an {@link ApplicationReport} for applications.</p>
 *
 * <p>The <code>ApplicationReport</code> for each application includes details
 * such as user, queue, name, host on which the <code>ApplicationMaster</code>
 * is running, RPC port, tracking URL, diagnostics, start time etc.</p>
 *
 * @see ApplicationReport
 * @see ApplicationClientProtocol#getApplications(GetApplicationsRequest)
 */
GetAttributesToNodesRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetAttributesToNodesRequest.java)/**
 * <p>
 * The request from clients to get node to attribute value mapping for all or
 * given set of Node AttributeKey's in the cluster from the
 * <code>ResourceManager</code>.
 * </p>
 *
 * @see ApplicationClientProtocol#getAttributesToNodes
 *      (GetAttributesToNodesRequest)
 */
GetAttributesToNodesResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetAttributesToNodesResponse.java)/**
 * <p>
 * The response sent by the <code>ResourceManager</code> to a client requesting
 * node to attribute value mapping for all or given set of Node AttributeKey's.
 * </p>
 *
 * @see ApplicationClientProtocol#getAttributesToNodes
 *      (GetAttributesToNodesRequest)
 */
GetClusterMetricsRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetClusterMetricsRequest.java)/**
 * <p>The request sent by clients to get cluster metrics from the 
 * <code>ResourceManager</code>.</p>
 * 
 * <p>Currently, this is empty.</p>
 *
 * @see ApplicationClientProtocol#getClusterMetrics(GetClusterMetricsRequest)
 */
GetClusterMetricsResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetClusterMetricsResponse.java)/**
 * The response sent by the <code>ResourceManager</code> to a client
 * requesting cluster metrics.
 * 
 * @see YarnClusterMetrics
 * @see ApplicationClientProtocol#getClusterMetrics(GetClusterMetricsRequest)
 */
GetClusterNodeAttributesRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetClusterNodeAttributesRequest.java)/**
 * <p>
 * The request from clients to get node attributes in the cluster from the
 * <code>ResourceManager</code>.
 * </p>
 *
 * @see ApplicationClientProtocol#getClusterNodeAttributes
 * (GetClusterNodeAttributesRequest)
 */
GetClusterNodeAttributesResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetClusterNodeAttributesResponse.java)/**
 * <p>
 * The response sent by the <code>ResourceManager</code> to a client requesting
 * a node attributes in cluster.
 * </p>
 *
 * @see ApplicationClientProtocol#getClusterNodeAttributes
 * (GetClusterNodeAttributesRequest)
 */
GetClusterNodesRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetClusterNodesRequest.java)/**
 * <p>The request from clients to get a report of all nodes
 * in the cluster from the <code>ResourceManager</code>.</p>
 *
 * The request will ask for all nodes in the given {@link NodeState}s.
 *
 * @see ApplicationClientProtocol#getClusterNodes(GetClusterNodesRequest) 
 */
GetClusterNodesResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetClusterNodesResponse.java)/**
 * <p>The response sent by the <code>ResourceManager</code> to a client
 * requesting a {@link NodeReport} for all nodes.</p>
 * 
 * <p>The <code>NodeReport</code> contains per-node information such as 
 * available resources, number of containers, tracking url, rack name, health
 * status etc.
 * 
 * @see NodeReport
 * @see ApplicationClientProtocol#getClusterNodes(GetClusterNodesRequest)
 */
GetContainerReportRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetContainerReportRequest.java)/**
 * <p>
 * The request sent by a client to the <code>ResourceManager</code> to get an
 * {@link ContainerReport} for a container.
 * </p>
 */
GetContainerReportResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetContainerReportResponse.java)/**
 * <p>
 * The response sent by the <code>ResourceManager</code> to a client requesting
 * a container report.
 * </p>
 * 
 * <p>
 * The response includes a {@link ContainerReport} which has details of a
 * container.
 * </p>
 * 
 */
GetContainersRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetContainersRequest.java)/**
 * <p>
 * The request from clients to get a list of container reports, which belong to
 * an application attempt from the <code>ResourceManager</code>.
 * </p>
 * 
 * @see ApplicationHistoryProtocol#getContainers(GetContainersRequest)
 */
GetContainersResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetContainersResponse.java)/**
 * <p>
 * The response sent by the <code>ResourceManager</code> to a client requesting
 * a list of {@link ContainerReport} for containers.
 * </p>
 * 
 * <p>
 * The <code>ContainerReport</code> for each container includes the container
 * details.
 * </p>
 * 
 * @see ContainerReport
 * @see ApplicationHistoryProtocol#getContainers(GetContainersRequest)
 */
GetContainerStatusesRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetContainerStatusesRequest.java)/**
 * The request sent by the <code>ApplicationMaster</code> to the
 * <code>NodeManager</code> to get {@link ContainerStatus} of requested
 * containers.
 * 
 * @see ContainerManagementProtocol#getContainerStatuses(GetContainerStatusesRequest)
 */
GetContainerStatusesResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetContainerStatusesResponse.java)/**
 * The response sent by the <code>NodeManager</code> to the
 * <code>ApplicationMaster</code> when asked to obtain the
 * <code>ContainerStatus</code> of requested containers.
 * 
 * @see ContainerManagementProtocol#getContainerStatuses(GetContainerStatusesRequest)
 */
GetDelegationTokenRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetDelegationTokenRequest.java)/**
 * The request issued by the client to get a delegation token from
 * the {@code ResourceManager}.
 * for more information.
 */
GetDelegationTokenResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetDelegationTokenResponse.java)/**
 * Response to a {@link GetDelegationTokenRequest} request 
 * from the client. The response contains the token that 
 * can be used by the containers to talk to  ClientRMService.
 *
 */
GetLocalizationStatusesRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetLocalizationStatusesRequest.java)/**
 * The request sent by an application master to the node manager to get
 * {@link LocalizationStatus}es of containers.
 *
 * @see ContainerManagementProtocol#getLocalizationStatuses(
 *        GetLocalizationStatusesRequest)
 */
GetLocalizationStatusesResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetLocalizationStatusesResponse.java)/**
 * The response sent by the node manager to an application master when
 * localization statuses are requested.
 *
 * @see ContainerManagementProtocol#getLocalizationStatuses(
 *        GetLocalizationStatusesRequest)
 */
GetNewApplicationRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetNewApplicationRequest.java)/**
 * <p>The request sent by clients to get a new {@link ApplicationId} for
 * submitting an application.</p>
 * 
 * <p>Currently, this is empty.</p>
 * 
 * @see ApplicationClientProtocol#getNewApplication(GetNewApplicationRequest)
 */
GetNewApplicationResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetNewApplicationResponse.java)/**
 * <p>The response sent by the <code>ResourceManager</code> to the client for 
 * a request to get a new {@link ApplicationId} for submitting applications.</p>
 * 
 * <p>Clients can submit an application with the returned
 * {@link ApplicationId}.</p>
 *
 * @see ApplicationClientProtocol#getNewApplication(GetNewApplicationRequest)
 */
GetNewReservationRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetNewReservationRequest.java)/**
 * <p>The request sent by clients to get a new {@code ReservationId} for
 * submitting an reservation.</p>
 *
 * {@code ApplicationClientProtocol#getNewReservation(GetNewReservationRequest)}
 */
GetNewReservationResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetNewReservationResponse.java)/**
 * <p>The response sent by the <code>ResourceManager</code> to the client for
 * a request to get a new {@link ReservationId} for submitting reservations.</p>
 *
 * <p>Clients can submit an reservation with the returned
 * {@link ReservationId}.</p>
 *
 * {@code ApplicationClientProtocol#getNewReservation(GetNewReservationRequest)}
 */
GetNodesToAttributesRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetNodesToAttributesRequest.java)/**
 * <p>
 * The request from clients to get nodes to attributes mapping
 * in the cluster from the <code>ResourceManager</code>.
 * </p>
 *
 * @see ApplicationClientProtocol#getNodesToAttributes
 * (GetNodesToAttributesRequest)
 */
GetNodesToAttributesResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetNodesToAttributesResponse.java)/**
 * <p>
 * The response sent by the <code>ResourceManager</code> to a client requesting
 * nodes to attributes mapping.
 * </p>
 *
 * @see ApplicationClientProtocol#getNodesToAttributes
 * (GetNodesToAttributesRequest)
 */
GetPluginInfoRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetPluginInfoRequest.java)/**
 * Get plugin info request.
 */
GetPluginInfoResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetPluginInfoResponse.java)/**
 * Get plugin info response.
 */
GetQueueInfoRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetQueueInfoRequest.java)/**
 * <p>The request sent by clients to get <em>queue information</em>
 * from the <code>ResourceManager</code>.</p>
 *
 * @see ApplicationClientProtocol#getQueueInfo(GetQueueInfoRequest)
 */
GetQueueInfoResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetQueueInfoResponse.java)/**
 * The response sent by the {@code ResourceManager} to a client
 * requesting information about queues in the system.
 * <p>
 * The response includes a {@link QueueInfo} which has details such as
 * queue name, used/total capacities, running applications, child queues etc.
 * 
 * @see QueueInfo
 * @see ApplicationClientProtocol#getQueueInfo(GetQueueInfoRequest)
 */
GetQueueUserAclsInfoRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetQueueUserAclsInfoRequest.java)/**
 * <p>The request sent by clients to the <code>ResourceManager</code> to 
 * get queue acls for the <em>current user</em>.</p>
 *
 * <p>Currently, this is empty.</p>
 * 
 * @see ApplicationClientProtocol#getQueueUserAcls(GetQueueUserAclsInfoRequest)
 */
GetQueueUserAclsInfoResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetQueueUserAclsInfoResponse.java)/**
 * <p>The response sent by the <code>ResourceManager</code> to clients
 * seeking queue acls for the user.</p>
 *
 * <p>The response contains a list of {@link QueueUserACLInfo} which
 * provides information about {@link QueueACL} per queue.</p>
 * 
 * @see QueueACL
 * @see QueueUserACLInfo
 * @see ApplicationClientProtocol#getQueueUserAcls(GetQueueUserAclsInfoRequest)
 */
GetResourceProfileRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetResourceProfileRequest.java)/**
 * Request class for getting the details for a particular resource profile.
 */
GetResourceProfileResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/GetResourceProfileResponse.java)/**
 * Response class for getting the details for a particular resource profile.
 */
IncreaseContainersResourceRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/IncreaseContainersResourceRequest.java)/**
 * <p>The request sent by <code>Application Master</code> to the
 * <code>Node Manager</code> to change the resource quota of a container.</p>
 *
 * @see ContainerManagementProtocol#increaseContainersResource(IncreaseContainersResourceRequest)
 */
IncreaseContainersResourceResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/IncreaseContainersResourceResponse.java)/**
 * <p>
 * The response sent by the <code>NodeManager</code> to the
 * <code>ApplicationMaster</code> when asked to increase container resource.
 * </p>
 *
 * @see ContainerManagementProtocol#increaseContainersResource(IncreaseContainersResourceRequest)
 */
KillApplicationRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/KillApplicationRequest.java)/**
 * <p>The request sent by the client to the <code>ResourceManager</code>
 * to abort a submitted application.</p>
 * 
 * <p>The request includes the {@link ApplicationId} of the application to be
 * aborted.</p>
 * 
 * @see ApplicationClientProtocol#forceKillApplication(KillApplicationRequest)
 */
KillApplicationResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/KillApplicationResponse.java)/**
 * The response sent by the <code>ResourceManager</code> to the client aborting
 * a submitted application.
 * <p>
 * The response, includes:
 * <ul>
 *   <li>
 *     A flag which indicates that the process of killing the application is
 *     completed or not.
 *   </li>
 * </ul>
 * Note: user is recommended to wait until this flag becomes true, otherwise if
 * the <code>ResourceManager</code> crashes before the process of killing the
 * application is completed, the <code>ResourceManager</code> may retry this
 * application on recovery.
 * 
 * @see ApplicationClientProtocol#forceKillApplication(KillApplicationRequest)
 */
MoveApplicationAcrossQueuesRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/MoveApplicationAcrossQueuesRequest.java)/**
 * <p>The request sent by the client to the <code>ResourceManager</code>
 * to move a submitted application to a different queue.</p>
 * 
 * <p>The request includes the {@link ApplicationId} of the application to be
 * moved and the queue to place it in.</p>
 * 
 * @see ApplicationClientProtocol#moveApplicationAcrossQueues(MoveApplicationAcrossQueuesRequest)
 */
MoveApplicationAcrossQueuesResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/MoveApplicationAcrossQueuesResponse.java)/**
 * <p>
 * The response sent by the <code>ResourceManager</code> to the client moving
 * a submitted application to a different queue.
 * </p>
 * <p>
 * A response without exception means that the move has completed successfully.
 * </p>
 * 
 * @see ApplicationClientProtocol#moveApplicationAcrossQueues(MoveApplicationAcrossQueuesRequest)
 */
NodePublishVolumeRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/NodePublishVolumeRequest.java)/**
 * The request sent by node manager to CSI driver adaptor
 * to publish a volume on a node.
 */
NodePublishVolumeResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/NodePublishVolumeResponse.java)/**
 * The response sent by a CSI driver adaptor to the node manager
 * after publishing a volume on the node.
 */
NodeUnpublishVolumeRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/NodeUnpublishVolumeRequest.java)/**
 * The request sent by node manager to CSI driver adaptor
 * to un-publish a volume on a node.
 */
NodeUnpublishVolumeResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/NodeUnpublishVolumeResponse.java)/**
 * The response sent by a CSI driver adaptor to the node manager
 * after un-publishing a volume on the node.
 */
RegisterApplicationMasterRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/RegisterApplicationMasterRequest.java)/**
 * The request sent by the {@code ApplicationMaster} to {@code ResourceManager}
 * on registration.
 * <p>
 * The registration includes details such as:
 * <ul>
 *   <li>Hostname on which the AM is running.</li>
 *   <li>RPC Port</li>
 *   <li>Tracking URL</li>
 * </ul>
 * 
 * @see ApplicationMasterProtocol#registerApplicationMaster(RegisterApplicationMasterRequest)
 */
RegisterApplicationMasterResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/RegisterApplicationMasterResponse.java)/**
 * The response sent by the {@code ResourceManager} to a new
 * {@code ApplicationMaster} on registration.
 * <p>
 * The response contains critical details such as:
 * <ul>
 *   <li>Maximum capability for allocated resources in the cluster.</li>
 *   <li>{@code ApplicationACL}s for the application.</li>
 *   <li>ClientToAMToken master key.</li>
 * </ul>
 * 
 * @see ApplicationMasterProtocol#registerApplicationMaster(RegisterApplicationMasterRequest)
 */
ReInitializeContainerRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ReInitializeContainerRequest.java)/**
 * This encapsulates all the required fields needed for a Container
 * ReInitialization.
 */
ReInitializeContainerResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ReInitializeContainerResponse.java)/**
 * The response to the {@link ReInitializeContainerRequest}.
 */
ReleaseSharedCacheResourceRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ReleaseSharedCacheResourceRequest.java)/**
 * <p>The request from clients to release a resource in the shared cache.</p>
 */
ReleaseSharedCacheResourceResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ReleaseSharedCacheResourceResponse.java)/**
 * <p>
 * The response to clients from the <code>SharedCacheManager</code> when
 * releasing a resource in the shared cache.
 * </p>
 *
 * <p>
 * Currently, this is empty.
 * </p>
 */
RenewDelegationTokenRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/RenewDelegationTokenRequest.java)/**
 * The request issued by the client to renew a delegation token from
 * the {@code ResourceManager}.
 */
RenewDelegationTokenResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/RenewDelegationTokenResponse.java)/**
 * The response to a renewDelegationToken call to the {@code ResourceManager}.
 */
ReservationDeleteRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ReservationDeleteRequest.java)/**
 * {@link ReservationDeleteRequest} captures the set of requirements the user
 * has to delete an existing reservation.
 * 
 * @see ReservationDefinition
 * 
 */
ReservationDeleteResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ReservationDeleteResponse.java)/**
 * {@link ReservationDeleteResponse} contains the answer of the admission
 * control system in the {@code ResourceManager} to a reservation delete
 * operation. Currently response is empty if the operation was successful, if
 * not an exception reporting reason for a failure.
 * 
 * @see ReservationDefinition
 * 
 */
ReservationListRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ReservationListRequest.java)/**
 * {@link ReservationListRequest} captures the set of requirements the
 * user has to list reservations.
 */
ReservationListResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ReservationListResponse.java)/**
 * {@link ReservationListResponse} captures the list of reservations that the
 * user has queried.
 *
 * The resulting list of {@link ReservationAllocationState} contains a list of
 * {@code ResourceAllocationRequest} representing the current state of the
 * reservation resource allocations will be returned. This is subject to change
 * in the event of re-planning a described by {@code ReservationDefinition}
 *
 * @see ReservationAllocationState
 *
 */
ReservationSubmissionRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ReservationSubmissionRequest.java)/**
 * {@link ReservationSubmissionRequest} captures the set of requirements the
 * user has to create a reservation.
 * 
 * @see ReservationDefinition
 * 
 */
ReservationSubmissionResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ReservationSubmissionResponse.java)/**
 * <p>The response sent by the <code>ResourceManager</code> to a client on
 * reservation submission.</p>
 *
 * <p>Currently, this is empty.</p>
 *
 * {@code ApplicationClientProtocol#submitReservation(
 * ReservationSubmissionRequest)}
 *
 */
ReservationUpdateRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ReservationUpdateRequest.java)/**
 * {@link ReservationUpdateRequest} captures the set of requirements the user
 * has to update an existing reservation.
 * 
 * @see ReservationDefinition
 * 
 */
ReservationUpdateResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ReservationUpdateResponse.java)/**
 * {@link ReservationUpdateResponse} contains the answer of the admission
 * control system in the {@code ResourceManager} to a reservation update
 * operation. Currently response is empty if the operation was successful, if
 * not an exception reporting reason for a failure.
 * 
 * @see ReservationDefinition
 * 
 */
ResourceLocalizationRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ResourceLocalizationRequest.java)/**
 * The request sent by the ApplicationMaster to ask for localizing resources.
 */
ResourceLocalizationResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ResourceLocalizationResponse.java)/**
 * The response to the {@link ResourceLocalizationRequest}
 */
RestartContainerResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/RestartContainerResponse.java)/**
 * The response to a restart Container request.
 */
RollbackResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/RollbackResponse.java)/**
 * Response to a Rollback request.
 */
SignalContainerRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/SignalContainerRequest.java)/**
 * <p>The request sent by the client to the <code>ResourceManager</code>
 * or by the <code>ApplicationMaster</code> to the <code>NodeManager</code>
 * to signal a container.
 * @see SignalContainerCommand </p>
 */
SignalContainerResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/SignalContainerResponse.java)/**
 * <p>The response sent by the <code>ResourceManager</code> to the client
 * signalling a container.</p>
 *
 * <p>Currently it's empty.</p>
 *
 * @see ApplicationClientProtocol#signalToContainer(SignalContainerRequest)
 */
StartContainerRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/StartContainerRequest.java)/**
 * <p>The request sent by the <code>ApplicationMaster</code> to the
 * <code>NodeManager</code> to <em>start</em> a container.</p>
 * 
 * <p>The <code>ApplicationMaster</code> has to provide details such as
 * allocated resource capability, security tokens (if enabled), command
 * to be executed to start the container, environment for the process, 
 * necessary binaries/jar/shared-objects etc. via the 
 * {@link ContainerLaunchContext}.</p>
 *
 * @see ContainerManagementProtocol#startContainers(StartContainersRequest)
 */
StartContainersRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/StartContainersRequest.java)/**
 * <p>
 * The request which contains a list of {@link StartContainerRequest} sent by
 * the <code>ApplicationMaster</code> to the <code>NodeManager</code> to
 * <em>start</em> containers.
 * </p>
 * 
 * <p>
 * In each {@link StartContainerRequest}, the <code>ApplicationMaster</code> has
 * to provide details such as allocated resource capability, security tokens (if
 * enabled), command to be executed to start the container, environment for the
 * process, necessary binaries/jar/shared-objects etc. via the
 * {@link ContainerLaunchContext}.
 * </p>
 * 
 * @see ContainerManagementProtocol#startContainers(StartContainersRequest)
 */
StartContainersResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/StartContainersResponse.java)/**
 * <p>
 * The response sent by the <code>NodeManager</code> to the
 * <code>ApplicationMaster</code> when asked to <em>start</em> an allocated
 * container.
 * </p>
 * 
 * @see ContainerManagementProtocol#startContainers(StartContainersRequest)
 */
StopContainersRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/StopContainersRequest.java)/**
 * <p>The request sent by the <code>ApplicationMaster</code> to the
 * <code>NodeManager</code> to <em>stop</em> containers.</p>
 * 
 * @see ContainerManagementProtocol#stopContainers(StopContainersRequest)
 */
StopContainersResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/StopContainersResponse.java)/**
 * <p>
 * The response sent by the <code>NodeManager</code> to the
 * <code>ApplicationMaster</code> when asked to <em>stop</em> allocated
 * containers.
 * </p>
 * 
 * @see ContainerManagementProtocol#stopContainers(StopContainersRequest)
 */
SubmitApplicationRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/SubmitApplicationRequest.java)/**
 * <p>The request sent by a client to <em>submit an application</em> to the 
 * <code>ResourceManager</code>.</p>
 * 
 * <p>The request, via {@link ApplicationSubmissionContext}, contains
 * details such as queue, {@link Resource} required to run the 
 * <code>ApplicationMaster</code>, the equivalent of 
 * {@link ContainerLaunchContext} for launching the 
 * <code>ApplicationMaster</code> etc.
 * 
 * @see ApplicationClientProtocol#submitApplication(SubmitApplicationRequest)
 */
SubmitApplicationResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/SubmitApplicationResponse.java)/**
 * <p>The response sent by the <code>ResourceManager</code> to a client on
 * application submission.</p>
 * 
 * <p>Currently, this is empty.</p>
 * 
 * @see ApplicationClientProtocol#submitApplication(SubmitApplicationRequest)
 */
UpdateApplicationTimeoutsRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/UpdateApplicationTimeoutsRequest.java)/**
 * <p>
 * The request sent by the client to the <code>ResourceManager</code> to set or
 * update the application timeout.
 * </p>
 * <p>
 * The request includes the {@link ApplicationId} of the application and timeout
 * to be set for an application
 * </p>
 */
UpdateApplicationTimeoutsResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/UpdateApplicationTimeoutsResponse.java)/**
 * <p>
 * The response sent by the <code>ResourceManager</code> to the client on update
 * application timeout.
 * </p>
 * <p>
 * A response without exception means that the update has completed
 * successfully.
 * </p>
 */
UseSharedCacheResourceRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/UseSharedCacheResourceRequest.java)/**
 * <p>
 * The request from clients to the <code>SharedCacheManager</code> that claims a
 * resource in the shared cache.
 * </p>
 */
UseSharedCacheResourceResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/UseSharedCacheResourceResponse.java)/**
 * <p>
 * The response from the SharedCacheManager to the client that indicates whether
 * a requested resource exists in the cache.
 * </p>
 */
VolumeCapability (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ValidateVolumeCapabilitiesRequest.java)/**
   * Volume capability.
   */
ValidateVolumeCapabilitiesRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ValidateVolumeCapabilitiesRequest.java)/**
 * YARN internal message used to validate volume capabilities
 * with a CSI driver controller plugin.
 */
ValidateVolumeCapabilitiesResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/ValidateVolumeCapabilitiesResponse.java)/**
 * YARN internal message used to represent the response of
 * volume capabilities validation with a CSI driver controller plugin.
 */
ApplicationAttemptId (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ApplicationAttemptId.java)/**
 * <p><code>ApplicationAttemptId</code> denotes the particular <em>attempt</em>
 * of an <code>ApplicationMaster</code> for a given {@link ApplicationId}.</p>
 * 
 * <p>Multiple attempts might be needed to run an application to completion due
 * to temporal failures of the <code>ApplicationMaster</code> such as hardware
 * failures, connectivity issues etc. on the node on which it was scheduled.</p>
 */
ApplicationAttemptReport (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ApplicationAttemptReport.java)/**
 * {@code ApplicationAttemptReport} is a report of an application attempt.
 * <p>
 * It includes details such as:
 * <ul>
 *   <li>{@link ApplicationAttemptId} of the application.</li>
 *   <li>Host on which the <code>ApplicationMaster</code> of this attempt is
 *   running.</li>
 *   <li>RPC port of the <code>ApplicationMaster</code> of this attempt.</li>
 *   <li>Tracking URL.</li>
 *   <li>Diagnostic information in case of errors.</li>
 *   <li>{@link YarnApplicationAttemptState} of the application attempt.</li>
 *   <li>{@link ContainerId} of the master Container.</li>
 * </ul>
 */
ApplicationId (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ApplicationId.java)/**
 * <p><code>ApplicationId</code> represents the <em>globally unique</em> 
 * identifier for an application.</p>
 * 
 * <p>The globally unique nature of the identifier is achieved by using the 
 * <em>cluster timestamp</em> i.e. start-time of the 
 * <code>ResourceManager</code> along with a monotonically increasing counter
 * for the application.</p>
 */
ApplicationReport (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ApplicationReport.java)/**
 * {@code ApplicationReport} is a report of an application.
 * <p>
 * It includes details such as:
 * <ul>
 *   <li>{@link ApplicationId} of the application.</li>
 *   <li>Applications user.</li>
 *   <li>Application queue.</li>
 *   <li>Application name.</li>
 *   <li>Host on which the <code>ApplicationMaster</code> is running.</li>
 *   <li>RPC port of the <code>ApplicationMaster</code>.</li>
 *   <li>Tracking URL.</li>
 *   <li>{@link YarnApplicationState} of the application.</li>
 *   <li>Diagnostic information in case of errors.</li>
 *   <li>Start time of the application.</li>
 *   <li>Client {@link Token} of the application (if security is enabled).</li>
 * </ul>
 *
 * @see ApplicationClientProtocol#getApplicationReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationReportRequest)
 */
ApplicationResourceUsageReport (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ApplicationResourceUsageReport.java)/**
 * Contains various scheduling metrics to be reported by UI and CLI.
 */
ApplicationSubmissionContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ApplicationSubmissionContext.java)/**
 * {@code ApplicationSubmissionContext} represents all of the
 * information needed by the {@code ResourceManager} to launch
 * the {@code ApplicationMaster} for an application.
 * <p>
 * It includes details such as:
 * <ul>
 *   <li>{@link ApplicationId} of the application.</li>
 *   <li>Application user.</li>
 *   <li>Application name.</li>
 *   <li>{@link Priority} of the application.</li>
 *   <li>
 *     {@link ContainerLaunchContext} of the container in which the
 *     <code>ApplicationMaster</code> is executed.
 *   </li>
 *   <li>
 *     maxAppAttempts. The maximum number of application attempts.
 *     It should be no larger than the global number of max attempts in the
 *     YARN configuration.
 *   </li>
 *   <li>
 *     attemptFailuresValidityInterval. The default value is -1.
 *     when attemptFailuresValidityInterval in milliseconds is set to
 *     {@literal >} 0, the failure number will no take failures which happen
 *     out of the validityInterval into failure count. If failure count
 *     reaches to maxAppAttempts, the application will be failed.
 *   </li>
 *   <li>Optional, application-specific {@link LogAggregationContext}</li>
 * </ul>
 * 
 * @see ContainerLaunchContext
 * @see ApplicationClientProtocol#submitApplication(org.apache.hadoop.yarn.api.protocolrecords.SubmitApplicationRequest)
 */
ApplicationTimeout (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ApplicationTimeout.java)/**
 * {@code ApplicationTimeout} is a report for configured application timeouts.
 * It includes details such as:
 * <ul>
 * <li>{@link ApplicationTimeoutType} of the timeout type.</li>
 * <li>Expiry time in ISO8601 standard with format
 * <b>yyyy-MM-dd'T'HH:mm:ss.SSSZ</b> or "UNLIMITED".</li>
 * <li>Remaining time in seconds.</li>
 * </ul>
 * The possible values for {ExpiryTime, RemainingTimeInSeconds} are
 * <ul>
 * <li>{UNLIMITED,-1} : Timeout is not configured for given timeout type
 * (LIFETIME).</li>
 * <li>{ISO8601 date string, 0} : Timeout is configured and application has
 * completed.</li>
 * <li>{ISO8601 date string, greater than zero} : Timeout is configured and
 * application is RUNNING. Application will be timed out after configured
 * value.</li>
 * </ul>
 */
CollectorInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/CollectorInfo.java)/**
 * Collector info containing collector address and collector token passed from
 * RM to AM in Allocate Response.
 */
Container (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/Container.java)/**
 * {@code Container} represents an allocated resource in the cluster.
 * <p>
 * The {@code ResourceManager} is the sole authority to allocate any
 * {@code Container} to applications. The allocated {@code Container}
 * is always on a single node and has a unique {@link ContainerId}. It has
 * a specific amount of {@link Resource} allocated.
 * <p>
 * It includes details such as:
 * <ul>
 *   <li>{@link ContainerId} for the container, which is globally unique.</li>
 *   <li>
 *     {@link NodeId} of the node on which it is allocated.
 *   </li>
 *   <li>HTTP uri of the node.</li>
 *   <li>{@link Resource} allocated to the container.</li>
 *   <li>{@link Priority} at which the container was allocated.</li>
 *   <li>
 *     Container {@link Token} of the container, used to securely verify
 *     authenticity of the allocation.
 *   </li>
 * </ul>
 * 
 * Typically, an {@code ApplicationMaster} receives the {@code Container}
 * from the {@code ResourceManager} during resource-negotiation and then
 * talks to the {@code NodeManager} to start/stop containers.
 * 
 * @see ApplicationMasterProtocol#allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)
 * @see ContainerManagementProtocol#startContainers(org.apache.hadoop.yarn.api.protocolrecords.StartContainersRequest)
 * @see ContainerManagementProtocol#stopContainers(org.apache.hadoop.yarn.api.protocolrecords.StopContainersRequest)
 */
ContainerExitStatus (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ContainerExitStatus.java)/**
 * Container exit statuses indicating special exit circumstances.
 */
ContainerId (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ContainerId.java)/**
 * <p><code>ContainerId</code> represents a globally unique identifier
 * for a {@link Container} in the cluster.</p>
 */
ContainerLaunchContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ContainerLaunchContext.java)/**
 * {@code ContainerLaunchContext} represents all of the information
 * needed by the {@code NodeManager} to launch a container.
 * <p>
 * It includes details such as:
 * <ul>
 *   <li>{@link ContainerId} of the container.</li>
 *   <li>{@link Resource} allocated to the container.</li>
 *   <li>User to whom the container is allocated.</li>
 *   <li>Security tokens (if security is enabled).</li>
 *   <li>
 *     {@link LocalResource} necessary for running the container such
 *     as binaries, jar, shared-objects, side-files etc.
 *   </li>
 *   <li>Optional, application-specific binary service data.</li>
 *   <li>Environment variables for the launched process.</li>
 *   <li>Command to launch the container.</li>
 *   <li>Retry strategy when container exits with failure.</li>
 * </ul>
 * 
 * @see ContainerManagementProtocol#startContainers(org.apache.hadoop.yarn.api.protocolrecords.StartContainersRequest)
 */
ContainerRetryContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ContainerRetryContext.java)/**
 * {@code ContainerRetryContext} indicates how container retry after it fails
 * to run.
 * <p>
 * It provides details such as:
 * <ul>
 *   <li>
 *     {@link ContainerRetryPolicy} :
 *     - NEVER_RETRY(DEFAULT value): no matter what error code is when container
 *       fails to run, just do not retry.
 *     - RETRY_ON_ALL_ERRORS: no matter what error code is, when container fails
 *       to run, just retry.
 *     - RETRY_ON_SPECIFIC_ERROR_CODES: when container fails to run, do retry if
 *       the error code is one of <em>errorCodes</em>, otherwise do not retry.
 *
 *     Note: if error code is 137(SIGKILL) or 143(SIGTERM), it will not retry
 *     because it is usually killed on purpose.
 *   </li>
 *   <li>
 *     <em>maxRetries</em> specifies how many times to retry if need to retry.
 *     If the value is -1, it means retry forever.
 *   </li>
 *   <li><em>retryInterval</em> specifies delaying some time before relaunch
 *   container, the unit is millisecond.</li>
 *   <li>
 *     <em>failuresValidityInterval</em>: default value is -1.
 *     When failuresValidityInterval in milliseconds is set to {@literal >} 0,
 *     the failure number will not take failures which happen out of the
 *     failuresValidityInterval into failure count. If failure count
 *     reaches to <em>maxRetries</em>, the container will be failed.
 *   </li>
 * </ul>
 */
ContainerStatus (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ContainerStatus.java)/**
 * {@code ContainerStatus} represents the current status of a
 * {@code Container}.
 * <p>
 * It provides details such as:
 * <ul>
 *   <li>{@code ContainerId} of the container.</li>
 *   <li>{@code ExecutionType} of the container.</li>
 *   <li>{@code ContainerState} of the container.</li>
 *   <li><em>Exit status</em> of a completed container.</li>
 *   <li><em>Diagnostic</em> message for a failed container.</li>
 *   <li>{@link Resource} allocated to the container.</li>
 * </ul>
 */
ExecutionTypeRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ExecutionTypeRequest.java)/**
 * An object of this class represents a specification of the execution
 * guarantee of the Containers associated with a ResourceRequest. It consists
 * of an <code>ExecutionType</code> as well as flag that explicitly asks the
 * configuredScheduler to return Containers of exactly the Execution Type
 * requested.
 */
LightWeightResource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/impl/LightWeightResource.java)/**
 * <p>
 * <code>LightWeightResource</code> extends Resource to handle base resources such
 * as memory and CPU.
 * TODO: We have a long term plan to use AbstractResource when additional
 * resource types are to be handled as well.
 * This will be used to speed up internal calculation to avoid creating
 * costly PB-backed Resource object: <code>ResourcePBImpl</code>
 * </p>
 *
 * <p>
 * Currently it models both <em>memory</em> and <em>CPU</em>.
 * </p>
 *
 * <p>
 * The unit for memory is megabytes. CPU is modeled with virtual cores (vcores),
 * a unit for expressing parallelism. A node's capacity should be configured
 * with virtual cores equal to its number of physical cores. A container should
 * be requested with the number of cores it can saturate, i.e. the average
 * number of threads it expects to have runnable at a time.
 * </p>
 *
 * <p>
 * Virtual cores take integer values and thus currently CPU-scheduling is very
 * coarse. A complementary axis for CPU requests that represents processing
 * power will likely be added in the future to enable finer-grained resource
 * configuration.
 * </p>
 *
 * @see Resource
 */
LocalizationStatus (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/LocalizationStatus.java)/**
 * Represents the localization status of a resource.
 * The status of the localization includes:
 * <ul>
 *   <li>resource key</li>
 *   <li>{@link LocalizationState} of the resource</li>
 * </ul>
 */
LocalResource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/LocalResource.java)/**
 * <p><code>LocalResource</code> represents a local resource required to
 * run a container.</p>
 * 
 * <p>The <code>NodeManager</code> is responsible for localizing the resource 
 * prior to launching the container.</p>
 * 
 * <p>Applications can specify {@link LocalResourceType} and 
 * {@link LocalResourceVisibility}.</p>
 * 
 * @see LocalResourceType
 * @see LocalResourceVisibility
 * @see ContainerLaunchContext
 * @see ApplicationSubmissionContext
 * @see ContainerManagementProtocol#startContainers(org.apache.hadoop.yarn.api.protocolrecords.StartContainersRequest)
 */
NMToken (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/NMToken.java)/**
 * <p>The NMToken is used for authenticating communication with
 * <code>NodeManager</code></p>
 * <p>It is issued by <code>ResourceMananger</code> when <code>ApplicationMaster</code>
 * negotiates resource with <code>ResourceManager</code> and
 * validated on <code>NodeManager</code> side.</p>
 * @see  AllocateResponse#getNMTokens()
 */
NodeAttribute (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/NodeAttribute.java)/**
 * <p>
 * Node Attribute is a kind of a label which represents one of the
 * attribute/feature of a Node. Its different from node partition label as
 * resource guarantees across the queues will not be maintained for these type
 * of labels.
 * </p>
 * <p>
 * A given Node can be mapped with any kind of attribute, few examples are
 * HAS_SSD=true, JAVA_VERSION=JDK1.8, OS_TYPE=WINDOWS.
 * </p>
 * <p>
 * Its not compulsory for all the attributes to have value, empty string is the
 * default value of the <code>NodeAttributeType.STRING</code>
 * </p>
 * <p>
 * Node Attribute Prefix is used as namespace to segregate the attributes.
 * </p>
 */
NodeAttributeInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/NodeAttributeInfo.java)/**
 * <p>
 * Node Attribute Info describes a NodeAttribute.
 * </p>
 */
NodeAttributeKey (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/NodeAttributeKey.java)/**
 * <p>
 * Node AttributeKey uniquely identifies a given Node Attribute. Node Attribute
 * is identified based on attribute prefix and name.
 * </p>
 * <p>
 * Node Attribute Prefix is used as namespace to segregate the attributes.
 * </p>
 */
NodeId (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/NodeId.java)/**
 * <p><code>NodeId</code> is the unique identifier for a node.</p>
 * 
 * <p>It includes the <em>hostname</em> and <em>port</em> to uniquely 
 * identify the node. Thus, it is unique across restarts of any 
 * <code>NodeManager</code>.</p>
 */
NodeReport (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/NodeReport.java)/**
 * {@code NodeReport} is a summary of runtime information of a node
 * in the cluster.
 * <p>
 * It includes details such as:
 * <ul>
 *   <li>{@link NodeId} of the node.</li>
 *   <li>HTTP Tracking URL of the node.</li>
 *   <li>Rack name for the node.</li>
 *   <li>Used {@link Resource} on the node.</li>
 *   <li>Total available {@link Resource} of the node.</li>
 *   <li>Number of running containers on the node.</li>
 * </ul>
 *
 * @see ApplicationClientProtocol#getClusterNodes(org.apache.hadoop.yarn.api.protocolrecords.GetClusterNodesRequest)
 */
NodeToAttributeValue (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/NodeToAttributeValue.java)/**
 * <p>
 * Mapping of Attribute Value to a Node.
 * </p>
 */
PreemptionContainer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/PreemptionContainer.java)/**
 * Specific container requested back by the <code>ResourceManager</code>.
 * @see PreemptionContract
 * @see StrictPreemptionContract
 */
PreemptionContract (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/PreemptionContract.java)/**
 * Description of resources requested back by the <code>ResourceManager</code>.
 * The <code>ApplicationMaster</code> (AM) can satisfy this request according
 * to its own priorities to prevent containers from being forcibly killed by
 * the platform.
 * @see PreemptionMessage
 */
PreemptionMessage (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/PreemptionMessage.java)/**
 * A {@link PreemptionMessage} is part of the RM-AM protocol, and it is used by
 * the RM to specify resources that the RM wants to reclaim from this
 * {@code ApplicationMaster} (AM). The AM receives a {@link
 * StrictPreemptionContract} message encoding which containers the platform may
 * forcibly kill, granting it an opportunity to checkpoint state or adjust its
 * execution plan. The message may also include a {@link PreemptionContract}
 * granting the AM more latitude in selecting which resources to return to the
 * cluster.
 * <p>
 * The AM should decode both parts of the message. The {@link
 * StrictPreemptionContract} specifies particular allocations that the RM
 * requires back. The AM can checkpoint containers' state, adjust its execution
 * plan to move the computation, or take no action and hope that conditions that
 * caused the RM to ask for the container will change.
 * <p>
 * In contrast, the {@link PreemptionContract} also includes a description of
 * resources with a set of containers. If the AM releases containers matching
 * that profile, then the containers enumerated in {@link
 * PreemptionContract#getContainers()} may not be killed.
 * <p>
 * Each preemption message reflects the RM's current understanding of the
 * cluster state, so a request to return <em>N</em> containers may not
 * reflect containers the AM is releasing, recently exited containers the RM has
 * yet to learn about, or new containers allocated before the message was
 * generated. Conversely, an RM may request a different profile of containers in
 * subsequent requests.
 * <p>
 * The policy enforced by the RM is part of the scheduler. Generally, only
 * containers that have been requested consistently should be killed, but the
 * details are not specified.
 */
PreemptionResourceRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/PreemptionResourceRequest.java)/**
 * Description of resources requested back by the cluster.
 * @see PreemptionContract
 * @see AllocateRequest#setAskList(java.util.List)
 */
Priority (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/Priority.java)/**
 * The priority assigned to a ResourceRequest or Application or Container 
 * allocation 
 *
 */
QueueConfigurations (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/QueueConfigurations.java)/**
 * QueueConfigurations contain information about the configuration percentages
 * of a queue.
 * <p>
 * It includes information such as:
 * <ul>
 *   <li>Capacity of the queue.</li>
 *   <li>Absolute capacity of the queue.</li>
 *   <li>Maximum capacity of the queue.</li>
 *   <li>Absolute maximum capacity of the queue.</li>
 *   <li>Maximum ApplicationMaster resource percentage of the queue.</li>
 * </ul>
 */
QueueInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/QueueInfo.java)/**
 * QueueInfo is a report of the runtime information of the queue.
 * <p>
 * It includes information such as:
 * <ul>
 *   <li>Queue name.</li>
 *   <li>Capacity of the queue.</li>
 *   <li>Maximum capacity of the queue.</li>
 *   <li>Current capacity of the queue.</li>
 *   <li>Child queues.</li>
 *   <li>Running applications.</li>
 *   <li>{@link QueueState} of the queue.</li>
 *   <li>{@link QueueConfigurations} of the queue.</li>
 * </ul>
 *
 * @see QueueState
 * @see QueueConfigurations
 * @see ApplicationClientProtocol#getQueueInfo(org.apache.hadoop.yarn.api.protocolrecords.GetQueueInfoRequest)
 */
QueueUserACLInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/QueueUserACLInfo.java)/**
 * <p><code>QueueUserACLInfo</code> provides information {@link QueueACL} for
 * the given user.</p>
 * 
 * @see QueueACL
 * @see ApplicationClientProtocol#getQueueUserAcls(org.apache.hadoop.yarn.api.protocolrecords.GetQueueUserAclsInfoRequest)
 */
RejectedSchedulingRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/RejectedSchedulingRequest.java)/**
 * This encapsulates a Rejected SchedulingRequest. It contains the offending
 * Scheduling Request along with the reason for rejection.
 */
ReservationAllocationState (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ReservationAllocationState.java)/**
 * {@code ReservationAllocationState} represents the reservation that is
 * made by a user.
 * <p>
 * It includes:
 * <ul>
 *   <li>Duration of the reservation.</li>
 *   <li>Acceptance time of the duration.</li>
 *   <li>
 *       List of {@link ResourceAllocationRequest}, which includes the time
 *       interval, and capability of the allocation.
 *       {@code ResourceAllocationRequest} represents an allocation
 *       made for a reservation for the current state of the queue. This can be
 *       changed for reasons such as re-planning, but will always be subject to
 *       the constraints of the user contract as described by
 *       {@link ReservationDefinition}
 *   </li>
 *   <li>{@link ReservationId} of the reservation.</li>
 *   <li>{@link ReservationDefinition} used to make the reservation.</li>
 * </ul>
 *
 * @see ResourceAllocationRequest
 * @see ReservationId
 * @see ReservationDefinition
 */
ReservationDefinition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ReservationDefinition.java)/**
 * {@link ReservationDefinition} captures the set of resource and time
 * constraints the user cares about regarding a reservation.
 * 
 * @see ResourceRequest
 * 
 */
ReservationId (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ReservationId.java)/**
 * <p>
 * {@link ReservationId} represents the <em>globally unique</em> identifier for
 * a reservation.
 * </p>
 *
 * <p>
 * The globally unique nature of the identifier is achieved by using the
 * <em>cluster timestamp</em> i.e. start-time of the {@code ResourceManager}
 * along with a monotonically increasing counter for the reservation.
 * </p>
 */
ReservationRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ReservationRequest.java)/**
 * {@link ReservationRequest} represents the request made by an application to
 * the {@code ResourceManager} to reserve {@link Resource}s.
 * <p>
 * It includes:
 * <ul>
 *   <li>{@link Resource} required for each request.</li>
 *   <li>
 *     Number of containers, of above specifications, which are required by the
 *     application.
 *   </li>
 *   <li>Concurrency that indicates the gang size of the request.</li>
 * </ul>
 */
ReservationRequests (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ReservationRequests.java)/**
 * {@link ReservationRequests} captures the set of resource and constraints the
 * user cares about regarding a reservation.
 * 
 * @see ReservationRequest
 * 
 */
Resource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/Resource.java)/**
 * <p><code>Resource</code> models a set of computer resources in the 
 * cluster.</p>
 * 
 * <p>Currently it models both <em>memory</em> and <em>CPU</em>.</p>
 * 
 * <p>The unit for memory is megabytes. CPU is modeled with virtual cores
 * (vcores), a unit for expressing parallelism. A node's capacity should
 * be configured with virtual cores equal to its number of physical cores. A
 * container should be requested with the number of cores it can saturate, i.e.
 * the average number of threads it expects to have runnable at a time.</p>
 * 
 * <p>Virtual cores take integer values and thus currently CPU-scheduling is
 * very coarse.  A complementary axis for CPU requests that represents
 * processing power will likely be added in the future to enable finer-grained
 * resource configuration.</p>
 *
 * <p>Typically, applications request <code>Resource</code> of suitable
 * capability to run their component tasks.</p>
 * 
 * @see ResourceRequest
 * @see ApplicationMasterProtocol#allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)
 */
ResourceAllocationRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ResourceAllocationRequest.java)/**
 * {@code ResourceAllocationRequest} represents an allocation
 * made for a reservation for the current state of the plan. This can be
 * changed for reasons such as re-planning, but will always be subject to the
 * constraints of the user contract as described by
 * {@link ReservationDefinition}
 * {@link Resource}
 *
 * <p>
 * It includes:
 * <ul>
 *   <li>StartTime of the allocation.</li>
 *   <li>EndTime of the allocation.</li>
 *   <li>{@link Resource} reserved for the allocation.</li>
 * </ul>
 *
 * @see Resource
 */
ResourceBlacklistRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ResourceBlacklistRequest.java)/**
 * {@link ResourceBlacklistRequest} encapsulates the list of resource-names 
 * which should be added or removed from the <em>blacklist</em> of resources 
 * for the application.
 * 
 * @see ResourceRequest
 * @see ApplicationMasterProtocol#allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)
 */
ResourceInformation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ResourceInformation.java)/**
 * Class to encapsulate information about a Resource - the name of the resource,
 * the units(milli, micro, etc), the type(countable), and the value.
 */
ResourceRequestBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ResourceRequest.java)/**
   * Class to construct instances of {@link ResourceRequest} with specific
   * options.
   */
ResourceRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ResourceRequest.java)/**
 * {@code ResourceRequest} represents the request made
 * by an application to the {@code ResourceManager}
 * to obtain various {@code Container} allocations.
 * <p>
 * It includes:
 * <ul>
 *   <li>{@link Priority} of the request.</li>
 *   <li>
 *     The <em>name</em> of the host or rack on which the allocation is
 *     desired. A special value of <em>*</em> signifies that
 *     <em>any</em> host/rack is acceptable to the application.
 *   </li>
 *   <li>{@link Resource} required for each request.</li>
 *   <li>
 *     Number of containers, of above specifications, which are required
 *     by the application.
 *   </li>
 *   <li>
 *     A boolean <em>relaxLocality</em> flag, defaulting to {@code true},
 *     which tells the {@code ResourceManager} if the application wants
 *     locality to be loose (i.e. allows fall-through to rack or <em>any</em>)
 *     or strict (i.e. specify hard constraint on resource allocation).
 *   </li>
 * </ul>
 * 
 * @see Resource
 * @see ApplicationMasterProtocol#allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)
 */
ResourceSizing (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ResourceSizing.java)/**
 * {@code ResourceSizing} contains information for the size of a
 * {@link SchedulingRequest}, such as the number of requested allocations and
 * the resources for each allocation.
 */
ResourceTypeInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ResourceTypeInfo.java)/**
 * Class to encapsulate information about a ResourceType - the name of the
 * resource, the units(milli, micro, etc), the type(countable).
 */
ResourceUtilization (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ResourceUtilization.java)/**
 * <p>
 * <code>ResourceUtilization</code> models the utilization of a set of computer
 * resources in the cluster.
 * </p>
 */
SchedulingRequestBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/SchedulingRequest.java)/**
   * Class to construct instances of {@link SchedulingRequest} with specific
   * options.
   */
SchedulingRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/SchedulingRequest.java)/**
 * {@code SchedulingRequest} represents a request made by an application to the
 * {@code ResourceManager} to obtain an allocation. It is similar to the
 * {@link ResourceRequest}. However, it is more complete than the latter, as it
 * allows applications to specify allocation tags (e.g., to express that an
 * allocation belongs to {@code Spark} or is an {@code HBase-master}), as well
 * as involved {@link PlacementConstraint}s (e.g., anti-affinity between Spark
 * and HBase allocations).
 *
 * The size specification of the allocation is in {@code ResourceSizing}.
 */
StrictPreemptionContract (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/StrictPreemptionContract.java)/**
 * Enumeration of particular allocations to be reclaimed. The platform will
 * reclaim exactly these resources, so the <code>ApplicationMaster</code> (AM)
 * may attempt to checkpoint work or adjust its execution plan to accommodate
 * it. In contrast to {@link PreemptionContract}, the AM has no flexibility in
 * selecting which resources to return to the cluster.
 * @see PreemptionMessage
 */
TimelineDelegationTokenResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timeline/TimelineDelegationTokenResponse.java)/**
 * The response of delegation token related request
 */
TimelineDomain (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timeline/TimelineDomain.java)/**
 * <p>
 * This class contains the information about a timeline domain, which is used
 * to a user to host a number of timeline entities, isolating them from others'.
 * The user can also define the reader and writer users/groups for the the
 * domain, which is used to control the access to its entities.
 * </p>
 * 
 * <p>
 * The reader and writer users/groups pattern that the user can supply is the
 * same as what <code>AccessControlList</code> takes.
 * </p>
 * 
 */
TimelineDomains (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timeline/TimelineDomains.java)/**
 * The class that hosts a list of timeline domains.
 */
TimelineEntities (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timeline/TimelineEntities.java)/**
 * The class that hosts a list of timeline entities.
 */
TimelineEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timeline/TimelineEntity.java)/**
 * <p>
 * The class that contains the the meta information of some conceptual entity
 * and its related events. The entity can be an application, an application
 * attempt, a container or whatever the user-defined object.
 * </p>
 * 
 * <p>
 * Primary filters will be used to index the entities in
 * <code>TimelineStore</code>, such that users should carefully choose the
 * information they want to store as the primary filters. The remaining can be
 * stored as other information.
 * </p>
 */
TimelineEntityGroupId (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timeline/TimelineEntityGroupId.java)/**
 * <p><code>TimelineEntityGroupId</code> is an abstract way for
 * timeline service users to represent a group of related timeline data.
 * For example, all entities that represents one data flow DAG execution
 * can be grouped into one timeline entity group. </p>
 */
TimelineEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timeline/TimelineEvent.java)/**
 * The class that contains the information of an event that is related to some
 * conceptual entity of an application. Users are free to define what the event
 * means, such as starting an application, getting allocated a container and
 * etc.
 */
EventsOfOneEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timeline/TimelineEvents.java)/**
   * The class that hosts a list of events that are only related to one entity.
   */
TimelineEvents (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timeline/TimelineEvents.java)/**
 * The class that hosts a list of events, which are categorized according to
 * their related entities.
 */
TimelineHealth (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timeline/TimelineHealth.java)/**
 * This class holds health information for ATS.
 */
TimelinePutError (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timeline/TimelinePutResponse.java)/**
   * A class that holds the error code for one entity.
   */
TimelinePutResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timeline/TimelinePutResponse.java)/**
 * A class that holds a list of put errors. This is the response returned when a
 * list of {@link TimelineEntity} objects is added to the timeline. If there are errors
 * in storing individual entity objects, they will be indicated in the list of
 * errors.
 */
ApplicationAttemptEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/ApplicationAttemptEntity.java)/**
 * This entity represents an application attempt.
 */
ApplicationEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/ApplicationEntity.java)/**
 * This entity represents an application.
 */
ClusterEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/ClusterEntity.java)/**
 * This entity represents a YARN cluster.
 */
ContainerEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/ContainerEntity.java)/**
 * This entity represents a container belonging to an application attempt.
 */
FlowActivityEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/FlowActivityEntity.java)/**
 * Entity that represents a record for flow activity. It's essentially a
 * container entity for flow runs with limited information.
 */
FlowRunEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/FlowRunEntity.java)/**
 * This entity represents a flow run.
 */
HierarchicalTimelineEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/HierarchicalTimelineEntity.java)/**
 * This class extends timeline entity and defines parent-child relationships
 * with other entities.
 */
QueueEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/QueueEntity.java)/**
 * This entity represents a queue.
 */
SubApplicationEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/SubApplicationEntity.java)/**
 * This entity represents a user defined entities to be stored under sub
 * application table.
 */
TimelineDomain (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineDomain.java)/**
 * <p>
 * This class contains the information about a timeline service domain, which is
 * used to a user to host a number of timeline entities, isolating them from
 * others'. The user can also define the reader and writer users/groups for
 * the domain, which is used to control the access to its entities.
 * </p>
 * <p>
 * The reader and writer users/groups pattern that the user can supply is the
 * same as what <code>AccessControlList</code> takes.
 * </p>
 */
TimelineEntities (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineEntities.java)/**
 * This class hosts a set of timeline entities.
 */
Identifier (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineEntity.java)/**
   * Identifier of timeline entity(entity id + entity type).
   */
TimelineEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineEntity.java)/**
 * The basic timeline entity data structure for timeline service v2. Timeline
 * entity objects are not thread safe and should not be accessed concurrently.
 * All collection members will be initialized into empty collections. Two
 * timeline entities are equal iff. their type and id are identical.
 *
 * All non-primitive type, non-collection members will be initialized into null.
 * User should set the type and id of a timeline entity to make it valid (can be
 * checked by using the {@link #isValid()} method). Callers to the getters
 * should perform null checks for non-primitive type, non-collection members.
 *
 * Callers are recommended not to alter the returned collection objects from the
 * getters.
 */
TimelineEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineEvent.java)/**
 * This class contains the information of an event that belongs to an entity.
 * Users are free to define what the event means, such as starting an
 * application, container being allocated, etc.
 */
TimelineMetric (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineMetric.java)/**
 * This class contains the information of a metric that is related to some
 * entity. Metric can either be a time series or single value.
 */
TimelineMetricCalculator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineMetricCalculator.java)/**
 * A calculator for timeline metrics.
 */
TimelineWriteError (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineWriteResponse.java)/**
   * A class that holds the error code for one entity.
   */
TimelineWriteResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineWriteResponse.java)/**
 * A class that holds a list of put errors. This is the response returned when a
 * list of {@link TimelineEntity} objects is added to the timeline. If there are
 * errors in storing individual entity objects, they will be indicated in the
 * list of errors.
 */
UserEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/UserEntity.java)/**
 * This entity represents a user.
 */
Token (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/Token.java)/**
 * <p><code>Token</code> is the security entity used by the framework
 * to verify authenticity of any resource.</p>
 */
UpdateContainerError (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/UpdateContainerError.java)/**
 * {@code UpdateContainerError} is used by the Scheduler to notify the
 * ApplicationMaster of an UpdateContainerRequest it cannot satisfy due to
 * an error in the request. It includes the update request as well as
 * a reason for why the request was not satisfiable.
 */
UpdateContainerRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/UpdateContainerRequest.java)/**
 * {@code UpdateContainerRequest} represents the request made by an
 * application to the {@code ResourceManager} to update an attribute of a
 * {@code Container} such as its Resource allocation or (@code ExecutionType}
 * <p>
 * It includes:
 * <ul>
 *   <li>version for the container.</li>
 *   <li>{@link ContainerId} for the container.</li>
 *   <li>
 *     {@link Resource} capability of the container after the update request
 *     is completed.
 *   </li>
 *   <li>
 *     {@link ExecutionType} of the container after the update request is
 *     completed.
 *   </li>
 * </ul>
 *
 * Update rules:
 * <ul>
 *   <li>
 *     Currently only ONE aspect of the container can be updated per request
 *     (user can either update Capability OR ExecutionType in one request..
 *     not both).
 *   </li>
 *   <li>
 *     There must be only 1 update request per container in an allocate call.
 *   </li>
 *   <li>
 *     If a new update request is sent for a container (in a subsequent allocate
 *     call) before the first one is satisfied by the Scheduler, it will
 *     overwrite the previous request.
 *   </li>
 * </ul>
 * @see ApplicationMasterProtocol#allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)
 */
UpdatedContainer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/UpdatedContainer.java)/**
 * An object that encapsulates an updated container and the
 * type of Update.
 */
URL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/URL.java)/**
 * <p><code>URL</code> represents a serializable {@link java.net.URL}.</p>
 */
YarnClusterMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/YarnClusterMetrics.java)/**
 * <p><code>YarnClusterMetrics</code> represents cluster metrics.</p>
 * 
 * <p>Currently only number of <code>NodeManager</code>s is provided.</p>
 */
Visitable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraint.java)/**
   * Interface used to enable the elements of the constraint tree to be visited.
   */
Visitor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraint.java)/**
   * Visitor API for a constraint tree.
   *
   * @param <T> determines the return type of the visit methods.
   */
AbstractConstraint (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraint.java)/**
   * Abstract class that acts as the superclass of all placement constraint
   * classes.
   */
SingleConstraint (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraint.java)/**
   * Consider a set of nodes N that belongs to the scope specified in the
   * constraint. If the target expressions are satisfied at least minCardinality
   * times and at most max-cardinality times in the node set N, then the
   * constraint is satisfied.
   *
   * For example, a constraint of the form {@code {RACK, 2, 10,
   * allocationTag("zk")}}, requires an allocation to be placed within a rack
   * that has at least 2 and at most 10 other allocations with tag "zk".
   */
TargetExpression (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraint.java)/**
   * Class representing the target expressions that are used in placement
   * constraints. They might refer to expressions on node attributes, allocation
   * tags, or be self-targets (referring to the allocation to which the
   * constraint is attached).
   */
TargetConstraint (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraint.java)/**
   * Class that represents a target constraint. Such a constraint requires an
   * allocation to be placed within a scope that satisfies some specified
   * expressions on node attributes and allocation tags.
   *
   * It is a specialized version of the {@link SingleConstraint}, where the
   * minimum and the maximum cardinalities take specific values based on the
   * {@link TargetOperator} used.
   */
CardinalityConstraint (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraint.java)/**
   * Class that represents a cardinality constraint. Such a constraint allows
   * the number of allocations with a specific set of tags and within a given
   * scope to be between some minimum and maximum values.
   *
   * It is a specialized version of the {@link SingleConstraint}, where the
   * target is a set of allocation tags.
   */
CompositeConstraint (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraint.java)/**
   * Class that represents composite constraints, which comprise other
   * constraints, forming a constraint tree.
   *
   * @param <R> the type of constraints that are used as children of the
   *          specific composite constraint
   */
And (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraint.java)/**
   * Class that represents a composite constraint that is a conjunction of other
   * constraints.
   */
Or (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraint.java)/**
   * Class that represents a composite constraint that is a disjunction of other
   * constraints.
   */
DelayedOr (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraint.java)/**
   * Class that represents a composite constraint that comprises a list of timed
   * placement constraints (see {@link TimedPlacementConstraint}). The scheduler
   * should try to satisfy first the first timed child constraint within the
   * specified time window. If this is not possible, it should attempt to
   * satisfy the second, and so on.
   */
TimedPlacementConstraint (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraint.java)/**
   * Represents a timed placement constraint that has to be satisfied within a
   * time window.
   */
PlacementConstraint (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraint.java)/**
 * {@code PlacementConstraint} represents a placement constraint for a resource
 * allocation.
 */
PlacementTargets (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraints.java)/**
   * Class with static methods for constructing target expressions to be used in
   * placement constraints.
   */
PlacementConstraints (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraints.java)/**
 * This class contains various static methods for the applications to create
 * placement constraints (see also {@link PlacementConstraint}).
 */
ApplicationAttemptNotFoundException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/ApplicationAttemptNotFoundException.java)/**
 * This exception is thrown on
 * {@link ApplicationHistoryProtocol#getApplicationAttemptReport (GetApplicationAttemptReportRequest)}
 * API when the Application Attempt doesn't exist in Application History Server or
 * {@link ApplicationMasterProtocol#allocate(AllocateRequest)} if application
 * doesn't exist in RM.
 */
ApplicationIdNotProvidedException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/ApplicationIdNotProvidedException.java)/**
 * Exception to be thrown when Client submit an application without
 * providing {@link ApplicationId} in {@link ApplicationSubmissionContext}.
 */
ApplicationNotFoundException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/ApplicationNotFoundException.java)/**
 * This exception is thrown on
 * {@link ApplicationClientProtocol#getApplicationReport
 * (GetApplicationReportRequest)} API
 * when the Application doesn't exist in RM and AHS
 */
ConfigurationException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/ConfigurationException.java)/**
 * This exception is thrown on unrecoverable configuration errors.
 * An example is container launch error due to configuration.
 */
ContainerNotFoundException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/ContainerNotFoundException.java)/**
 * This exception is thrown on
 * {@link ApplicationHistoryProtocol#getContainerReport (GetContainerReportRequest)}
 * API when the container doesn't exist in AHS
 */
InvalidApplicationMasterRequestException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/InvalidApplicationMasterRequestException.java)/**
 * This exception is thrown when an ApplicationMaster asks for resources by
 * calling {@link ApplicationMasterProtocol#allocate(AllocateRequest)}
 * without first registering by calling
 * {@link ApplicationMasterProtocol#registerApplicationMaster(RegisterApplicationMasterRequest)}
 * or if it tries to register more than once.
 */
InvalidAuxServiceException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/InvalidAuxServiceException.java)/**
 * This exception is thrown by a NodeManager that is rejecting start-container
 * requests via
 * {@link ContainerManagementProtocol#startContainers(StartContainersRequest)}
 * for auxservices does not exist.
 */
InvalidContainerException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/InvalidContainerException.java)/**
 * This exception is thrown by a NodeManager that is rejecting start-container
 * requests via
 * {@link ContainerManagementProtocol#startContainers(StartContainersRequest)}
 * for containers allocated by a previous instance of the RM.
 */
InvalidContainerReleaseException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/InvalidContainerReleaseException.java)/**
 * This exception is thrown when an Application Master tries to release
 * containers not belonging to it using
 * {@link ApplicationMasterProtocol#allocate(AllocateRequest)} API.
 */
InvalidLabelResourceRequestException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/InvalidLabelResourceRequestException.java)/**
 * This exception is thrown when a resource requested via
 * {@link ResourceRequest} in the
 * {@link ApplicationMasterProtocol#allocate(AllocateRequest)} when requested
 * label is not having permission to access.
 *
 */
InvalidResourceBlacklistRequestException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/InvalidResourceBlacklistRequestException.java)/**
 * This exception is thrown when an application provides an invalid
 * {@link ResourceBlacklistRequest} specification for blacklisting of resources
 * in {@link ApplicationMasterProtocol#allocate(AllocateRequest)} API.
 * 
 * Currently this exceptions is thrown when an application tries to
 * blacklist {@link ResourceRequest#ANY}.
 */
InvalidResourceRequestException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/InvalidResourceRequestException.java)/**
 * This exception is thrown when a resource requested via
 * {@link ResourceRequest} in the
 * {@link ApplicationMasterProtocol#allocate(AllocateRequest)} API is out of the
 * range of the configured lower and upper limits on resources.
 * 
 */
NMNotYetReadyException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/NMNotYetReadyException.java)/**
 * This exception is thrown on
 * {@link ContainerManagementProtocol#startContainers(StartContainersRequest)} API
 * when an NM starts from scratch but has not yet connected with RM.
 */
ResourceNotFoundException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/ResourceNotFoundException.java)/**
 * This exception is thrown when details of an unknown resource type
 * are requested.
 */
ResourceProfilesNotEnabledException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/ResourceProfilesNotEnabledException.java)/**
 * This exception is thrown when the client requests information about
 * ResourceProfiles in the
 * {@link org.apache.hadoop.yarn.api.ApplicationClientProtocol} but resource
 * profiles is not enabled on the RM.
 *
 */
SchedulerInvalidResoureRequestException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/SchedulerInvalidResoureRequestException.java)/**
 * This exception is thrown when any issue inside scheduler to handle a new or
 * updated {@link org.apache.hadoop.yarn.api.records.SchedulingRequest}/
 * {@link org.apache.hadoop.yarn.api.records.ResourceRequest} add to the
 * scheduler.
 */
YarnException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/YarnException.java)/**
 * YarnException indicates exceptions from yarn servers. On the other hand,
 * IOExceptions indicates exceptions from RPC layer.
 */
YARNFeatureNotEnabledException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/YARNFeatureNotEnabledException.java)/**
 * This exception is thrown when a feature is being used which is not enabled
 * yet.
 */
YarnRuntimeException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/exceptions/YarnRuntimeException.java)/** Base YARN Exception.
 *
 * NOTE: All derivatives of this exception, which may be thrown by a remote
 * service, must include a String only constructor for the exception to be
 * unwrapped on the client.
 */
ApplicationInitializationContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/ApplicationInitializationContext.java)/**
 * Initialization context for {@link AuxiliaryService} when starting an
 * application.
 */
ApplicationTerminationContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/ApplicationTerminationContext.java)/**
 * Initialization context for {@link AuxiliaryService} when stopping an
 * application.
 * 
 */
AuxiliaryLocalPathHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/AuxiliaryLocalPathHandler.java)/** An Interface that can retrieve local directories to read from or write to.
 *  Components can implement this interface to link it to
 *  their own Directory Handler Service
 */
AuxiliaryService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/AuxiliaryService.java)/**
 * A generic service that will be started by the NodeManager. This is a service
 * that administrators have to configure on each node by setting
 * {@link YarnConfiguration#NM_AUX_SERVICES}.
 * 
 */
ContainerContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/ContainerContext.java)/**
 * Base context class for {@link AuxiliaryService} initializing and stopping a
 * container.
 */
ContainerInitializationContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/ContainerInitializationContext.java)/**
 * Initialization context for {@link AuxiliaryService} when starting a
 * container.
 * 
 */
ContainerLogAggregationPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/ContainerLogAggregationPolicy.java)/**
 * This API is used by NodeManager to decide if a given container's logs
 * should be aggregated at run time.
 */
ContainerLogContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/ContainerLogContext.java)/**
 * Context class for {@link ContainerLogAggregationPolicy}.
 */
ContainerTerminationContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/ContainerTerminationContext.java)/**
 * Termination context for {@link AuxiliaryService} when stopping a
 * container.
 * 
 */
NodesToAttributesMappingRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodesToAttributesMappingRequest.java)/**
 * list of node-attribute mapping request info.
 */
NodesToAttributesMappingResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodesToAttributesMappingResponse.java)/**
 * NodesToAttributesMappingResponse holds response object for attribute
 * mapping.
 */
NodeToAttributes (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeToAttributes.java)/**
 * Represents a mapping of Node id to list of attributes.
 */
RefreshNodesResourcesRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/RefreshNodesResourcesRequest.java)/**
 * Request to refresh the resources of a node.
 */
RefreshNodesResourcesResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/RefreshNodesResourcesResponse.java)/**
 * Response to a request to refresh the resources of a node.
 */
RunSharedCacheCleanerTaskRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/RunSharedCacheCleanerTaskRequest.java)/**
 * <p>
 * The request from admin to ask the <code>SharedCacheManager</code> to run
 * cleaner service right away.
 * </p>
 * 
 * <p>
 * Currently, this is empty.
 * </p>
 */
RunSharedCacheCleanerTaskResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/RunSharedCacheCleanerTaskResponse.java)/**
 * <p>
 * The response to admin from the <code>SharedCacheManager</code> when
 * is asked to run the cleaner service.
 * </p>
 * 
 * <p>
 * Currently, this is empty.
 * </p>
 */
UpdateNodeResourceRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/UpdateNodeResourceRequest.java)/**
 * <p>The request sent by admin to change a list of nodes' resource to the 
 * <code>ResourceManager</code>.</p>
 * 
 * <p>The request contains details such as a map from {@link NodeId} to 
 * {@link ResourceOption} for updating the RMNodes' resources in 
 * <code>ResourceManager</code>.
 * 
 * @see ResourceManagerAdministrationProtocol#updateNodeResource(
 *      UpdateNodeResourceRequest)
 */
UpdateNodeResourceResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/UpdateNodeResourceResponse.java)/**
 * <p>The response sent by the <code>ResourceManager</code> to Admin client on
 * node resource change.</p>
 * 
 * <p>Currently, this is empty.</p>
 * 
 * @see ResourceManagerAdministrationProtocol#updateNodeResource(
 *      UpdateNodeResourceRequest)
 */
SCMAdminProtocol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/server/api/SCMAdminProtocol.java)/**
 * <p>
 * The protocol between administrators and the <code>SharedCacheManager</code>
 * </p>
 */
PlacementConstraintParseException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/constraint/PlacementConstraintParseException.java)/**
 * Exception when the placement constraint parser fails to parse an expression.
 */
ConstraintParser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/constraint/PlacementConstraintParser.java)/**
   * Constraint Parser used to parse placement constraints from a
   * given expression.
   */
ConstraintTokenizer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/constraint/PlacementConstraintParser.java)/**
   * Tokenizer interface that used to parse an expression. It first
   * validates if the syntax of the given expression is valid, then traverse
   * the expression and parse it to an enumeration of strings. Each parsed
   * string can be further consumed by a {@link ConstraintParser} and
   * transformed to a {@link AbstractConstraint}.
   */
BaseStringTokenizer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/constraint/PlacementConstraintParser.java)/**
   * A basic tokenizer that splits an expression by a given delimiter.
   */
ConjunctionTokenizer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/constraint/PlacementConstraintParser.java)/**
   * Tokenizer used to parse conjunction form of a constraint expression,
   * [AND|OR](C1:C2:...:Cn). Each Cn is a constraint expression.
   */
SourceTagsTokenizer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/constraint/PlacementConstraintParser.java)/**
   * Tokenizer used to parse allocation tags expression, which should be
   * in tag(numOfAllocations) syntax.
   */
MultipleConstraintsTokenizer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/constraint/PlacementConstraintParser.java)/**
   * Tokenizer used to handle a placement spec composed by multiple
   * constraint expressions. Each of them is delimited with the
   * given delimiter, e.g ':'.
   */
NodeConstraintParser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/constraint/PlacementConstraintParser.java)/**
   * Constraint parser used to parse a given target expression.
   */
TargetConstraintParser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/constraint/PlacementConstraintParser.java)/**
   * Constraint parser used to parse a given target expression, such as
   * "NOTIN, NODE, foo, bar".
   */
CardinalityConstraintParser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/constraint/PlacementConstraintParser.java)/**
   * Constraint parser used to parse a given target expression, such as
   * "cardinality, NODE, foo, 0, 1".
   */
ConjunctionConstraintParser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/constraint/PlacementConstraintParser.java)/**
   * Parser used to parse conjunction form of constraints, such as
   * AND(A, ..., B), OR(A, ..., B).
   */
SourceTags (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/constraint/PlacementConstraintParser.java)/**
   * A helper class to encapsulate source tags and allocations in the
   * placement specification.
   */
PlacementConstraintParser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/constraint/PlacementConstraintParser.java)/**
 * Placement constraint expression parser.
 */
CsiConfigUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/csi/CsiConfigUtils.java)/**
 * Utility class for CSI in the API level.
 */
Records (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/Records.java)/**
 * Convenient API record utils
 */
ResourceUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/resource/ResourceUtils.java)/**
 * Helper class to read the resource-types to be supported by the system.
 */
TimelineServiceHelper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/TimelineServiceHelper.java)/**
 * Helper class for Timeline service.
 */
Converter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/UnitsConversionUtil.java)/**
   * Helper class for encapsulating conversion values.
   */
UnitsConversionUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/UnitsConversionUtil.java)/**
 * A util to convert values in one unit to another. Units refers to whether
 * the value is expressed in pico, nano, etc.
 */
TestResource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/test/java/org/apache/hadoop/yarn/api/records/TestResource.java)/**
 * The class to test {@link Resource}.
 */
RecordFactoryForTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/test/java/org/apache/hadoop/yarn/api/records/TestURL.java)/** Record factory that instantiates URLs for this test. */
URLForTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/test/java/org/apache/hadoop/yarn/api/records/TestURL.java)/** URL fake for this test; sidesteps proto-URL dependency. */
TestURL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/test/java/org/apache/hadoop/yarn/api/records/TestURL.java)/** Test for the URL class. */
TestApplicationEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/test/java/org/apache/hadoop/yarn/api/records/timelineservice/TestApplicationEntity.java)/**
 * Various tests for the ApplicationEntity class.
 *
 */
TestPlacementConstraintParser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/test/java/org/apache/hadoop/yarn/api/resource/TestPlacementConstraintParser.java)/**
 * Class to test placement constraint parser.
 */
TestPlacementConstraints (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/test/java/org/apache/hadoop/yarn/api/resource/TestPlacementConstraints.java)/**
 * Test class for the various static methods in
 * {@link org.apache.hadoop.yarn.api.resource.PlacementConstraints}.
 */
TestResourceInformation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/test/java/org/apache/hadoop/yarn/conf/TestResourceInformation.java)/**
 * Test class to verify various resource informations in a given resource.
 */
TestYarnConfigurationFields (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/test/java/org/apache/hadoop/yarn/conf/TestYarnConfigurationFields.java)/**
 * Unit test class to compare
 * {@link org.apache.hadoop.yarn.conf.YarnConfiguration} and
 * yarn-default.xml for missing properties.  Currently only throws an error
 * if the class is missing a property.
 * <p></p>
 * Refer to {@link org.apache.hadoop.conf.TestConfigurationFieldsBase}
 * for how this class works.
 */
TestUnitsConversionUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/test/java/org/apache/hadoop/yarn/util/TestUnitsConversionUtil.java)/**
 * Test class to handle all test cases needed to verify basic unit conversion
 * scenarios.
 */
AppCatalog (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/main/java/org/apache/hadoop/yarn/appcatalog/application/AppCatalog.java)/**
 * Jackson resource configuration class for Application Catalog.
 */
AppCatalogInitializer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/main/java/org/apache/hadoop/yarn/appcatalog/application/AppCatalogInitializer.java)/**
 * Initialization class for setting Kerberos configuration.
 */
AppCatalogSolrClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/main/java/org/apache/hadoop/yarn/appcatalog/application/AppCatalogSolrClient.java)/**
 * Driver class for accessing Solr.
 */
YarnServiceClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/main/java/org/apache/hadoop/yarn/appcatalog/application/YarnServiceClient.java)/**
 * Driver class for calling YARN Resource Manager REST API.
 */
AppDetailsController (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/main/java/org/apache/hadoop/yarn/appcatalog/controller/AppDetailsController.java)/**
 * Application catalog REST API for displaying application details.
 */
AppListController (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/main/java/org/apache/hadoop/yarn/appcatalog/controller/AppListController.java)/**
 * Application deployment module.
 */
AppStoreController (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/main/java/org/apache/hadoop/yarn/appcatalog/controller/AppStoreController.java)/**
 * Application catalog REST API for searching and recommending
 * applications.
 *
 */
AppDetails (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/main/java/org/apache/hadoop/yarn/appcatalog/model/AppDetails.java)/**
 * Data model for user defined application configuration.
 */
AppEntry (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/main/java/org/apache/hadoop/yarn/appcatalog/model/AppEntry.java)/**
 * Data model for deployed application.
 *
 */
Application (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/main/java/org/apache/hadoop/yarn/appcatalog/model/Application.java)/**
 * Data model of display recommended applications and descriptions.
 */
AppStoreEntry (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/main/java/org/apache/hadoop/yarn/appcatalog/model/AppStoreEntry.java)/**
 * Data model of application template stored in application catalog.
 */
RandomWord (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/main/java/org/apache/hadoop/yarn/appcatalog/utils/RandomWord.java)/**
 * Random word generator utility.
 */
WordLengthException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/main/java/org/apache/hadoop/yarn/appcatalog/utils/WordLengthException.java)/**
 * Word length exception class.
 */
EmbeddedSolrServerFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/application/EmbeddedSolrServerFactory.java)/**
 * Embedded solr server factory class for unit tests.
 */
TestAppCatalogSolrClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/application/TestAppCatalogSolrClient.java)/**
 * Unit test for AppCatalogSolrClient.
 */
AppDetailsControllerTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppDetailsControllerTest.java)/**
 * Unit test for AppDetailsController.
 */
AppListControllerTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppListControllerTest.java)/**
 * Unit test for AppListController.
 */
AppStoreControllerTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/src/test/java/org/apache/hadoop/yarn/appcatalog/controller/AppStoreControllerTest.java)/**
 * Unit tests for AppStoreController.
 */
LaunchContainerRunnable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java)/**
   * Thread to connect to the {@link ContainerManagementProtocol} and launch the container
   * that will execute the shell command.
   */
ApplicationMaster (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java)/**
 * An ApplicationMaster for executing shell commands on a set of launched
 * containers using the YARN framework.
 * 
 * <p>
 * This class is meant to act as an example on how to write yarn-based
 * application masters.
 * </p>
 * 
 * <p>
 * The ApplicationMaster is started on a container by the
 * <code>ResourceManager</code>'s launcher. The first thing that the
 * <code>ApplicationMaster</code> needs to do is to connect and register itself
 * with the <code>ResourceManager</code>. The registration sets up information
 * within the <code>ResourceManager</code> regarding what host:port the
 * ApplicationMaster is listening on to provide any form of functionality to a
 * client as well as a tracking url that a client can use to keep track of
 * status/job history if needed. However, in the distributedshell, trackingurl
 * and appMasterHost:appMasterRpcPort are not supported.
 * </p>
 * 
 * <p>
 * The <code>ApplicationMaster</code> needs to send a heartbeat to the
 * <code>ResourceManager</code> at regular intervals to inform the
 * <code>ResourceManager</code> that it is up and alive. The
 * {@link ApplicationMasterProtocol#allocate} to the <code>ResourceManager</code> from the
 * <code>ApplicationMaster</code> acts as a heartbeat.
 * 
 * <p>
 * For the actual handling of the job, the <code>ApplicationMaster</code> has to
 * request the <code>ResourceManager</code> via {@link AllocateRequest} for the
 * required no. of containers using {@link ResourceRequest} with the necessary
 * resource specifications such as node location, computational
 * (memory/disk/cpu) resource requirements. The <code>ResourceManager</code>
 * responds with an {@link AllocateResponse} that informs the
 * <code>ApplicationMaster</code> of the set of newly allocated containers,
 * completed containers as well as current state of available resources.
 * </p>
 * 
 * <p>
 * For each allocated container, the <code>ApplicationMaster</code> can then set
 * up the necessary launch context via {@link ContainerLaunchContext} to specify
 * the allocated container id, local resources required by the executable, the
 * environment to be setup for the executable, commands to execute, etc. and
 * submit a {@link StartContainerRequest} to the {@link ContainerManagementProtocol} to
 * launch and execute the defined commands on the given allocated container.
 * </p>
 * 
 * <p>
 * The <code>ApplicationMaster</code> can monitor the launched container by
 * either querying the <code>ResourceManager</code> using
 * {@link ApplicationMasterProtocol#allocate} to get updates on completed containers or via
 * the {@link ContainerManagementProtocol} by querying for the status of the allocated
 * container's {@link ContainerId}.
 *
 * <p>
 * After the job has been completed, the <code>ApplicationMaster</code> has to
 * send a {@link FinishApplicationMasterRequest} to the
 * <code>ResourceManager</code> to inform it that the
 * <code>ApplicationMaster</code> has been completed.
 */
Client (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java)/**
 * Client for Distributed Shell application submission to YARN.
 * 
 * <p> The distributed shell client allows an application master to be launched that in turn would run 
 * the provided shell command on a set of containers. </p>
 * 
 * <p>This client is meant to act as an example on how to write yarn-based applications. </p>
 *
 * <p> To submit an application, a client first needs to connect to the <code>ResourceManager</code> 
 * aka ApplicationsManager or ASM via the {@link ApplicationClientProtocol}. The {@link ApplicationClientProtocol} 
 * provides a way for the client to get access to cluster information and to request for a
 * new {@link ApplicationId}. <p>
 * 
 * <p> For the actual job submission, the client first has to create an {@link ApplicationSubmissionContext}. 
 * The {@link ApplicationSubmissionContext} defines the application details such as {@link ApplicationId} 
 * and application name, the priority assigned to the application and the queue
 * to which this application needs to be assigned. In addition to this, the {@link ApplicationSubmissionContext}
 * also defines the {@link ContainerLaunchContext} which describes the <code>Container</code> with which 
 * the {@link ApplicationMaster} is launched. </p>
 * 
 * <p> The {@link ContainerLaunchContext} in this scenario defines the resources to be allocated for the 
 * {@link ApplicationMaster}'s container, the local resources (jars, configuration files) to be made available 
 * and the environment to be set for the {@link ApplicationMaster} and the commands to be executed to run the 
 * {@link ApplicationMaster}. <p>
 * 
 * <p> Using the {@link ApplicationSubmissionContext}, the client submits the application to the 
 * <code>ResourceManager</code> and then monitors the application by requesting the <code>ResourceManager</code> 
 * for an {@link ApplicationReport} at regular time intervals. In case of the application taking too long, the client 
 * kills the application by submitting a {@link KillApplicationRequest} to the <code>ResourceManager</code>. </p>
 *
 */
DistributedShellTimelinePlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DistributedShellTimelinePlugin.java)/**
 * Timeline v1.5 reader plugin for YARN distributed shell. It tranlsates an
 * incoming getEntity request to a set of related timeline entity groups, via
 * the information provided in the primary filter or entity id field.
 */
DSConstants (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DSConstants.java)/**
 * Constants used in both Client and Application Master
 */
PlacementSpec (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/PlacementSpec.java)/**
 * Class encapsulating a SourceTag, number of container and a Placement
 * Constraint.
 */
TestDSAppMaster (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSAppMaster.java)/**
 * A bunch of tests to make sure that the container allocations
 * and releases occur correctly.
 */
NMContainerMonitor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSWithMultipleNodeManager.java)/**
   * Monitor containers running on NMs
   */
AbstractTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/main/java/org/apache/hadoop/applications/mawo/server/common/AbstractTask.java)/**
 * Abstract class for MaWo Task.
 */
CompositeTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/main/java/org/apache/hadoop/applications/mawo/server/common/CompositeTask.java)/**
 * Composite Task is Task with multiple commands.
 */
DieTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/main/java/org/apache/hadoop/applications/mawo/server/common/DieTask.java)/**
 * Die Task is a type of task which indicates app to die.
 */
MawoConfiguration (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/main/java/org/apache/hadoop/applications/mawo/server/common/MawoConfiguration.java)/**
 * MaWo configuration class.
 */
NullTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/main/java/org/apache/hadoop/applications/mawo/server/common/NullTask.java)/**
 * Define Null Task.
 */
SimpleTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/main/java/org/apache/hadoop/applications/mawo/server/common/SimpleTask.java)/**
 * Define Simple Task.
 * Each Task has only one command
 */
Task (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/main/java/org/apache/hadoop/applications/mawo/server/common/Task.java)/**
 * Define Task Interface.
 */
TaskId (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/main/java/org/apache/hadoop/applications/mawo/server/common/TaskId.java)/**
 * Defines TaskId for MaWo app.
 */
TaskStatus (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/main/java/org/apache/hadoop/applications/mawo/server/common/TaskStatus.java)/**
 * Defines TaskStatus for MaWo app.
 */
TeardownTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/main/java/org/apache/hadoop/applications/mawo/server/common/TeardownTask.java)/**
 * Define Teardown Task.
 */
WorkAssignmentProtocol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/main/java/org/apache/hadoop/applications/mawo/server/common/WorkAssignmentProtocol.java)/**
 * Define work assignment protocol.
 */
JobId (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/main/java/org/apache/hadoop/applications/mawo/server/master/job/JobId.java)/**
 * Define MaWo JobId.
 */
WorkerId (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/main/java/org/apache/hadoop/applications/mawo/server/worker/WorkerId.java)/**
 * Define WorkerId for Workers.
 */
TestMaWoConfiguration (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-mawo/hadoop-yarn-applications-mawo-core/src/test/java/org/apache/hadoop/applications/mawo/server/common/TestMaWoConfiguration.java)/**
 * Test MaWo configuration.
 */
UnmanagedAMLauncher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/src/main/java/org/apache/hadoop/yarn/applications/unmanagedamlauncher/UnmanagedAMLauncher.java)/**
 * The UnmanagedLauncher is a simple client that launches and unmanaged AM. An
 * unmanagedAM is an AM that is not launched and managed by the RM. The client
 * creates a new application on the RM and negotiates a new attempt id. Then it
 * waits for the RM app state to reach be YarnApplicationState.ACCEPTED after
 * which it spawns the AM in another process and passes it the container id via
 * env variable Environment.CONTAINER_ID. The AM can be in any
 * language. The AM can register with the RM using the attempt id obtained
 * from the container id and proceed as normal.
 * The client redirects app stdout and stderr to its own stdout and
 * stderr and waits for the AM process to exit. Then it waits for the RM to
 * report app completion.
 */
ApiServiceClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api/src/main/java/org/apache/hadoop/yarn/service/client/ApiServiceClient.java)/**
 * The rest API client for users to manage services on YARN.
 */
SystemServiceManagerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api/src/main/java/org/apache/hadoop/yarn/service/client/SystemServiceManagerImpl.java)/**
 * SystemServiceManager implementation.
 * Scan for configure system service path.
 *
 * The service path structure is as follows:
 * SYSTEM_SERVICE_DIR_PATH
 * |---- sync
 * |     |--- user1
 * |     |    |---- service1.yarnfile
 * |     |    |---- service2.yarnfile
 * |     |--- user2
 * |     |    |---- service1.yarnfile
 * |     |    ....
 * |     |
 * |---- async
 * |     |--- user3
 * |     |    |---- service1.yarnfile
 * |     |    |---- service2.yarnfile
 * |     |--- user4
 * |     |    |---- service1.yarnfile
 * |     |    ....
 * |     |
 *
 * sync: These services are launched at the time of service start synchronously.
 *       It is a blocking service start.
 * async: These services are launched in separate thread without any delay after
 *       service start. Non-blocking service start.
 */
ApiServer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api/src/main/java/org/apache/hadoop/yarn/service/webapp/ApiServer.java)/**
 * The rest API endpoints for users to manage services on YARN.
 */
ApiServerWebApp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api/src/main/java/org/apache/hadoop/yarn/service/webapp/ApiServerWebApp.java)/**
 * This class launches the web service using Hadoop HttpServer2 (which uses
 * an embedded Jetty container). This is the entry point to your service.
 * The Java command used to launch this app should call the main method.
 */
TestServlet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api/src/test/java/org/apache/hadoop/yarn/service/client/TestApiServiceClient.java)/**
   * A mocked version of API Service for testing purpose.
   *
   */
TestApiServiceClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api/src/test/java/org/apache/hadoop/yarn/service/client/TestApiServiceClient.java)/**
 * Test case for CLI to API Service.
 *
 */
TestServlet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api/src/test/java/org/apache/hadoop/yarn/service/client/TestSecureApiServiceClient.java)/**
   * A mocked version of API Service for testing purpose.
   *
   */
TestSecureApiServiceClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api/src/test/java/org/apache/hadoop/yarn/service/client/TestSecureApiServiceClient.java)/**
 * Test Spnego Client Login.
 */
TestSystemServiceManagerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api/src/test/java/org/apache/hadoop/yarn/service/client/TestSystemServiceManagerImpl.java)/**
 * Test class for system service manager.
 */
ServiceClientTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api/src/test/java/org/apache/hadoop/yarn/service/ServiceClientTest.java)/**
 * A mock version of ServiceClient - This class is design
 * to simulate various error conditions that will happen
 * when a consumer class calls ServiceClient.
 */
TestApiServer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api/src/test/java/org/apache/hadoop/yarn/service/TestApiServer.java)/**
 * Test case for ApiServer REST API.
 *
 */
TestCleanupAfterKill (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-api/src/test/java/org/apache/hadoop/yarn/service/TestCleanupAfterKill.java)/**
 * Minicluster test that verifies registry cleanup when app lifetime is
 * exceeded.
 */
Artifact (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/Artifact.java)/**
 * Artifact of an service component.
 **/
Component (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/Component.java)/**
 * One or more components of the service. If the service is HBase say,
 * then the component can be a simple role like master or regionserver. If the
 * service is a complex business webapp then a component can be other
 * services say Kafka or Storm. Thereby it opens up the support for complex
 * and nested services.
 **/
ComponentContainers (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/ComponentContainers.java)/**
 * Containers of a component.
 */
ConfigFile (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/ConfigFile.java)/**
 * A config file that needs to be created and made available as a volume in an
 * service component container.
 **/
Configuration (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/Configuration.java)/**
 * Set of configuration properties that can be injected into the service
 * components via envs, files and custom pluggable helper docker containers.
 * Files of several standard formats like xml, properties, json, yaml and
 * templates will be supported.
 **/
Container (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/Container.java)/**
 * An instance of a running service container.
 **/
KerberosPrincipal (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/KerberosPrincipal.java)/**
 * The kerberos principal of the service.
 */
LocalizationStatus (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/LocalizationStatus.java)/**
 * The status of localization.
 */
PlacementConstraint (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/PlacementConstraint.java)/**
 * Placement constraint details.
 **/
PlacementPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/PlacementPolicy.java)/**
 * Advanced placement policy of the components of a service.
 **/
ReadinessCheck (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/ReadinessCheck.java)/**
 * A custom command or a pluggable helper container to determine the readiness
 * of a container of a component. Readiness for every service is different.
 * Hence the need for a simple interface, with scope to support advanced
 * usecases.
 **/
Resource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/Resource.java)/**
 * Resource determines the amount of resources (vcores, memory, network, etc.)
 * usable by a container. This field determines the resource to be applied for
 * all the containers of a component or service. The resource specified at
 * the service (or global) level can be overriden at the component level. Only one
 * of profile OR cpu &amp; memory are expected. It raises a validation
 * exception otherwise.
 **/
ResourceInformation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/ResourceInformation.java)/**
 * ResourceInformation determines unit/name/value of resource types in addition to memory and vcores. It will be part of Resource object
 */
Service (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/Service.java)/**
 * An Service resource has the following attributes.
 **/
ServiceStatus (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/records/ServiceStatus.java)/**
 * The current status of a submitted service, returned as a response to the
 * GET API.
 **/
ServiceApiConstants (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/api/ServiceApiConstants.java)/**
 * This class defines constants that can be used in input spec for
 * variable substitutions
 */
ClientAMPolicyProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ClientAMPolicyProvider.java)/**
 * PolicyProvider for Client to Service AM protocol.
 */
ClientAMSecurityInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ClientAMSecurityInfo.java)/**
 * Security Info for Client to Service AM protocol.
 */
AlwaysRestartPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/AlwaysRestartPolicy.java)/**
 * Always restart policy allows for restarts for long live components which
 * never terminate.
 */
UpgradeStatus (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java)/**
   * Status of upgrade.
   */
ComponentRestartPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/ComponentRestartPolicy.java)/**
 * Interface for Component Restart policies.
 * Which is used to make decisions on termination/restart of components and
 * their instances.
 */
NeverRestartPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/NeverRestartPolicy.java)/**
 * Policy for components with instances that do not require/support a restart.
 */
OnFailureRestartPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/OnFailureRestartPolicy.java)/**
 * Policy for components that require restarts for instances on failure.
 */
ContainerFailureTracker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ContainerFailureTracker.java)/**
 * This tracks the container failures per node. If the failure counter exceeds
 * the maxFailurePerNode limit, it'll blacklist that node.
 *
 */
AbstractLauncher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/containerlaunch/AbstractLauncher.java)/**
 * Launcher of applications: base class
 */
ClasspathConstructor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/containerlaunch/ClasspathConstructor.java)/**
 * build a classpath -allows for entries to be injected in front of
 * YARN classpath as well as behind, adds appropriate separators, 
 * extraction of local classpath, etc.
 */
CommandLineBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/containerlaunch/CommandLineBuilder.java)/**
 * Build a single command line to include in the container commands;
 * Special support for JVM command buildup.
 */
ComponentLaunchContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/containerlaunch/ContainerLaunchService.java)/**
   * Launch context of a component.
   */
JavaCommandLineBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/containerlaunch/JavaCommandLineBuilder.java)/**
 * Command line builder purely for the Java CLI.
 * Some of the <code>define</code> methods are designed to work with Hadoop tool and
 * Slider launcher applications.
 */
BadClusterStateException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/BadClusterStateException.java)/**
 * The system is in a bad state
 */
BadConfigException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/BadConfigException.java)/**
 * An exception to raise on a bad configuration
 */
ExitCodeProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/ExitCodeProvider.java)/**
 * Get the exit code of an exception. Making it an interface allows
 * us to retrofit exit codes onto existing classes
 */
ServiceLaunchException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/ServiceLaunchException.java)/**
 * A service launch exception that includes an exit code;
 * when caught by the ServiceLauncher, it will convert that
 * into a process exit code.
 */
UsageException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/UsageException.java)/**
 * Used to raise a usage exception ... this has the exit code
 * {@link #EXIT_USAGE}
 */
ComponentHealthThresholdMonitor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/monitor/ComponentHealthThresholdMonitor.java)/**
 * Monitors the health of containers of a specific component at a regular
 * interval. It takes necessary actions when the health of a component drops
 * below a desired threshold.
 */
DefaultProbe (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/monitor/probe/DefaultProbe.java)/**
 * A probe that checks whether the AM has retrieved an IP for a container.
 * Optional parameters enable a subsequent check for whether a DNS lookup can
 * be performed for the container's hostname. Configurable properties include:
 *
 *   dns.check.enabled - true if DNS check should be performed (default false)
 *   dns.address - optional IP:port address of DNS server to use for DNS check
 */
HttpProbe (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/monitor/probe/HttpProbe.java)/**
 * A probe that checks whether a successful HTTP response code can be obtained
 * from a container. A well-formed URL must be provided. The URL is intended
 * to contain a token ${THIS_HOST} that will be replaced by the IP of the
 * container. This probe also performs the checks of the {@link DefaultProbe}.
 * Additional configurable properties include:
 *
 *   url - required URL for HTTP connection, e.g. http://${THIS_HOST}:8080
 *   timeout - connection timeout (default 1000)
 *   min.success - minimum response code considered successful (default 200)
 *   max.success - maximum response code considered successful (default 299)
 *
 */
LogEntryBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/monitor/probe/LogEntryBuilder.java)/**
 * Build up log entries for ease of splunk
 */
MonitorKeys (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/monitor/probe/MonitorKeys.java)/**
 * Config keys for monitoring
 */
MonitorUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/monitor/probe/MonitorUtils.java)/**
 * Various utils to work with the monitor
 */
PortProbe (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/monitor/probe/PortProbe.java)/**
 * A probe that checks whether a container has a specified port open. This
 * probe also performs the checks of the {@link DefaultProbe}. Additional
 * configurable properties include:
 *
 *   port - required port for socket connection
 *   timeout - connection timeout (default 1000)
 */
Probe (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/monitor/probe/Probe.java)/**
 * Base class of all probes.
 */
ProbeStatus (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/monitor/probe/ProbeStatus.java)/**
 * Status message of a probe. This is designed to be sent over the wire, though the exception
 * Had better be unserializable at the far end if that is to work.
 */
ProviderFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderFactory.java)/**
 * Base class for factories.
 */
ResolvedLaunchParams (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderService.java)/**
   * This holds any information that is resolved during building the launch
   * context for a container.
   * <p>
   * Right now it contains a mapping of resource keys to destination files
   * for resources that need to be localized.
   */
ProviderUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderUtils.java)/**
 * This is a factoring out of methods handy for providers. It's bonded to a log
 * at construction time.
 */
CustomRegistryConstants (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/registry/CustomRegistryConstants.java)/**
 * These are constants unique to the Slider AM
 */
YarnRegistryViewForProviders (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/registry/YarnRegistryViewForProviders.java)/**
 * Registry view for providers. This tracks where the service
 * is registered, offers access to the record and other things.
 */
ServiceEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceEvent.java)/**
 * Events are handled by {@link ServiceManager} to manage the service
 * state.
 */
ServiceManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceManager.java)/**
 * Manages the state of Service.
 */
ServiceScheduler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java)/**
 *
 */
ServiceMetricsSink (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/timelineservice/ServiceMetricsSink.java)/**
 * Write the metrics to a ATSv2. Generally, this class is instantiated via
 * hadoop-metrics2 property files. Specifically, you would create this class by
 * adding the following to by This would actually be set as: <code>
 * [prefix].sink.[some instance name].class
 * =org.apache.hadoop.yarn.service.timelineservice.ServiceMetricsSink
 * </code>, where <tt>prefix</tt> is "atsv2": and <tt>some instance name</tt> is
 * just any unique name, so properties can be differentiated if there are
 * multiple sinks of the same type created
 */
ServiceTimelineMetricsConstants (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/timelineservice/ServiceTimelineMetricsConstants.java)/**
 * Constants which are stored as key in ATS
 */
ServiceTimelinePublisher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/timelineservice/ServiceTimelinePublisher.java)/**
 * A single service that publishes all the Timeline Entities.
 */
DefaultUpgradeComponentsFinder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/UpgradeComponentsFinder.java)/**
   * Default implementation of {@link UpgradeComponentsFinder} that finds all
   * the target component specs.
   */
UpgradeComponentsFinder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/UpgradeComponentsFinder.java)/**
 * Finds all the the target component specs.
 */
ApplicationReportSerDeser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ApplicationReportSerDeser.java)/**
 * Persistence of {@link SerializedApplicationReport}
 * 
 */
ClientRegistryBinder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ClientRegistryBinder.java)/**
 * Generic code to get the URLs for clients via the registry
 */
ComparatorReverser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/Comparators.java)/**
   * Little template class to reverse any comparitor
   * @param <CompareType> the type that is being compared
   */
Comparators (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/Comparators.java)/**
 * Some general comparators
 */
ConfigHelper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ConfigHelper.java)/**
 * Methods to aid in config, both in the Configuration class and
 * with other parts of setting up Slider-initated processes.
 *
 * Some of the methods take an argument of a map iterable for their sources; this allows
 * the same method
 */
Duration (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/Duration.java)/**
 * A duration in milliseconds. This class can be used
 * to count time, and to be polled to see if a time limit has
 * passed.
 */
HttpUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/HttpUtil.java)/**
 * Http connection utilities.
 *
 */
JsonSerDeser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/JsonSerDeser.java)/**
 * Support for marshalling objects to and from JSON.
 * This class is NOT thread safe; it constructs an object mapper
 * as an instance field.
 * @param <T>
 */
PatternValidator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/PatternValidator.java)/**
 * Utility class to validate strings against a predefined pattern.
 */
PortScanner (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/PortScanner.java)/**
 * a scanner which can take an input string for a range or scan the lot.
 */
PublishedConfiguration (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/PublishedConfiguration.java)/**
 * JSON-serializable description of a published key-val configuration.
 * 
 * The values themselves are not serialized in the external view; they have
 * to be served up by the far end
 */
PublishedConfigurationOutputter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/PublishedConfigurationOutputter.java)/**
 * Output a published configuration
 */
ProcessTerminationHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceUtils.java)/**
   * Process termination handler - exist with specified exit code after
   * waiting a while for ATS state to be in sync.
   */
ServiceUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceUtils.java)/**
 * These are slider-specific Util methods
 */
SliderFileSystem (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/SliderFileSystem.java)/**
 * Extends Core Filesystem with operations to manipulate ClusterDescription
 * persistent state
 */
TestBuildExternalComponents (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/client/TestBuildExternalComponents.java)/**
 * Test for building / resolving components of type SERVICE.
 */
DummyServiceClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/client/TestServiceCLI.java)/**
   * Dummy service client for test purpose.
   */
TestServiceClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/client/TestServiceClient.java)/**
 * Tests for {@link ServiceClient}.
 */
TestComponentInstance (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/component/instance/TestComponentInstance.java)/**
 * Tests for {@link ComponentInstance}.
 */
TestComponent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/component/TestComponent.java)/**
 * Tests for {@link Component}.
 */
TestComponentDecommissionInstances (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/component/TestComponentDecommissionInstances.java)/**
 * Test decommissioning component instances.
 */
TestComponentRestartPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/component/TestComponentRestartPolicy.java)/**
 * Tests for ComponentRestartPolicy implementations.
 */
ExampleAppJson (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/conf/ExampleAppJson.java)/**
 * Names of the example configs.
 */
TestAppJsonResolve (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/conf/TestAppJsonResolve.java)/**
 * Test global configuration resolution.
 */
TestLoadExampleAppJson (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/conf/TestLoadExampleAppJson.java)/**
 * Test loading example resources.
 */
TestValidateServiceNames (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/conf/TestValidateServiceNames.java)/**
 * Test cluster name validation.
 */
TestAbstractLauncher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/containerlaunch/TestAbstractLauncher.java)/**
 * Tests for {@link AbstractLauncher}.
 */
MockRunningServiceContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/MockRunningServiceContext.java)/**
 * Mocked service context for a running service.
 */
TestDefaultProbe (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/monitor/probe/TestDefaultProbe.java)/**
 * Tests for default probe.
 */
TestAbstractProviderService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/provider/TestAbstractProviderService.java)/**
 * Tests for {@link AbstractProviderService}
 */
TestProviderUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/provider/TestProviderUtils.java)/**
 * Test functionality of ProviderUtils.
 */
TestAbstractClientProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/providers/TestAbstractClientProvider.java)/**
 * Test the AbstractClientProvider shared methods.
 */
TestProviderFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/providers/TestProviderFactory.java)/**
 * Test provider factories.
 */
ServiceFSWatcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/ServiceTestUtils.java)/**
   * Watcher to initialize yarn service base path under target and deletes the
   * the test directory when finishes.
   */
TestDefaultUpgradeComponentsFinder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestDefaultUpgradeComponentsFinder.java)/**
 * Tests for {@link UpgradeComponentsFinder.DefaultUpgradeComponentsFinder}.
 */
TestServiceManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceManager.java)/**
 * Tests for {@link ServiceManager}.
 */
TestYarnNativeServices (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestYarnNativeServices.java)/**
 * End to end tests to test deploying services with MiniYarnCluster and a in-JVM
 * ZK testing cluster.
 */
TestServiceTimelinePublisher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/timelineservice/TestServiceTimelinePublisher.java)/**
 * Test class for ServiceTimelinePublisher.
 */
TestCoreFileSystem (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/utils/TestCoreFileSystem.java)/**
 * Tests for {@link CoreFileSystem}.
 */
TestServiceApiUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/utils/TestServiceApiUtil.java)/**
 * Test for ServiceApiUtil helper methods.
 */
ContainerRequestBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java)/**
     * Class to construct instances of {@link ContainerRequest} with specific
     * options.
     */
ContainerRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java)/**
   * Object to represent a single container request for resources. Scheduler
   * documentation should be consulted for the specifics of how the parameters
   * are honored.
   * 
   * By default, YARN schedulers try to allocate containers at the requested
   * locations but they may relax the constraints in order to expedite meeting
   * allocations limits. They first relax the constraint to the same rack as the
   * requested node and then to anywhere in the cluster. The relaxLocality flag
   * may be used to disable locality relaxation and request containers at only 
   * specific locations. The following conditions apply.
   * <ul>
   * <li>Within a priority, all container requests must have the same value for
   * locality relaxation. Either enabled or disabled.</li>
   * <li>If locality relaxation is disabled, then across requests, locations at
   * different network levels may not be specified. E.g. its invalid to make a
   * request for a specific node and another request for a specific rack.</li>
   * <li>If locality relaxation is disabled, then only within the same request,  
   * a node and its rack may be specified together. This allows for a specific   
   * rack with a preference for a specific node within that rack.</li>
   * <li></li>
   * </ul>
   * To re-enable locality relaxation at a given priority, all pending requests 
   * with locality relaxation disabled must be first removed. Then they can be 
   * added back with locality relaxation enabled.
   * 
   * All getters return immutable values.
   */
AbstractCallbackHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/async/AMRMClientAsync.java)/**
   * <p>
   * The callback abstract class. The callback functions need to be implemented
   * by {@link AMRMClientAsync} users. The APIs are called when responses from
   * the <code>ResourceManager</code> are available.
   * </p>
   */
CallbackHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/async/AMRMClientAsync.java)/**
   * @deprecated Use {@link AMRMClientAsync.AbstractCallbackHandler} instead.
   */
AMRMClientAsync (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/async/AMRMClientAsync.java)/**
 * <code>AMRMClientAsync</code> handles communication with the ResourceManager
 * and provides asynchronous updates on events such as container allocations and
 * completions.  It contains a thread that sends periodic heartbeats to the
 * ResourceManager.
 * 
 * It should be used by implementing a CallbackHandler:
 * <pre>
 * {@code
 * class MyCallbackHandler extends AMRMClientAsync.AbstractCallbackHandler {
 *   public void onContainersAllocated(List<Container> containers) {
 *     [run tasks on the containers]
 *   }
 *
 *   public void onContainersUpdated(List<Container> containers) {
 *     [determine if resource allocation of containers have been increased in
 *      the ResourceManager, and if so, inform the NodeManagers to increase the
 *      resource monitor/enforcement on the containers]
 *   }
 *
 *   public void onContainersCompleted(List<ContainerStatus> statuses) {
 *     [update progress, check whether app is done]
 *   }
 *   
 *   public void onNodesUpdated(List<NodeReport> updated) {}
 *   
 *   public void onReboot() {}
 * }
 * }
 * </pre>
 * 
 * The client's lifecycle should be managed similarly to the following:
 * 
 * <pre>
 * {@code
 * AMRMClientAsync asyncClient = 
 *     createAMRMClientAsync(appAttId, 1000, new MyCallbackhandler());
 * asyncClient.init(conf);
 * asyncClient.start();
 * RegisterApplicationMasterResponse response = asyncClient
 *    .registerApplicationMaster(appMasterHostname, appMasterRpcPort,
 *       appMasterTrackingUrl);
 * asyncClient.addContainerRequest(containerRequest);
 * [... wait for application to complete]
 * asyncClient.unregisterApplicationMaster(status, appMsg, trackingUrl);
 * asyncClient.stop();
 * }
 * </pre>
 */
AbstractCallbackHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/async/NMClientAsync.java)/**
   * <p>
   * The callback abstract class. The callback functions need to be implemented
   * by {@link NMClientAsync} users. The APIs are called when responses from
   * <code>NodeManager</code> are available.
   * </p>
   *
   * <p>
   * Once a callback happens, the users can chose to act on it in blocking or
   * non-blocking manner. If the action on callback is done in a blocking
   * manner, some of the threads performing requests on NodeManagers may get
   * blocked depending on how many threads in the pool are busy.
   * </p>
   *
   * <p>
   * The implementation of the callback functions should not throw the
   * unexpected exception. Otherwise, {@link NMClientAsync} will just
   * catch, log and then ignore it.
   * </p>
   */
CallbackHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/async/NMClientAsync.java)/**
   * @deprecated Use {@link NMClientAsync.AbstractCallbackHandler} instead.
   *
   * <p>
   * The callback interface needs to be implemented by {@link NMClientAsync}
   * users. The APIs are called when responses from <code>NodeManager</code> are
   * available.
   * </p>
   *
   * <p>
   * Once a callback happens, the users can chose to act on it in blocking or
   * non-blocking manner. If the action on callback is done in a blocking
   * manner, some of the threads performing requests on NodeManagers may get
   * blocked depending on how many threads in the pool are busy.
   * </p>
   *
   * <p>
   * The implementation of the callback function should not throw the
   * unexpected exception. Otherwise, {@link NMClientAsync} will just
   * catch, log and then ignore it.
   * </p>
   */
NMClientAsync (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/async/NMClientAsync.java)/**
 * <code>NMClientAsync</code> handles communication with all the NodeManagers
 * and provides asynchronous updates on getting responses from them. It
 * maintains a thread pool to communicate with individual NMs where a number of
 * worker threads process requests to NMs by using {@link NMClientImpl}. The max
 * size of the thread pool is configurable through
 * {@link YarnConfiguration#NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE}.
 *
 * It should be used in conjunction with a CallbackHandler. For example
 *
 * <pre>
 * {@code
 * class MyCallbackHandler extends NMClientAsync.AbstractCallbackHandler {
 *   public void onContainerStarted(ContainerId containerId,
 *       Map<String, ByteBuffer> allServiceResponse) {
 *     [post process after the container is started, process the response]
 *   }

 *   public void onContainerResourceIncreased(ContainerId containerId,
 *       Resource resource) {
 *     [post process after the container resource is increased]
 *   }
 *
 *   public void onContainerStatusReceived(ContainerId containerId,
 *       ContainerStatus containerStatus) {
 *     [make use of the status of the container]
 *   }
 *
 *   public void onContainerStopped(ContainerId containerId) {
 *     [post process after the container is stopped]
 *   }
 *
 *   public void onStartContainerError(
 *       ContainerId containerId, Throwable t) {
 *     [handle the raised exception]
 *   }
 *
 *   public void onGetContainerStatusError(
 *       ContainerId containerId, Throwable t) {
 *     [handle the raised exception]
 *   }
 *
 *   public void onStopContainerError(
 *       ContainerId containerId, Throwable t) {
 *     [handle the raised exception]
 *   }
 * }
 * }
 * </pre>
 *
 * The client's life-cycle should be managed like the following:
 *
 * <pre>
 * {@code
 * NMClientAsync asyncClient = 
 *     NMClientAsync.createNMClientAsync(new MyCallbackhandler());
 * asyncClient.init(conf);
 * asyncClient.start();
 * asyncClient.startContainer(container, containerLaunchContext);
 * [... wait for container being started]
 * asyncClient.getContainerStatus(container.getId(), container.getNodeId(),
 *     container.getContainerToken());
 * [... handle the status in the callback instance]
 * asyncClient.stopContainer(container.getId(), container.getNodeId(),
 *     container.getContainerToken());
 * [... wait for container being stopped]
 * asyncClient.stop();
 * }
 * </pre>
 */
ContainerShellWebSocket (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/ContainerShellWebSocket.java)/**
 * Web socket for establishing interactive command shell connection through
 * Node Manage to container executor.
 */
AHSv2ClientImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/AHSv2ClientImpl.java)/**
 * This class provides Application History client implementation which uses
 * ATS v2 as backend.
 */
ResourceReverseComparator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/AMRMClientImpl.java)/**
   * Class compares Resource by memory, then cpu and then the remaining resource
   * types in reverse order.
   */
ContainerManagementProtocolProxy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/ContainerManagementProtocolProxy.java)/**
 * Helper class to manage container manager proxies
 */
NMClientImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/NMClientImpl.java)/**
 * <p>
 * This class implements {@link NMClient}. All the APIs are blocking.
 * </p>
 *
 * <p>
 * By default, this client stops all the running containers that are started by
 * it when it stops. It can be disabled via
 * {@link #cleanupRunningContainersOnStop}, in which case containers will
 * continue to run even after this client is stopped and till the application
 * runs at which point ResourceManager will forcefully kill them.
 * </p>
 *
 * <p>
 * Note that the blocking APIs ensure the RPC calls to <code>NodeManager</code>
 * are executed immediately, and the responses are received before these APIs
 * return. However, when {@link #startContainer} or {@link #stopContainer}
 * returns, <code>NodeManager</code> may still need some time to either start
 * or stop the container because of its asynchronous implementation. Therefore,
 * {@link #getContainerStatus} is likely to return a transit container status
 * if it is executed immediately after {@link #startContainer} or
 * {@link #stopContainer}.
 * </p>
 */
RequestInfoIterator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/RemoteRequestsTable.java)/**
   * Nested Iterator that iterates over just the ResourceRequestInfo
   * object.
   */
SharedCacheClientImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/SharedCacheClientImpl.java)/**
 * An implementation of the SharedCacheClient API.
 */
InvalidContainerRequestException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/InvalidContainerRequestException.java)/**
 * Thrown when an arguments are combined to construct a
 * <code>AMRMClient.ContainerRequest</code> in an invalid way.
 */
NMTokenCache (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/NMTokenCache.java)/**
 * NMTokenCache manages NMTokens required for an Application Master
 * communicating with individual NodeManagers.
 * <p>
 * By default YARN client libraries {@link AMRMClient} and {@link NMClient} use
 * {@link #getSingleton()} instance of the cache.
 * <ul>
 *   <li>
 *     Using the singleton instance of the cache is appropriate when running a
 *     single ApplicationMaster in the same JVM.
 *   </li>
 *   <li>
 *     When using the singleton, users don't need to do anything special,
 *     {@link AMRMClient} and {@link NMClient} are already set up to use the
 *     default singleton {@link NMTokenCache}
 *     </li>
 * </ul>
 * If running multiple Application Masters in the same JVM, a different cache
 * instance should be used for each Application Master.
 * <ul>
 *   <li>
 *     If using the {@link AMRMClient} and the {@link NMClient}, setting up
 *     and using an instance cache is as follows:
 * <pre>
 *   NMTokenCache nmTokenCache = new NMTokenCache();
 *   AMRMClient rmClient = AMRMClient.createAMRMClient();
 *   NMClient nmClient = NMClient.createNMClient();
 *   nmClient.setNMTokenCache(nmTokenCache);
 *   ...
 * </pre>
 *   </li>
 *   <li>
 *     If using the {@link AMRMClientAsync} and the {@link NMClientAsync},
 *     setting up and using an instance cache is as follows:
 * <pre>
 *   NMTokenCache nmTokenCache = new NMTokenCache();
 *   AMRMClient rmClient = AMRMClient.createAMRMClient();
 *   NMClient nmClient = NMClient.createNMClient();
 *   nmClient.setNMTokenCache(nmTokenCache);
 *   AMRMClientAsync rmClientAsync = new AMRMClientAsync(rmClient, 1000, [AMRM_CALLBACK]);
 *   NMClientAsync nmClientAsync = new NMClientAsync("nmClient", nmClient, [NM_CALLBACK]);
 *   ...
 * </pre>
 *   </li>
 *   <li>
 *     If using {@link ApplicationMasterProtocol} and
 *     {@link ContainerManagementProtocol} directly, setting up and using an
 *     instance cache is as follows:
 * <pre>
 *   NMTokenCache nmTokenCache = new NMTokenCache();
 *   ...
 *   ApplicationMasterProtocol amPro = ClientRMProxy.createRMProxy(conf, ApplicationMasterProtocol.class);
 *   ...
 *   AllocateRequest allocateRequest = ...
 *   ...
 *   AllocateResponse allocateResponse = rmClient.allocate(allocateRequest);
 *   for (NMToken token : allocateResponse.getNMTokens()) {
 *     nmTokenCache.setToken(token.getNodeId().toString(), token.getToken());
 *   }
 *   ...
 *   ContainerManagementProtocolProxy nmPro = ContainerManagementProtocolProxy(conf, nmTokenCache);
 *   ...
 *   nmPro.startContainer(container, containerContext);
 *   ...
 * </pre>
 *   </li>
 * </ul>
 * It is also possible to mix the usage of a client ({@code AMRMClient} or
 * {@code NMClient}, or the async versions of them) with a protocol proxy
 * ({@code ContainerManagementProtocolProxy} or
 * {@code ApplicationMasterProtocol}).
 */
SharedCacheClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/SharedCacheClient.java)/**
 * This is the client for YARN's shared cache.
 */
ClusterCLI (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ClusterCLI.java)/**
 * Cluster CLI used to get over all information of the cluster
 */
CommandHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/NodeAttributesCLI.java)/**
   * Abstract class for command handler.
   */
ClientCommandHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/NodeAttributesCLI.java)/**
   * Client commands handler.
   */
AdminCommandHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/NodeAttributesCLI.java)/**
   * Admin commands handler.
   */
NodeAttributesCLI (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/NodeAttributesCLI.java)/**
 * CLI to map attributes to Nodes.
 */
SchedConfCLI (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/SchedConfCLI.java)/**
 * CLI for modifying scheduler configuration.
 */
YarnClientUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/util/YarnClientUtils.java)/**
 * This class is a container for utility methods that are useful when creating
 * YARN clients.
 */
BaseAMRMClientTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/BaseAMRMClientTest.java)/**
 * Base class for testing AMRMClient.
 */
BaseAMRMProxyE2ETest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/BaseAMRMProxyE2ETest.java)/**
 * Base test case to be used for Testing frameworks that use AMRMProxy.
 */
TestAHSv2ClientImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAHSv2ClientImpl.java)/**
 * This class is to test class {@link AHSv2ClientImpl).
 */
TestAMRMClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClient.java)/**
 * Test application master client class to resource manager.
 */
TestAMRMClientPlacementConstraints (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientPlacementConstraints.java)/**
 * Test Placement Constraints and Scheduling Requests.
 */
TestAMRMProxy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMProxy.java)/**
 * End-to-End test cases for the AMRMProxy Service.
 */
DebugSumContainerStateListener (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestNMClient.java)/**
   * Container State transition listener to track the number of times
   * a container has transitioned into a state.
   */
TestOpportunisticContainerAllocationE2E (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestOpportunisticContainerAllocationE2E.java)/**
 * Class that tests the allocation of OPPORTUNISTIC containers through the
 * centralized ResourceManager.
 */
TestYarnClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestYarnClient.java)/**
 * This class is to test class {@link YarnClient).
 */
TestYarnClientImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestYarnClientImpl.java)/**
 * This class is to test class {@link YarnClientImpl ).
 */
TestYarnClientWithReservation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestYarnClientWithReservation.java)/**
 * This class is to test class {@link YarnClient) and {@link YarnClientImpl}
 * with Reservation.
 */
ApplicationMasterServiceProtoTestBase (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/ApplicationMasterServiceProtoTestBase.java)/**
 * Test Base for Application Master Service Protocol.
 */
TestNodeAttributesCLI (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/cli/TestNodeAttributesCLI.java)/**
 * Test class for TestNodeAttributesCLI.
 */
TestRMCustomAuthFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/cli/TestSchedConfCLI.java)/**
   * Custom filter which sets the Remote User for testing purpose.
   */
TestSchedConfCLI (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/cli/TestSchedConfCLI.java)/**
 * Class for testing {@link SchedConfCLI}.
 */
TestTopCLI (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/cli/TestTopCLI.java)/**
 * Test class for TopCli.
 *
 */
ProtocolHATestBase (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/ProtocolHATestBase.java)/**
 * Test Base for ResourceManager's Protocol on HA.
 *
 * Limited scope:
 * For all the test cases, we only test whether the method will be re-entered
 * when failover happens. Does not cover the entire logic of test.
 *
 * Test strategy:
 * Create a separate failover thread with a trigger flag,
 * override all APIs that are added trigger flag.
 * When the APIs are called, we will set trigger flag as true to kick off
 * the failover. So We can make sure the failover happens during process
 * of the method. If this API is marked as @Idempotent or @AtMostOnce,
 * the test cases will pass; otherwise, they will throw the exception.
 *
 */
TestApplicationMasterServiceProtocolForTimelineV2 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/TestApplicationMasterServiceProtocolForTimelineV2.java)/**
 * Tests Application Master Protocol with timeline service v2 enabled.
 */
TestFederationRMFailoverProxyProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/TestFederationRMFailoverProxyProvider.java)/**
 * Unit tests for FederationRMFailoverProxyProvider.
 */
TestResourceManagerAdministrationProtocolPBClientImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/TestResourceManagerAdministrationProtocolPBClientImpl.java)/**
 * Test ResourceManagerAdministrationProtocolPBClientImpl. Test a methods and the proxy without  logic.
 */
TestYarnClientUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/util/TestYarnClientUtils.java)/**
 * Tests for the YarnClientUtils class
 */
CsiAdaptorProtocolPBClientImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/impl/pb/client/CsiAdaptorProtocolPBClientImpl.java)/**
 * CSI adaptor client implementation.
 */
CsiAdaptorProtocolPBServiceImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/impl/pb/service/CsiAdaptorProtocolPBServiceImpl.java)/**
 * CSI adaptor server side implementation, this is hosted on a node manager.
 */
PlacementConstraintFromProtoConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/pb/PlacementConstraintFromProtoConverter.java)/**
 * {@code PlacementConstraintFromProtoConverter} generates an
 * {@link PlacementConstraint.AbstractConstraint} given a
 * {@link PlacementConstraintProto}.
 */
PlacementConstraintToProtoConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/pb/PlacementConstraintToProtoConverter.java)/**
 * {@code PlacementConstraintToProtoConverter} generates a
 * {@link PlacementConstraintProto} given a
 * {@link PlacementConstraint.AbstractConstraint}.
 */
ContainerUpdateRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/ContainerUpdateRequestPBImpl.java)/**
 * <p>An implementation of <code>ContainerUpdateRequest</code>.</p>
 *
 * @see ContainerUpdateRequest
 */
ContainerUpdateResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/ContainerUpdateResponsePBImpl.java)/**
 * <p>An implementation of <code>ContainerUpdateResponse</code>.</p>
 *
 * @see ContainerUpdateResponse
 */
GetAllResourceProfilesRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetAllResourceProfilesRequestPBImpl.java)/**
 * Protobuf implementation class for GetAllResourceProfilesRequest.
 */
GetAllResourceProfilesResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetAllResourceProfilesResponsePBImpl.java)/**
 * Protobuf implementation class for the GetAllResourceProfilesResponse.
 */
GetAllResourceTypeInfoRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetAllResourceTypeInfoRequestPBImpl.java)/**
 * Protobuf implementation class for GetAllResourceTypeInfoRequest.
 */
GetAllResourceTypeInfoResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetAllResourceTypeInfoResponsePBImpl.java)/**
 * Protobuf implementation class for the GetAllResourceTypeInfoResponse.
 */
GetAttributesToNodesRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetAttributesToNodesRequestPBImpl.java)/**
 * Attributes to nodes mapping request.
 */
GetAttributesToNodesResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetAttributesToNodesResponsePBImpl.java)/**
 * Attributes to nodes response.
 */
GetClusterNodeAttributesRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetClusterNodeAttributesRequestPBImpl.java)/**
 * Request to get cluster node attributes.
 */
GetClusterNodeAttributesResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetClusterNodeAttributesResponsePBImpl.java)/**
 * Cluster node attributes response.
 */
GetLocalizationStatusesRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetLocalizationStatusesRequestPBImpl.java)/**
 * PB Impl of {@link GetLocalizationStatusesRequest}.
 */
GetLocalizationStatusesResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetLocalizationStatusesResponsePBImpl.java)/**
 * PB Impl of {@link GetLocalizationStatusesResponse}.
 */
GetNewReservationRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetNewReservationRequestPBImpl.java)/**
 * <p>The implementation of the request sent by clients to get a
 * new {@code ReservationId} for submitting an reservation.</p>
 *
 * {@code ApplicationClientProtocol#getNewReservation(GetNewReservationRequest)}
 */
GetNewReservationResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetNewReservationResponsePBImpl.java)/**
 * <p>The implementation of the response sent by the
 * <code>ResourceManager</code> to the client for a request to get a new
 * {@link ReservationId} for submitting reservations.</p>
 *
 * <p>Clients can submit an reservation with the returned
 * {@link ReservationId}.</p>
 *
 * {@code ApplicationClientProtocol#getNewReservation(GetNewReservationRequest)}
 */
GetNodesToAttributesRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetNodesToAttributesRequestPBImpl.java)/**
 * Request to get hostname to attributes mapping.
 */
GetNodesToAttributesResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetNodesToAttributesResponsePBImpl.java)/**
 * Nodes to attributes request response.
 */
GetPluginInfoRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetPluginInfoRequestPBImpl.java)/**
 * Get plugin info request protobuf impl.
 */
GetPluginInfoResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetPluginInfoResponsePBImpl.java)/**
 * Get plugin info response protobuf impl.
 */
GetResourceProfileRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetResourceProfileRequestPBImpl.java)/**
 * Protobuf implementation for the GetResourceProfileRequest class.
 */
GetResourceProfileResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/GetResourceProfileResponsePBImpl.java)/**
 * Protobuf implementation for the GetResourceProfileResponse class.
 */
NodePublishVolumeRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/NodePublishVolumeRequestPBImpl.java)/**
 * Request to publish volume on node manager.
 */
NodePublishVolumeResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/NodePublishVolumeResponsePBImpl.java)/**
 * Protobuf record class for node publish response.
 */
NodeUnpublishVolumeRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/NodeUnpublishVolumeRequestPBImpl.java)/**
 * The protobuf record class for request to un-publish volume on node manager.
 */
NodeUnpublishVolumeResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/NodeUnpublishVolumeResponsePBImpl.java)/**
 * Response to the un-publish volume request on node manager.
 */
ReservationListRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/ReservationListRequestPBImpl.java)/**
 * {@link ReservationListRequestPBImpl} implements the {@link
 * ReservationListRequest} abstract class which captures the set of requirements
 * the user has to list reservations.
 *
 * @see ReservationListRequest
 */
ReservationListResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/ReservationListResponsePBImpl.java)/**
 * {@link ReservationListResponsePBImpl} is the implementation of the
 * {@link ReservationListResponse} which captures  the list of reservations
 * that the user has queried.
 */
ValidateVolumeCapabilitiesRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/ValidateVolumeCapabilitiesRequestPBImpl.java)/**
 * PB wrapper for CsiAdaptorProtos.ValidateVolumeCapabilitiesRequest.
 */
ValidateVolumeCapabilitiesResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/protocolrecords/impl/pb/ValidateVolumeCapabilitiesResponsePBImpl.java)/**
 * PB wrapper for CsiAdaptorProtos.ValidateVolumeCapabilitiesResponse.
 */
ApplicationTimeoutPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ApplicationTimeoutPBImpl.java)/**
 * PB implementation for ApplicationTimeout class.
 */
CollectorInfoPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/CollectorInfoPBImpl.java)/**
 * Protocol record implementation of {@link CollectorInfo}.
 */
ContainerRetryContextPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ContainerRetryContextPBImpl.java)/**
 * Implementation of ContainerRetryContext.
 */
ExecutionTypeRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ExecutionTypeRequestPBImpl.java)/**
 * Implementation of <code>ExecutionTypeRequest</code>.
 */
LocalizationStatusPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/LocalizationStatusPBImpl.java)/**
 * PB Impl of {@link LocalizationStatus}.
 */
NodeAttributeInfoPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/NodeAttributeInfoPBImpl.java)/**
 * Implementation for NodeAttributeInfo.
 *
 */
NodeAttributeKeyPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/NodeAttributeKeyPBImpl.java)/**
 * Implementation for NodeAttributeKey.
 *
 */
NodeAttributePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/NodeAttributePBImpl.java)/**
 * Implementation for NodeAttribute.
 */
NodeToAttributeValuePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/NodeToAttributeValuePBImpl.java)/**
 * PB Implementation for NodeToAttributeValue.
 *
 */
ProtoUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ProtoUtils.java)/**
 * Utils to convert enum protos to corresponding java enums and vice versa.
 */
RejectedSchedulingRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/RejectedSchedulingRequestPBImpl.java)/**
 * Implementation of RejectedSchedulingRequest.
 */
ReservationAllocationStatePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ReservationAllocationStatePBImpl.java)/**
 * {@code ReservationAllocationStatePBImpl} implements the {@link
 * ReservationAllocationState} that represents the  reservation  that is
 * made by a user.
 *
 * <p>
 * It includes:
 * <ul>
 *   <li>Duration of the reservation.</li>
 *   <li>Acceptance time of the duration.</li>
 *   <li>
 *       List of {@link ResourceAllocationRequest}, which includes the time
 *       interval, and capability of the allocation.
 *       {@code ResourceAllocationRequest} represents an allocation
 *       made for a reservation for the current state of the plan. This can be
 *       changed for reasons such as re-planning, but will always be subject to
 *       the constraints of the user contract as described by
 *       {@link ReservationDefinition}
 *   </li>
 *   <li>{@link ReservationId} of the reservation.</li>
 *   <li>{@link ReservationDefinition} used to make the reservation.</li>
 * </ul>
 *
 * @see ResourceAllocationRequest
 * @see ReservationId
 * @see ReservationDefinition
 */
ResourceAllocationRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ResourceAllocationRequestPBImpl.java)/**
 * {@code ResourceAllocationRequestPBImpl} which implements the
 * {@link ResourceAllocationRequest} class which represents an allocation
 * made for a reservation for the current state of the plan. This can be
 * changed for reasons such as re-planning, but will always be subject to the
 * constraints of the user contract as described by a
 * {@code ReservationDefinition}
 * {@link Resource}
 *
 * <p>
 * It includes:
 * <ul>
 *   <li>StartTime of the allocation.</li>
 *   <li>EndTime of the allocation.</li>
 *   <li>{@link Resource} reserved for the allocation.</li>
 * </ul>
 *
 * @see Resource
 */
ResourceSizingPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ResourceSizingPBImpl.java)/**
 * Proto Implementation for {@link ResourceSizing} interface.
 */
ResourceTypeInfoPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ResourceTypeInfoPBImpl.java)/**
 * {@code ResourceTypeInfoPBImpl} which implements the
 * {@link ResourceTypeInfo} class which represents different resource types
 * supported in YARN.
 */
SchedulingRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/SchedulingRequestPBImpl.java)/**
 * Proto implementation for {@link SchedulingRequest} interface.
 */
UpdateContainerErrorPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/UpdateContainerErrorPBImpl.java)/**
 * Implementation of <code>UpdateContainerError</code>.
 */
UpdateContainerRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/UpdateContainerRequestPBImpl.java)/**
 * Implementation of <code>UpdateContainerRequest</code>.
 */
UpdatedContainerPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/UpdatedContainerPBImpl.java)/**
 * Implementation of <code>UpdatedContainer</code>.
 */
AbstractTransformer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraintTransformations.java)/**
   * The default implementation of the {@link PlacementConstraint.Visitor} that
   * does a traversal of the constraint tree, performing no action for the lead
   * constraints.
   */
SingleConstraintTransformer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraintTransformations.java)/**
   * Visits a {@link PlacementConstraint} tree and substitutes each
   * {@link TargetConstraint} and {@link CardinalityConstraint} with an
   * equivalent {@link SingleConstraint}.
   */
SpecializedConstraintTransformer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraintTransformations.java)/**
   * Visits a {@link PlacementConstraint} tree and, whenever possible,
   * substitutes each {@link SingleConstraint} with a {@link TargetConstraint}.
   * When such a substitution is not possible, we keep the original
   * {@link SingleConstraint}.
   */
PlacementConstraintTransformations (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/resource/PlacementConstraintTransformations.java)/**
 * This class contains inner classes that define transformation on a
 * {@link PlacementConstraint} expression.
 */
AMRMClientUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/AMRMClientUtils.java)/**
 * Utility class for AMRMClient.
 */
AppAdminClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/AppAdminClient.java)/**
 * Client for managing applications.
 */
DirectTimelineWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/DirectTimelineWriter.java)/**
 * A simple writer class for storing Timeline data into Leveldb store.
 */
FileSystemTimelineWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/FileSystemTimelineWriter.java)/**
 * A simple writer class for storing Timeline data in any storage that
 * implements a basic FileSystem interface.
 * This writer is used for ATSv1.5.
 */
TimelineClientRetryOp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineConnector.java)/**
   * Abstract class for an operation that should be retried by timeline client.
   */
TimelineConnector (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineConnector.java)/**
 * Utility Connector class which is used by timeline clients to securely get
 * connected to the timeline server.
 *
 */
TimelineReaderClientImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineReaderClientImpl.java)/**
 * Implementation of TimelineReaderClient interface.
 */
TimelineEntityDispatcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineV2ClientImpl.java)/**
   * This class is responsible for collecting the timeline entities and
   * publishing them in async.
   */
TimelineV2ClientImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineV2ClientImpl.java)/**
 * Implementation of timeline v2 client interface.
 *
 */
TimelineWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineWriter.java)/**
 * Base writer class to write the Timeline data.
 */
TimelineClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/TimelineClient.java)/**
 * A client library that can be used to post some information in terms of a
 * number of conceptual entities. This client library needs to be used along
 * with Timeline V.1.x server versions.
 * Refer {@link TimelineV2Client} for ATS V2 interface.
 */
TimelineReaderClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/TimelineReaderClient.java)/**
 * A client library that can be used to get Timeline Entities associated with
 * application, application attempt or containers. This client library needs to
 * be used along with time line v.2 server version.
 */
TimelineV2Client (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/TimelineV2Client.java)/**
 * A client library that can be used to post some information in terms of a
 * number of conceptual entities. This client library needs to be used along
 * with time line v.2 server version.
 * Refer {@link TimelineClient} for ATS V1 interface.
 */
RequestHedgingRMFailoverProxyProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/RequestHedgingRMFailoverProxyProvider.java)/**
 * A FailoverProxyProvider implementation that technically does not "failover"
 * per-se. It constructs a wrapper proxy that sends the request to ALL
 * underlying proxies simultaneously. Each proxy inside the wrapper proxy will
 * retry the corresponding target. It assumes the in an HA setup, there will be
 * only one Active, and the active should respond faster than any configured
 * standbys. Once it receives a response from any one of the configured proxies,
 * outstanding requests to other proxies are immediately cancelled.
 */
ContainerLogAppender (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerLogAppender.java)/**
 * A simple log4j-appender for container's logs.
 */
ContainerRollingLogAppender (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerRollingLogAppender.java)/**
 * A simple log4j-appender for container's logs.
 *
 */
AbstractEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AbstractEvent.java)/**
 * Parent class of all the events. All events extend this class.
 */
MultiListenerHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java)/**
   * Multiplexing an event. Sending it to different handlers that
   * are interested in the event.
   * @param <T> the type of event these multiple handlers are interested in.
   */
AsyncDispatcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java)/**
 * Dispatches {@link Event}s in a separate thread. Currently only single thread
 * does that. Potentially there could be multiple channels for each event type
 * class and a thread pool can be used to dispatch the events.
 */
Dispatcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/Dispatcher.java)/**
 * Event Dispatcher interface. It dispatches events to registered 
 * event handlers based on event types.
 * 
 */
Event (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/Event.java)/**
 * Interface defining events api.
 *
 */
EventDispatcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/EventDispatcher.java)/**
 * This is a specialized EventHandler to be used by Services that are expected
 * handle a large number of events efficiently by ensuring that the caller
 * thread is not blocked. Events are immediately stored in a BlockingQueue and
 * a separate dedicated Thread consumes events from the queue and handles
 * appropriately
 * @param <T> Type of Event
 */
EventHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/EventHandler.java)/**
 * Interface for handling events of type T
 *
 * @param <T> parameterized event of type T
 */
RpcFactoryProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/factory/providers/RpcFactoryProvider.java)/**
 * A public static get() method must be present in the Client/Server Factory implementation.
 */
HadoopYarnProtoRPC (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ipc/HadoopYarnProtoRPC.java)/**
 * This uses Hadoop RPC. Uses a tunnel ProtoSpecificRpcEngine over 
 * Hadoop connection.
 * This does not give cross-language wire compatibility, since the Hadoop 
 * RPC wire format is non-standard, but it does permit use of Protocol Buffers
 *  protocol versioning features for inter-Java RPCs.
 */
YarnRPC (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ipc/YarnRPC.java)/**
 * Abstraction to get the RPC implementation for Yarn.
 */
AggregatedLogDeletionService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogDeletionService.java)/**
 * A service that periodically deletes aggregated logs.
 */
LogRetentionContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogFormat.java)/**
   * A context for log retention to determine if files are older than
   * the retention policy configured in YarnConfiguration.
   */
LogWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogFormat.java)/**
   * The writer that writes out the aggregated logs.
   */
ContainerLogFileInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/ContainerLogFileInfo.java)/**
 * ContainerLogFileInfo represents the meta data for a container log file,
 * which includes:
 * <ul>
 *   <li>The filename of the container log.</li>
 *   <li>The size of the container log.</li>
 *   <li>The last modification time of the container log.</li>
 * </ul>
 *
 */
ContainerLogMeta (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/ContainerLogMeta.java)/**
 * The ContainerLogMeta includes:
 * <ul>
 *   <li>The Container Id.</li>
 *   <li>The NodeManager Id.</li>
 *   <li>A list of {@link ContainerLogFileInfo}.</li>
 * </ul>
 *
 */
IndexedFileAggregatedLogsBlock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/ifile/IndexedFileAggregatedLogsBlock.java)/**
 * The Aggregated Logs Block implementation for Indexed File.
 */
IndexedLogsMeta (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/ifile/LogAggregationIndexedFileController.java)/**
   * This IndexedLogsMeta includes all the meta information
   * for the aggregated log file.
   */
IndexedPerAggregationLogMeta (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/ifile/LogAggregationIndexedFileController.java)/**
   * This IndexedPerAggregationLogMeta includes the meta information
   * for all files which would be aggregated in one
   * Log aggregation cycle.
   */
IndexedFileLogMeta (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/ifile/LogAggregationIndexedFileController.java)/**
   * This IndexedFileLogMeta includes the meta information
   * for a single file which would be aggregated in one
   * Log aggregation cycle.
   *
   */
LogAggregationIndexedFileController (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/ifile/LogAggregationIndexedFileController.java)/**
 * The Indexed Log Aggregation File Format implementation.
 *
 */
LogAggregationDFSException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/LogAggregationDFSException.java)/**
 * This exception class indicates an issue during log aggregation.
 */
LogAggregationFileController (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/LogAggregationFileController.java)/**
 * Base class to implement Log Aggregation File Controller.
 */
LogAggregationFileControllerContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/LogAggregationFileControllerContext.java)/**
 * {@code LogAggregationFileControllerContext} is a record used in
 * the log aggregation process.
 */
LogAggregationFileControllerFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/LogAggregationFileControllerFactory.java)/**
 * Use {@code LogAggregationFileControllerFactory} to get the correct
 * {@link LogAggregationFileController} for write and read.
 *
 */
LogAggregationHtmlBlock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/LogAggregationHtmlBlock.java)/**
 * Base class to implement Aggregated Logs Block.
 */
LogAggregationTFileController (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/tfile/LogAggregationTFileController.java)/**
 * The TFile log aggregation file Controller implementation.
 */
TFileAggregatedLogsBlock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/tfile/TFileAggregatedLogsBlock.java)/**
 * The Aggregated Logs Block implementation for TFile.
 */
LogAggregationWebUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/LogAggregationWebUtils.java)/**
 * Utils for rendering aggregated logs block.
 *
 */
LogToolUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/LogToolUtils.java)/**
 * This class contains several utility function which could be used in different
 * log tools.
 *
 */
AbstractLabel (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/AbstractLabel.java)/**
 * Generic class capturing the information required commonly across Partitions
 * and Attributes.
 */
AttributeValue (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/AttributeValue.java)/**
 * Interface to capture operations on AttributeValue.
 */
Host (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java)/**
   * A <code>Host</code> can have multiple <code>Node</code>s 
   */
FileSystemNodeLabelsStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java)/**
 * FileSystemNodeLabelsStore for storing node labels.
 */
NodeAttributesManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/NodeAttributesManager.java)/**
 * This class captures all interactions for Attributes with RM.
 */
NodeAttributeStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/NodeAttributeStore.java)/**
 * Interface class for Node label store.
 */
NodeLabelsStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/NodeLabelsStore.java)/**
 * Interface class for Node label store.
 */
NodeLabelUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/NodeLabelUtil.java)/**
 * Utility class for all NodeLabel and NodeAttribute operations.
 */
NonAppendableFSNodeLabelStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/NonAppendableFSNodeLabelStore.java)/**
 * Store implementation for Non Appendable File Store.
 */
RMNodeAttribute (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/RMNodeAttribute.java)/**
 * Reference of NodeAttribute in RM.
 */
RMNodeLabel (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/RMNodeLabel.java)/**
 * Partition representation in RM.
 */
StoreSchema (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/store/AbstractFSNodeStore.java)/**
   * Filesystem store schema define the log name and mirror name.
   */
AbstractFSNodeStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/store/AbstractFSNodeStore.java)/**
 * Abstract class for File System based store.
 *
 * @param <M> manager filesystem store.Currently nodelabel will use
 *           CommonNodeLabelManager.
 */
FSStoreOpHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/store/FSStoreOpHandler.java)/**
 * File system store op handler.
 */
AddClusterLabelOp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/store/op/AddClusterLabelOp.java)/**
 * Add label operation for file system.
 */
AddNodeToAttributeLogOp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/store/op/AddNodeToAttributeLogOp.java)/**
 * File system Add Node to attribute mapping.
 */
FSNodeStoreLogOp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/store/op/FSNodeStoreLogOp.java)/**
 * Defines all FileSystem editlog operation. All node label and attribute
 * store write or read operation will be defined in this class.
 *
 * @param <M> Manager used for each operation.
 */
NodeAttributeMirrorOp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/store/op/NodeAttributeMirrorOp.java)/**
 * File System Node Attribute Mirror read and write operation.
 */
NodeLabelMirrorOp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/store/op/NodeLabelMirrorOp.java)/**
 * NodeLabel Mirror Op class.
 */
NodeToLabelOp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/store/op/NodeToLabelOp.java)/**
 * Node to label mapping store operation for label.
 */
RemoveClusterLabelOp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/store/op/RemoveClusterLabelOp.java)/**
 * Remove label from cluster log store operation.
 */
RemoveNodeToAttributeLogOp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/store/op/RemoveNodeToAttributeLogOp.java)/**
 * File system remove node attribute from node operation.
 */
ReplaceNodeToAttributeLogOp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/store/op/ReplaceNodeToAttributeLogOp.java)/**
 * File system replace node attribute from node operation.
 */
StoreOp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/store/StoreOp.java)/**
 * Define the interface for store activity.
 * Used by for FileSystem based operation.
 *
 * @param <W> write to be done to
 * @param <R> read to be done from
 * @param <M> manager used
 */
StringAttributeValue (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/StringAttributeValue.java)/**
 * Attribute value for String NodeAttributeType.
 */
AccessRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/security/AccessRequest.java)/**
 * This request object contains all the context information to determine whether
 * a user has permission to access the target entity.
 * user       : the user who's currently accessing
 * accessType : the access type against the entity.
 * entity     : the target object user is accessing.
 * appId      : the associated app Id for current access. This could be null
 *              if no app is associated.
 * appName    : the associated app name for current access. This could be null if
 *              no app is associated.
 * remoteAddress : The caller's remote ip address.
 * forwardedAddresses : In case this is an http request, this contains the
 *                    originating IP address of a client connecting to a web
 *                    server through an HTTP proxy or load balancer. This
 *                    parameter is null, if it's a RPC request.
 */
AMRMTokenIdentifier (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/security/AMRMTokenIdentifier.java)/**
 * AMRMTokenIdentifier is the TokenIdentifier to be used by
 * ApplicationMasters to authenticate to the ResourceManager.
 */
BaseClientToAMTokenSecretManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/security/client/BaseClientToAMTokenSecretManager.java)/**
 * A base {@link SecretManager} for AMs to extend and validate Client-RM tokens
 * issued to clients by the RM using the underlying master-key shared by RM to
 * the AMs on their launch. All the methods are called by either Hadoop RPC or
 * YARN, so this class is strictly for the purpose of inherit/extend and
 * register with Hadoop RPC.
 */
ClientToAMTokenSecretManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/security/client/ClientToAMTokenSecretManager.java)/**
 * A simple {@link SecretManager} for AMs to validate Client-RM tokens issued to
 * clients by the RM using the underlying master-key shared by RM to the AMs on
 * their launch. All the methods are called by either Hadoop RPC or YARN, so
 * this class is strictly for the purpose of inherit/extend and register with
 * Hadoop RPC.
 */
RMDelegationTokenIdentifier (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/security/client/RMDelegationTokenIdentifier.java)/**
 * Delegation Token Identifier that identifies the delegation tokens from the 
 * Resource Manager. 
 */
ConfiguredYarnAuthorizer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/security/ConfiguredYarnAuthorizer.java)/**
 * A YarnAuthorizationProvider implementation based on configuration files.
 *
 */
ContainerTokenIdentifier (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/security/ContainerTokenIdentifier.java)/**
 * TokenIdentifier for a container. Encodes {@link ContainerId},
 * {@link Resource} needed by the container and the target NMs host-address.
 * 
 */
DockerCredentialTokenIdentifier (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/security/DockerCredentialTokenIdentifier.java)/**
 * TokenIdentifier for Docker registry credentials.
 */
Permission (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/security/Permission.java)/**
 * This class contains permissions info for the target object.
 */
PrivilegedEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/security/PrivilegedEntity.java)/**
 * An entity in YARN that can be guarded with ACLs. The entity could be an
 * application or a queue etc. An application entity has access types defined in
 * {@link ApplicationAccessType}, a queue entity has access types defined in
 * {@link QueueACL}.  
 */
YarnAuthorizationProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/security/YarnAuthorizationProvider.java)/**
 * An implementation of the interface will provide authorization related
 * information and enforce permission check. It is excepted that any of the
 * methods defined in this interface should be non-blocking call and should not
 * involve expensive computation as these method could be invoked in RPC.
 */
NodesToAttributesMappingRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodesToAttributesMappingRequestPBImpl.java)/**
 * Proto class for node to attributes mapping request.
 */
NodesToAttributesMappingResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodesToAttributesMappingResponsePBImpl.java)/**
 * Proto class for node to attributes mapping response.
 */
NodeToAttributesPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeToAttributesPBImpl.java)/**
 * Proto class for Node to attributes mapping.
 */
RemoveFromClusterNodeLabelsRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/RemoveFromClusterNodeLabelsRequestPBImpl.java)/**
 * Proto class to handlde RemoveFromClusterNodeLabels request.
 */
InvalidStateTransitionException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/InvalidStateTransitionException.java)/**
 * The exception that happens when you call invalid state transition.
 *
 */
MultipleArcTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/MultipleArcTransition.java)/**
 * Hook for Transition. 
 * Post state is decided by Transition hook. Post state must be one of the 
 * valid post states registered in StateMachine.
 */
MultiStateTransitionListener (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/MultiStateTransitionListener.java)/**
 * A {@link StateTransitionListener} that dispatches the pre and post
 * state transitions to multiple registered listeners.
 * NOTE: The registered listeners are called in a for loop. Clients should
 *       know that a listener configured earlier might prevent a later listener
 *       from being called, if for instance it throws an un-caught Exception.
 */
SingleArcTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/SingleArcTransition.java)/**
 * Hook for Transition. This lead to state machine to move to 
 * the post state as registered in the state machine.
 */
StateMachineFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateMachineFactory.java)/**
 * State machine topology.
 * This object is semantically immutable.  If you have a
 * StateMachineFactory there's no operation in the API that changes
 * its semantic properties.
 *
 * @param <OPERAND> The object type on which this state machine operates.
 * @param <STATE> The state of the entity.
 * @param <EVENTTYPE> The external eventType to be handled.
 * @param <EVENT> The event object.
 *
 */
StateTransitionListener (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateTransitionListener.java)/**
 * A State Transition Listener.
 * It exposes a pre and post transition hook called before and
 * after the transition.
 */
AbstractLivelinessMonitor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java)/**
 * A simple liveliness monitor with which clients can register, trust the
 * component to monitor liveliness, get a call-back on expiry and then finally
 * unregister.
 */
ApplicationClassLoader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ApplicationClassLoader.java)/**
 * This type has been deprecated in favor of
 * {@link org.apache.hadoop.util.ApplicationClassLoader}. All new uses of
 * ApplicationClassLoader should use that type instead.
 */
Apps (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/Apps.java)/**
 * YARN internal application-related utilities
 */
AsyncCallback (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AsyncCallback.java)/**
 * Generic interface that can be used for calling back when a corresponding
 * asynchronous operation completes.
 *
 * @param <T> parameter type for the callback
 */
Clock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/Clock.java)/**
 * A simple clock interface that gives you time.
 */
ConverterUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java)/**
 * This class contains a set of utilities which help converting data structures
 * from/to 'serializableFormat' to/from hadoop/nativejava data structures.
 *
 */
DockerClientConfigHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/DockerClientConfigHandler.java)/**
 * Commonly needed actions for handling the Docker client configurations.
 *
 * Credentials that are used to access private Docker registries are supplied.
 * Actions include:
 * <ul>
 *   <li>Read the Docker client configuration json file from a
 *   {@link FileSystem}.</li>
 *   <li>Extract the authentication information from the configuration into
 *   {@link Token} and {@link Credentials} objects.</li>
 *   <li>Tokens are commonly shipped via the
 *   {@link org.apache.hadoop.yarn.api.records.ContainerLaunchContext} as a
 *   {@link ByteBuffer}, extract the {@link Credentials}.</li>
 *   <li>Write the Docker client configuration json back to the local filesystem
 *   to be used by the Docker command line.</li>
 * </ul>
 */
FSDownload (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/FSDownload.java)/**
 * Download a single URL to the local disk.
 *
 */
LinuxResourceCalculatorPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/LinuxResourceCalculatorPlugin.java)/**
 * Plugin to calculate resource information on Linux systems.
 */
LRUCacheHashMap (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/LRUCacheHashMap.java)/**
 * LRU cache with a configurable maximum cache size and access order.
 */
MonotonicClock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/MonotonicClock.java)/**
 * A monotonic clock from some arbitrary time base in the past, counting in
 * milliseconds, and not affected by settimeofday or similar system clock
 * changes.
 * This is appropriate to use when computing how much longer to wait for an
 * interval to expire.
 * This function can return a negative value and it must be handled correctly
 * by callers. See the documentation of System#nanoTime for caveats.
 */
ProcessInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ProcfsBasedProcessTree.java)/**
   *
   * Class containing information of a process.
   *
   */
ProcessTreeSmapMemInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ProcfsBasedProcessTree.java)/**
   * Placeholder for process's SMAPS information
   */
ProcessSmapMemoryInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ProcfsBasedProcessTree.java)/**
   * <pre>
   * Private Pages : Pages that were mapped only by the process
   * Shared Pages : Pages that were shared with other processes
   *
   * Clean Pages : Pages that have not been modified since they were mapped
   * Dirty Pages : Pages that have been modified since they were mapped
   *
   * Private RSS = Private Clean Pages + Private Dirty Pages
   * Shared RSS = Shared Clean Pages + Shared Dirty Pages
   * RSS = Private RSS + Shared RSS
   * PSS = The count of all pages mapped uniquely by the process, 
   *  plus a fraction of each shared page, said fraction to be 
   *  proportional to the number of processes which have mapped the page.
   * 
   * </pre>
   */
ProcfsBasedProcessTree (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ProcfsBasedProcessTree.java)/**
 * A Proc file-system based ProcessTree. Works only on Linux.
 */
DominantResourceCalculator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/DominantResourceCalculator.java)/**
 * A {@link ResourceCalculator} which uses the concept of
 * <em>dominant resource</em> to compare multi-dimensional resources.
 *
 * Essentially the idea is that the in a multi-resource environment,
 * the resource allocation should be determined by the dominant share
 * of an entity (user or queue), which is the maximum share that the
 * entity has been allocated of any resource.
 *
 * In a nutshell, it seeks to maximize the minimum dominant share across
 * all entities.
 *
 * For example, if user A runs CPU-heavy tasks and user B runs
 * memory-heavy tasks, it attempts to equalize CPU share of user A
 * with Memory-share of user B.
 *
 * In the single resource case, it reduces to max-min fairness for that resource.
 *
 * See the Dominant Resource Fairness paper for more details:
 * www.cs.berkeley.edu/~matei/papers/2011/nsdi_drf.pdf
 */
ResourceCalculator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/ResourceCalculator.java)/**
 * A set of {@link Resource} comparison and manipulation interfaces.
 */
FixedValueResource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/Resources.java)/**
   * Helper class to create a resource with a fixed value for all resource
   * types. For example, a NONE resource which returns 0 for any resource type.
   */
Resources (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/Resources.java)/**
 * Resources is a computation class which provides a set of apis to do
 * mathematical operations on Resource object.
 */
ResourceCalculatorPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ResourceCalculatorPlugin.java)/**
 * Plugin to calculate resource information on the system.
 */
ResourceCalculatorProcessTree (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ResourceCalculatorProcessTree.java)/**
 * Interface class to obtain process resource usage
 * NOTE: This class should not be used by external users, but only by external
 * developers to extend and include their own process-tree implementation, 
 * especially for platforms other than Linux and Windows.
 */
StringHelper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/StringHelper.java)/**
 * Common string manipulation helpers
 */
SystemClock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/SystemClock.java)/**
 * Implementation of {@link Clock} that gives the current time from the system
 * clock in milliseconds.
 * 
 * NOTE: Do not use this to calculate a duration of expire or interval to sleep,
 * because it will be broken by settimeofday. Please use {@link MonotonicClock}
 * instead.
 */
TimelineEntityV2Converter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/timeline/TimelineEntityV2Converter.java)/**
 * Utility class to generate reports from timeline entities.
 */
TimelineUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/timeline/TimelineUtils.java)/**
 * The helper class for the timeline module.
 * 
 */
TrackingUriPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/TrackingUriPlugin.java)/**
 * Plugin to derive a tracking URL from a YARN Application ID
 *
 */
UTCClock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/UTCClock.java)/**
 * Implementation of {@link Clock} that gives the current UTC time in
 * milliseconds.
 */
YarnVersionInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/YarnVersionInfo.java)/**
 * This class finds the package info for Yarn.
 */
QueueConfigInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/dao/QueueConfigInfo.java)/**
 * Information for adding or updating a queue to scheduler configuration
 * for this queue.
 */
SchedConfUpdateInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/dao/SchedConfUpdateInfo.java)/**
 * Information for making scheduler configuration changes (supports adding,
 * removing, or updating a queue, as well as global scheduler conf changes).
 */
Dispatcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/Dispatcher.java)/**
 * The servlet that dispatch request to various controllers
 * according to the user defined routes in the router.
 */
HelloWorld (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/example/HelloWorld.java)/**
 * The obligatory example. No xml/jsp/templates/config files! No
 * proliferation of strange annotations either :)
 *
 * <p>3 in 1 example. Check results at
 * <br>http://localhost:8888/hello and
 * <br>http://localhost:8888/hello/html
 * <br>http://localhost:8888/hello/json
 */
MyApp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/example/MyApp.java)/**
 * The embedded UI serves two pages at:
 * <br>http://localhost:8888/my and
 * <br>http://localhost:8888/my/anythingYouWant
 */
GenericExceptionHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/GenericExceptionHandler.java)/**
 * Handle webservices jersey exceptions and create json or xml response
 * with the ExceptionData.
 */
Hamlet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/Hamlet.java)/**
 * @deprecated Use org.apache.hadoop.yarn.webapp.hamlet2 package instead.
 */
HamletGen (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletGen.java)/**
 * Generates a specific hamlet implementation class from a spec class
 * using a generic hamlet implementation class.
 * @deprecated Use org.apache.hadoop.yarn.webapp.hamlet2 package instead.
 */
EImp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletImpl.java)/**
   * The base class for elements
   * @param <T> type of the parent (containing) element for the element
   */
HamletImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletImpl.java)/**
 * A simple unbuffered generic hamlet implementation.
 *
 * Zero copy but allocation on every element, which could be
 * optimized to use a thread-local element pool.
 *
 * Prints HTML as it builds. So the order is important.
 * @deprecated Use org.apache.hadoop.yarn.webapp.hamlet2 package instead.
 */
_ (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Child (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Script (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Object (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
HeadMisc (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %head.misc */
Heading (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %heading */
Listing (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %list */
Preformatted (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** % preformatted */
CoreAttrs (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %coreattrs */
I18nAttrs (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %i18n */
EventsAttrs (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %events */
Attrs (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %attrs */
_FontSize (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** Part of %pre.exclusion */
_FontStyle (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %fontstyle -(%pre.exclusion) */
FontStyle (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %fontstyle */
Phrase (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %phrase */
_ImgObject (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** Part of %pre.exclusion */
_SubSup (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** Part of %pre.exclusion */
_Anchor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_InsDel (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   * INS and DEL are unusual for HTML
   * "in that they may serve as either block-level or inline elements
   * (but not both)".
   * <br>cf. http://www.w3.org/TR/html4/struct/text.html#h-9.4
   * <br>cf. http://www.w3.org/TR/html5/edits.html#edits
   */
_Special (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %special -(A|%pre.exclusion) */
Special (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %special */
_Label (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_FormCtrl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
FormCtrl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %formctrl */
_Content (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_RawContent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
PCData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** #PCDATA */
Inline (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %inline */
I (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
B (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
SMALL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
EM (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
STRONG (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
DFN (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
CODE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
SAMP (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
KBD (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
VAR (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
CITE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
ABBR (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
ACRONYM (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
SUB (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
SUP (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
SPAN (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
BDO (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** The dir attribute is required for the BDO element */
BR (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Form (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_FieldSet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Block (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %block -(FORM|FIELDSET) */
Block (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %block */
Flow (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/** %flow */
_Body (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
BODY (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
ADDRESS (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
DIV (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
A (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
MAP (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
AREA (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
LINK (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
IMG (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Param (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
OBJECT (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
PARAM (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
HR (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
P (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
H1 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
H2 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
H3 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
H4 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
H5 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
H6 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
PRE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
Q (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
BLOCKQUOTE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
INS (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   * @see _InsDel INS/DEL quirks.
   */
DEL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   * @see _InsDel INS/DEL quirks.
   */
_Dl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
DL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
DT (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
DD (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Li (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
OL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
UL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
LI (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
FORM (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
LABEL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
INPUT (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Option (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
SELECT (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
OPTGROUP (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
OPTION (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
TEXTAREA (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Legend (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
FIELDSET (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
LEGEND (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
BUTTON (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_TableRow (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_TableCol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Table (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
TABLE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   * TBODY should be used after THEAD/TFOOT, iff there're no TABLE.TR elements.
   */
CAPTION (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
THEAD (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
TFOOT (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
TBODY (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
COLGROUP (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
COL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Tr (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
TR (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Cell (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
TH (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
TD (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Head (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
HEAD (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
TITLE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
BASE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
META (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
STYLE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
SCRIPT (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
_Html (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   *
   */
HTML (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
   * The root element
   */
HamletSpec (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet/HamletSpec.java)/**
 * HTML5 compatible HTML4 builder interfaces.
 *
 * <p>Generated from HTML 4.01 strict DTD and HTML5 diffs.
 * <br>cf. http://www.w3.org/TR/html4/
 * <br>cf. http://www.w3.org/TR/html5-diff/
 * <p> The omitted attributes and elements (from the 4.01 DTD)
 * are for HTML5 compatibility.
 *
 * <p>Note, the common argument selector uses the same syntax as Haml/Sass:
 * <pre>  selector ::= (#id)?(.class)*</pre>
 * cf. http://haml-lang.com/
 *
 * <p>The naming convention used in this class is slightly different from
 * normal classes. A CamelCase interface corresponds to an entity in the DTD.
 * _CamelCase is for internal refactoring. An element builder interface is in
 * UPPERCASE, corresponding to an element definition in the DTD. $lowercase is
 * used as attribute builder methods to differentiate from element builder
 * methods.
 * @deprecated Use org.apache.hadoop.yarn.webapp.hamlet2 package instead.
 */
HamletGen (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletGen.java)/**
 * Generates a specific hamlet implementation class from a spec class
 * using a generic hamlet implementation class.
 */
EImp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletImpl.java)/**
   * The base class for elements
   * @param <T> type of the parent (containing) element for the element
   */
HamletImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletImpl.java)/**
 * A simple unbuffered generic hamlet implementation.
 *
 * Zero copy but allocation on every element, which could be
 * optimized to use a thread-local element pool.
 *
 * Prints HTML as it builds. So the order is important.
 */
__ (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Child (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Script (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Object (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
HeadMisc (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %head.misc */
Heading (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %heading */
Listing (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %list */
Preformatted (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** % preformatted */
CoreAttrs (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %coreattrs */
I18nAttrs (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %i18n */
EventsAttrs (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %events */
Attrs (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %attrs */
_FontSize (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** Part of %pre.exclusion */
_FontStyle (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %fontstyle -(%pre.exclusion) */
FontStyle (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %fontstyle */
Phrase (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %phrase */
_ImgObject (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** Part of %pre.exclusion */
_SubSup (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** Part of %pre.exclusion */
_Anchor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_InsDel (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   * INS and DEL are unusual for HTML
   * "in that they may serve as either block-level or inline elements
   * (but not both)".
   * <br>cf. http://www.w3.org/TR/html4/struct/text.html#h-9.4
   * <br>cf. http://www.w3.org/TR/html5/edits.html#edits
   */
_Special (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %special -(A|%pre.exclusion) */
Special (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %special */
_Label (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_FormCtrl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
FormCtrl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %formctrl */
_Content (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_RawContent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
PCData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** #PCDATA */
Inline (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %inline */
I (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
B (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
SMALL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
EM (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
STRONG (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
DFN (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
CODE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
SAMP (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
KBD (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
VAR (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
CITE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
ABBR (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
ACRONYM (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
SUB (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
SUP (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
SPAN (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
BDO (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** The dir attribute is required for the BDO element */
BR (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Form (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_FieldSet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Block (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %block -(FORM|FIELDSET) */
Block (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %block */
Flow (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/** %flow */
_Body (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
BODY (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
ADDRESS (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
DIV (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
A (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
MAP (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
AREA (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
LINK (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
IMG (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Param (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
OBJECT (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
PARAM (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
HR (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
P (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
H1 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
H2 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
H3 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
H4 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
H5 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
H6 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
PRE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
Q (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
BLOCKQUOTE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
INS (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   * @see _InsDel INS/DEL quirks.
   */
DEL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   * @see _InsDel INS/DEL quirks.
   */
_Dl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
DL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
DT (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
DD (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Li (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
OL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
UL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
LI (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
FORM (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
LABEL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
INPUT (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Option (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
SELECT (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
OPTGROUP (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
OPTION (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
TEXTAREA (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Legend (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
FIELDSET (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
LEGEND (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
BUTTON (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_TableRow (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_TableCol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Table (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
TABLE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   * TBODY should be used after THEAD/TFOOT, iff there're no TABLE.TR elements.
   */
CAPTION (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
THEAD (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
TFOOT (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
TBODY (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
COLGROUP (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
COL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Tr (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
TR (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Cell (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
TH (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
TD (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Head (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
HEAD (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
TITLE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
BASE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
META (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
STYLE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
SCRIPT (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
_Html (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   *
   */
HTML (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
   * The root element
   */
HamletSpec (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/hamlet2/HamletSpec.java)/**
 * HTML5 compatible HTML4 builder interfaces.
 *
 * <p>Generated from HTML 4.01 strict DTD and HTML5 diffs.
 * <br>cf. http://www.w3.org/TR/html4/
 * <br>cf. http://www.w3.org/TR/html5-diff/
 * <p> The omitted attributes and elements (from the 4.01 DTD)
 * are for HTML5 compatibility.
 *
 * <p>Note, the common argument selector uses the same syntax as Haml/Sass:
 * <pre>  selector ::= (#id)?(.class)*</pre>
 * cf. http://haml-lang.com/
 *
 * <p>The naming convention used in this class is slightly different from
 * normal classes. A CamelCase interface corresponds to an entity in the DTD.
 * _CamelCase is for internal refactoring. An element builder interface is in
 * UPPERCASE, corresponding to an element definition in the DTD. $lowercase is
 * used as attribute builder methods to differentiate from element builder
 * methods.
 */
Params (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/Params.java)/**
 * Public static constants for webapp parameters. Do NOT put any
 * private or application specific constants here as they're part of
 * the API for users of the controllers and views.
 */
RemoteExceptionData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/RemoteExceptionData.java)/**
 * Contains the exception information from an exception thrown
 * by the web service REST API's.
 * Fields include:
 *   exception - exception type
 *   javaClassName - java class name of the exception
 *   message - a detailed message explaining the exception
 *
 */
ResponseInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/ResponseInfo.java)/**
 * A class to help passing around request scoped info
 */
Router (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/Router.java)/**
 * Manages path info to controller#action routing.
 */
SubView (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/SubView.java)/**
 * Interface for SubView to avoid top-level inclusion
 */
ToJSON (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/ToJSON.java)/**
 * A light-weight JSON rendering interface
 */
ThrowingBiFunction (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/util/WebAppUtils.java)/** A BiFunction which throws on Exception. */
YarnWebServiceUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/util/YarnWebServiceUtils.java)/**
 * This class contains several utility function which could be used to generate
 * Restful calls to RM/NM/AHS.
 *
 */
ErrorPage (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/view/ErrorPage.java)/**
 * A jquery-ui themeable error page
 */
Html (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/view/Html.java)/**
 * This class holds utility functions for HTML
 */
HtmlPage (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/view/HtmlPage.java)/**
 * The parent class of all HTML pages.  Override 
 * {@link #render(org.apache.hadoop.yarn.webapp.hamlet2.Hamlet.HTML)}
 * to actually render the page.
 */
TwoColumnCssLayout (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/view/TwoColumnCssLayout.java)/**
 * A reusable, pure-css, cross-browser, left nav, 2 column,
 * supposedly liquid layout.
 * Doesn't quite work with resizable themes, kept as an example of the
 * sad state of css (v2/3 anyway) layout.
 * @see TwoColumnLayout
 */
TwoColumnLayout (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/view/TwoColumnLayout.java)/**
 * A simpler two column layout implementation with a header, a navigation bar
 * on the left, content on the right, and a footer. Works with resizable themes.
 * @see TwoColumnCssLayout
 */
View (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/View.java)/**
 * Base class for all views
 */
WebApp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/WebApp.java)/**
 * @see WebApps for a usage example
 */
WebApps (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/WebApps.java)/**
 * Helpers to create an embedded webapp.
 *
 * <b>Quick start:</b>
 * <pre>
 *   WebApp wa = WebApps.$for(myApp).start();</pre>
 * Starts a webapp with default routes binds to 0.0.0.0 (all network interfaces)
 * on an ephemeral port, which can be obtained with:<pre>
 *   int port = wa.port();</pre>
 * <b>With more options:</b>
 * <pre>
 *   WebApp wa = WebApps.$for(myApp).at(address, port).
 *                        with(configuration).
 *                        start(new WebApp() {
 *     &#064;Override public void setup() {
 *       route("/foo/action", FooController.class);
 *       route("/foo/:id", FooController.class, "show");
 *     }
 *   });</pre>
 */
YarnJacksonJaxbJsonProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/YarnJacksonJaxbJsonProvider.java)/**
 * YARN's implementation of JAX-RS abstractions based on
 * {@link JacksonJaxbJsonProvider} needed for deserialize JSON content to or
 * serialize it from POJO objects.
 */
YarnUncaughtExceptionHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/YarnUncaughtExceptionHandler.java)/**
 * This class is intended to be installed by calling 
 * {@link Thread#setDefaultUncaughtExceptionHandler(UncaughtExceptionHandler)}
 * In the main entry point.  It is intended to try and cleanly shut down
 * programs using the YARN Event framework.
 * 
 * Note: Right now it only will shut down the program if a Error is caught, but
 * not any other exception.  Anything else is just logged.
 */
BasePBImplRecordsTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/api/BasePBImplRecordsTest.java)/**
 * Generic helper class to validate protocol records.
 */
TestPlacementConstraintTransformations (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/api/resource/TestPlacementConstraintTransformations.java)/**
 * Test class for {@link PlacementConstraintTransformations}.
 */
TestPBImplRecords (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/api/TestPBImplRecords.java)/**
 * Test class for YARN API protocol records.
 */
TestPlacementConstraintPBConversion (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/api/TestPlacementConstraintPBConversion.java)/**
 * Test class for {@link PlacementConstraintToProtoConverter} and
 * {@link PlacementConstraintFromProtoConverter}.
 */
TestResourcePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/api/TestResourcePBImpl.java)/**
 * Test class to handle various proto related tests for resources.
 */
TestResourceRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/api/TestResourceRequest.java)/**
 * The class to test {@link ResourceRequest}.
 */
TestTimelineReaderClientImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineReaderClientImpl.java)/**
 * Test class for Timeline Reader Client.
 */
UGICapturingHadoopYarnProtoRPC (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/client/TestClientRMProxy.java)/**
   * Subclass of {@link HadoopYarnProtoRPC} which captures the current UGI in
   * a static variable.  Used by {@link #testProxyUserCorrectUGI()}.
   */
TestRpcClientFactoryPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/factories/impl/pb/TestRpcClientFactoryPBImpl.java)/**
 * Test class for RpcClientFactoryPBImpl.
 */
TestRpcServerFactoryPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/factories/impl/pb/TestRpcServerFactoryPBImpl.java)/**
 * Test class for RpcServerFactoryPBImpl.
 */
TestLogAggregationIndexedFileController (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/logaggregation/filecontroller/ifile/TestLogAggregationIndexedFileController.java)/**
 * Function test for {@link LogAggregationIndexedFileController}.
 *
 */
TestLogAggregationFileController (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/logaggregation/filecontroller/TestLogAggregationFileController.java)/**
 * Test for the abstract {@link LogAggregationFileController} class,
 * checking its core functionality.
 */
TestLogAggregationFileControllerFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/logaggregation/filecontroller/TestLogAggregationFileControllerFactory.java)/**
 * Test LogAggregationFileControllerFactory.
 */
TestAggregatedLogsBlock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/logaggregation/TestAggregatedLogsBlock.java)/**
 * Test AggregatedLogsBlock. AggregatedLogsBlock should check user, aggregate a
 * logs into one file and show this logs or errors into html code
 * 
 */
TestContainerLogsUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/logaggregation/TestContainerLogsUtils.java)/**
 * This class contains several utility functions for log aggregation tests.
 */
MockApps (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/MockApps.java)/**
 * Utilities to generate fake test apps
 */
TestNodeLabelUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/nodelabels/TestNodeLabelUtil.java)/**
 * Test class to verify node label util ops.
 */
ResourceTypesTestHelper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/resourcetypes/ResourceTypesTestHelper.java)/**
 * Contains helper methods to create Resource and ResourceInformation objects.
 * ResourceInformation can be created from a resource name
 * and a resource descriptor as well that comprises amount and unit.
 */
TestDockerClientConfigHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/security/TestDockerClientConfigHandler.java)/**
 * Test the functionality of the DockerClientConfigHandler.
 */
CustomResourceTypesConfigurationProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/resource/CustomResourceTypesConfigurationProvider.java)/**
 * This class can generate an XML configuration file of custom resource types.
 * See createInitial ResourceTypes for the default values. All custom resource
 * type is prefixed with CUSTOM_RESOURCE_PREFIX. Please use the
 * getConfigurationInputStream method to get an InputStream of the XML.
 *
 */
TestResourceUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/resource/TestResourceUtils.java)/**
 * Test class to verify all resource utility methods.
 */
TestBoundedAppender (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestBoundedAppender.java)/**
 * Test class for {@link BoundedAppender}.
 */
TestFSDownload (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestFSDownload.java)/**
 * Unit test for the FSDownload class.
 */
TestLRUCacheHashMap (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestLRUCacheHashMap.java)/**
 * Test class to validate the correctness of the {@code LRUCacheHashMap}.
 *
 */
TestProcfsBasedProcessTree (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestProcfsBasedProcessTree.java)/**
 * A JUnit test to test ProcfsBasedProcessTree.
 */
MultipleResolver (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java)/**
   * This class is to test the resolve method which accepts a list of hosts
   * in RackResolver.
   */
TestResourceCalculatorProcessTree (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestResourceCalculatorProcessTree.java)/**
 * A JUnit test to test {@link ResourceCalculatorPlugin}
 */
TestYarnVersionInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestYarnVersionInfo.java)/**
 * A JUnit test to test {@link YarnVersionInfo}
 */
TestShortenedFlowName (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/timeline/TestShortenedFlowName.java)/**
 * Test case for limiting flow name size.
 */
GuiceServletConfig (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/webapp/GuiceServletConfig.java)/**
 * GuiceServletConfig is a wrapper class to have a static Injector instance
 * instead of having the instance inside test classes. This allow us to use
 * Jersey test framework after 1.13.
 * Please check test cases to know how to use this class:
 * e.g. TestRMWithCSRFFilter.java
 */
CsiAdaptorFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/adaptor/CsiAdaptorFactory.java)/**
 * Desired csi-adaptor implementation is configurable, default to
 * CsiAdaptorProtocolService. If user wants to have a different implementation,
 * just to configure a different class for the csi-driver.
 */
CsiAdaptorProtocolService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/adaptor/CsiAdaptorProtocolService.java)/**
 * This is a Hadoop RPC server, we uses the Hadoop RPC framework here
 * because we need to stick to the security model current Hadoop supports.
 */
CsiAdaptorServices (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/adaptor/CsiAdaptorServices.java)/**
 * NM manages csi-adaptors as a single NM AUX service, this service
 * manages a set of rpc services and each of them serves one particular
 * csi-driver. It loads all available drivers from configuration, and
 * find a csi-driver-adaptor implementation class for each of them. At last
 * it brings up all of them as a composite service.
 */
DefaultCsiAdaptorImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/adaptor/DefaultCsiAdaptorImpl.java)/**
 * The default implementation of csi-driver-adaptor service.
 */
CsiClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/client/CsiClient.java)/**
 * General interface for a CSI client. This interface defines all APIs
 * that CSI spec supports, including both identity/controller/node service
 * APIs.
 */
CsiClientImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/client/CsiClientImpl.java)/**
 * A CSI client implementation that communicates with a CSI driver via
 * unix domain socket. It leverages gRPC blocking stubs to synchronize
 * the call with CSI driver. CSI spec is designed as a set of synchronized
 * APIs, in order to make the call idempotent for failure recovery,
 * so the client does the same.
 */
GrpcClientBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/client/CsiGrpcClient.java)/**
   * The Grpc Client builder.
   */
CsiGrpcClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/client/CsiGrpcClient.java)/**
 * A CSI gRPC client, it connects a CSI driver via a given unix domain socket.
 */
GetPluginInfoResponseProtoTranslator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/translator/GetPluginInfoResponseProtoTranslator.java)/**
 * Protobuf message translator for GetPluginInfoResponse and
 * Csi.GetPluginInfoResponse.
 */
NodePublishVolumeRequestProtoTranslator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/translator/NodePublishVolumeRequestProtoTranslator.java)/**
 * This class helps to transform a YARN side NodePublishVolumeRequest
 * to corresponding CSI protocol message.
 * @param <A> YARN NodePublishVolumeRequest
 * @param <B> CSI NodePublishVolumeRequest
 */
NodeUnpublishVolumeRequestProtoTranslator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/translator/NodeUnpublishVolumeRequestProtoTranslator.java)/**
 * This class helps to transform a YARN side NodeUnpublishVolumeRequest
 * to corresponding CSI protocol message.
 * @param <A> YARN NodeUnpublishVolumeRequest
 * @param <B> CSI NodeUnpublishVolumeRequest
 */
ProtoTranslator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/translator/ProtoTranslator.java)/**
 * ProtoTranslator converts a YARN side message to CSI proto message
 * and vice versa. Each CSI proto message should have a corresponding
 * YARN side message implementation, and a transformer to convert them
 * one to the other. This layer helps we to hide CSI spec messages
 * from YARN components.
 *
 * @param <A> YARN side internal messages
 * @param <B> CSI proto messages
 */
ProtoTranslatorFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/translator/ProtoTranslatorFactory.java)/**
 * Factory class to get desired proto transformer instance.
 */
ValidateVolumeCapabilitiesRequestProtoTranslator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/translator/ValidateVolumeCapabilitiesRequestProtoTranslator.java)/**
 * Proto message translator for ValidateVolumeCapabilitiesRequest.
 * @param <A> ValidateVolumeCapabilitiesRequest
 * @param <B> Csi.ValidateVolumeCapabilitiesRequest
 */
ValidationVolumeCapabilitiesResponseProtoTranslator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/translator/ValidationVolumeCapabilitiesResponseProtoTranslator.java)/**
 * Proto message translator for ValidateVolumeCapabilitiesResponse.
 * @param <A> ValidateVolumeCapabilitiesResponse
 * @param <B> Csi.ValidateVolumeCapabilitiesResponse
 */
GrpcHelper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/main/java/org/apache/hadoop/yarn/csi/utils/GrpcHelper.java)/**
 * Helper classes for gRPC utility functions.
 */
MockCsiAdaptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/test/java/org/apache/hadoop/yarn/csi/adaptor/MockCsiAdaptor.java)/**
 * This class is used by {@link TestCsiAdaptorService} for testing.
 * It gives some dummy implementation for a adaptor plugin, and used to
 * verify the plugin can be properly loaded by NM and execution logic is
 * as expected.
 *
 * This is created as a separated class instead of an inner class, because
 * {@link CsiAdaptorServices} is loading classes using conf.getClass(),
 * the utility class is unable to resolve inner classes.
 */
TestCsiAdaptorService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/test/java/org/apache/hadoop/yarn/csi/adaptor/TestCsiAdaptorService.java)/**
 * UT for {@link CsiAdaptorProtocolService}.
 */
TestGetPluginInfoRequestResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/test/java/org/apache/hadoop/yarn/csi/adaptor/TestGetPluginInfoRequestResponse.java)/**
 * Verify the integrity of GetPluginInfoRequest and GetPluginInfoResponse.
 */
TestNodePublishVolumeRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/test/java/org/apache/hadoop/yarn/csi/adaptor/TestNodePublishVolumeRequest.java)/**
 * UT for NodePublishVolumeRequest.
 */
TestValidateVolumeCapabilityRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/test/java/org/apache/hadoop/yarn/csi/adaptor/TestValidateVolumeCapabilityRequest.java)/**
 * UT for message exchanges.
 */
TestValidateVolumeCapabilityResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/test/java/org/apache/hadoop/yarn/csi/adaptor/TestValidateVolumeCapabilityResponse.java)/**
 * UT for message exchanges.
 */
FakeCsiDriver (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/test/java/org/apache/hadoop/yarn/csi/client/FakeCsiDriver.java)/**
 * A fake implementation of CSI driver.
 * This is for testing purpose only.
 */
FakeCsiIdentityService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/test/java/org/apache/hadoop/yarn/csi/client/FakeCsiIdentityService.java)/**
 * A fake implementation of CSI identity plugin gRPC service.
 * This is for testing purpose only.
 */
ICsiClientTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/test/java/org/apache/hadoop/yarn/csi/client/ICsiClientTest.java)/**
 * This interface is used only in testing. It gives default implementation
 * of all methods.
 */
TestCsiClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-csi/src/test/java/org/apache/hadoop/yarn/csi/client/TestCsiClient.java)/**
 * Test class for CSI client.
 */
ApplicationHistoryServer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryServer.java)/**
 * History server that keeps track of all types of history in the cluster.
 * Application specific history to start with.
 */
ApplicationHistoryStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryStore.java)/**
 * This class is the abstract of the storage of the application history data. It
 * is a {@link Service}, such that the implementation of this class can make use
 * of the service life cycle to initialize and cleanup the storage. Users can
 * access the storage via {@link ApplicationHistoryReader} and
 * {@link ApplicationHistoryWriter} interfaces.
 * 
 */
ApplicationHistoryWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryWriter.java)/**
 * It is the interface of writing the application history, exposing the methods
 * of writing {@link ApplicationStartData}, {@link ApplicationFinishData}
 * {@link ApplicationAttemptStartData}, {@link ApplicationAttemptFinishData},
 * {@link ContainerStartData} and {@link ContainerFinishData}.
 */
FileSystemApplicationHistoryStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/FileSystemApplicationHistoryStore.java)/**
 * File system implementation of {@link ApplicationHistoryStore}. In this
 * implementation, one application will have just one file in the file system,
 * which contains all the history data of one application, and its attempts and
 * containers. {@link #applicationStarted(ApplicationStartData)} is supposed to
 * be invoked first when writing any history data of one application and it will
 * open a file, while {@link #applicationFinished(ApplicationFinishData)} is
 * supposed to be last writing operation and will close the file.
 */
MemoryApplicationHistoryStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/MemoryApplicationHistoryStore.java)/**
 * In-memory implementation of {@link ApplicationHistoryStore}. This
 * implementation is for test purpose only. If users improperly instantiate it,
 * they may encounter reading and writing history data in different memory
 * store.
 * 
 */
NullApplicationHistoryStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/NullApplicationHistoryStore.java)/**
 * Dummy implementation of {@link ApplicationHistoryStore}. If this
 * implementation is used, no history data will be persisted.
 * 
 */
ApplicationAttemptFinishData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/records/ApplicationAttemptFinishData.java)/**
 * The class contains the fields that can be determined when
 * <code>RMAppAttempt</code> finishes, and that need to be stored persistently.
 */
ApplicationAttemptHistoryData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/records/ApplicationAttemptHistoryData.java)/**
 * The class contains all the fields that are stored persistently for
 * <code>RMAppAttempt</code>.
 */
ApplicationAttemptStartData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/records/ApplicationAttemptStartData.java)/**
 * The class contains the fields that can be determined when
 * <code>RMAppAttempt</code> starts, and that need to be stored persistently.
 */
ApplicationFinishData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/records/ApplicationFinishData.java)/**
 * The class contains the fields that can be determined when <code>RMApp</code>
 * finishes, and that need to be stored persistently.
 */
ApplicationHistoryData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/records/ApplicationHistoryData.java)/**
 * The class contains all the fields that are stored persistently for
 * <code>RMApp</code>.
 */
ApplicationStartData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/records/ApplicationStartData.java)/**
 * The class contains the fields that can be determined when <code>RMApp</code>
 * starts, and that need to be stored persistently.
 */
ContainerFinishData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/records/ContainerFinishData.java)/**
 * The class contains the fields that can be determined when
 * <code>RMContainer</code> finishes, and that need to be stored persistently.
 */
ContainerHistoryData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/records/ContainerHistoryData.java)/**
 * The class contains all the fields that are stored persistently for
 * <code>RMContainer</code>.
 */
ContainerStartData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/records/ContainerStartData.java)/**
 * The class contains the fields that can be determined when
 * <code>RMContainer</code> starts, and that need to be stored persistently.
 */
AHSErrorsAndWarningsPage (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/webapp/AHSErrorsAndWarningsPage.java)/**
 * Class to display the Errors and Warnings page for the AHS.
 */
ContextFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/webapp/ContextFactory.java)/**
 * ContextFactory to reuse JAXBContextImpl for DAO Classes.
 */
EntityIdentifier (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityIdentifier.java)/**
 * The unique identifier for an entity
 */
GenericObjectMapper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/GenericObjectMapper.java)/**
 * A utility class providing methods for serializing and deserializing
 * objects. The {@link #write(Object)} and {@link #read(byte[])} methods are
 * used by the {@link LeveldbTimelineStore} to store and retrieve arbitrary
 * JSON, while the {@link #writeReverseOrderedLong} and {@link
 * #readReverseOrderedLong} methods are used to sort entities in descending
 * start time order.
 */
KeyValueBasedTimelineStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/KeyValueBasedTimelineStore.java)/**
 * Map based implementation of {@link TimelineStore}. A hash map
 * implementation should be connected to this implementation through a
 * {@link TimelineStoreMapAdapter}.
 *
 * The methods are synchronized to avoid concurrent modifications.
 *
 */
LeveldbTimelineStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/LeveldbTimelineStore.java)/**
 * <p>An implementation of an application timeline store backed by leveldb.</p>
 *
 * <p>There are three sections of the db, the start time section,
 * the entity section, and the indexed entity section.</p>
 *
 * <p>The start time section is used to retrieve the unique start time for
 * a given entity. Its values each contain a start time while its keys are of
 * the form:</p>
 * <pre>
 *   START_TIME_LOOKUP_PREFIX + entity type + entity id</pre>
 *
 * <p>The entity section is ordered by entity type, then entity start time
 * descending, then entity ID. There are four sub-sections of the entity
 * section: events, primary filters, related entities,
 * and other info. The event entries have event info serialized into their
 * values. The other info entries have values corresponding to the values of
 * the other info name/value map for the entry (note the names are contained
 * in the key). All other entries have empty values. The key structure is as
 * follows:</p>
 * <pre>
 *   ENTITY_ENTRY_PREFIX + entity type + revstarttime + entity id
 *
 *   ENTITY_ENTRY_PREFIX + entity type + revstarttime + entity id +
 *     EVENTS_COLUMN + reveventtimestamp + eventtype
 *
 *   ENTITY_ENTRY_PREFIX + entity type + revstarttime + entity id +
 *     PRIMARY_FILTERS_COLUMN + name + value
 *
 *   ENTITY_ENTRY_PREFIX + entity type + revstarttime + entity id +
 *     OTHER_INFO_COLUMN + name
 *
 *   ENTITY_ENTRY_PREFIX + entity type + revstarttime + entity id +
 *     RELATED_ENTITIES_COLUMN + relatedentity type + relatedentity id
 *
 *   ENTITY_ENTRY_PREFIX + entity type + revstarttime + entity id +
 *     DOMAIN_ID_COLUMN
 *
 *   ENTITY_ENTRY_PREFIX + entity type + revstarttime + entity id +
 *     INVISIBLE_REVERSE_RELATED_ENTITIES_COLUMN + relatedentity type +
 *     relatedentity id</pre>
 *
 * <p>The indexed entity section contains a primary filter name and primary
 * filter value as the prefix. Within a given name/value, entire entity
 * entries are stored in the same format as described in the entity section
 * above (below, "key" represents any one of the possible entity entry keys
 * described above).</p>
 * <pre>
 *   INDEXED_ENTRY_PREFIX + primaryfilter name + primaryfilter value +
 *     key</pre>
 */
MemoryTimelineStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/MemoryTimelineStore.java)/**
 * In-memory implementation of {@link TimelineStore}. This
 * implementation is for test purpose only. If users improperly instantiate it,
 * they may encounter reading and writing history data in different memory
 * store.
 * 
 * The methods are synchronized to avoid concurrent modification on the memory.
 * 
 */
NameValuePair (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/NameValuePair.java)/**
 * A class holding a name and value pair, used for specifying filters in
 * {@link TimelineReader}.
 */
LeveldbTimelineStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/recovery/LeveldbTimelineStateStore.java)/**
 * A timeline service state storage implementation that supports any persistent
 * storage that adheres to the LevelDB interface.
 */
MemoryTimelineStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/recovery/MemoryTimelineStateStore.java)/**
 * A state store backed by memory for unit tests
 */
RollingWriteBatch (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDB.java)/**
   * Convenience class for associating a write batch with its rolling leveldb
   * instance.
   */
RollingLevelDB (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDB.java)/**
 * Contains the logic to lookup a leveldb by timestamp so that multiple smaller
 * databases can roll according to the configured period and evicted efficiently
 * via operating system directory removal.
 */
RollingLevelDBTimelineStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java)/**
 * <p>
 * An implementation of an application timeline store backed by leveldb.
 * </p>
 *
 * <p>
 * There are three sections of the db, the start time section, the entity
 * section, and the indexed entity section.
 * </p>
 *
 * <p>
 * The start time section is used to retrieve the unique start time for a given
 * entity. Its values each contain a start time while its keys are of the form:
 * </p>
 *
 * <pre>
 *   START_TIME_LOOKUP_PREFIX + entity type + entity id
 * </pre>
 *
 * <p>
 * The entity section is ordered by entity type, then entity start time
 * descending, then entity ID. There are four sub-sections of the entity
 * section: events, primary filters, related entities, and other info. The event
 * entries have event info serialized into their values. The other info entries
 * have values corresponding to the values of the other info name/value map for
 * the entry (note the names are contained in the key). All other entries have
 * empty values. The key structure is as follows:
 * </p>
 *
 * <pre>
 *   ENTITY_ENTRY_PREFIX + entity type + revstarttime + entity id
 *
 *   ENTITY_ENTRY_PREFIX + entity type + revstarttime + entity id +
 *     DOMAIN_ID_COLUMN
 *
 *   ENTITY_ENTRY_PREFIX + entity type + revstarttime + entity id +
 *     EVENTS_COLUMN + reveventtimestamp + eventtype
 *
 *   ENTITY_ENTRY_PREFIX + entity type + revstarttime + entity id +
 *     PRIMARY_FILTERS_COLUMN + name + value
 *
 *   ENTITY_ENTRY_PREFIX + entity type + revstarttime + entity id +
 *     OTHER_INFO_COLUMN + name
 *
 *   ENTITY_ENTRY_PREFIX + entity type + revstarttime + entity id +
 *     RELATED_ENTITIES_COLUMN + relatedentity type + relatedentity id
 * </pre>
 *
 * <p>
 * The indexed entity section contains a primary filter name and primary filter
 * value as the prefix. Within a given name/value, entire entity entries are
 * stored in the same format as described in the entity section above (below,
 * "key" represents any one of the possible entity entry keys described above).
 * </p>
 *
 * <pre>
 *   INDEXED_ENTRY_PREFIX + primaryfilter name + primaryfilter value +
 *     key
 * </pre>
 */
TimelinePolicyProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/security/authorize/TimelinePolicyProvider.java)/**
 * {@link PolicyProvider} for YARN timeline server protocols.
 */
TimelineACLsManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/security/TimelineACLsManager.java)/**
 * <code>TimelineACLsManager</code> check the entity level timeline data access.
 */
TimelineV1DelegationTokenSecretManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/security/TimelineV1DelegationTokenSecretManagerService.java)/**
   * Delegation token secret manager for ATSv1 and ATSv1.5.
   */
TimelineV1DelegationTokenSecretManagerService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/security/TimelineV1DelegationTokenSecretManagerService.java)/**
 * The service wrapper of {@link TimelineV1DelegationTokenSecretManager}.
 */
TimelineDataManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineDataManager.java)/**
 * The class wrap over the timeline store and the ACLs manager. It does some non
 * trivial manipulation of the timeline data before putting or after getting it
 * from the timeline store, and checks the user's access to it.
 *
 */
TimelineDataManagerMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineDataManagerMetrics.java)/** This class tracks metrics for the TimelineDataManager. */
TimelineReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineReader.java)/**
 * This interface is for retrieving timeline information.
 */
TimelineStoreMapAdapter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreMapAdapter.java)/**
 * An adapter for map timeline store implementations
 * @param <K> the type of the key set
 * @param <V> the type of the value set
 */
TimelineWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineWriter.java)/**
 * This interface is for storing timeline information.
 */
KeyBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/util/LeveldbUtils.java)/** A string builder utility for building timeline server leveldb keys. */
TestTimelineAuthenticationFilterForV1 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/security/TestTimelineAuthenticationFilterForV1.java)/**
 * Test cases for authentication via TimelineAuthenticationFilter while
 * publishing entities for ATSv1.
 */
MyRollingLevelDB (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestRollingLevelDB.java)/** RollingLevelDB for testing that has a setting current time. */
TestRollingLevelDB (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestRollingLevelDB.java)/** Test class for verification of RollingLevelDB. */
TestRollingLevelDBTimelineStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestRollingLevelDBTimelineStore.java)/** Test class to verify RollingLevelDBTimelineStore. */
ZKWatcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/lib/ZKClient.java)/**
   * a watcher class that handles what events from
   * zookeeper.
   *
   */
ZKClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/lib/ZKClient.java)/** ZK Registration Library
 * currently does not use any authorization
 */
AsyncAllocateRequestInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/AMHeartbeatRequestHandler.java)/**
   * Data structure that encapsulates AllocateRequest and AsyncCallback
   * instance.
   */
HeartBeatThreadUncaughtExceptionHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/AMHeartbeatRequestHandler.java)/**
   * Uncaught exception handler for the background heartbeat thread.
   */
AMHeartbeatRequestHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/AMHeartbeatRequestHandler.java)/**
 * Extends Thread and provides an implementation that is used for processing the
 * AM heart beat request asynchronously and sending back the response using the
 * callback method registered with the system.
 */
AMRMClientRelayer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/AMRMClientRelayer.java)/**
 * A component that sits in between AMRMClient(Impl) and Yarn RM. It remembers
 * pending requests similar to AMRMClient, and handles RM re-sync automatically
 * without propagate the re-sync exception back to AMRMClient.
 */
CollectorNodemanagerProtocol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/CollectorNodemanagerProtocol.java)/**
 * <p>The protocol between an <code>TimelineCollectorManager</code> and a
 * <code>NodeManager</code> to report a new application collector get launched.
 * </p>
 *
 */
DistributedSchedulingAMProtocol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/DistributedSchedulingAMProtocol.java)/**
 * <p>
 * This protocol extends the <code>ApplicationMasterProtocol</code>. It is used
 * by the <code>DistributedScheduler</code> running on the NodeManager to wrap
 * the request / response objects of the <code>registerApplicationMaster</code>
 * and <code>allocate</code> methods of the protocol with additional information
 * required to perform distributed scheduling.
 * </p>
 */
DistributedSchedulingAMProtocolPBClientImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/impl/pb/client/DistributedSchedulingAMProtocolPBClientImpl.java)/**
 * Implementation of {@link DistributedSchedulingAMProtocol}, used when
 * distributed scheduling is enabled.
 */
DistributedSchedulingAMProtocolPBServiceImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/impl/pb/service/DistributedSchedulingAMProtocolPBServiceImpl.java)/**
 * Implementation of {@link DistributedSchedulingAMProtocolPB}.
 */
DistributedSchedulingAllocateRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/DistributedSchedulingAllocateRequest.java)/**
 * Object used by the Application Master when distributed scheduling is enabled,
 * in order to forward the {@link AllocateRequest} for GUARANTEED containers to
 * the Resource Manager, and to notify the Resource Manager about the allocation
 * of OPPORTUNISTIC containers through the Distributed Scheduler.
 */
DistributedSchedulingAllocateResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/DistributedSchedulingAllocateResponse.java)/**
 * This is the response of the Resource Manager to the
 * {@link DistributedSchedulingAllocateRequest}, when distributed scheduling is
 * enabled. It includes the {@link AllocateResponse} for the GUARANTEED
 * containers allocated by the Resource Manager. Moreover, it includes a list
 * with the nodes that can be used by the Distributed Scheduler when allocating
 * containers.
 */
DistributedSchedulingAllocateRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/DistributedSchedulingAllocateRequestPBImpl.java)/**
 * Implementation of {@link DistributedSchedulingAllocateRequest}.
 */
DistributedSchedulingAllocateResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/DistributedSchedulingAllocateResponsePBImpl.java)/**
 * Implementation of {@link DistributedSchedulingAllocateResponse}.
 */
NodeHeartbeatResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatResponsePBImpl.java)/**
 * PBImpl class for NodeHeartbeatResponse.
 */
RegisterDistributedSchedulingAMResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/RegisterDistributedSchedulingAMResponsePBImpl.java)/**
 * Implementation of {@link RegisterDistributedSchedulingAMResponse}.
 */
RegisterNodeManagerResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/RegisterNodeManagerResponsePBImpl.java)/**
 * PBImpl class for RegisterNodeManagerResponse.
 */
RemoteNodePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/RemoteNodePBImpl.java)/**
 * Implementation of {@link RemoteNode}.
 */
UnRegisterNodeManagerRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/UnRegisterNodeManagerRequestPBImpl.java)/**
 * PBImpl class for UnRegisterNodeManagerRequest.
 */
UnRegisterNodeManagerResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/UnRegisterNodeManagerResponsePBImpl.java)/**
 * PBImpl class for UnRegisterNodeManagerResponse.
 */
LogAggregationReport (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/LogAggregationReport.java)/**
 * {@code LogAggregationReport} is a report for log aggregation status
 * in one NodeManager of an application.
 * <p>
 * It includes details such as:
 * <ul>
 *   <li>{@link ApplicationId} of the application.</li>
 *   <li>{@link LogAggregationStatus}</li>
 *   <li>Diagnostic information</li>
 * </ul>
 *
 */
NMContainerStatus (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NMContainerStatus.java)/**
 * NMContainerStatus includes the current information of a container. This
 * record is used by YARN only, whereas {@link ContainerStatus} is used both
 * inside YARN and by end-users.
 */
NodeHeartbeatResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatResponse.java)/**
 * Node Manager's heartbeat response.
 */
RegisterDistributedSchedulingAMResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/RegisterDistributedSchedulingAMResponse.java)/**
 * This is the response to registering an Application Master when distributed
 * scheduling is enabled. Apart from the
 * {@link RegisterApplicationMasterResponse}, it includes various parameters
 * to be used during distributed scheduling, such as the min and max resources
 * that can be requested by containers.
 */
RegisterNodeManagerResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/RegisterNodeManagerResponse.java)/**
 * Node Manager's register response.
 */
RemoteNode (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/RemoteNode.java)/**
 * This class is used to encapsulate the {@link NodeId} as well as the HTTP
 * address that can be used to communicate with the Node.
 */
SCMUploaderCanUploadRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/SCMUploaderCanUploadRequest.java)/**
 * <p>
 * The request from the NodeManager to the <code>SharedCacheManager</code> that
 * requests whether it can upload a resource in the shared cache.
 * </p>
 */
SCMUploaderCanUploadResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/SCMUploaderCanUploadResponse.java)/**
 * <p>
 * The response from the SharedCacheManager to the NodeManager that indicates
 * whether the NodeManager can upload the resource to the shared cache. If it is
 * not accepted by SCM, the NodeManager should not upload it to the shared
 * cache.
 * </p>
 */
SCMUploaderNotifyRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/SCMUploaderNotifyRequest.java)/**
 * <p>
 * The request from the NodeManager to the <code>SharedCacheManager</code> that
 * notifies that a resource has been uploaded to the shared cache. The
 * <code>SharedCacheManager</code> may reject the resource for various reasons,
 * in which case the NodeManager should remove it from the shared cache.
 * </p>
 */
SCMUploaderNotifyResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/SCMUploaderNotifyResponse.java)/**
 * <p>
 * The response from the SharedCacheManager to the NodeManager that indicates
 * whether the NodeManager needs to delete the cached resource it was sending
 * the notification for.
 * </p>
 */
UnRegisterNodeManagerRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/UnRegisterNodeManagerRequest.java)/**
 * Node Manager's unregister request.
 */
UnRegisterNodeManagerResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/UnRegisterNodeManagerResponse.java)/**
 * Node Manager's unregister response.
 */
ContainerQueuingLimit (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/records/ContainerQueuingLimit.java)/**
 * Used to hold max wait time / queue length information to be
 * passed back to the NodeManager.
 */
ContainerQueuingLimitPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/records/impl/pb/ContainerQueuingLimitPBImpl.java)/**
 * Implementation of ContainerQueuingLimit interface.
 */
OpportunisticContainersStatusPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/records/impl/pb/OpportunisticContainersStatusPBImpl.java)/**
 * Protocol Buffer implementation of OpportunisticContainersStatus.
 */
NodeHealthStatus (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/records/NodeHealthStatus.java)/**
 * {@code NodeHealthStatus} is a summary of the health status of the node.
 * <p>
 * It includes information such as:
 * <ul>
 *   <li>
 *     An indicator of whether the node is healthy, as determined by the
 *     health-check script.
 *   </li>
 *   <li>The previous time at which the health status was reported.</li>
 *   <li>A diagnostic report on the health status.</li>
 * </ul>
 * 
 * @see NodeReport
 * @see ApplicationClientProtocol#getClusterNodes(org.apache.hadoop.yarn.api.protocolrecords.GetClusterNodesRequest)
 */
NodeStatus (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/records/NodeStatus.java)/**
 * {@code NodeStatus} is a summary of the status of the node.
 * <p>
 * It includes information such as:
 * <ul>
 *   <li>Node information and status..</li>
 *   <li>Container status.</li>
 * </ul>
 */
OpportunisticContainersStatus (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/records/OpportunisticContainersStatus.java)/**
 * <p> <code>OpportunisticContainersStatus</code> captures information
 * pertaining to the state of execution of the opportunistic containers within a
 * node. </p>
 */
ResourceTracker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/ResourceTracker.java)/**
 * This is used by the Node Manager to register/nodeHeartbeat/unregister with
 * the ResourceManager.
 */
SCMUploaderProtocol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/SCMUploaderProtocol.java)/**
 * <p>
 * The protocol between a <code>NodeManager's</code>
 * <code>SharedCacheUploadService</code> and the
 * <code>SharedCacheManager.</code>
 * </p>
 */
FederationProxyProviderUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/failover/FederationProxyProviderUtil.java)/**
 * Utility class that creates proxy for specified protocols when federation is
 * enabled. The class creates a federation aware failover provider, i.e. the
 * failover provider uses the {@code FederationStateStore} to determine the
 * current active ResourceManager
 */
FederationRMFailoverProxyProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/failover/FederationRMFailoverProxyProvider.java)/**
 * A FailoverProxyProvider implementation that uses the
 * {@code FederationStateStore} to determine the ResourceManager to connect to.
 * This supports both HA and regular mode which is controlled by configuration.
 */
AbstractConfigurableFederationPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/AbstractConfigurableFederationPolicy.java)/**
 * Base abstract class for a weighted {@link ConfigurableFederationPolicy}.
 */
AbstractAMRMProxyPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/amrmproxy/AbstractAMRMProxyPolicy.java)/**
 * Base abstract class for {@link FederationAMRMProxyPolicy} implementations,
 * that provides common validation for reinitialization.
 */
BroadcastAMRMProxyPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/amrmproxy/BroadcastAMRMProxyPolicy.java)/**
 * An implementation of the {@link FederationAMRMProxyPolicy} that simply
 * broadcasts each {@link ResourceRequest} to all the available sub-clusters.
 */
FederationAMRMProxyPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/amrmproxy/FederationAMRMProxyPolicy.java)/**
 * Implementors of this interface provide logic to split the list of
 * {@link ResourceRequest}s received by the AM among various RMs.
 */
HomeAMRMProxyPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/amrmproxy/HomeAMRMProxyPolicy.java)/**
 * An implementation of the {@link FederationAMRMProxyPolicy} that simply
 * sends the {@link ResourceRequest} to the home subcluster.
 */
AllocationBookkeeper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/amrmproxy/LocalityMulticastAMRMProxyPolicy.java)/**
   * This helper class is used to book-keep the requests made to each
   * subcluster, and maintain useful statistics to split ANY requests.
   */
LocalityMulticastAMRMProxyPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/amrmproxy/LocalityMulticastAMRMProxyPolicy.java)/**
 * An implementation of the {@link FederationAMRMProxyPolicy} interface that
 * carefully multicasts the requests with the following behavior:
 *
 * <p>
 * Host localized {@link ResourceRequest}s are always forwarded to the RM that
 * owns the corresponding node, based on the feedback of a
 * {@link SubClusterResolver}. If the {@link SubClusterResolver} cannot resolve
 * this node we default to forwarding the {@link ResourceRequest} to the home
 * sub-cluster.
 * </p>
 *
 * <p>
 * Rack localized {@link ResourceRequest}s are forwarded to the RMs that owns
 * the corresponding rack. Note that in some deployments each rack could be
 * striped across multiple RMs. This policy respects that. If the
 * {@link SubClusterResolver} cannot resolve this rack we default to forwarding
 * the {@link ResourceRequest} to the home sub-cluster.
 * </p>
 *
 * <p>
 * ANY requests corresponding to node/rack local requests are forwarded only to
 * the set of RMs that owns the corresponding localized requests. The number of
 * containers listed in each ANY is proportional to the number of localized
 * container requests (associated to this ANY via the same allocateRequestId).
 * </p>
 *
 * <p>
 * ANY that are not associated to node/rack local requests are split among RMs
 * based on the "weights" in the {@link WeightedPolicyInfo} configuration *and*
 * headroom information. The {@code headroomAlpha} parameter of the policy
 * configuration indicates how much headroom contributes to the splitting
 * choice. Value of 1.0f indicates the weights are interpreted only as 0/1
 * boolean but all splitting is based on the advertised headroom (fallback to
 * 1/N for RMs that we don't have headroom info from). An {@code headroomAlpha}
 * value of 0.0f means headroom is ignored and all splitting decisions are
 * proportional to the "weights" in the configuration of the policy.
 * </p>
 *
 * <p>
 * ANY of zero size are forwarded to all known subclusters (i.e., subclusters
 * where we scheduled containers before), as they may represent a user attempt
 * to cancel a previous request (and we are mostly stateless now, so should
 * forward to all known RMs).
 * </p>
 *
 * <p>
 * Invariants:
 * </p>
 *
 * <p>
 * The policy always excludes non-active RMs.
 * </p>
 *
 * <p>
 * The policy always excludes RMs that do not appear in the policy configuration
 * weights, or have a weight of 0 (even if localized resources explicit refer to
 * it).
 * </p>
 *
 * <p>
 * (Bar rounding to closest ceiling of fractional containers) The sum of
 * requests made to multiple RMs at the ANY level "adds-up" to the user request.
 * The maximum possible excess in a given request is a number of containers less
 * or equal to number of sub-clusters in the federation.
 * </p>
 */
RejectAMRMProxyPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/amrmproxy/RejectAMRMProxyPolicy.java)/**
 * An implementation of the {@link FederationAMRMProxyPolicy} that simply
 * rejects all requests. Useful to prevent apps from accessing any sub-cluster.
 */
ConfigurableFederationPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/ConfigurableFederationPolicy.java)/**
 * This interface provides a general method to reinitialize a policy. The
 * semantics are try-n-swap, so in case of an exception is thrown the
 * implmentation must ensure the previous state and configuration is preserved.
 */
FederationPolicyException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/exceptions/FederationPolicyException.java)/**
 * Generic policy exception.
 */
FederationPolicyInitializationException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/exceptions/FederationPolicyInitializationException.java)/**
 * This exception is thrown when the initialization of a federation policy is
 * not successful.
 */
NoActiveSubclustersException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/exceptions/NoActiveSubclustersException.java)/**
 * This exception is thrown when policies cannot locate any active cluster.
 */
UnknownSubclusterException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/exceptions/UnknownSubclusterException.java)/**
 * This exception is thrown whenever a policy is given a {@code SubClusterId}
 * that is unknown.
 */
FederationPolicyInitializationContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/FederationPolicyInitializationContext.java)/**
 * Context to (re)initialize a {@code FederationAMRMProxyPolicy} and {@code
 * FederationRouterPolicy}.
 */
FederationPolicyInitializationContextValidator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/FederationPolicyInitializationContextValidator.java)/**
 * Helper class used to factor out common validation steps for policies.
 */
FederationPolicyUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/FederationPolicyUtils.java)/**
 * Utility class for Federation policy.
 */
AbstractPolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/manager/AbstractPolicyManager.java)/**
 * This class provides basic implementation for common methods that multiple
 * policies will need to implement.
 */
FederationPolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/manager/FederationPolicyManager.java)/**
 *
 * Implementors need to provide the ability to serliaze a policy and its
 * configuration as a {@link SubClusterPolicyConfiguration}, as well as provide
 * (re)initialization mechanics for the underlying
 * {@link FederationAMRMProxyPolicy} and {@link FederationRouterPolicy}.
 *
 * The serialization aspects are used by admin APIs or a policy engine to store
 * a serialized configuration in the {@code FederationStateStore}, while the
 * getters methods are used to obtain a propertly inizialized policy in the
 * {@code Router} and {@code AMRMProxy} respectively.
 *
 * This interface by design binds together {@link FederationAMRMProxyPolicy} and
 * {@link FederationRouterPolicy} and provide lifecycle support for
 * serialization and deserialization, to reduce configuration mistakes
 * (combining incompatible policies).
 *
 */
HashBroadcastPolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/manager/HashBroadcastPolicyManager.java)/**
 * Policy that routes applications via hashing of their queuename, and broadcast
 * resource requests. This picks a {@link HashBasedRouterPolicy} for the router
 * and a {@link BroadcastAMRMProxyPolicy} for the amrmproxy as they are designed
 * to work together.
 */
HomePolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/manager/HomePolicyManager.java)/**
 * Policy manager which uses the {@link UniformRandomRouterPolicy} for the
 * Router and {@link HomeAMRMProxyPolicy} as the AMRMProxy policy to find the
 * RM.
 */
PriorityBroadcastPolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/manager/PriorityBroadcastPolicyManager.java)/**
 * Policy that allows operator to configure "weights" for routing. This picks a
 * {@link PriorityRouterPolicy} for the router and a
 * {@link BroadcastAMRMProxyPolicy} for the amrmproxy as they are designed to
 * work together.
 */
RejectAllPolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/manager/RejectAllPolicyManager.java)/**
 * This class represents a simple implementation of a {@code
 * FederationPolicyManager}.
 *
 * This policy rejects all reuqests for both router and amrmproxy routing. This
 * is to be used to prevent applications in a specific queue (or if used as
 * default for non-configured queues) from accessing cluster resources.
 */
UniformBroadcastPolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/manager/UniformBroadcastPolicyManager.java)/**
 * This class represents a simple implementation of a {@code
 * FederationPolicyManager}.
 *
 * It combines the basic policies: {@link UniformRandomRouterPolicy} and
 * {@link BroadcastAMRMProxyPolicy}, which are designed to work together and
 * "spread" the load among sub-clusters uniformly.
 *
 * This simple policy might impose heavy load on the RMs and return more
 * containers than a job requested as all requests are (replicated and)
 * broadcasted.
 */
WeightedLocalityPolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/manager/WeightedLocalityPolicyManager.java)/**
 * Policy that allows operator to configure "weights" for routing. This picks a
 * {@link WeightedRandomRouterPolicy} for the router and a {@link
 * LocalityMulticastAMRMProxyPolicy} for the amrmproxy as they are designed to
 * work together.
 */
AbstractRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/AbstractRouterPolicy.java)/**
 * Base abstract class for {@link FederationRouterPolicy} implementations, that
 * provides common validation for reinitialization.
 */
FederationRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/FederationRouterPolicy.java)/**
 * Implements the logic for determining the routing of an application submission
 * based on a policy.
 */
HashBasedRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/HashBasedRouterPolicy.java)/**
 * This {@link FederationRouterPolicy} pick a subcluster based on the hash of
 * the job's queue name. Useful to provide a default behavior when too many
 * queues exist in a system. This also ensures that all jobs belonging to a
 * queue are mapped to the same sub-cluster (likely help with locality).
 */
LoadBasedRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/LoadBasedRouterPolicy.java)/**
 * This implements a simple load-balancing policy. The policy "weights" are
 * binary 0/1 values that enable/disable each sub-cluster, and the policy peaks
 * the sub-cluster with the least load to forward this application.
 */
PriorityRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/PriorityRouterPolicy.java)/**
 * This implements a policy that interprets "weights" as a ordered list of
 * preferences among sub-clusters. Highest weight among active subclusters is
 * chosen.
 */
RejectRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/RejectRouterPolicy.java)/**
 * This {@link FederationRouterPolicy} simply rejects all incoming requests.
 * This is useful to prevent applications running in a queue to be run anywhere
 * in the federated cluster.
 */
UniformRandomRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/UniformRandomRouterPolicy.java)/**
 * This simple policy picks at uniform random among any of the currently active
 * subclusters. This policy is easy to use and good for testing.
 *
 * NOTE: this is "almost" subsumed by the {@code WeightedRandomRouterPolicy}.
 * Behavior only diverges when there are active sub-clusters that are not part
 * of the "weights", in which case the {@link UniformRandomRouterPolicy} send
 * load to them, while {@code WeightedRandomRouterPolicy} does not.
 */
WeightedRandomRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/WeightedRandomRouterPolicy.java)/**
 * This policy implements a weighted random sample among currently active
 * sub-clusters.
 */
RouterPolicyFacade (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/RouterPolicyFacade.java)/**
 * This class provides a facade to the policy subsystem, and handles the
 * lifecycle of policies (e.g., refresh from remote, default behaviors etc.).
 */
AbstractSubClusterResolver (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/resolver/AbstractSubClusterResolver.java)/**
 * Partial implementation of {@link SubClusterResolver}, containing basic
 * implementations of the read methods.
 */
DefaultSubClusterResolverImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/resolver/DefaultSubClusterResolverImpl.java)/**
 *
 * Default simple sub-cluster and rack resolver class.
 *
 * This class expects a three-column comma separated file, specified in
 * yarn.federation.machine-list. Each line of the file should be of the format:
 *
 * nodeName, subClusterId, rackName
 *
 * Lines that do not follow this format will be ignored. This resolver only
 * loads the file when load() is explicitly called; it will not react to changes
 * to the file.
 *
 * It is case-insensitive on the rack and node names and ignores
 * leading/trailing whitespace.
 *
 */
SubClusterResolver (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/resolver/SubClusterResolver.java)/**
 * An utility that helps to determine the sub-cluster that a specified node or
 * rack belongs to. All implementing classes should be thread-safe.
 */
FederationStateStoreException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/exception/FederationStateStoreException.java)/**
 * Exception thrown by the <code>FederationStateStore</code>.
 *
 */
FederationStateStoreInvalidInputException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/exception/FederationStateStoreInvalidInputException.java)/**
 * Exception thrown by the {@code FederationMembershipStateStoreInputValidator},
 * {@code FederationApplicationHomeSubClusterStoreInputValidator},
 * {@code FederationPolicyStoreInputValidator} if the input is invalid.
 *
 */
FederationStateStoreRetriableException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/exception/FederationStateStoreRetriableException.java)/**
 * Exception thrown by the {@code FederationStateStore}, if it is a retriable
 * exception.
 *
 */
FederationApplicationHomeSubClusterStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/FederationApplicationHomeSubClusterStore.java)/**
 * FederationApplicationHomeSubClusterStore maintains the state of all
 * <em>Applications</em> that have been submitted to the federated cluster.
 *
 * *
 * <p>
 * The mapping details contains:
 * <ul>
 * <li>{@code ApplicationId}</li>
 * <li>{@code SubClusterId}</li>
 * </ul>
 *
 */
FederationMembershipStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/FederationMembershipStateStore.java)/**
 * FederationMembershipStateStore maintains the state of all
 * <em>subcluster(s)</em> as encapsulated by {@code SubClusterInfo} for all the
 * subcluster(s) that are participating in federation.
 */
FederationPolicyStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/FederationPolicyStore.java)/**
 * The FederationPolicyStore provides a key-value interface to access the
 * policies configured for the system. The key is a "queue" name, i.e., the
 * system allows to configure a different policy for each queue in the system
 * (though each policy can make dynamic run-time decisions on a per-job/per-task
 * basis). The value is a {@code SubClusterPolicyConfiguration}, a serialized
 * representation of the policy type and its parameters.
 */
FederationStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/FederationStateStore.java)/**
 * FederationStore extends the three interfaces used to coordinate the state of
 * a federated cluster: {@link FederationApplicationHomeSubClusterStore},
 * {@link FederationMembershipStateStore}, and {@link FederationPolicyStore}.
 *
 */
MemoryFederationStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/MemoryFederationStateStore.java)/**
 * In-memory implementation of {@link FederationStateStore}.
 */
SQLFederationStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/SQLFederationStateStore.java)/**
 * SQL implementation of {@link FederationStateStore}.
 */
ZookeeperFederationStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/ZookeeperFederationStateStore.java)/**
 * ZooKeeper implementation of {@link FederationStateStore}.
 *
 * The znode structure is as follows:
 * ROOT_DIR_PATH
 * |--- MEMBERSHIP
 * |     |----- SC1
 * |     |----- SC2
 * |--- APPLICATION
 * |     |----- APP1
 * |     |----- APP2
 * |--- POLICY
 *       |----- QUEUE1
 *       |----- QUEUE1
 */
FederationStateStoreClientMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/metrics/FederationStateStoreClientMetrics.java)/**
 * Performance metrics for FederationStateStore implementations.
 */
AddApplicationHomeSubClusterRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/AddApplicationHomeSubClusterRequest.java)/**
 * <p>
 * The request sent by the <code>Router</code> to <code>Federation state
 * store</code> to map the home subcluster of a newly submitted application.
 *
 * <p>
 * The request includes the mapping details, i.e.:
 * <ul>
 * <li>{@code ApplicationId}</li>
 * <li>{@code SubClusterId}</li>
 * </ul>
 */
AddApplicationHomeSubClusterResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/AddApplicationHomeSubClusterResponse.java)/**
 * AddApplicationHomeSubClusterResponse contains the answer from the
 * {@code FederationApplicationHomeSubClusterStore} to a request to insert a
 * newly generated applicationId and its owner.
 *
 * The response contains application's home sub-cluster as it is stored in the
 * {@code FederationApplicationHomeSubClusterStore}. If a mapping for the
 * application already existed, the {@code SubClusterId} in this response will
 * return the existing mapping which might be different from that in the
 * {@code AddApplicationHomeSubClusterRequest}.
 */
ApplicationHomeSubCluster (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/ApplicationHomeSubCluster.java)/**
 * <p>
 * ApplicationHomeSubCluster is a report of the runtime information of the
 * application that is running in the federated cluster.
 *
 * <p>
 * It includes information such as:
 * <ul>
 * <li>{@link ApplicationId}</li>
 * <li>{@link SubClusterId}</li>
 * </ul>
 *
 */
DeleteApplicationHomeSubClusterRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/DeleteApplicationHomeSubClusterRequest.java)/**
 * The request to <code>Federation state store</code> to delete the mapping of
 * home subcluster of a submitted application.
 */
DeleteApplicationHomeSubClusterResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/DeleteApplicationHomeSubClusterResponse.java)/**
 * DeleteApplicationHomeSubClusterResponse contains the answer from the {@code
 * FederationApplicationHomeSubClusterStore} to a request to delete the mapping
 * of home subcluster of a submitted application. Currently response is empty if
 * the operation was successful, if not an exception reporting reason for a
 * failure.
 */
GetApplicationHomeSubClusterRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/GetApplicationHomeSubClusterRequest.java)/**
 * Request class to obtain the home sub-cluster for the specified
 * {@link ApplicationId}.
 */
GetApplicationHomeSubClusterResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/GetApplicationHomeSubClusterResponse.java)/**
 * <p>
 * The response sent by <code>Federation state
 * store</code> to a query for the home subcluster of a newly submitted
 * application.
 *
 * <p>
 * The request includes the mapping details, i.e.:
 * <ul>
 * <li>{@code ApplicationId}</li>
 * <li>{@code SubClusterId}</li>
 * </ul>
 */
GetApplicationsHomeSubClusterRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/GetApplicationsHomeSubClusterRequest.java)/**
 * Request class to obtain the home sub-cluster mapping of all active
 * applications.
 */
GetApplicationsHomeSubClusterResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/GetApplicationsHomeSubClusterResponse.java)/**
 * <p>
 * The response sent by <code>Federation state
 * store</code> to a query for the home subcluster of all submitted
 * applications.
 *
 * <p>
 * The response includes the mapping details, i.e.:
 * <ul>
 * <li>{@code ApplicationId}</li>
 * <li>{@code SubClusterId}</li>
 * </ul>
 */
GetSubClusterInfoRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/GetSubClusterInfoRequest.java)/**
 * Request class to obtain information about a sub-cluster identified by its
 * {@link SubClusterId}.
 */
GetSubClusterInfoResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/GetSubClusterInfoResponse.java)/**
 * Response to a query with {@link SubClusterInfo} about a sub-cluster.
 */
GetSubClusterPoliciesConfigurationsRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/GetSubClusterPoliciesConfigurationsRequest.java)/**
 * GetSubClusterPoliciesConfigurationsRequest is a request to the
 * {@code FederationPolicyStore} to obtain all policy configurations.
 */
GetSubClusterPoliciesConfigurationsResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/GetSubClusterPoliciesConfigurationsResponse.java)/**
 * GetSubClusterPolicyConfigurationResponse contains the answer from the {@code
 * FederationPolicyStore} to a request to get all the policies configured in the
 * system via a {@link SubClusterPolicyConfiguration}.
 */
GetSubClusterPolicyConfigurationRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/GetSubClusterPolicyConfigurationRequest.java)/**
 * GetSubClusterPolicyConfigurationRequest is a request to the
 * {@code FederationPolicyStore} to get the configuration of a policy for a
 * given queue.
 */
GetSubClusterPolicyConfigurationResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/GetSubClusterPolicyConfigurationResponse.java)/**
 * GetSubClusterPolicyConfigurationResponse contains the answer from the {@code
 * FederationPolicyStore} to a request to get the information about how a policy
 * should be configured via a {@link SubClusterPolicyConfiguration}.
 */
GetSubClustersInfoRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/GetSubClustersInfoRequest.java)/**
 * Request class to obtain information about all sub-clusters that are
 * participating in federation.
 *
 * If filterInactiveSubClusters is set to true, only active sub-clusters will be
 * returned; otherwise, all sub-clusters will be returned regardless of state.
 * By default, filterInactiveSubClusters is true.
 */
GetSubClustersInfoResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/GetSubClustersInfoResponse.java)/**
 * Response to a query with list of {@link SubClusterInfo} about all
 * sub-clusters that are currently participating in Federation.
 */
AddApplicationHomeSubClusterRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/AddApplicationHomeSubClusterRequestPBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link AddApplicationHomeSubClusterRequest}.
 */
AddApplicationHomeSubClusterResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/AddApplicationHomeSubClusterResponsePBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link AddApplicationHomeSubClusterResponse}.
 */
ApplicationHomeSubClusterPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/ApplicationHomeSubClusterPBImpl.java)/**
 * Protocol buffer based implementation of {@link ApplicationHomeSubCluster}.
 */
DeleteApplicationHomeSubClusterRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/DeleteApplicationHomeSubClusterRequestPBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link DeleteApplicationHomeSubClusterRequest}.
 */
DeleteApplicationHomeSubClusterResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/DeleteApplicationHomeSubClusterResponsePBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link DeleteApplicationHomeSubClusterResponse}.
 */
GetApplicationHomeSubClusterRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/GetApplicationHomeSubClusterRequestPBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link GetApplicationHomeSubClusterRequest}.
 */
GetApplicationHomeSubClusterResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/GetApplicationHomeSubClusterResponsePBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link GetApplicationHomeSubClusterResponse}.
 */
GetApplicationsHomeSubClusterRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/GetApplicationsHomeSubClusterRequestPBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link GetApplicationsHomeSubClusterRequest}.
 */
GetApplicationsHomeSubClusterResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/GetApplicationsHomeSubClusterResponsePBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link GetApplicationsHomeSubClusterResponse}.
 */
GetSubClusterInfoRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/GetSubClusterInfoRequestPBImpl.java)/**
 * Protocol buffer based implementation of {@link GetSubClusterInfoRequest}.
 */
GetSubClusterInfoResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/GetSubClusterInfoResponsePBImpl.java)/**
 * Protocol buffer based implementation of {@link GetSubClusterInfoResponse}.
 */
GetSubClusterPoliciesConfigurationsRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/GetSubClusterPoliciesConfigurationsRequestPBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link GetSubClusterPoliciesConfigurationsRequest}.
 */
GetSubClusterPoliciesConfigurationsResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/GetSubClusterPoliciesConfigurationsResponsePBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link GetSubClusterPoliciesConfigurationsResponse}.
 */
GetSubClusterPolicyConfigurationRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/GetSubClusterPolicyConfigurationRequestPBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link GetSubClusterPolicyConfigurationRequest}.
 */
GetSubClusterPolicyConfigurationResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/GetSubClusterPolicyConfigurationResponsePBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link GetSubClusterPolicyConfigurationResponse}.
 */
GetSubClustersInfoRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/GetSubClustersInfoRequestPBImpl.java)/**
 * Protocol buffer based implementation of {@link GetSubClustersInfoRequest}.
 */
GetSubClustersInfoResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/GetSubClustersInfoResponsePBImpl.java)/**
 * Protocol buffer based implementation of {@link GetSubClustersInfoResponse}.
 */
SetSubClusterPolicyConfigurationRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/SetSubClusterPolicyConfigurationRequestPBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link SetSubClusterPolicyConfigurationRequest}.
 */
SetSubClusterPolicyConfigurationResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/SetSubClusterPolicyConfigurationResponsePBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link SetSubClusterPolicyConfigurationResponse}.
 */
SubClusterDeregisterRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/SubClusterDeregisterRequestPBImpl.java)/**
 * Protocol buffer based implementation of {@link SubClusterDeregisterRequest}.
 */
SubClusterDeregisterResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/SubClusterDeregisterResponsePBImpl.java)/**
 * Protocol buffer based implementation of {@link SubClusterDeregisterResponse}.
 */
SubClusterHeartbeatRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/SubClusterHeartbeatRequestPBImpl.java)/**
 * Protocol buffer based implementation of {@link SubClusterHeartbeatRequest}.
 */
SubClusterHeartbeatResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/SubClusterHeartbeatResponsePBImpl.java)/**
 * Protocol buffer based implementation of {@link SubClusterHeartbeatResponse}.
 */
SubClusterIdPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/SubClusterIdPBImpl.java)/**
 * Protocol buffer based implementation of {@link SubClusterId}.
 */
SubClusterInfoPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/SubClusterInfoPBImpl.java)/**
 * Protocol buffer based implementation of {@link SubClusterInfo}.
 */
SubClusterPolicyConfigurationPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/SubClusterPolicyConfigurationPBImpl.java)/**
 * Protobuf based implementation of {@link SubClusterPolicyConfiguration}.
 *
 */
SubClusterRegisterRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/SubClusterRegisterRequestPBImpl.java)/**
 * Protocol buffer based implementation of {@link SubClusterRegisterRequest}.
 */
SubClusterRegisterResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/SubClusterRegisterResponsePBImpl.java)/**
 * Protocol buffer based implementation of {@link SubClusterRegisterResponse}.
 */
UpdateApplicationHomeSubClusterRequestPBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/UpdateApplicationHomeSubClusterRequestPBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link UpdateApplicationHomeSubClusterRequest} .
 */
UpdateApplicationHomeSubClusterResponsePBImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/impl/pb/UpdateApplicationHomeSubClusterResponsePBImpl.java)/**
 * Protocol buffer based implementation of
 * {@link UpdateApplicationHomeSubClusterResponse}.
 */
SetSubClusterPolicyConfigurationRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/SetSubClusterPolicyConfigurationRequest.java)/**
 * SetSubClusterPolicyConfigurationRequest is a request to the
 * {@code FederationPolicyStore} to set the policy configuration corresponding
 * to a queue.
 */
SetSubClusterPolicyConfigurationResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/SetSubClusterPolicyConfigurationResponse.java)/**
 * SetSubClusterPolicyConfigurationResponse contains the answer from the
 * {@code FederationPolicyStore} to a request to set for a policy configuration
 * for a given queue.
 */
SubClusterDeregisterRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/SubClusterDeregisterRequest.java)/**
 * <p>
 * The request sent to set the state of a subcluster to either
 * SC_DECOMMISSIONED, SC_LOST, or SC_DEREGISTERED.
 *
 * <p>
 * The update includes details such as:
 * <ul>
 * <li>{@link SubClusterId}</li>
 * <li>{@link SubClusterState}</li>
 * </ul>
 */
SubClusterDeregisterResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/SubClusterDeregisterResponse.java)/**
 * SubClusterDeregisterResponse contains the answer from the {@code
 * FederationMembershipStateStore} to a request to deregister the sub cluster.
 * Currently response is empty if the operation was successful, if not an
 * exception reporting reason for a failure.
 */
SubClusterHeartbeatRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/SubClusterHeartbeatRequest.java)/**
 * <p>
 * SubClusterHeartbeatRequest is a report of the runtime information of the
 * subcluster that is participating in federation.
 *
 * <p>
 * It includes information such as:
 * <ul>
 * <li>{@link SubClusterId}</li>
 * <li>The URL of the subcluster</li>
 * <li>The timestamp representing the last start time of the subCluster</li>
 * <li>{@code FederationsubClusterState}</li>
 * <li>The current capacity and utilization of the subCluster</li>
 * </ul>
 */
SubClusterHeartbeatResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/SubClusterHeartbeatResponse.java)/**
 * SubClusterHeartbeatResponse contains the response from the {@code
 * FederationMembershipStateStore} to a periodic heartbeat to indicate
 * liveliness from a <code>ResourceManager</code> participating in federation.
 * Currently response is empty if the operation was successful, if not an
 * exception reporting reason for a failure.
 * <p>
 * NOTE: This can be extended to push down policies in future
 */
SubClusterId (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/SubClusterId.java)/**
 * <p>
 * SubClusterId represents the <em>globally unique</em> identifier for a
 * subcluster that is participating in federation.
 *
 * <p>
 * The globally unique nature of the identifier is obtained from the
 * <code>FederationMembershipStateStore</code> on initialization.
 */
SubClusterIdInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/SubClusterIdInfo.java)/**
 * This class represent a sub-cluster identifier in the JSON representation
 * of the policy configuration.
 */
SubClusterInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/SubClusterInfo.java)/**
 * <p>
 * SubClusterInfo is a report of the runtime information of the subcluster that
 * is participating in federation.
 *
 * <p>
 * It includes information such as:
 * <ul>
 * <li>{@link SubClusterId}</li>
 * <li>The URL of the subcluster</li>
 * <li>The timestamp representing the last start time of the subCluster</li>
 * <li>{@code FederationsubClusterState}</li>
 * <li>The current capacity and utilization of the subCluster</li>
 * </ul>
 */
SubClusterPolicyConfiguration (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/SubClusterPolicyConfiguration.java)/**
 * {@link SubClusterPolicyConfiguration} is a class that represents a
 * configuration of a policy. For a single queue, it contains a policy type
 * (resolve to a class name) and its params as an opaque {@link ByteBuffer}.
 *
 * Note: by design the params are an opaque ByteBuffer, this allows for enough
 * flexibility to evolve the policies without impacting the protocols to/from
 * the federation state store.
 */
SubClusterRegisterRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/SubClusterRegisterRequest.java)/**
 * <p>
 * SubClusterRegisterRequest is a request by a sub-cluster
 * {@code ResourceManager} to participate in federation.
 *
 * <p>
 * It includes information such as:
 * <ul>
 * <li>{@link SubClusterId}</li>
 * <li>The URL of the subcluster</li>
 * <li>The timestamp representing the last start time of the subCluster</li>
 * <li>{@code FederationsubClusterState}</li>
 * <li>The current capacity and utilization of the subCluster</li>
 * </ul>
 */
SubClusterRegisterResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/SubClusterRegisterResponse.java)/**
 * SubClusterRegisterResponse contains the response from the {@code
 * FederationMembershipStateStore} to a registration request from a
 * <code>ResourceManager</code> to participate in federation.
 *
 * Currently response is empty if the operation was successful, if not an
 * exception reporting reason for a failure.
 */
UpdateApplicationHomeSubClusterRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/UpdateApplicationHomeSubClusterRequest.java)/**
 * <p>
 * The request sent by the <code>Router</code> to
 * <code>Federation state store</code> to update the home subcluster of a newly
 * submitted application.
 *
 * <p>
 * The request includes the mapping details, i.e.:
 * <ul>
 * <li>{@code ApplicationId}</li>
 * <li>{@code SubClusterId}</li>
 * </ul>
 */
UpdateApplicationHomeSubClusterResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/records/UpdateApplicationHomeSubClusterResponse.java)/**
 * UpdateApplicationHomeSubClusterResponse contains the answer from the
 * {@code FederationApplicationHomeSubClusterStore} to a request to register the
 * home subcluster of a submitted application. Currently response is empty if
 * the operation was successful, if not an exception reporting reason for a
 * failure.
 */
FederationApplicationHomeSubClusterStoreInputValidator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/utils/FederationApplicationHomeSubClusterStoreInputValidator.java)/**
 * Utility class to validate the inputs to
 * {@code FederationApplicationHomeSubClusterStore}, allows a fail fast
 * mechanism for invalid user inputs.
 *
 */
FederationMembershipStateStoreInputValidator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/utils/FederationMembershipStateStoreInputValidator.java)/**
 * Utility class to validate the inputs to
 * {@code FederationMembershipStateStore}, allows a fail fast mechanism for
 * invalid user inputs.
 *
 */
FederationPolicyStoreInputValidator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/utils/FederationPolicyStoreInputValidator.java)/**
 * Utility class to validate the inputs to {@code FederationPolicyStore}, allows
 * a fail fast mechanism for invalid user inputs.
 *
 */
FederationStateStoreUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/utils/FederationStateStoreUtils.java)/**
 * Common utility methods used by the store implementations.
 *
 */
FederationRegistryClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/utils/FederationRegistryClient.java)/**
 * Helper class that handles reads and writes to Yarn Registry to support UAM HA
 * and second attempt.
 */
CacheLoaderImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/utils/FederationStateStoreFacade.java)/**
   * Internal class that implements the CacheLoader interface that can be
   * plugged into the CacheManager to load objects into the cache for specified
   * keys.
   */
CacheRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/utils/FederationStateStoreFacade.java)/**
   * Internal class that encapsulates the cache key and a function that returns
   * the value for the specified key.
   */
Func (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/utils/FederationStateStoreFacade.java)/**
   * Encapsulates a method that has one parameter and returns a value of the
   * type specified by the TResult parameter.
   */
FederationStateStoreFacade (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/utils/FederationStateStoreFacade.java)/**
 *
 * The FederationStateStoreFacade is an utility wrapper that provides singleton
 * access to the Federation state store. It abstracts out retries and in
 * addition, it also implements the caching for various objects.
 *
 */
AMRMClientRelayerMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/metrics/AMRMClientRelayerMetrics.java)/**
 * Metrics for FederationInterceptor Internals.
 */
OpportunisticSchedulerMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/metrics/OpportunisticSchedulerMetrics.java)/**
 * Metrics for Opportunistic Scheduler.
 */
Version (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/records/Version.java)/**
 * The version information for state get stored in YARN components,
 * i.e. RMState, NMState, etc., which include: majorVersion and 
 * minorVersion.
 * The major version update means incompatible changes happen while
 * minor version update indicates compatible changes.
 */
DistributedOpportunisticContainerAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/DistributedOpportunisticContainerAllocator.java)/**
 * <p>
 * The DistributedOpportunisticContainerAllocator allocates containers on a
 * given list of nodes, after modifying the container sizes to respect the
 * limits set by the ResourceManager. It tries to distribute the containers
 * as evenly as possible.
 * </p>
 */
AllocationParams (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java)/**
   * This class encapsulates application specific parameters used to build a
   * Container.
   */
ContainerIdGenerator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java)/**
   * A Container Id Generator.
   */
PartitionedResourceRequests (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java)/**
   * Class that includes two lists of {@link ResourceRequest}s: one for
   * GUARANTEED and one for OPPORTUNISTIC {@link ResourceRequest}s.
   */
EnrichedResourceRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java)/**
   * This class encapsulates Resource Request and provides requests per
   * node and rack.
   */
OpportunisticContainerAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java)/**
 * <p>
 * Base abstract class for Opportunistic container allocations, that provides
 * common functions required for Opportunistic container allocation.
 * </p>
 */
OpportunisticContainerContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerContext.java)/**
 * This encapsulates application specific information used by the
 * Opportunistic Container Allocator to allocate containers.
 */
ResourceRequestSet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/ResourceRequestSet.java)/**
 * A set of resource requests of the same scheduler key
 * {@link ResourceRequestSetKey}.
 */
ResourceRequestSetKey (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/ResourceRequestSetKey.java)/**
 * The scheduler key for a group of {@link ResourceRequest}.
 *
 * TODO: after YARN-7631 is fixed by adding Resource and ExecType into
 * SchedulerRequestKey, then we can directly use that.
 */
SchedulerRequestKey (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/SchedulerRequestKey.java)/**
 * Composite key for outstanding scheduler requests for any schedulable entity.
 * Currently it includes {@link Priority}.
 */
AMSecretKeys (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/security/AMSecretKeys.java)/**
 * Constants for AM Secret Keys.
 */
BaseContainerTokenSecretManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/security/BaseContainerTokenSecretManager.java)/**
 * SecretManager for ContainerTokens. Extended by both RM and NM and hence is
 * present in yarn-server-common package.
 * 
 */
SystemServiceManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/service/SystemServiceManager.java)/**
 * Marker interface for starting services from RM. The implementation should
 * launch configured services.
 */
SharedCacheUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/sharedcache/SharedCacheUtil.java)/**
 * A utility class that contains helper methods for dealing with the internal
 * shared cache structure.
 */
TimelineAuthenticationFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/timeline/security/TimelineAuthenticationFilter.java)/**
 * Timeline authentication filter provides delegation token support for ATSv1
 * and ATSv2.
 */
TimelineAuthenticationFilterInitializer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/timeline/security/TimelineAuthenticationFilterInitializer.java)/**
 * Initializes {@link TimelineAuthenticationFilter} which provides support for
 * Kerberos HTTP SPNEGO authentication.
 * <p>
 * It enables Kerberos HTTP SPNEGO plus delegation token authentication for the
 * timeline server.
 * <p>
 * Refer to the {@code core-default.xml} file, after the comment 'HTTP
 * Authentication' for details on the configuration options. All related
 * configuration properties have {@code hadoop.http.authentication.} as prefix.
 */
TimelineDelgationTokenSecretManagerService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/timeline/security/TimelineDelgationTokenSecretManagerService.java)/**
 * Abstract implementation of delegation token manager service for different
 * versions of timeline service.
 */
UnmanagedAMPoolManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/uam/UnmanagedAMPoolManager.java)/**
 * A service that manages a pool of UAM managers in
 * {@link UnmanagedApplicationManager}.
 */
UnmanagedApplicationManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/uam/UnmanagedApplicationManager.java)/**
 * UnmanagedApplicationManager is used to register unmanaged application and
 * negotiate for resources from resource managers. An unmanagedAM is an AM that
 * is not launched and managed by the RM. Allocate calls are handled
 * asynchronously using {@link AsyncCallback}.
 */
TimelineServerUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/util/timeline/TimelineServerUtils.java)/**
 * Set of utility methods to be used across timeline reader and collector.
 */
BuilderUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/utils/BuilderUtils.java)/**
 * Builder utilities to construct various objects.
 *
 */
LeveldbIterator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/utils/LeveldbIterator.java)/**
 * A wrapper for a DBIterator to translate the raw RuntimeExceptions that
 * can be thrown into DBExceptions.
 */
YarnServerBuilderUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/utils/YarnServerBuilderUtils.java)/**
 * Server Builder utilities to construct various objects.
 *
 */
YarnServerSecurityUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/utils/YarnServerSecurityUtils.java)/**
 * Utility class that contains commonly used server methods.
 *
 */
CsiConstants (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/volume/csi/CsiConstants.java)/**
 * CSI constants.
 */
InvalidVolumeException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/volume/csi/exception/InvalidVolumeException.java)/**
 * This exception is thrown when a volume is found not valid.
 */
VolumeException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/volume/csi/exception/VolumeException.java)/**
 * Base class for all volume related exceptions.
 */
VolumeProvisioningException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/volume/csi/exception/VolumeProvisioningException.java)/**
 * Exception throws when volume provisioning is failed.
 */
VolumeCapabilityBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/volume/csi/VolumeCapabilityRange.java)/**
   * The builder used to build a VolumeCapabilityRange instance.
   */
VolumeCapabilityRange (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/volume/csi/VolumeCapabilityRange.java)/**
 * Volume capability range that specified in a volume resource request,
 * this range defines the desired min/max capacity.
 */
VolumeId (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/volume/csi/VolumeId.java)/**
 * Unique ID for a volume. This may or may not come from a storage system,
 * YARN depends on this ID to recognized volumes and manage their states.
 */
VolumeSpecBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/volume/csi/VolumeMetaData.java)/**
   * The builder used to build a VolumeMetaData instance.
   */
VolumeMetaData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/volume/csi/VolumeMetaData.java)/**
 * VolumeMetaData defines all valid info for a CSI compatible volume.
 */
LogWebService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/webapp/LogWebService.java)/**
 * Support only ATSv2 client only.
 */
LogWebServiceUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/webapp/LogWebServiceUtils.java)/**
 * Log web service utils class.
 */
YarnWebServiceParams (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/webapp/YarnWebServiceParams.java)/**
 * Common web service parameters which could be used in
 * RM/NM/AHS WebService.
 *
 */
TestServerRMProxy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/api/TestServerRMProxy.java)/**
 * Test ServerRMProxy.
 */
TestBroadcastAMRMProxyFederationPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/amrmproxy/TestBroadcastAMRMProxyFederationPolicy.java)/**
 * Simple test class for the {@link BroadcastAMRMProxyPolicy}.
 */
TestHomeAMRMProxyPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/amrmproxy/TestHomeAMRMProxyPolicy.java)/**
 * Simple test class for the {@link HomeAMRMProxyPolicy}.
 */
TestableLocalityMulticastAMRMProxyPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/amrmproxy/TestLocalityMulticastAMRMProxyPolicy.java)/**
   * A testable version of LocalityMulticastAMRMProxyPolicy that
   * deterministically falls back to home sub-cluster for unresolved requests.
   */
TestLocalityMulticastAMRMProxyPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/amrmproxy/TestLocalityMulticastAMRMProxyPolicy.java)/**
 * Simple test class for the {@link LocalityMulticastAMRMProxyPolicy}.
 */
TestRejectAMRMProxyPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/amrmproxy/TestRejectAMRMProxyPolicy.java)/**
 * Simple test class for the {@link RejectAMRMProxyPolicy}.
 */
BaseFederationPoliciesTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/BaseFederationPoliciesTest.java)/**
 * Base class for policies tests, tests for common reinitialization cases.
 */
BasePolicyManagerTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/manager/BasePolicyManagerTest.java)/**
 * This class provides common test methods for testing {@code
 * FederationPolicyManager}s.
 */
TestHashBasedBroadcastPolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/manager/TestHashBasedBroadcastPolicyManager.java)/**
 * Simple test of {@link HashBroadcastPolicyManager}.
 */
TestHomePolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/manager/TestHomePolicyManager.java)/**
 * Simple test of {@link HomePolicyManager}.
 */
TestPriorityBroadcastPolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/manager/TestPriorityBroadcastPolicyManager.java)/**
 * Simple test of {@link PriorityBroadcastPolicyManager}.
 */
TestRejectAllPolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/manager/TestRejectAllPolicyManager.java)/**
 * Simple test of {@link RejectAllPolicyManager}.
 */
TestUniformBroadcastPolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/manager/TestUniformBroadcastPolicyManager.java)/**
 * Simple test of {@link UniformBroadcastPolicyManager}.
 */
TestWeightedLocalityPolicyManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/manager/TestWeightedLocalityPolicyManager.java)/**
 * Simple test of {@link WeightedLocalityPolicyManager}.
 */
BaseRouterPoliciesTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/BaseRouterPoliciesTest.java)/**
 * Base class for router policies tests, tests for null input cases.
 */
TestHashBasedRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestHashBasedRouterPolicy.java)/**
 * Simple test class for the {@link HashBasedRouterPolicy}. Tests that one of
 * the active sub-cluster is chosen.
 */
TestLoadBasedRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestLoadBasedRouterPolicy.java)/**
 * Simple test class for the {@link LoadBasedRouterPolicy}. Test that the load
 * is properly considered for allocation.
 */
TestPriorityRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestPriorityRouterPolicy.java)/**
 * Simple test class for the {@link PriorityRouterPolicy}. Tests that the
 * weights are correctly used for ordering the choice of sub-clusters.
 */
TestRejectRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestRejectRouterPolicy.java)/**
 * Simple test class for the {@link RejectRouterPolicy}. Tests that one of the
 * active subcluster is chosen.
 */
TestUniformRandomRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestUniformRandomRouterPolicy.java)/**
 * Simple test class for the {@link UniformRandomRouterPolicy}. Tests that one
 * of the active subcluster is chosen.
 */
TestWeightedRandomRouterPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestWeightedRandomRouterPolicy.java)/**
 * Simple test class for the {@link WeightedRandomRouterPolicy}. Generate large
 * number of randomized tests to check we are weighiting correctly even if
 * clusters go inactive.
 */
TestFederationPolicyInitializationContextValidator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/TestFederationPolicyInitializationContextValidator.java)/**
 * Test class for {@link FederationPolicyInitializationContextValidator}.
 */
TestFederationPolicyUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/TestFederationPolicyUtils.java)/**
 * Unit test for {@link FederationPolicyUtils}.
 */
TestRouterPolicyFacade (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/TestRouterPolicyFacade.java)/**
 * Simple test of {@link RouterPolicyFacade}.
 */
TestDefaultSubClusterResolver (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/resolver/TestDefaultSubClusterResolver.java)/**
 * Test {@link SubClusterResolver} against correct and malformed Federation
 * machine lists.
 */
FederationStateStoreBaseTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/store/impl/FederationStateStoreBaseTest.java)/**
 * Base class for FederationMembershipStateStore implementations.
 */
HSQLDBFederationStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/store/impl/HSQLDBFederationStateStore.java)/**
 * HSQLDB implementation of {@link FederationStateStore}.
 */
TestMemoryFederationStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/store/impl/TestMemoryFederationStateStore.java)/**
 * Unit tests for MemoryFederationStateStore.
 */
TestSQLFederationStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/store/impl/TestSQLFederationStateStore.java)/**
 * Unit tests for SQLFederationStateStore.
 */
TestZookeeperFederationStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/store/impl/TestZookeeperFederationStateStore.java)/**
 * Unit tests for ZookeeperFederationStateStore.
 */
TestFederationStateStoreClientMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/store/metrics/TestFederationStateStoreClientMetrics.java)/**
 * Unittests for {@link FederationStateStoreClientMetrics}.
 *
 */
TestFederationProtocolRecords (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/store/records/TestFederationProtocolRecords.java)/**
 * Test class for federation protocol records.
 */
TestFederationStateStoreInputValidator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/store/utils/TestFederationStateStoreInputValidator.java)/**
 * Unit tests for FederationApplicationInputValidator,
 * FederationMembershipInputValidator, and FederationPolicyInputValidator.
 */
FederationPoliciesTestUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/utils/FederationPoliciesTestUtil.java)/**
 * Support class providing common initialization methods to test federation
 * policies.
 */
FederationStateStoreTestUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/utils/FederationStateStoreTestUtil.java)/**
 * Utility class for FederationStateStore unit tests.
 */
TestFederationRegistryClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/utils/TestFederationRegistryClient.java)/**
 * Unit test for FederationRegistryClient.
 */
TestFederationStateStoreFacade (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/utils/TestFederationStateStoreFacade.java)/**
 * Unit tests for FederationStateStoreFacade.
 */
TestFederationStateStoreFacadeRetry (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/utils/TestFederationStateStoreFacadeRetry.java)/**
 * Test class to validate FederationStateStoreFacade retry policy.
 */
MockApplicationMasterService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/metrics/TestAMRMClientRelayerMetrics.java)/**
   * Mock AMS for easier testing and mocking of request/responses.
   */
TestAMRMClientRelayerMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/metrics/TestAMRMClientRelayerMetrics.java)/**
 * Unit test for AMRMClientRelayer.
 */
MockResourceManagerFacade (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/MockResourceManagerFacade.java)/**
 * Mock Resource Manager facade implementation that exposes all the methods
 * implemented by the YARN RM. The behavior and the values returned by this mock
 * implementation is expected by the Router/AMRMProxy unit test cases. So please
 * change the implementation with care.
 */
MockApplicationMasterService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/TestAMRMClientRelayer.java)/**
   * Mocked ApplicationMasterService in RM.
   */
TestAMRMClientRelayer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/TestAMRMClientRelayer.java)/**
 * Unit test for AMRMClientRelayer.
 */
TestTimelineAuthenticationFilterInitializer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/timeline/security/TestTimelineAuthenticationFilterInitializer.java)/**
 * Tests {@link TimelineAuthenticationFilterInitializer}.
 */
TestableUnmanagedApplicationManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/uam/TestUnmanagedApplicationManager.java)/**
   * Testable UnmanagedApplicationManager that talks to a mock RM.
   */
TestableAMRequestHandlerThread (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/uam/TestUnmanagedApplicationManager.java)/**
   * Wrap the handler thread so it calls from the same user.
   */
TestUnmanagedApplicationManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/uam/TestUnmanagedApplicationManager.java)/**
 * Unit test for UnmanagedApplicationManager.
 */
ContainerBlockTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/webapp/ContainerBlockTest.java)/**
 * Tests for ContainerBlock.
 */
TestLogWebService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/webapp/TestLogWebService.java)/**
 * test class for log web service.
 */
TestResourceTrackerPBClientImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/TestResourceTrackerPBClientImpl.java)/**
 * Test ResourceTrackerPBClientImpl. this class should have methods
 * registerNodeManager and newRecordInstance.
 */
TestYarnServerApiClasses (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/TestYarnServerApiClasses.java)/**
 * Simple test classes from org.apache.hadoop.yarn.server.api
 */
AbstractRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/AbstractRequestInterceptor.java)/**
 * Implements the RequestInterceptor interface and provides common functionality
 * which can can be used and/or extended by other concrete intercepter classes.
 *
 */
AMRMProxyApplicationContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/AMRMProxyApplicationContext.java)/**
 * Interface that can be used by the intercepter plugins to get the information
 * about one application.
 *
 */
AMRMProxyApplicationContextImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/AMRMProxyApplicationContextImpl.java)/**
 * Encapsulates the information about one application that is needed by the
 * request intercepters.
 *
 */
ApplicationEventHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/AMRMProxyService.java)/**
   * Private class for handling application stop events.
   *
   */
RequestInterceptorChainWrapper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/AMRMProxyService.java)/**
   * Private structure for encapsulating RequestInterceptor and
   * ApplicationAttemptId instances.
   *
   */
AMRMProxyService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/AMRMProxyService.java)/**
 * AMRMProxyService is a service that runs on each node manager that can be used
 * to intercept and inspect messages from application master to the cluster
 * resource manager. It listens to messages from the application master and
 * creates a request intercepting pipeline instance for each application. The
 * pipeline is a chain of interceptor instances that can inspect and modify the
 * request/response as needed.
 */
AMRMProxyTokenSecretManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/AMRMProxyTokenSecretManager.java)/**
 * This secret manager instance is used by the AMRMProxyService to generate and
 * manage tokens.
 */
DefaultRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/DefaultRequestInterceptor.java)/**
 * Extends the AbstractRequestInterceptor class and provides an implementation
 * that simply forwards the AM requests to the cluster resource manager.
 *
 */
HeartbeatCallBack (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/FederationInterceptor.java)/**
   * Async callback handler for heart beat response from all sub-clusters.
   */
FinishApplicationMasterResponseInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/FederationInterceptor.java)/**
   * Private structure for encapsulating SubClusterId and
   * FinishApplicationMasterResponse instances.
   */
FederationInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/FederationInterceptor.java)/**
 * Extends the AbstractRequestInterceptor and provides an implementation for
 * federation of YARN RM and scaling an application across multiple YARN
 * sub-clusters. All the federation specific implementation is encapsulated in
 * this class. This is always the last intercepter in the chain.
 */
RequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/RequestInterceptor.java)/**
 * Defines the contract to be implemented by the request intercepter classes,
 * that can be used to intercept and inspect messages sent from the application
 * master to the resource manager.
 */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/Device.java)/**
   * Builder for Device.
   * */
Device (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/Device.java)/**
 * Represent one "device" resource.
 * */
DevicePlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/DevicePlugin.java)/**
 * A must interface for vendor plugin to implement.
 * */
DevicePluginScheduler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/DevicePluginScheduler.java)/**
 * An optional interface to implement if custom device scheduling is needed.
 * If this is not implemented, the device framework will do scheduling.
 * */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/DeviceRegisterRequest.java)/**
   * Builder class for construct {@link DeviceRegisterRequest}.
   * */
DeviceRegisterRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/DeviceRegisterRequest.java)/**
 * Contains plugin register request info.
 * */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/DeviceRuntimeSpec.java)/**
   * Builder for DeviceRuntimeSpec.
   * */
DeviceRuntimeSpec (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/DeviceRuntimeSpec.java)/**
 * This is a spec used to prepare and run container.
 * It's return value of onDeviceAllocated invoked by the framework.
 * For preparation, if volumeSpecs is populated then the framework will
 * create the volume before using the device
 * When running container, the envs indicates environment variable needed.
 * The containerRuntime indicates what Docker runtime to use.
 * The volume and device mounts describes key isolation requirements
 * */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/MountDeviceSpec.java)/**
   * Builder for MountDeviceSpec.
   * */
MountDeviceSpec (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/MountDeviceSpec.java)/**
 * Describe one device mount.
 * */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/MountVolumeSpec.java)/**
   * Builder for MountVolumeSpec.
   * */
MountVolumeSpec (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/MountVolumeSpec.java)/**
 * Describe one volume mount.
 * */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/VolumeSpec.java)/**
   * Builder for VolumeSpec.
   * */
VolumeSpec (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/VolumeSpec.java)/**
 * Describe one volume creation or deletion.
 * */
NMProtoUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/impl/pb/NMProtoUtils.java)/**
 * Utilities for converting from PB representations.
 */
CMgrUpdateContainersEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/CMgrUpdateContainersEvent.java)/**
 * Event used by the NodeStatusUpdater to notify the ContainerManager of
 * container update commands it received from the RM.
 */
NMCollectorService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/collectormanager/NMCollectorService.java)/**
 * Service that handles collector information. It is used only if the timeline
 * service v.2 is enabled.
 */
DelayedProcessKiller (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java)/**
   * This class will signal a target container after a specified delay.
   * @see #signalContainer
   */
ContainerExecutor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java)/**
 * This class is abstraction of the mechanism used to launch a container on the
 * underlying OS.  All executor implementations must extend ContainerExecutor.
 */
ApplicationContainerInitEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationContainerInitEvent.java)/**
 * Event sent from {@link ContainerManagerImpl} to {@link ApplicationImpl} to
 * request the initialization of a container. This is funneled through
 * the Application so that the application life-cycle can be checked, and container
 * launches can be delayed until the application is fully initialized.
 * 
 * Once the application is initialized,
 * {@link ApplicationImpl.InitContainerTransition} simply passes this event on as a
 * {@link ContainerInitEvent}.
 *  
 */
ApplicationFinishEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationFinishEvent.java)/**
 * Finish/abort event
 */
FlowContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java)/**
   * Data object that encapsulates the flow context for the application purpose.
   */
AppInitTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java)/**
   * Notify services of new application.
   * 
   * In particular, this initializes the {@link LogAggregationService}
   */
AppLogInitDoneTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java)/**
   * Handles the APPLICATION_LOG_HANDLING_INITED event that occurs after
   * {@link LogAggregationService} has created the directories for the app
   * and started the aggregation thread for the app.
   * 
   * In particular, this requests that the {@link ResourceLocalizationService}
   * localize the application-scoped resources.
   */
AppLogInitFailTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java)/**
   * Handles the APPLICATION_LOG_HANDLING_FAILED event that occurs after
   * {@link LogAggregationService} has failed to initialize the log 
   * aggregation service
   * 
   * In particular, this requests that the {@link ResourceLocalizationService}
   * localize the application-scoped resources.
   */
InitContainerTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java)/**
   * Handles INIT_CONTAINER events which request that we launch a new
   * container. When we're still in the INITTING state, we simply
   * queue these up. When we're in the RUNNING state, we pass along
   * an ContainerInitEvent to the appropriate ContainerImpl.
   */
ApplicationImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java)/**
 * The state machine for the representation of an Application
 * within the NodeManager.
 */
ManifestReloadTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/AuxServices.java)/**
   * Class which is used by the {@link Timer} class to periodically execute the
   * manifest reload.
   */
RequestResourcesTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * State transition when a NEW container receives the INIT_CONTAINER
   * message.
   * 
   * If there are resources to localize, sends a
   * ContainerLocalizationRequest (LOCALIZE_CONTAINER_RESOURCES)
   * to the ResourceLocalizationManager and enters LOCALIZING state.
   * 
   * If there are no resources to localize, sends LAUNCH_CONTAINER event
   * and enters SCHEDULED state directly.
   * 
   * If there are any invalid resources specified, enters LOCALIZATION_FAILED
   * directly.
   */
LocalizedTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transition when one of the requested resources for this container
   * has been successfully localized.
   */
ReInitializeContainerTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transition to start the Re-Initialization process.
   */
RollbackContainerTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transition to start the Rollback process.
   */
ResourceLocalizedWhileReInitTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Resource requested for Container Re-initialization has been localized.
   * If all dependencies are met, then restart Container with new bits.
   */
ResourceLocalizedWhileRunningTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Resource is localized while the container is running - create symlinks.
   */
ResourceLocalizationFailedWhileRunningTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Resource localization failed while the container is running.
   */
ResourceLocalizationFailedWhileReInitTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Resource localization failed while the container is reinitializing.
   */
LaunchTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transition from SCHEDULED state to RUNNING state upon receiving
   * a CONTAINER_LAUNCHED event.
   */
RecoveredContainerTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transition from SCHEDULED state to PAUSED state on recovery
   */
ExitedWithSuccessTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transition from RUNNING or KILLING state to
   * EXITED_WITH_SUCCESS state upon EXITED_WITH_SUCCESS message.
   */
ExitedWithFailureTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transition to EXITED_WITH_FAILURE state upon
   * CONTAINER_EXITED_WITH_FAILURE state.
   **/
RetryFailureTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transition to EXITED_WITH_FAILURE or RELAUNCHING state upon
   * CONTAINER_EXITED_WITH_FAILURE state.
   **/
KilledExternallyTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transition to EXITED_WITH_FAILURE
   */
KilledForReInitializationTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transition to SCHEDULED and wait for RE-LAUNCH
   */
ResourceFailedTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transition from LOCALIZING to LOCALIZATION_FAILED upon receiving
   * RESOURCE_FAILED event.
   */
KillBeforeRunningTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transition from LOCALIZING to KILLING upon receiving
   * KILL_CONTAINER event.
   */
LocalizedResourceDuringKillTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Remain in KILLING state when receiving a RESOURCE_LOCALIZED request
   * while in the process of killing.
   */
KillTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transitions upon receiving KILL_CONTAINER.
   * - SCHEDULED -> KILLING.
   * - RUNNING -> KILLING.
   * - REINITIALIZING -> KILLING.
   */
KillOnPauseTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transitions upon receiving PAUSE_CONTAINER.
   * - LOCALIZED -> KILLING.
   * - REINITIALIZING -> KILLING.
   */
ContainerKilledTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transition from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
   * upon receiving CONTAINER_KILLED_ON_REQUEST.
   */
ContainerDoneTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Handle the following transitions:
   * - {LOCALIZATION_FAILED, EXITED_WITH_SUCCESS, EXITED_WITH_FAILURE,
   *    KILLING, CONTAINER_CLEANEDUP_AFTER_KILL}
   *   -> DONE upon CONTAINER_RESOURCES_CLEANEDUP
   */
KillOnNewTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Handle the following transition:
   * - NEW -> DONE upon KILL_CONTAINER
   */
LocalizationFailedToDoneTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Handle the following transition:
   * - LOCALIZATION_FAILED -> DONE upon CONTAINER_RESOURCES_CLEANEDUP
   */
ExitedWithSuccessToDoneTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Handle the following transition:
   * - EXITED_WITH_SUCCESS -> DONE upon CONTAINER_RESOURCES_CLEANEDUP
   */
ExitedWithFailureToDoneTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Handle the following transition:
   * - EXITED_WITH_FAILURE -> DONE upon CONTAINER_RESOURCES_CLEANEDUP
   */
KillingToDoneTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Handle the following transition:
   * - KILLING -> DONE upon CONTAINER_RESOURCES_CLEANEDUP
   */
ContainerCleanedupAfterKillToDoneTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Handle the following transition:
   * CONTAINER_CLEANEDUP_AFTER_KILL -> DONE upon CONTAINER_RESOURCES_CLEANEDUP
   */
ContainerDiagnosticsUpdateTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Update diagnostics, staying in the same state.
   */
PauseContainerTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transitions upon receiving PAUSE_CONTAINER.
   * - RUNNING -> PAUSING
   */
PausedContainerTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transitions upon receiving PAUSED_CONTAINER.
   */
ResumeContainerTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerImpl.java)/**
   * Transitions upon receiving RESUME_CONTAINER.
   * - PAUSED -> RUNNING
   */
ContainerPauseEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerPauseEvent.java)/**
 * ContainerEvent for ContainerEventType.PAUSE_CONTAINER.
 */
ContainerReInitEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerReInitEvent.java)/**
 * ContainerEvent sent by ContainerManager to ContainerImpl to
 * re-initiate Container.
 */
ContainerResumeEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ContainerResumeEvent.java)/**
 * ContainerEvent for ContainerEventType.RESUME_CONTAINER.
 */
AssignedResources (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ResourceMappings.java)/**
   * Stores resources assigned to a container for a given resource type.
   */
ResourceMappings (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/ResourceMappings.java)/**
 * This class is used to store assigned resource to a single container by
 * resource types.
 *
 * Assigned resource could be list of String
 *
 * For example, we can assign container to:
 * "numa": ["numa0"]
 * "gpu": ["0", "1", "2", "3"]
 * "fpga": ["1", "3"]
 *
 * This will be used for NM restart container recovery.
 */
RetryContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/SlidingWindowRetryPolicy.java)/**
   * Sliding window container retry context.
   * <p>
   * Besides {@link ContainerRetryContext}, it also provide details such as:
   * <ul>
   * <li>
   * <em>remainingRetries</em>: specifies the number of pending retries. It is
   * initially set to <code>containerRetryContext.maxRetries</code>.
   * </li>
   * <li>
   * <em>restartTimes</em>: when
   * <code>containerRetryContext.failuresValidityInterval</code> is set,
   * then this records the times when the container is set to restart.
   * </li>
   * </ul>
   */
SlidingWindowRetryPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/SlidingWindowRetryPolicy.java)/**
 * <p>Sliding window retry policy for relaunching a
 * <code>Container</code> in Yarn.</p>
 */
UpdateContainerTokenEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/UpdateContainerTokenEvent.java)/**
 * Update Event consumed by the Container.
 */
ContainerManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManager.java)/**
 * The ContainerManager is an entity that manages the life cycle of Containers.
 */
AuxiliaryLocalPathHandlerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java)/**
   * Implements AuxiliaryLocalPathHandler.
   * It links NodeManager's LocalDirsHandlerService to the Auxiliary Services
   */
DeletionTaskRecoveryInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/deletion/recovery/DeletionTaskRecoveryInfo.java)/**
 * Encapsulates the recovery info needed to recover a DeletionTask from the NM
 * state store.
 */
DeletionTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/deletion/task/DeletionTask.java)/**
 * DeletionTasks are supplied to the {@link DeletionService} for deletion.
 */
DockerContainerDeletionTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/deletion/task/DockerContainerDeletionTask.java)/**
 * {@link DeletionTask} handling the removal of Docker containers.
 */
FileDeletionTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/deletion/task/FileDeletionTask.java)/**
 * {@link DeletionTask} handling the removal of files (and directories).
 */
AbstractContainersLauncher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/AbstractContainersLauncher.java)/**
 * Pluggable ContainersLauncher interface for processing
 * ContainersLauncherEvents.
 */
ContainerCleanup (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerCleanup.java)/**
 * Cleanup the container.
 * Cancels the launch if launch has not started yet or signals
 * the executor to not execute the process if not already done so.
 * Also, sends a SIGTERM followed by a SIGKILL to the process if
 * the process id is available.
 */
ContainerRelaunch (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerRelaunch.java)/**
 * Relaunch container.
 */
ContainersLauncher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainersLauncher.java)/**
 * The launcher for the containers. This service should be started only after
 * the {@link ResourceLocalizationService} is started as it depends on creation
 * of system directories on the local file-system.
 * 
 */
RecoveredContainerLaunch (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/RecoveredContainerLaunch.java)/**
 * This is a ContainerLaunch which has been recovered after an NM restart (for
 * rolling upgrades).
 */
RecoverPausedContainerLaunch (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/RecoverPausedContainerLaunch.java)/**
 * This is a ContainerLaunch which has been recovered after an NM restart for
 * pause containers (for rolling upgrades)
 */
OOMNotResolvedException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/CGroupElasticMemoryController.java)/**
   * Exception thrown if the OOM situation is not resolved.
   */
CGroupElasticMemoryController (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/CGroupElasticMemoryController.java)/**
 * This thread controls memory usage using cgroups. It listens to out of memory
 * events of all the containers together, and if we go over the limit picks
 * a container to kill. The algorithm that picks the container is a plugin.
 */
CGroupsBlkioResourceHandlerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/CGroupsBlkioResourceHandlerImpl.java)/**
 * Handler class to handle the blkio controller. Currently it splits resources
 * evenly across all containers. Once we have scheduling sorted out, we can
 * modify the function to represent the disk resources allocated.
 */
CGroupsCpuResourceHandlerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/CGroupsCpuResourceHandlerImpl.java)/**
 * An implementation for using CGroups to restrict CPU usage on Linux. The
 * implementation supports 3 different controls - restrict usage of all YARN
 * containers, restrict relative usage of individual YARN containers and
 * restrict usage of individual YARN containers. Admins can set the overall CPU
 * to be used by all YARN containers - this is implemented by setting
 * cpu.cfs_period_us and cpu.cfs_quota_us to the ratio desired. If strict
 * resource usage mode is not enabled, cpu.shares is set for individual
 * containers - this prevents containers from exceeding the overall limit for
 * YARN containers but individual containers can use as much of the CPU as
 * available(under the YARN limit). If strict resource usage is enabled, then
 * container can only use the percentage of CPU allocated to them and this is
 * again implemented using cpu.cfs_period_us and cpu.cfs_quota_us.
 *
 */
CGroupsMemoryResourceHandlerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/CGroupsMemoryResourceHandlerImpl.java)/**
 * Handler class to handle the memory controller. YARN already ships a
 * physical memory monitor in Java but it isn't as
 * good as CGroups. This handler sets the soft and hard memory limits. The soft
 * limit is set to 90% of the hard limit.
 */
CGroupsMountConfig (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/CGroupsMountConfig.java)/**
 * Stores config related to cgroups.
 */
CGroupsResourceCalculator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/CGroupsResourceCalculator.java)/**
 * A cgroups file-system based Resource calculator without the process tree
 * features.
 *
 * CGroups has its limitations. It can only be enabled, if both CPU and memory
 * cgroups are enabled with yarn.nodemanager.resource.cpu.enabled and
 * yarn.nodemanager.resource.memory.enabled respectively. This means that
 * memory limits are enforced by default. You can turn this off and keep
 * memory reporting only with yarn.nodemanager.resource.memory.enforced.
 *
 * Another limitation is virtual memory measurement. CGroups does not have the
 * ability to measure virtual memory usage. This includes memory reserved but
 * not used. CGroups measures used memory as sa sum of
 * physical memory and swap usage. This will be returned in the virtual
 * memory counters.
 * If the real virtual memory is required please use the legacy procfs based
 * resource calculator or CombinedResourceCalculator.
 */
CombinedResourceCalculator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/CombinedResourceCalculator.java)/**
 * CombinedResourceCalculator is a resource calculator that uses cgroups but
 * it is backward compatible with procfs in terms of virtual memory usage.
 */
CpuResourceHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/CpuResourceHandler.java)/**
 * Resource handler for cpu resources.
 */
ContainerCandidate (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/DefaultOOMHandler.java)/**
   * Note: this class has a natural ordering that is inconsistent with equals.
   */
DefaultOOMHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/DefaultOOMHandler.java)/**
 * A very basic OOM handler implementation.
 * See the javadoc on the run() method for details.
 */
DiskResourceHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/DiskResourceHandler.java)/**
 * Resource handler for disk resources.
 */
FpgaDevice (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/fpga/FpgaResourceAllocator.java)/** A class that represents an FPGA card. */
FpgaResourceAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/fpga/FpgaResourceAllocator.java)/**
 * This FPGA resource allocator tends to be used by different FPGA vendor's plugin
 * A "type" parameter is taken into consideration when allocation
 * */
GpuAllocation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/gpu/GpuResourceAllocator.java)/**
   * Contains allowed and denied devices.
   * Denied devices will be useful for cgroups devices module to do blacklisting
   */
GpuResourceAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/gpu/GpuResourceAllocator.java)/**
 * Allocate GPU resources according to requirements.
 */
NetworkPacketTaggingHandlerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/NetworkPacketTaggingHandlerImpl.java)/**
 * The network packet tagging handler implementation.
 *
 */
NetworkTagMapping (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/NetworkTagMappingJsonManager.java)/**
   * The NetworkTagMapping object.
   *
   */
User (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/NetworkTagMappingJsonManager.java)/**
   * The user object.
   *
   */
Group (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/NetworkTagMappingJsonManager.java)/**
   * The group object.
   *
   */
NetworkTagMappingJsonManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/NetworkTagMappingJsonManager.java)/**
 * The NetworkTagMapping JsonManager implementation.
 */
NetworkTagMappingManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/NetworkTagMappingManager.java)/**
 * Base interface for network tag mapping manager.
 */
NetworkTagMappingManagerFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/NetworkTagMappingManagerFactory.java)/**
 * Use {@code NetworkTagMappingManagerFactory} to get the correct
 * {@link NetworkTagMappingManager}.
 *
 */
NumaNodeResource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/numa/NumaNodeResource.java)/**
 * NumaNodeResource class holds the NUMA node topology with the total and used
 * resources.
 */
NumaResourceAllocation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/numa/NumaResourceAllocation.java)/**
 * NumaResourceAllocation contains Memory nodes and CPU nodes assigned to a
 * container.
 */
NumaResourceAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/numa/NumaResourceAllocator.java)/**
 * NUMA Resources Allocator reads the NUMA topology and assigns NUMA nodes to
 * the containers.
 */
NumaResourceHandlerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/numa/NumaResourceHandlerImpl.java)/**
 * ResourceHandler implementation for allocating NUMA Resources to each
 * container.
 */
ResourcesExceptionUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/ResourcesExceptionUtil.java)/**
 * Small utility class which only re-throws YarnException if
 * NM_RESOURCE_PLUGINS_FAIL_FAST property is true.
 *
 */
DefaultLinuxContainerRuntime (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DefaultLinuxContainerRuntime.java)/**
 * This class is a {@link ContainerRuntime} implementation that uses the
 * native {@code container-executor} binary via a
 * {@link PrivilegedOperationExecutor} instance to launch processes using the
 * standard process model.
 */
DelegatingLinuxContainerRuntime (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DelegatingLinuxContainerRuntime.java)/**
 * This class is a {@link ContainerRuntime} implementation that delegates all
 * operations to a {@link DefaultLinuxContainerRuntime} instance, a
 * {@link DockerLinuxContainerRuntime} instance, a
 * {@link JavaSandboxLinuxContainerRuntime} instance, or a custom instance
 * depending on whether each instance believes the operation to be within its
 * scope.
 *
 * @see LinuxContainerRuntime#isRuntimeRequested
 */
DockerCommandExecutor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommandExecutor.java)/**
 * Utility class for executing common docker operations.
 */
DockerExecCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerExecCommand.java)/**
 * Encapsulates the docker exec command and its command
 * line arguments.
 */
DockerImagesCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerImagesCommand.java)/**
 * Encapsulates the docker images command and its command
 * line arguments.
 */
DockerInspectCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerInspectCommand.java)/**
 * Encapsulates the docker inspect command and its command
 * line arguments.
 */
DockerKillCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerKillCommand.java)/**
 * Encapsulates the docker kill command and its command line arguments.
 */
DockerPullCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerPullCommand.java)/**
 * Encapsulates the docker pull command and its command
 * line arguments.
 */
DockerRmCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerRmCommand.java)/**
 * Encapsulates the docker rm command and its command
 * line arguments.
 */
DockerStartCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerStartCommand.java)/**
 * Encapsulates the docker start command and its command line arguments.
 */
DockerStopCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerStopCommand.java)/**
 * Encapsulates the docker stop command and its command
 * line arguments.
 */
DockerVolumeCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerVolumeCommand.java)/**
 * Docker Volume Command, run "docker volume --help" for more details.
 */
DockerLinuxContainerRuntime (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java)/**
 * <p>This class is an extension of {@link OCIContainerRuntime} that uses the
 * native {@code container-executor} binary via a
 * {@link PrivilegedOperationExecutor} instance to launch processes inside
 * Docker containers.</p>
 *
 * <p>The following environment variables are used to configure the Docker
 * engine:</p>
 *
 * <ul>
 *   <li>
 *     {@code YARN_CONTAINER_RUNTIME_TYPE} ultimately determines whether a
 *     Docker container will be used. If the value is {@code docker}, a Docker
 *     container will be used. Otherwise a regular process tree container will
 *     be used. This environment variable is checked by the
 *     {@link #isDockerContainerRequested} method, which is called by the
 *     {@link DelegatingLinuxContainerRuntime}.
 *   </li>
 *   <li>
 *     {@code YARN_CONTAINER_RUNTIME_DOCKER_IMAGE} names which image
 *     will be used to launch the Docker container.
 *   </li>
 *   <li>
 *     {@code YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE} controls
 *     whether the Docker container's default command is overridden.  When set
 *     to {@code true}, the Docker container's command will be
 *     {@code bash <path_to_launch_script>}. When unset or set to {@code false}
 *     the Docker container's default command is used.
 *   </li>
 *   <li>
 *     {@code YARN_CONTAINER_RUNTIME_DOCKER_CONTAINER_NETWORK} sets the
 *     network type to be used by the Docker container. It must be a valid
 *     value as determined by the
 *     {@code yarn.nodemanager.runtime.linux.docker.allowed-container-networks}
 *     property.
 *   </li>
 *   <li>
 *     {@code YARN_CONTAINER_RUNTIME_DOCKER_PORTS_MAPPING} allows users to
 *     specify ports mapping for the bridge network Docker container. The value
 *     of the environment variable should be a comma-separated list of ports
 *     mapping. It's the same to "-p" option for the Docker run command. If the
 *     value is empty, "-P" will be added.
 *   </li>
 *   <li>
 *     {@code YARN_CONTAINER_RUNTIME_DOCKER_CONTAINER_PID_NAMESPACE}
 *     controls which PID namespace will be used by the Docker container. By
 *     default, each Docker container has its own PID namespace. To share the
 *     namespace of the host, the
 *     {@code yarn.nodemanager.runtime.linux.docker.host-pid-namespace.allowed}
 *     property must be set to {@code true}. If the host PID namespace is
 *     allowed and this environment variable is set to {@code host}, the
 *     Docker container will share the host's PID namespace. No other value is
 *     allowed.
 *   </li>
 *   <li>
 *     {@code YARN_CONTAINER_RUNTIME_DOCKER_CONTAINER_HOSTNAME} sets the
 *     hostname to be used by the Docker container. If not specified, a
 *     hostname will be derived from the container ID and set as default
 *     hostname for networks other than 'host'.
 *   </li>
 *   <li>
 *     {@code YARN_CONTAINER_RUNTIME_DOCKER_RUN_PRIVILEGED_CONTAINER}
 *     controls whether the Docker container is a privileged container. In order
 *     to use privileged containers, the
 *     {@code yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed}
 *     property must be set to {@code true}, and the application owner must
 *     appear in the value of the
 *     {@code yarn.nodemanager.runtime.linux.docker.privileged-containers.acl}
 *     property. If this environment variable is set to {@code true}, a
 *     privileged Docker container will be used if allowed. No other value is
 *     allowed, so the environment variable should be left unset rather than
 *     setting it to false.
 *   </li>
 *   <li>
 *     {@code YARN_CONTAINER_RUNTIME_DOCKER_MOUNTS} allows users to specify
 *     additional volume mounts for the Docker container. The value of the
 *     environment variable should be a comma-separated list of mounts.
 *     All such mounts must be given as {@code source:dest[:mode]} and the mode
 *     must be "ro" (read-only) or "rw" (read-write) to specify the type of
 *     access being requested. If neither is specified, read-write will be
 *     assumed. The mode may include a bind propagation option. In that case,
 *     the mode should either be of the form [option], rw+[option], or
 *     ro+[option]. Valid bind propagation options are shared, rshared, slave,
 *     rslave, private, and rprivate. The requested mounts will be validated by
 *     container-executor based on the values set in container-executor.cfg for
 *     {@code docker.allowed.ro-mounts} and {@code docker.allowed.rw-mounts}.
 *   </li>
 *   <li>
 *     {@code YARN_CONTAINER_RUNTIME_DOCKER_TMPFS_MOUNTS} allows users to
 *     specify additional tmpfs mounts for the Docker container. The value of
 *     the environment variable should be a comma-separated list of mounts.
 *   </li>
 *   <li>
 *     {@code YARN_CONTAINER_RUNTIME_DOCKER_DELAYED_REMOVAL} allows a user
 *     to request delayed deletion of the Docker containers on a per
 *     container basis. If true, Docker containers will not be removed until
 *     the duration defined by {@code yarn.nodemanager.delete.debug-delay-sec}
 *     has elapsed. Administrators can disable this feature through the
 *     yarn-site property
 *     {@code yarn.nodemanager.runtime.linux.docker.delayed-removal.allowed}.
 *     This feature is disabled by default. When this feature is disabled or set
 *     to false, the container will be removed as soon as it exits.
 *   </li>
 *   <li>
 *     {@code YARN_CONTAINER_RUNTIME_YARN_SYSFS_ENABLE} allows export yarn
 *     service json to docker container.  This feature is disabled by default.
 *     When this feature is set, app.json will be available in
 *     /hadoop/yarn/sysfs/app.json.
 *   </li>
 * </ul>
 */
NMContainerPolicyUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/JavaSandboxLinuxContainerRuntime.java)/**
   * Static utility class defining String constants and static methods for the
   * use of the {@link JavaSandboxLinuxContainerRuntime}.
   */
JavaSandboxLinuxContainerRuntime (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/JavaSandboxLinuxContainerRuntime.java)/**
 * <p>This class extends the {@link DefaultLinuxContainerRuntime} specifically
 * for containers which run Java commands.  It generates a new java security
 * policy file per container and modifies the java command to enable the
 * Java Security Manager with the generated policy.</p>
 *
 * The behavior of the {@link JavaSandboxLinuxContainerRuntime} can be modified
 * using the following settings:
 *
 * <ul>
 *   <li>
 *     {@value
 *     org.apache.hadoop.yarn.conf.YarnConfiguration#YARN_CONTAINER_SANDBOX} :
 *     This yarn-site.xml setting has three options:
 *     <ul>
 *     <li>disabled - Default behavior. {@link LinuxContainerRuntime}
 *     is disabled</li>
 *     <li>permissive - JVM containers will run with Java Security Manager
 *     enabled.  Non-JVM containers will run normally</li>
 *     <li>enforcing - JVM containers will run with Java Security Manager
 *     enabled.  Non-JVM containers will be prevented from executing and an
 *     {@link ContainerExecutionException} will be thrown.</li>
 *     </ul>
 *   </li>
 *   <li>
 *     {@value
 *     org.apache.hadoop.yarn.conf.YarnConfiguration#YARN_CONTAINER_SANDBOX_FILE_PERMISSIONS}
 *     :
 *     Determines the file permissions for the application directories.  The
 *     permissions come in the form of comma separated values
 *     (e.g. read,write,execute,delete). Defaults to {@code read} for read-only.
 *   </li>
 *   <li>
 *     {@value
 *     org.apache.hadoop.yarn.conf.YarnConfiguration#YARN_CONTAINER_SANDBOX_POLICY}
 *     :
 *     Accepts canonical path to a java policy file on the local filesystem.
 *     This file will be loaded as the base policy, any additional container
 *     grants will be appended to this base file.  If not specified, the default
 *     java.policy file provided with hadoop resources will be used.
 *   </li>
 *   <li>
 *     {@value
 *     org.apache.hadoop.yarn.conf.YarnConfiguration#YARN_CONTAINER_SANDBOX_WHITELIST_GROUP}
 *     :
 *     Optional setting to specify a YARN queue which will be exempt from the
 *     sand-boxing process.
 *   </li>
 *   <li>
 *     {@value
 *     org.apache.hadoop.yarn.conf.YarnConfiguration#YARN_CONTAINER_SANDBOX_POLICY_GROUP_PREFIX}$groupName
 *     :
 *     Optional setting to map groups to java policy files.  The value is a path
 *     to the java policy file for $groupName.  A user which is a member of
 *     multiple groups with different policies will receive the superset of all
 *     the permissions across their groups.
 *   </li>
 * </ul>
 */
ContainerLocalizationRequestEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/event/ContainerLocalizationRequestEvent.java)/**
 * Event that requests that the {@link ResourceLocalizationService} localize
 * a set of resources for the given container. This is generated by
 * {@link ContainerImpl} during container initialization.
 */
LocalizationEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/event/LocalizationEvent.java)/**
 * Events handled by {@link ResourceLocalizationService}
 */
LocalizerEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/event/LocalizerEvent.java)/**
 * Events delivered to the {@link ResourceLocalizationService}
 */
LocalizerResourceRequestEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/event/LocalizerResourceRequestEvent.java)/**
 * Event indicating that the {@link ResourceLocalizationService}
 * should fetch this resource.
 */
ResourceFailedLocalizationEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/event/ResourceFailedLocalizationEvent.java)/**
 * This event is sent by the localizer in case resource localization fails for
 * the requested resource.
 */
LocalCacheCleaner (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalCacheCleaner.java)/**
 * A class responsible for cleaning the PUBLIC and PRIVATE local caches on a
 * node manager.
 */
LocalCacheDirectoryManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalCacheDirectoryManager.java)/**
 * {@link LocalCacheDirectoryManager} is used for managing hierarchical
 * directories for local cache. It will allow to restrict the number of files in
 * a directory to
 * {@link YarnConfiguration#NM_LOCAL_CACHE_MAX_FILES_PER_DIRECTORY} which
 * includes 36 sub-directories (named from 0 to 9 and a to z). Root directory is
 * represented by an empty string. It internally maintains a vacant directory
 * queue. As soon as the file count for the directory reaches its limit; new
 * files will not be created in it until at least one file is deleted from it.
 * New sub directories are not created unless a
 * {@link LocalCacheDirectoryManager#getRelativePathForLocalization()} request
 * is made and nonFullDirectories are empty.
 * 
 * Note : this structure only returns relative localization path but doesn't
 * create one on disk.
 */
FetchResourceTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalizedResource.java)/**
   * Transition from INIT to DOWNLOADING.
   * Sends a {@link LocalizerResourceRequestEvent} to the
   * {@link ResourceLocalizationService}.
   */
FetchSuccessTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalizedResource.java)/**
   * Resource localized, notify waiting containers.
   */
FetchFailedTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalizedResource.java)/**
   * Resource localization failed, notify waiting containers.
   */
LocalizedResourceTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalizedResource.java)/**
   * Resource already localized, notify immediately.
   */
ReleaseTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalizedResource.java)/**
   * Decrement resource count, update timestamp.
   */
LocalizedResource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalizedResource.java)/**
 * Datum representing a localized resource. Holds the statemachine of a
 * resource. State of the resource is one of {@link ResourceState}.
 * 
 */
LocalResourcesTracker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTracker.java)/**
 * Component tracking resources all of the same {@link LocalResourceVisibility}
 * 
 */
LocalizerTracker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java)/**
   * Sub-component handling the spawning of {@link ContainerLocalizer}s
   */
LocalizerRunner (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java)/**
   * Runs the {@link ContainerLocalizer} itself in a separate process with
   * access to user's credentials. One {@link LocalizerRunner} per localizerId.
   * 
   */
ResourceSet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceSet.java)/**
 * All Resources requested by the container.
 */
SharedCacheUploader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/sharedcache/SharedCacheUploader.java)/**
 * The callable class that handles the actual upload to the shared cache.
 */
SampleContainerLogAggregationPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/SampleContainerLogAggregationPolicy.java)/**
 * The sample policy samples logs of successful worker containers to aggregate.
 * It always aggregates AM container and failed/killed worker
 * containers' logs. To make sure small applications have enough logs, it only
 * applies sampling beyond minimal number of containers. The parameters can be
 * configured by SAMPLE_RATE and MIN_THRESHOLD. For example if SAMPLE_RATE is
 * 0.2 and MIN_THRESHOLD is 20, for an application with 100 successful
 * worker containers, 20 + (100-20) * 0.2 = 36 containers's logs will be
 * aggregated.
 */
NonAggregatingLogHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java)/**
 * Log Handler which schedules deletion of log files based on the configured log
 * retention time.
 */
ProcessTreeInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/monitor/ContainersMonitorImpl.java)/**
   * Encapsulates resource requirements of a process and its tree.
   */
ContainersMonitorImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/monitor/ContainersMonitorImpl.java)/**
 * Monitors containers collecting resource usage and preempting the container
 * if it exceeds its limits.
 */
AuxServiceConfiguration (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/records/AuxServiceConfiguration.java)/**
 * Set of configuration properties that can be injected into the service
 * components via envs, files and custom pluggable helper docker containers.
 * Files of several standard formats like xml, properties, json, yaml and
 * templates will be supported.
 **/
AuxServiceFile (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/records/AuxServiceFile.java)/**
 * A config file that needs to be created and made available as a volume in an
 * service component container.
 **/
AuxServiceRecord (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/records/AuxServiceRecord.java)/**
 * An Service resource has the following attributes.
 **/
AuxServiceRecords (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/records/AuxServiceRecords.java)/**
 * A list of Services.
 **/
NECVEPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/com/nec/NECVEPlugin.java)/**
 * A device framework plugin which supports NEC Vector Engine.
 *
 */
NvidiaCommandExecutor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/com/nvidia/NvidiaGPUPluginForRuntimeV2.java)/**
   * A shell wrapper class easy for test.
   * */
NvidiaGPUPluginForRuntimeV2 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/com/nvidia/NvidiaGPUPluginForRuntimeV2.java)/**
 * Nvidia GPU plugin supporting both Nvidia container runtime v2 for Docker and
 * non-Docker container.
 * It has topology aware as well as simple scheduling ability.
 * */
AssignedDevice (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/deviceframework/AssignedDevice.java)/**
 * Device wrapper class used for NM REST API.
 * */
DeviceMappingManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/deviceframework/DeviceMappingManager.java)/**
 * Schedule device resource based on requirements and do book keeping
 * It holds all device type resource and can do scheduling as a default
 * scheduler.
 * */
DevicePluginAdapter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/deviceframework/DevicePluginAdapter.java)/**
 * The {@link DevicePluginAdapter} will adapt existing hooks.
 * into vendor plugin's logic.
 * It decouples the vendor plugin from YARN's device framework
 *
 * */
DeviceResourceDockerRuntimePluginImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/deviceframework/DeviceResourceDockerRuntimePluginImpl.java)/**
 * Bridge DevicePlugin and the hooks related to lunch Docker container.
 * When launching Docker container, DockerLinuxContainerRuntime will invoke
 * this class's methods which get needed info back from DevicePlugin.
 * */
DeviceResourceHandlerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/deviceframework/DeviceResourceHandlerImpl.java)/**
 * The Hooks into container lifecycle.
 * Get device list from device plugin in {@code bootstrap}
 * Assign devices for a container in {@code preStart}
 * Restore statue in {@code reacquireContainer}
 * Recycle devices from container in {@code postComplete}
 * */
DeviceResourceUpdaterImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/deviceframework/DeviceResourceUpdaterImpl.java)/**
 * Hooks into NodeStatusUpdater to update resource.
 * */
ShellWrapper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/deviceframework/ShellWrapper.java)/**
 * A shell Wrapper to ease testing.
 * */
DockerCommandPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/DockerCommandPlugin.java)/**
 * Interface to make different resource plugins (e.g. GPU) can update docker run
 * command without adding logic to Docker runtime.
 */
AoclOutputBasedDiscoveryStrategy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/discovery/AoclOutputBasedDiscoveryStrategy.java)/**
 * FPGA device discovery strategy which invokes the "aocl" SDK command
 * to retrieve the list of available FPGA cards.
 */
DeviceSpecParser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/discovery/DeviceSpecParser.java)/**
 * Parses a string which specifies FPGA devices. Multiple devices should be
 * separated by a comma. A device specification should contain the
 * symbolic name of the device, major and minor device numbers.
 *
 * Example: "acl0/243:0,acl1/243:1".
 */
FPGADiscoveryStrategy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/discovery/FPGADiscoveryStrategy.java)/**
 * Interface for an FPGA device discovery strategy.
 */
ScriptBasedFPGADiscoveryStrategy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/discovery/ScriptBasedFPGADiscoveryStrategy.java)/**
 * FPGA device discovery strategy which invokes an external script.
 * The script must return a single line in given format.
 *
 * See DeviceSpecParser for details.
 */
SettingsBasedFPGADiscoveryStrategy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/discovery/SettingsBasedFPGADiscoveryStrategy.java)/**
 * FPGA device discovery strategy which parses a string.
 * The string must consist of a single line and be in a specific format.
 *
 * See DeviceSpecParser for details.
 */
InnerShellExecutor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/IntelFpgaOpenclPlugin.java)/**
   *  Helper class to run aocl diagnose &amp; determine major/minor numbers.
   */
IntelFpgaOpenclPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/IntelFpgaOpenclPlugin.java)/**
 * Intel FPGA for OpenCL plugin.
 * The key points are:
 * 1. It uses Intel's toolchain "aocl" to discover devices/reprogram IP
 *    to the device before container launch to achieve a quickest
 *    reprogramming path
 * 2. It avoids reprogramming by maintaining a mapping of device to FPGA IP ID
 * 3. It assume IP file is distributed to container directory
 */
AssignedGpuDevice (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/AssignedGpuDevice.java)/**
 * In addition to {@link GpuDevice}, this include container id and more runtime
 * information related to who is using the GPU device if possible
 */
GpuDevice (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/GpuDevice.java)/**
 * This class is used to represent GPU device while allocation.
 */
GpuDeviceSpecificationException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/GpuDeviceSpecificationException.java)/**
 * This exception is to be thrown when allowed GPU device specification
 * is empty or invalid.
 */
GpuDockerCommandPluginFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/GpuDockerCommandPluginFactory.java)/**
 * Factory to create GpuDocker Command Plugin instance
 */
NvidiaBinaryHelper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/NvidiaBinaryHelper.java)/**
 * Executes the "nvidia-smi" command and returns an object
 * based on its output.
 *
 */
NvidiaDockerV1CommandPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/NvidiaDockerV1CommandPlugin.java)/**
 * Implementation to use nvidia-docker v1 as GPU docker command plugin.
 */
NvidiaDockerV2CommandPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/NvidiaDockerV2CommandPlugin.java)/**
 * Implementation to use nvidia-docker v2 as GPU docker command plugin.
 */
NodeResourceUpdaterPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/NodeResourceUpdaterPlugin.java)/**
 * Plugins to handle resources on a node. This will be used by
 * {@link org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdater}
 */
ResourcePlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/ResourcePlugin.java)/**
 * {@link ResourcePlugin} is an interface for node manager to easier support
 * discovery/manage/isolation for new resource types.
 *
 * <p>
 * It has two major part: {@link ResourcePlugin#createResourceHandler(Context,
 * CGroupsHandler, PrivilegedOperationExecutor)} and
 * {@link ResourcePlugin#getNodeResourceHandlerInstance()}, see javadocs below
 * for more details.
 * </p>
 */
ResourcePluginManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/ResourcePluginManager.java)/**
 * Manages {@link ResourcePlugin} configured on this NodeManager.
 */
Attribute (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerRuntimeContext.java)/** An attribute class that attempts to provide better type safety as compared
   * with using a map of string to object.
   * @param <T>
   */
AllocationBasedResourceUtilizationTracker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/scheduler/AllocationBasedResourceUtilizationTracker.java)/**
 * An implementation of the {@link ResourceUtilizationTracker} that equates
 * resource utilization with the total resource allocated to the container.
 */
ContainerScheduler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/scheduler/ContainerScheduler.java)/**
 * The ContainerScheduler manages a collection of runnable containers. It
 * ensures that a container is launched only if all its launch criteria are
 * met. It also ensures that OPPORTUNISTIC containers are killed to make
 * room for GUARANTEED containers.
 */
ContainerSchedulerEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/scheduler/ContainerSchedulerEvent.java)/**
 * Events consumed by the {@link ContainerScheduler}.
 */
ResourceUtilizationTracker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/scheduler/ResourceUtilizationTracker.java)/**
 * This interface abstracts out how a container contributes to
 * Resource Utilization of the node.
 * It is used by the {@link ContainerScheduler} to determine which
 * OPPORTUNISTIC containers to be killed to make room for a GUARANTEED
 * container.
 */
UpdateContainerSchedulerEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/scheduler/UpdateContainerSchedulerEvent.java)/**
 * Update Event consumed by the {@link ContainerScheduler}.
 */
ContainerVolumePublisher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/volume/csi/ContainerVolumePublisher.java)/**
 * Publish/un-publish CSI volumes on node manager.
 */
ContainerStateTransitionListener (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerStateTransitionListener.java)/**
 * Interface to be used by external cluster operators to implement a
 * State Transition listener that is notified before and after a container
 * state transition.
 * NOTE: The pre and post transition callbacks will be made in the synchronized
 *       block as the call to the instrumented transition - Serially, in the
 *       order: preTransition, transition and postTransition. The implementor
 *       must ensure that the callbacks return in a timely manner to avoid
 *       blocking the state-machine.
 */
Context (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/Context.java)/**
 * Context interface for sharing information across components in the
 * NodeManager.
 */
LocalWrapperScriptBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DefaultContainerExecutor.java)/**
   * This class is a utility to create a wrapper script that is platform
   * appropriate.
   */
UnixLocalWrapperScriptBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DefaultContainerExecutor.java)/**
   * This class is an instance of {@link LocalWrapperScriptBuilder} for
   * non-Windows hosts.
   */
WindowsLocalWrapperScriptBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DefaultContainerExecutor.java)/**
   * This class is an instance of {@link LocalWrapperScriptBuilder} for
   * Windows hosts.
   */
DefaultContainerExecutor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DefaultContainerExecutor.java)/**
 * The {@code DefaultContainerExecuter} class offers generic container
 * execution services. Process execution is handled in a platform-independent
 * way via {@link ProcessBuilder}.
 */
DirsChangeListener (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DirectoryCollection.java)/**
   * The interface provides a callback when localDirs is changed.
   */
DirectoryCollection (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DirectoryCollection.java)/**
 * Manages a list of local storage directories.
 */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/executor/ContainerExecContext.java)/**
   *  Builder for ContainerExecContext.
   */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/executor/ContainerPrepareContext.java)/**
   * Builder for ContainerPrepareContext.
   */
ContainerPrepareContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/executor/ContainerPrepareContext.java)/**
 * Encapsulates information required for preparing containers.
 */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/executor/ContainerReapContext.java)/**
   * Builder for the ContainerReapContext.
   */
ContainerReapContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/executor/ContainerReapContext.java)/**
 * Encapsulate the details needed to reap a container.
 */
LinuxContainerExecutor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java)/**
 * <p>This class provides {@link Container} execution using a native
 * {@code container-executor} binary. By using a helper written it native code,
 * this class is able to do several things that the
 * {@link DefaultContainerExecutor} cannot, such as execution of applications
 * as the applications' owners, provide localization that takes advantage of
 * mapping the application owner to a UID on the execution host, resource
 * management through Linux CGROUPS, and Docker support.</p>
 *
 * <p>If {@code hadoop.security.authentication} is set to {@code simple},
 * then the
 * {@code yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users}
 * property will determine whether the {@code LinuxContainerExecutor} runs
 * processes as the application owner or as the default user, as set in the
 * {@code yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user}
 * property.</p>
 *
 * <p>The {@code LinuxContainerExecutor} will manage applications through an
 * appropriate {@link LinuxContainerRuntime} instance. This class uses a
 * {@link DelegatingLinuxContainerRuntime} instance, which will delegate calls
 * to either a {@link DefaultLinuxContainerRuntime} instance or a
 * {@link OCIContainerRuntime} instance, depending on the job's
 * configuration.</p>
 *
 * @see LinuxContainerRuntime
 * @see DelegatingLinuxContainerRuntime
 * @see DefaultLinuxContainerRuntime
 * @see DockerLinuxContainerRuntime
 * @see OCIContainerRuntime#isOCICompliantContainerRequested
 */
MonitoringTimerTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LocalDirsHandlerService.java)/**
   * Class which is used by the {@link Timer} class to periodically execute the
   * disks' health checker code.
   */
LocalDirsHandlerService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LocalDirsHandlerService.java)/**
 * The class which provides functionality of checking the health of the local
 * directories of a node. This specifically manages nodemanager-local-dirs and
 * nodemanager-log-dirs by periodically checking their health.
 */
NMLogAggregationStatusTracker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/logaggregation/tracker/NMLogAggregationStatusTracker.java)/**
 * {@link NMLogAggregationStatusTracker} is used to cache log aggregation
 * status for finished applications. It will also delete the old cached
 * log aggregation status periodically.
 *
 */
NMAuditLogger (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NMAuditLogger.java)/** 
 * Manages NodeManager audit logs.
 *
 * Audit log format is written as key=value pairs. Tab separated.
 */
NodeHealthCheckerService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeHealthCheckerService.java)/**
 * The class which provides functionality of checking the health of the node and
 * reporting back to the service for which the health checker has been asked to
 * report.
 */
AbstractNodeDescriptorsProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/nodelabels/AbstractNodeDescriptorsProvider.java)/**
 * Provides base implementation of NodeDescriptorsProvider with Timer and
 * expects subclass to provide TimerTask which can fetch node descriptors.
 */
ConfigurationNodeAttributesProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/nodelabels/ConfigurationNodeAttributesProvider.java)/**
 * Configuration based node attributes provider.
 */
ConfigurationNodeLabelsProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/nodelabels/ConfigurationNodeLabelsProvider.java)/**
 * Provides Node's Labels by constantly monitoring the configuration.
 */
NodeAttributesProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/nodelabels/NodeAttributesProvider.java)/**
 * Abstract class which will be responsible for fetching the node attributes.
 *
 */
NodeDescriptorsProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/nodelabels/NodeDescriptorsProvider.java)/**
 * Interface which will be responsible for fetching node descriptors,
 * a node descriptor could be a
 * {@link org.apache.hadoop.yarn.api.records.NodeLabel} or a
 * {@link org.apache.hadoop.yarn.api.records.NodeAttribute}.
 */
NodeDescriptorsScriptRunner (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/nodelabels/NodeDescriptorsScriptRunner.java)/**
 * A node descriptors script runner periodically runs a script,
 * parses the output to collect desired descriptors, and then
 * post these descriptors to the given {@link NodeDescriptorsProvider}.
 * @param <T> a certain type of descriptor.
 */
NodeLabelsProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/nodelabels/NodeLabelsProvider.java)/**
 * Abstract class which will be responsible for fetching the node labels.
 *
 */
ScriptBasedNodeAttributesProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/nodelabels/ScriptBasedNodeAttributesProvider.java)/**
 * Node attribute provider that periodically runs a script to collect
 * node attributes.
 */
ScriptBasedNodeLabelsProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/nodelabels/ScriptBasedNodeLabelsProvider.java)/**
 * The class which provides functionality of getting the labels of the node
 * using the configured node labels provider script. "NODE_PARTITION:" is the
 * pattern which will be used to search node label partition from the out put of
 * the NodeLabels provider script
 */
DefaultContainerStateListener (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java)/**
   * Default Container State transition listener.
   */
NodeManagerMXBean (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManagerMXBean.java)/**
 * This is the JMX management interface for NodeManager.
 * End users shouldn't be implementing these interfaces, and instead
 * access this information through the JMX APIs.
 */
NodeResourceMonitor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeResourceMonitor.java)/**
 * Interface for monitoring the resources of a node.
 */
MonitoringThread (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeResourceMonitorImpl.java)/**
   * Thread that monitors the resource utilization of this node.
   */
NodeResourceMonitorImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeResourceMonitorImpl.java)/**
 * Implementation of the node resource monitor. It periodically tracks the
 * resource utilization of the node and reports it to the NM.
 */
NMCentralizedNodeAttributesHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java)/**
   * In centralized configuration, NM need not send Node attributes or process
   * the response.
   */
NMCentralizedNodeLabelsHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java)/**
   * In centralized configuration, NM need not send Node labels or process the
   * response
   */
RecoveredAMRMProxyState (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/recovery/NMStateStoreService.java)/**
   * Recovered states for AMRMProxy.
   */
RecoveryIterator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/recovery/RecoveryIterator.java)/**
 * A wrapper for a Iterator to translate the raw RuntimeExceptions that
 * can be thrown into IOException.
 */
DistributedScheduler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/DistributedScheduler.java)/**
 * <p>The DistributedScheduler runs on the NodeManager and is modeled as an
 * <code>AMRMProxy</code> request interceptor. It is responsible for the
 * following:</p>
 * <ul>
 *   <li>Intercept <code>ApplicationMasterProtocol</code> calls and unwrap the
 *   response objects to extract instructions from the
 *   <code>ClusterMonitor</code> running on the ResourceManager to aid in making
 *   distributed scheduling decisions.</li>
 *   <li>Call the <code>OpportunisticContainerAllocator</code> to allocate
 *   containers for the outstanding OPPORTUNISTIC container requests.</li>
 * </ul>
 */
NMPolicyProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/authorize/NMPolicyProvider.java)/**
 * {@link PolicyProvider} for YARN NodeManager protocols.
 */
NMContainerTokenSecretManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java)/**
 * The NM maintains only two master-keys. The current key that RM knows and the
 * key from the previous rolling-interval.
 * 
 */
NMTimelineEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/NMTimelineEvent.java)/**
 * Event posted to NMTimelinePublisher which in turn publishes it to
 * timelineservice v2.
 */
ForwardingEventHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/NMTimelinePublisher.java)/**
   * EventHandler implementation which forward events to NMMetricsPublisher.
   * Making use of it, NMMetricsPublisher can avoid to have a public handle
   * method.
   */
NMTimelinePublisher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/NMTimelinePublisher.java)/**
 * Metrics publisher service that publishes data to the timeline service v.2. It
 * is used only if the timeline service v.2 is enabled and the system publishing
 * of events and metrics is enabled.
 */
CgroupsLCEResourcesHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/util/CgroupsLCEResourcesHandler.java)/**
 * Resource handler that lets you setup cgroups
 * to to handle cpu isolation. Please look at the ResourceHandlerModule
 * and CGroupsCpuResourceHandlerImpl classes which let you isolate multiple
 * resources using cgroups.
 * Deprecated - please look at ResourceHandlerModule and
 * CGroupsCpuResourceHandlerImpl
 */
NodeManagerHardwareUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/util/NodeManagerHardwareUtils.java)/**
 * Helper class to determine hardware related characteristics such as the
 * number of processors and the amount of memory on the node.
 */
ProcessIdFileReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/util/ProcessIdFileReader.java)/**
 * Helper functionality to read the pid from a file.
 */
ContainerLogsUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/ContainerLogsUtils.java)/**
 * Contains utilities for fetching a user's log file in a secure fashion.
 */
ContainerShellWebSocket (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/ContainerShellWebSocket.java)/**
 * Web socket for establishing interactive command shell connection through
 * Node Manage to container executor.
 */
ContainerShellWebSocketServlet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/ContainerShellWebSocketServlet.java)/**
 * Container shell web socket interface.
 */
AuxiliaryServiceInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/dao/AuxiliaryServiceInfo.java)/**
 * Information about a loaded auxiliary service.
 */
AuxiliaryServicesInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/dao/AuxiliaryServicesInfo.java)/**
 * A list of loaded auxiliary services.
 */
GpuDeviceInformation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/dao/gpu/GpuDeviceInformation.java)/**
 * All GPU Device Information in the system, fetched from nvidia-smi.
 */
GpuDeviceInformationParser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/dao/gpu/GpuDeviceInformationParser.java)/**
 * Parse XML and get GPU device information
 */
NMGpuResourceInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/dao/gpu/NMGpuResourceInfo.java)/**
 * Gpu device information return to client when
 * {@link org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices#getNMResourceInfo(String)}
 * is invoked.
 */
StrToFloatBeforeSpaceAdapter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/dao/gpu/PerGpuDeviceInformation.java)/**
   * Convert formats like "34 C", "75.6 %" to float.
   */
StrToMemAdapter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/dao/gpu/PerGpuDeviceInformation.java)/**
   * Convert formats like "725 MiB" to long.
   */
PerGpuDeviceInformation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/dao/gpu/PerGpuDeviceInformation.java)/**
 * Capture single GPU device information such as memory size, temperature,
 * utilization.
 */
PerGpuTemperature (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/dao/gpu/PerGpuTemperature.java)/**
 * Temperature of GPU
 */
PerGpuUtilizations (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/dao/gpu/PerGpuUtilizations.java)/**
 * GPU utilizations
 */
NMContainerLogsInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/dao/NMContainerLogsInfo.java)/**
 * NMContainerLogsInfo represents the meta data for container logs
 * which exist in NM local log directory.
 * This class extends {@link ContainerLogsInfo}.
 */
NMDeviceResourceInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/dao/NMDeviceResourceInfo.java)/**
 * Wrapper class of Device allocation for NMWebServices.
 * */
TerminalServlet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/webapp/TerminalServlet.java)/**
 * TerminalServlet host the static html and javascript for
 * connecting to a text terminal over websocket.
 */
Elevated (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/WindowsSecureContainerExecutor.java)/**
     * This class contains methods used by the WindowsSecureContainerExecutor
     * file system operations.
     */
WinutilsProcessStub (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/WindowsSecureContainerExecutor.java)/**
     * Wraps a process started by the winutils service helper.
     *
     */
Native (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/WindowsSecureContainerExecutor.java)/**
   * This class is a container for the JNI Win32 native methods used by WSCE.
   */
WindowsSecureWrapperScriptBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/WindowsSecureContainerExecutor.java)/**
   * A shell script wrapper builder for WSCE.  
   * Overwrites the default behavior to remove the creation of the PID file in 
   * the script wrapper. WSCE creates the pid file as part of launching the 
   * task in winutils.
   */
ElevatedRawLocalFilesystem (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/WindowsSecureContainerExecutor.java)/**
     * This overwrites certain RawLocalSystem operations to be performed by a 
     * privileged process.
     * 
     */
ElevatedFileSystem (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/WindowsSecureContainerExecutor.java)/**
   * This is a skeleton file system used to elevate certain operations.
   * WSCE has to create container dirs under local/userchache/$user but
   * this dir itself is owned by $user, with chmod 750. As ther NM has no
   * write access, it must delegate the write operations to the privileged
   * hadoopwintuilsvc.
   */
WindowsSecureContainerExecutor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/WindowsSecureContainerExecutor.java)/**
 * Windows secure container executor (WSCE).
 * This class offers a secure container executor on Windows, similar to the 
 * LinuxContainerExecutor. As the NM does not run on a high privileged context, 
 * this class delegates elevated operations to the helper hadoopwintuilsvc, 
 * implemented by the winutils.exe running as a service.
 * JNI and LRPC is used to communicate with the privileged service.
 */
Function (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/BaseAMRMProxyTest.java)/**
   * The Function interface is used for passing method pointers that can be
   * invoked asynchronously at a later point.
   */
BaseAMRMProxyTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/BaseAMRMProxyTest.java)/**
 * Base class for all the AMRMProxyService test cases. It provides utility
 * methods that can be used by the concrete test case classes
 *
 */
PassThroughRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/PassThroughRequestInterceptor.java)/**
 * Mock intercepter that does not do anything other than forwarding it to the
 * next intercepter in the chain
 *
 */
TestableUnmanagedAMPoolManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/TestableFederationInterceptor.java)/**
   * Extends the UnmanagedAMPoolManager and overrides methods to provide a
   * testable implementation of UnmanagedAMPoolManager.
   */
TestableUnmanagedApplicationManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/TestableFederationInterceptor.java)/**
   * Extends the UnmanagedApplicationManager and overrides methods to provide a
   * testable implementation.
   */
TestableAMRequestHandlerThread (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/TestableFederationInterceptor.java)/**
   * Wrap the handler thread so it calls from the same user.
   */
TestableFederationInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/TestableFederationInterceptor.java)/**
 * Extends the FederationInterceptor and overrides methods to provide a testable
 * implementation of FederationInterceptor.
 */
MockRequestInterceptorAcrossRestart (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/TestAMRMProxyService.java)/**
   * A mock intercepter implementation that uses the same mockRM instance across
   * restart.
   */
BadRequestInterceptorAcrossRestart (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/TestAMRMProxyService.java)/**
   * A mock intercepter implementation that throws when recovering.
   */
TestAMRMProxyTokenSecretManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/TestAMRMProxyTokenSecretManager.java)/**
 * Unit test for AMRMProxyTokenSecretManager.
 */
ConcurrentRegisterAMCallable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/TestFederationInterceptor.java)/**
   * A callable that calls registerAM to RM with blocking.
   */
TestFederationInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/TestFederationInterceptor.java)/**
 * Extends the TestAMRMProxyService and overrides methods in order to use the
 * AMRMProxyService's pipeline test cases for testing the FederationInterceptor
 * class. The tests for AMRMProxyService has been written cleverly so that it
 * can be reused to validate different request intercepter chains.
 */
TestNMProtoUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/api/impl/pb/TestNMProtoUtils.java)/**
 * Test conversion to {@link DeletionTask}.
 */
TestSlidingWindowRetryPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/TestSlidingWindowRetryPolicy.java)/**
 * Tests for {@link SlidingWindowRetryPolicy}.
 */
DockerContainerDeletionMatcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/deletion/task/DockerContainerDeletionMatcher.java)/**
 * ArgumentMatcher to check the arguments of the
 * {@link DockerContainerDeletionTask}.
 */
FileDeletionMatcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/deletion/task/FileDeletionMatcher.java)/**
 * ArgumentMatcher to check the arguments of the {@link FileDeletionTask}.
 */
TestDockerContainerDeletionTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/deletion/task/TestDockerContainerDeletionTask.java)/**
 * Test the attributes of the {@link DockerContainerDeletionTask} class.
 */
TestFileDeletionTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/deletion/task/TestFileDeletionTask.java)/**
 * Test the attributes of the {@link FileDeletionTask} class.
 */
TestContainerCleanup (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerCleanup.java)/**
 * Tests for {@link ContainerCleanup}.
 */
TestContainerRelaunch (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerRelaunch.java)/** Unit tests for relaunching containers. */
TestContainersLauncher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainersLauncher.java)/**
 * Tests to verify all the Container's Launcher Events in
 * {@link ContainersLauncher} are handled as expected.
 */
MockPrivilegedOperationCaptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/MockPrivilegedOperationCaptor.java)/**
 * Captures operations from mock {@link PrivilegedOperation} instances.
 */
DummyRunnableWithContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/DummyRunnableWithContext.java)/**
 * Runnable that does not do anything.
 */
TestGpuResourceAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/gpu/TestGpuResourceAllocator.java)/**
 * Unit tests for GpuResourceAllocator.
 */
TestNumaResourceAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/numa/TestNumaResourceAllocator.java)/**
 * Test class for NumaResourceAllocator.
 */
TestNumaResourceHandlerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/numa/TestNumaResourceHandlerImpl.java)/**
 * Test class for NumaResourceHandlerImpl.
 *
 */
TestCGroupElasticMemoryController (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/TestCGroupElasticMemoryController.java)/**
 * Test for elastic non-strict memory controller based on cgroups.
 */
TestCGroupsBlkioResourceHandlerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/TestCGroupsBlkioResourceHandlerImpl.java)/**
 * Tests for the cgroups disk handler implementation.
 */
MockSecurityManagerDenyWrite (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/TestCGroupsHandlerImpl.java)/**
   * Security manager simulating access denied.
   */
TestCGroupsHandlerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/TestCGroupsHandlerImpl.java)/**
 * Tests for the CGroups handler implementation.
 */
TestCGroupsMemoryResourceHandlerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/TestCGroupsMemoryResourceHandlerImpl.java)/**
 * Unit test for CGroupsMemoryResourceHandlerImpl.
 */
TestCGroupsResourceCalculator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/TestCGroupsResourceCalculator.java)/**
 * Unit test for CGroupsResourceCalculator.
 */
TestCompareResourceCalculators (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/TestCompareResourceCalculators.java)/**
 * Functional test for CGroupsResourceCalculator to compare two resource
 * calculators. It is OS dependent.
 * Ignored in automated tests due to flakiness by design.
 */
TestDefaultOOMHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/TestDefaultOOMHandler.java)/**
 * Test default out of memory handler.
 */
TestNetworkPacketTaggingHandlerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/TestNetworkPacketTaggingHandlerImpl.java)/**
 * Test NetworkPacketTagging Handler.
 *
 */
TestDockerClient (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerClient.java)/** Unit tests for DockerClient. */
TestDockerCommandExecutor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerCommandExecutor.java)/**
 * Test common docker commands.
 */
TestDockerImagesCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerImagesCommand.java)/**
 * Tests the docker images command and its command
 * line arguments.
 */
TestDockerInspectCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerInspectCommand.java)/**
 * Tests the docker inspect command and its command
 * line arguments.
 */
TestDockerKillCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerKillCommand.java)/**
 * Tests the docker kill command and its command line arguments.
 */
TestDockerLoadCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerLoadCommand.java)/**
 * Tests the docker load command and its command
 * line arguments.
 */
TestDockerPullCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerPullCommand.java)/**
 * Tests the docker pull command and its command
 * line arguments.
 */
TestDockerRmCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerRmCommand.java)/**
 * Tests the docker rm command and any command
 * line arguments.
 */
TestDockerStartCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerStartCommand.java)/**
 * Tests the docker start command and any command line arguments.
 */
TestDockerStopCommand (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerStopCommand.java)/**
 * Tests the docker stop command and its command
 * line arguments.
 */
TestDelegatingLinuxContainerRuntime (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/TestDelegatingLinuxContainerRuntime.java)/**
 * Test container runtime delegation.
 */
TestJavaSandboxLinuxContainerRuntime (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/TestJavaSandboxLinuxContainerRuntime.java)/**
 * Test policy file generation and policy enforcement for the
 * {@link JavaSandboxLinuxContainerRuntime}.
 */
FakeFSDataInputStream (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/FakeFSDataInputStream.java)/** mock streams in unit tests */
TestLocalCacheCleanup (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestLocalCacheCleanup.java)/**
 * This class tests the clean up of local caches the node manager uses for the
 * purpose of resource localization.
 */
TestResourceSet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceSet.java)/**
 * Tests of {@link ResourceSet}.
 */
TestAppLogAggregatorImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestAppLogAggregatorImpl.java)/**
 * Unit tests of AppLogAggregatorImpl class.
 */
MockCPUResourceCalculatorProcessTree (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/monitor/MockCPUResourceCalculatorProcessTree.java)/**
 * Mock class to obtain resource usage (CPU).
 */
TestNECVEPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/com/nec/TestNECVEPlugin.java)/**
 * Unit tests for NECVEPlugin class.
 *
 */
TestVEDeviceDiscoverer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/com/nec/TestVEDeviceDiscoverer.java)/**
 * Unit tests for VEDeviceDiscoverer class.
 *
 */
DataRecord (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/com/nvidia/TestNvidiaGPUPluginForRuntimeV2.java)/**
     * One line in the report.
     * */
ActualPerformanceReport (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/com/nvidia/TestNvidiaGPUPluginForRuntimeV2.java)/**
   * Representation of the performance data report.
   * */
TestNvidiaGPUPluginForRuntimeV2 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/com/nvidia/TestNvidiaGPUPluginForRuntimeV2.java)/**
 * Test case for NvidiaGPUPluginForRuntimeV2 device plugin.
 * */
FakeTestDevicePlugin1 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/deviceframework/FakeTestDevicePlugin1.java)/**
 * Used only for testing.
 * A fake normal vendor plugin
 * */
FakeTestDevicePlugin2 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/deviceframework/FakeTestDevicePlugin2.java)/**
 * Only used for testing.
 * This isn't a implementation of DevicePlugin
 * */
FakeTestDevicePlugin3 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/deviceframework/FakeTestDevicePlugin3.java)/**
 * Only used for testing.
 * This plugin register a same name with FakeTestDevicePlugin1
 * */
FakeTestDevicePlugin4 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/deviceframework/FakeTestDevicePlugin4.java)/**
 * Implement the interface but missed getRegisterRequestInfo method.
 * This is equivalent to implements a old version of DevicePlugin
 */
TestDeviceMappingManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/deviceframework/TestDeviceMappingManager.java)/**
 * Tests for DeviceMappingManager.
 * Note that we test it under multi-threaded situation
 * */
TestDevicePluginAdapter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/deviceframework/TestDevicePluginAdapter.java)/**
 * Unit tests for DevicePluginAdapter.
 * About interaction with vendor plugin
 * */
TestAoclOutputParser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/TestAoclOutputParser.java)/**
 * Tests for AoclDiagnosticOutputParser.
 */
TestNvidiaDockerV2CommandPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/TestNvidiaDockerV2CommandPlugin.java)/**
 * test for NvidiaDockerV2CommandPlugin.
 */
TestAllocationBasedResourceUtilizationTracker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/scheduler/TestAllocationBasedResourceUtilizationTracker.java)/**
 * Tests for the {@link AllocationBasedResourceUtilizationTracker} class.
 */
TestContainerSchedulerBehaviorCompatibility (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/scheduler/TestContainerSchedulerBehaviorCompatibility.java)/**
 * Make sure ContainerScheduler related changes are compatible
 * with old behavior.
 */
TestContainerSchedulerQueuing (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/scheduler/TestContainerSchedulerQueuing.java)/**
 * Tests to verify that the {@link ContainerScheduler} is able to queue and
 * make room for containers.
 */
TestContainerSchedulerRecovery (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/scheduler/TestContainerSchedulerRecovery.java)/**
 * Tests to verify that the {@link ContainerScheduler} is able to
 * recover active containers based on RecoveredContainerStatus and
 * ExecutionType.
 */
TestAuxServices (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestAuxServices.java)/**
 * Test for auxiliary services. Parameter 0 tests the Configuration-based aux
 * services and parameter 1 tests manifest-based aux services.
 */
TestContainerReapContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/executor/TestContainerReapContext.java)/**
 * Test the attributes of the {@link ContainerReapContext}.
 */
TestNMLogAggregationStatusTracker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/logaggregation/tracker/TestNMLogAggregationStatusTracker.java)/**
 * Function test for {@link NMLogAggregationStatusTracker}.
 *
 */
MockNodeStatusUpdater (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/MockNodeStatusUpdater.java)/**
 * This class allows a node manager to run without without communicating with a
 * real RM.
 */
TestConfigurationNodeAttributesProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/nodelabels/TestConfigurationNodeAttributesProvider.java)/**
 * Test class for node configuration node attributes provider.
 */
TestScriptBasedNodeAttributesProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/nodelabels/TestScriptBasedNodeAttributesProvider.java)/**
 * Test cases for script based node attributes provider.
 */
TestDistributedScheduler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/TestDistributedScheduler.java)/**
 * Test cases for {@link DistributedScheduler}.
 */
TestLinuxContainerExecutor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutor.java)/**
 * This is intended to test the LinuxContainerExecutor code, but because of some
 * security restrictions this can only be done with some special setup first. <br>
 * <ol>
 * <li>Compile the code with container-executor.conf.dir set to the location you
 * want for testing. <br>
 * 
 * <pre>
 * <code>
 * > mvn clean install -Pnative -Dcontainer-executor.conf.dir=/etc/hadoop
 *                          -DskipTests
 * </code>
 * </pre>
 * 
 * <li>Set up <code>${container-executor.conf.dir}/container-executor.cfg</code>
 * container-executor.cfg needs to be owned by root and have in it the proper
 * config values. <br>
 * 
 * <pre>
 * <code>
 * > cat /etc/hadoop/container-executor.cfg
 * yarn.nodemanager.linux-container-executor.group=mapred
 * #depending on the user id of the application.submitter option
 * min.user.id=1
 * > sudo chown root:root /etc/hadoop/container-executor.cfg
 * > sudo chmod 444 /etc/hadoop/container-executor.cfg
 * </code>
 * </pre>
 * 
 * <li>Move the binary and set proper permissions on it. It needs to be owned by
 * root, the group needs to be the group configured in container-executor.cfg,
 * and it needs the setuid bit set. (The build will also overwrite it so you
 * need to move it to a place that you can support it. <br>
 * 
 * <pre>
 * <code>
 * > cp ./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/c/container-executor/container-executor /tmp/
 * > sudo chown root:mapred /tmp/container-executor
 * > sudo chmod 4050 /tmp/container-executor
 * </code>
 * </pre>
 * 
 * <li>Run the tests with the execution enabled (The user you run the tests as
 * needs to be part of the group from the config. <br>
 * 
 * <pre>
 * <code>
 * mvn test -Dtest=TestLinuxContainerExecutor -Dapplication.submitter=nobody -Dcontainer-executor.path=/tmp/container-executor
 * </code>
 * </pre>
 *
 * <li>The test suite also contains tests to test mounting of CGroups. By
 * default, these tests are not run. To run them, add -Dcgroups.mount=<mount-point>
 * Please note that the test does not unmount the CGroups at the end of the test,
 * since that requires root permissions. <br>
 *
 * <li>The tests that are run are sensitive to directory permissions. All parent
 * directories must be searchable by the user that the tasks are run as. If you
 * wish to run the tests in a different directory, please set it using
 * -Dworkspace.dir
 * 
 * </ol>
 */
TestNetworkTagMappingJsonManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNetworkTagMappingJsonManager.java)/**
 * Test NetworkTagMapping Json Manager.
 *
 */
MyTestRPCServer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNMAuditLogger.java)/**
   * A special extension of {@link TestImpl} RPC server with 
   * {@link TestImpl#ping()} testing the audit logs.
   */
TestNMAuditLogger (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNMAuditLogger.java)/**
 * Tests {@link NMAuditLogger}.
 */
TestNodeManagerMXBean (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManagerMXBean.java)/**
 * Class for testing {@link NodeManagerMXBean} implementation.
 */
DummyNodeAttributesProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdaterForAttributes.java)/**
   * A dummy NodeAttributesProvider class for tests.
   */
TestNodeStatusUpdaterForAttributes (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdaterForAttributes.java)/**
 * Test NodeStatusUpdater for node attributes.
 */
DummyNodeLabelsProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdaterForLabels.java)/**
   * A dummy NodeLabelsProvider class for tests.
   */
TestNodeManagerHardwareUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/util/TestNodeManagerHardwareUtils.java)/**
 * Test the various functions provided by the NodeManagerHardwareUtils class.
 */
ContainerShellClientSocketTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/ContainerShellClientSocketTest.java)/**
 *  Container shell client socket interface.
 */
TestNMContainerWebSocket (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/TestNMContainerWebSocket.java)/**
 * Test class for Node Manager Container Web Socket.
 */
TestNMWebFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/TestNMWebFilter.java)/**
 * Basic sanity Tests for NMWebFilter.
 *
 */
TestNMWebServices (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/TestNMWebServices.java)/**
 * Test the nodemanager node info web services api's
 */
TestNMWebServicesAuxServices (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/TestNMWebServicesAuxServices.java)/**
 * Basic sanity Tests for AuxServices.
 *
 */
TestNMWebTerminal (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/TestNMWebTerminal.java)/**
 * Unit test for hosting web terminal servlet in node manager.
 */
ActiveStandbyElectorBasedElectorService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ActiveStandbyElectorBasedElectorService.java)/**
 * Leader election implementation that uses {@link ActiveStandbyElector}.
 */
ForwardingEventHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ahs/RMApplicationHistoryWriter.java)/**
   * EventHandler implementation which forward events to HistoryWriter Making
   * use of it, HistoryWriter can avoid to have a public handle method
   */
RMApplicationHistoryWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ahs/RMApplicationHistoryWriter.java)/**
 * <p>
 * {@link ResourceManager} uses this class to write the information of
 * {@link RMApp}, {@link RMAppAttempt} and {@link RMContainer}. These APIs are
 * non-blocking, and just schedule a writing history event. An self-contained
 * dispatcher vector will handle the event in separate threads, and extract the
 * required fields that are going to be persisted. Then, the extracted
 * information will be persisted via the implementation of
 * {@link ApplicationHistoryStore}.
 * </p>
 */
AMLauncher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java)/**
 * The launch of the AM itself.
 */
AMSProcessingChain (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AMSProcessingChain.java)/**
 * This maintains a chain of {@link ApplicationMasterServiceProcessor}s.
 */
BlacklistManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/blacklist/BlacklistManager.java)/**
 * Tracks blacklists based on failures reported on nodes.
 */
DisabledBlacklistManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/blacklist/DisabledBlacklistManager.java)/**
 * A {@link BlacklistManager} that returns no blacklists.
 */
SimpleBlacklistManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/blacklist/SimpleBlacklistManager.java)/**
 * Maintains a list of failed nodes and returns that as long as number of
 * blacklisted nodes is below a threshold percentage of total nodes. If more
 * than threshold number of nodes are marked as failure they all are returned
 * as removal from blacklist so previous additions are reversed.
 */
ClientRMService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java)/**
 * The client interface to the Resource Manager. This module handles all the rpc
 * interfaces to the resource manager from the client.
 */
ClusterMonitor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClusterMonitor.java)/**
 * Implementations of this class are notified of changes to the cluster's state,
 * such as node addition, removal and updates.
 */
CuratorBasedElectorService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/CuratorBasedElectorService.java)/**
 * Leader election implementation that uses Curator.
 */
PollTimerTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/DecommissioningNodesWatcher.java)/**
   * PollTimerTask periodically:
   *   1. log status of all DECOMMISSIONING nodes;
   *   2. identify and taken care of stale DECOMMISSIONING nodes
   *      (for example, node already terminated).
   */
DecommissioningNodesWatcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/DecommissioningNodesWatcher.java)/**
 * DecommissioningNodesWatcher is used by ResourceTrackerService to track
 * DECOMMISSIONING nodes to decide when, after all running containers on
 * the node have completed, will be transitioned into DECOMMISSIONED state
 * (NodeManager will be told to shutdown).
 * Under MR application, a node, after completes all its containers,
 * may still serve it map output data during the duration of the application
 * for reducers. A fully graceful mechanism would keep such DECOMMISSIONING
 * nodes until all involved applications complete. It could be however
 * undesirable under long-running applications scenario where a bunch
 * of "idle" nodes would stay around for long period of time.
 *
 * DecommissioningNodesWatcher balance such concern with a timeout policy ---
 * a DECOMMISSIONING node will be DECOMMISSIONED no later than
 * DECOMMISSIONING_TIMEOUT regardless of running containers or applications.
 *
 * DecommissioningNodesWatcher basically is no cost when no node is
 * DECOMMISSIONING.
 */
DefaultAMSProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/DefaultAMSProcessor.java)/**
 * This is the default Application Master Service processor. It has be the
 * last processor in the @{@link AMSProcessingChain}.
 */
EmbeddedElector (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/EmbeddedElector.java)/**
 * Interface that all embedded leader electors must implement.
 */
FederationStateStoreHeartbeat (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/federation/FederationStateStoreHeartbeat.java)/**
 * Periodic heart beat from a <code>ResourceManager</code> participating in
 * federation to indicate liveliness. The heart beat publishes the current
 * capabilities as represented by {@link ClusterMetricsInfo} of the sub cluster.
 *
 */
FederationStateStoreService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/federation/FederationStateStoreService.java)/**
 * Implements {@link FederationStateStore} and provides a service for
 * participating in the federation membership.
 */
IsResourceManagerActiveServlet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/IsResourceManagerActiveServlet.java)/**
 * Used by Load Balancers to find the active ResourceManager.
 */
MultiThreadedDispatcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/AbstractSystemMetricsPublisher.java)/**
   * Dispatches ATS related events using multiple threads.
   */
TimelinePublishEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/AbstractSystemMetricsPublisher.java)/**
   * TimelinePublishEvent's hash code should be based on application's id this
   * will ensure all the events related to a particular app goes to particular
   * thread of MultiThreaded dispatcher.
   */
AbstractSystemMetricsPublisher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/AbstractSystemMetricsPublisher.java)/**
 * Abstract implementation of SystemMetricsPublisher which is then extended by
 * metrics publisher implementations depending on timeline service version.
 */
CombinedSystemMetricsPublisher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/CombinedSystemMetricsPublisher.java)/**
 * A metrics publisher that can publish for a collection of publishers.
 */
NoOpSystemMetricPublisher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/NoOpSystemMetricPublisher.java)/**
 * This class does nothing when any of the methods are invoked on
 * SystemMetricsPublisher.
 */
SystemMetricsPublisher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java)/**
 * Interface used to publish app/container events to timelineservice.
 */
TimelineServiceV1Publisher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TimelineServiceV1Publisher.java)/**
 * This class is responsible for posting application, appattempt &amp; Container
 * lifecycle related events to timeline service v1.
 */
TimelineServiceV2Publisher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TimelineServiceV2Publisher.java)/**
 * This class is responsible for posting application, appattempt &amp; Container
 * lifecycle related events to timeline service v2.
 */
AbstractPreemptableResourceCalculator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/AbstractPreemptableResourceCalculator.java)/**
 * Calculate how much resources need to be preempted for each queue,
 * will be used by {@link PreemptionCandidatesSelector}.
 */
AbstractPreemptionEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/AbstractPreemptionEntity.java)/**
 * Abstract temporary data-structure for tracking resource availability,pending
 * resource need, current utilization for app/queue.
 */
CapacitySchedulerPreemptionContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/CapacitySchedulerPreemptionContext.java)/**
 * This interface provides context for the calculation of ideal allocation
 * and preemption for the {@code CapacityScheduler}.
 */
FifoIntraQueuePreemptionPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/FifoIntraQueuePreemptionPlugin.java)/**
 * FifoIntraQueuePreemptionPlugin will handle intra-queue preemption for
 * priority and user-limit.
 */
IntraQueueCandidatesSelector (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/IntraQueueCandidatesSelector.java)/**
 * Identifies over utilized resources within a queue and tries to normalize
 * them to resolve resource allocation anomalies w.r.t priority and user-limit.
 */
PreemptableResourceCalculator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/PreemptableResourceCalculator.java)/**
 * Calculate how much resources need to be preempted for each queue,
 * will be used by {@link PreemptionCandidatesSelector}
 */
ProportionalCapacityPreemptionPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java)/**
 * This class implement a {@link SchedulingEditPolicy} that is designed to be
 * paired with the {@code CapacityScheduler}. At every invocation of {@code
 * editSchedule()} it computes the ideal amount of resources assigned to each
 * queue (for each queue in the hierarchy), and determines whether preemption
 * is needed. Overcapacity is distributed among queues in a weighted fair manner,
 * where the weight is the amount of guaranteed capacity for the queue.
 * Based on this ideal assignment it determines whether preemption is required
 * and select a set of containers from each application that would be killed if
 * the corresponding amount of resources is not freed up by the application.
 *
 * If not in {@code observeOnly} mode, it triggers preemption requests via a
 * {@link ContainerPreemptEvent} that the {@code ResourceManager} will ensure
 * to deliver to the application (or to execute).
 *
 * If the deficit of resources is persistent over a long enough period of time
 * this policy will trigger forced termination of containers (again by generating
 * {@link ContainerPreemptEvent}).
 */
NodeForPreemption (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ReservedContainerCandidatesSelector.java)/**
   * A temporary data structure to remember what to preempt on a node
   */
TempAppPerPartition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TempAppPerPartition.java)/**
 * Temporary data-structure tracking resource availability, pending resource
 * need, current utilization for an application.
 */
TempQueuePerPartition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TempQueuePerPartition.java)/**
 * Temporary data-structure tracking resource availability, pending resource
 * need, current utilization. This is per-queue-per-partition data structure
 */
TempSchedulerNode (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TempSchedulerNode.java)/**
 * This class will save necessary information which copied from
 * FiCaSchedulerNode. This is added majorly for performance consideration, this
 * can be cached to avoid hitting scheduler again and again. In addition,
 * we can add some preemption-required fields to the class.
 */
TempUserPerPartition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TempUserPerPartition.java)/**
 * Temporary data-structure tracking resource availability, pending resource
 * need, current utilization for an application.
 */
InvariantsChecker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/invariants/InvariantsChecker.java)/**
 * Abstract invariant checker, that setup common context for invariants
 * checkers.
 */
InvariantViolationException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/invariants/InvariantViolationException.java)/**
 * This exception represents the violation of an internal invariant.
 */
MetricsInvariantChecker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/invariants/MetricsInvariantChecker.java)/**
 * This policy checks at every invocation that a given set of invariants
 * (specified in a file) are respected over QueueMetrics and JvmMetrics. The
 * file may contain arbitrary (Javascrip) boolean expression over the metrics
 * variables.
 *
 * The right set of invariants depends on the deployment environment, a large
 * number of complex invariant can make this check expensive.
 *
 * The MetricsInvariantChecker can be configured to throw a RuntimeException or
 * simlpy warn in the logs if an invariant is not respected.
 */
ReservationInvariantsChecker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/invariants/ReservationInvariantsChecker.java)/**
 * Invariant checker that checks certain reservation invariants are respected.
 */
SchedulingMonitorManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitorManager.java)/**
 * Manages scheduling monitors.
 */
FileSystemNodeAttributeStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/FileSystemNodeAttributeStore.java)/**
 * File system node attribute implementation.
 */
Host (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/NodeAttributesManagerImpl.java)/**
   * A <code>Host</code> can have multiple <code>Node</code>s.
   */
NodeAttributesManagerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/NodeAttributesManagerImpl.java)/**
 * Manager holding the attributes to Labels.
 */
NodeAttributesStoreEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/NodeAttributesStoreEvent.java)/**
 * Event capturing details to store the Node Attributes in the backend store.
 */
NodeLabelsUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/NodeLabelsUtils.java)/**
 * Node labels utilities.
 */
RMDelegatedNodeLabelsUpdater (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/RMDelegatedNodeLabelsUpdater.java)/**
 * Update nodes labels map for ResourceManager periodically. It collects
 * nodes labels from {@link RMNodeLabelsMappingProvider} and updates the
 * nodes {@literal ->} labels map via {@link RMNodeLabelsManager}.
 * This service is enabled when configuration
 * "yarn.node-labels.configuration-type" is set to "delegated-centralized".
 */
RMNodeLabelsMappingProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/RMNodeLabelsMappingProvider.java)/**
 * Interface which is responsible for providing
 * the node {@literal ->} labels map.
 */
UnknownNode (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java)/**
   * A Node instance needed upon startup for populating inactive nodes Map.
   * It only knows its hostname/ip.
   */
OpportunisticContainerAllocatorAMService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/OpportunisticContainerAllocatorAMService.java)/**
 * The OpportunisticContainerAllocatorAMService is started instead of the
 * ApplicationMasterService if opportunistic scheduling is enabled for the YARN
 * cluster (either centralized or distributed opportunistic scheduling).
 *
 * It extends the functionality of the ApplicationMasterService by servicing
 * clients (AMs and AMRMProxy request interceptors) that understand the
 * DistributedSchedulingProtocol.
 */
ApplicationPlacementContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/placement/ApplicationPlacementContext.java)/**
 * Each placement rule when it successfully places an application onto a queue
 * returns a PlacementRuleContext which encapsulates the queue the
 * application was mapped to and any parent queue for the queue (if configured)
 */
DefaultPlacementRule (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/placement/DefaultPlacementRule.java)/**
 * Places apps in the specified default queue. If no default queue is
 * specified the app is placed in root.default queue.
 */
FairQueuePlacementUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/placement/FairQueuePlacementUtils.java)/**
 * Utility methods used by Fair scheduler placement rules.
 * {@link
 * org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler}
 */
FSPlacementRule (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/placement/FSPlacementRule.java)/**
 * Abstract base for all {@link FairScheduler} Placement Rules.
 */
PlacementFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/placement/PlacementFactory.java)/**
 * Factory class for creating instances of {@link PlacementRule}.
 */
PlacementRule (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/placement/PlacementRule.java)/**
 * Abstract base for all Placement Rules.
 */
PrimaryGroupPlacementRule (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/placement/PrimaryGroupPlacementRule.java)/**
 * Places apps in queues by the primary group of the submitter.
 */
QueuePlacementRuleUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/placement/QueuePlacementRuleUtils.java)/**
 * Utility class for Capacity Scheduler queue PlacementRules.
 */
RejectPlacementRule (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/placement/RejectPlacementRule.java)/**
 * Rejects all placements.
 */
SecondaryGroupExistingPlacementRule (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/placement/SecondaryGroupExistingPlacementRule.java)/**
 * Places apps in queues by the secondary group of the submitter, if the
 * submitter is a member of more than one group.
 * The first "matching" queue based on the group list is returned. The match
 * takes into account the parent rule and create flag,
 */
SpecifiedPlacementRule (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/placement/SpecifiedPlacementRule.java)/**
 * Places apps in queues by requested queue of the submitter.
 */
UserPlacementRule (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/placement/UserPlacementRule.java)/**
 * Places apps in queues by username of the submitter.
 */
ContextProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/preprocessor/ContextProcessor.java)/**
 * This is the interface providing functionality to process
 * application submission context.
 */
NodeLabelProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/preprocessor/NodeLabelProcessor.java)/**
 * Processor will add the node label to application submission context.
 */
QueueProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/preprocessor/QueueProcessor.java)/**
 * Processor will add queue to application submission context.
 */
SubmissionContextPreProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/preprocessor/SubmissionContextPreProcessor.java)/**
 * Pre process the ApplicationSubmissionContext with server side info.
 */
TagAddProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/preprocessor/TagAddProcessor.java)/**
 * This processor will add the tag to application submission context.
 */
LeveldbRMStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/LeveldbRMStateStore.java)/**
 * Changes from 1.0 to 1.1, Addition of ReservationSystem state.
 */
AMRMTokenSecretManagerState (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/records/AMRMTokenSecretManagerState.java)/**
 * Contains all the state data that needs to be stored persistently 
 * for {@link AMRMTokenSecretManager}
 */
ApplicationStateData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/records/ApplicationStateData.java)/**
 * Contains all the state data that needs to be stored persistently 
 * for an Application
 */
Epoch (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/records/Epoch.java)/**
 * The epoch information of RM for work-preserving restart.
 * Epoch is incremented each time RM restart. It's used for assuring
 * uniqueness of <code>ContainerId</code>.
 */
RMState (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java)/**
   * State of the ResourceManager
   */
ForwardingEventHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java)/**
   * EventHandler implementation which forward events to the FSRMStateStore
   * This hides the EventHandle methods of the store from its public interface 
   */
RMStateStoreProxyCAEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStoreProxyCAEvent.java)/**
 * A event used to store ProxyCA information.
 */
RMStateStoreRemoveAppAttemptEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStoreRemoveAppAttemptEvent.java)/**
 * A event used to remove an attempt.
 */
RMStateStoreStoreReservationEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStoreStoreReservationEvent.java)/**
 * Event representing maintaining ReservationSystem state.
 */
RMStateStoreUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStoreUtils.java)/**
 * Utility methods for {@link RMStateStore} and subclasses.
 */
RMStateVersionIncompatibleException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateVersionIncompatibleException.java)/**
 * This exception is thrown by ResourceManager if it's loading an incompatible
 * version of storage on recovery.
 */
StoreLimitException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/StoreLimitException.java)/**
 * This exception is thrown when Application Data size exceeds limit RM state
 * store.
 *
 */
ZnodeSplitInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java)/**
   * Encapsulates znode path and corresponding split index for hierarchical
   * znode layouts.
   */
VerifyActiveStatusThread (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java)/**
   * Helper class that periodically attempts creating a znode to ensure that
   * this RM continues to be the Active.
   */
ZKRMStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java)/**
 * {@link RMStateStore} implementation backed by ZooKeeper.
 *
 * The znode structure is as follows:
 * ROOT_DIR_PATH
 * |--- VERSION_INFO
 * |--- EPOCH_NODE
 * |--- RM_ZK_FENCING_LOCK
 * |--- RM_APP_ROOT
 * |     |----- HIERARCHIES
 * |     |        |----- 1
 * |     |        |      |----- (#ApplicationId barring last character)
 * |     |        |      |       |----- (#Last character of ApplicationId)
 * |     |        |      |       |       |----- (#ApplicationAttemptIds)
 * |     |        |      ....
 * |     |        |
 * |     |        |----- 2
 * |     |        |      |----- (#ApplicationId barring last 2 characters)
 * |     |        |      |       |----- (#Last 2 characters of ApplicationId)
 * |     |        |      |       |       |----- (#ApplicationAttemptIds)
 * |     |        |      ....
 * |     |        |
 * |     |        |----- 3
 * |     |        |      |----- (#ApplicationId barring last 3 characters)
 * |     |        |      |       |----- (#Last 3 characters of ApplicationId)
 * |     |        |      |       |       |----- (#ApplicationAttemptIds)
 * |     |        |      ....
 * |     |        |
 * |     |        |----- 4
 * |     |        |      |----- (#ApplicationId barring last 4 characters)
 * |     |        |      |       |----- (#Last 4 characters of ApplicationId)
 * |     |        |      |       |       |----- (#ApplicationAttemptIds)
 * |     |        |      ....
 * |     |        |
 * |     |----- (#ApplicationId1)
 * |     |        |----- (#ApplicationAttemptIds)
 * |     |
 * |     |----- (#ApplicationId2)
 * |     |       |----- (#ApplicationAttemptIds)
 * |     ....
 * |
 * |--- RM_DT_SECRET_MANAGER_ROOT
 *        |----- RM_DT_SEQUENTIAL_NUMBER_ZNODE_NAME
 *        |----- RM_DELEGATION_TOKENS_ROOT_ZNODE_NAME
 *        |       |----- 1
 *        |       |      |----- (#TokenId barring last character)
 *        |       |      |       |----- (#Last character of TokenId)
 *        |       |      ....
 *        |       |----- 2
 *        |       |      |----- (#TokenId barring last 2 characters)
 *        |       |      |       |----- (#Last 2 characters of TokenId)
 *        |       |      ....
 *        |       |----- 3
 *        |       |      |----- (#TokenId barring last 3 characters)
 *        |       |      |       |----- (#Last 3 characters of TokenId)
 *        |       |      ....
 *        |       |----- 4
 *        |       |      |----- (#TokenId barring last 4 characters)
 *        |       |      |       |----- (#Last 4 characters of TokenId)
 *        |       |      ....
 *        |       |----- Token_1
 *        |       |----- Token_2
 *        |       ....
 *        |
 *        |----- RM_DT_MASTER_KEYS_ROOT_ZNODE_NAME
 *        |      |----- Key_1
 *        |      |----- Key_2
 *                ....
 * |--- AMRMTOKEN_SECRET_MANAGER_ROOT
 *        |----- currentMasterKey
 *        |----- nextMasterKey
 *
 * |-- RESERVATION_SYSTEM_ROOT
 *        |------PLAN_1
 *        |      |------ RESERVATION_1
 *        |      |------ RESERVATION_2
 *        |      ....
 *        |------PLAN_2
 *        ....
 * |-- PROXY_CA_ROOT
 *        |----- caCert
 *        |----- caPrivateKey
 *
 * Note: Changes from 1.1 to 1.2 - AMRMTokenSecretManager state has been saved
 * separately. The currentMasterkey and nextMasterkey have been stored.
 * Also, AMRMToken has been removed from ApplicationAttemptState.
 *
 * Changes from 1.2 to 1.3, Addition of ReservationSystem state.
 *
 * Changes from 1.3 to 1.4 - Change the structure of application znode by
 * splitting it in 2 parts, depending on a configurable split index. This limits
 * the number of application znodes returned in a single call while loading
 * app state.
 *
 * Changes from 1.4 to 1.5 - Change the structure of delegation token znode by
 * splitting it in 2 parts, depending on a configurable split index. This limits
 * the number of delegation token znodes returned in a single call while loading
 * tokens state.
 */
AbstractReservationSystem (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/AbstractReservationSystem.java)/**
 * This is the implementation of {@link ReservationSystem} based on the
 * {@link ResourceScheduler}
 */
IntegralResource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/CapacityOverTimePolicy.java)/**
   * This class provides support for Resource-like book-keeping, based on
   * long(s), as using Resource to store the "integral" of the allocation over
   * time leads to integer overflows for large allocations/clusters. (Evolving
   * Resource to use long is too disruptive at this point.)
   *
   * The comparison/multiplication behaviors of IntegralResource are consistent
   * with the DefaultResourceCalculator.
   */
CapacityOverTimePolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/CapacityOverTimePolicy.java)/**
 * This policy enforces a time-extended notion of Capacity. In particular it
 * guarantees that the allocation received in input when combined with all
 * previous allocation for the user does not violate an instantaneous max limit
 * on the resources received, and that for every window of time of length
 * validWindow, the integral of the allocations for a user (sum of the currently
 * submitted allocation and all prior allocations for the user) does not exceed
 * validWindow * maxAvg.
 *
 * This allows flexibility, in the sense that an allocation can instantaneously
 * use large portions of the available capacity, but prevents abuses by bounding
 * the average use over time.
 *
 * By controlling maxInst, maxAvg, validWindow the administrator configuring
 * this policy can obtain a behavior ranging from instantaneously enforced
 * capacity (akin to existing queues), or fully flexible allocations (likely
 * reserved to super-users, or trusted systems).
 */
CapacityReservationSystem (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/CapacityReservationSystem.java)/**
 * This is the implementation of {@link ReservationSystem} based on the
 * {@link CapacityScheduler}
 */
CapacitySchedulerPlanFollower (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/CapacitySchedulerPlanFollower.java)/**
 * This class implements a {@link PlanFollower}. This is invoked on a timer, and
 * it is in charge to publish the state of the {@link Plan}s to the underlying
 * {@link CapacityScheduler}. This implementation does so, by
 * adding/removing/resizing leaf queues in the scheduler, thus affecting the
 * dynamic behavior of the scheduler in a way that is consistent with the
 * content of the plan. It also updates the plan's view on how much resources
 * are available in the cluster.
 * 
 * This implementation of PlanFollower is relatively stateless, and it can
 * synchronize schedulers and Plans that have arbitrary changes (performing set
 * differences among existing queues). This makes it resilient to frequency of
 * synchronization, and RM restart issues (no "catch up" is necessary).
 */
ContractValidationException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/exceptions/ContractValidationException.java)/**
 * This exception is thrown if the request made is not syntactically valid.
 */
PlanningQuotaException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/exceptions/PlanningQuotaException.java)/**
 * This exception is thrown if the user quota is exceed while accepting or
 * updating a reservation.
 */
ResourceOverCommitException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/exceptions/ResourceOverCommitException.java)/**
 * This exception indicate that the reservation that has been attempted, would
 * exceed the physical resources available in the {@link Plan} at the moment.
 */
InMemoryPlan (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/InMemoryPlan.java)/**
 * This class represents an in memory representation of the state of our
 * reservation system, and provides accelerated access to both individual
 * reservations and aggregate utilization of resources over time.
 */
InMemoryReservationAllocation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/InMemoryReservationAllocation.java)/**
 * An in memory implementation of a reservation allocation using the
 * {@link RLESparseResourceAllocation}
 *
 */
NoOverCommitPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/NoOverCommitPolicy.java)/**
 * This policy enforce a simple physical cluster capacity constraints, by
 * validating that the allocation proposed fits in the current plan. This
 * validation is compatible with "updates" and in verifying the capacity
 * constraints it conceptually remove the prior version of the reservation.
 */
PeriodicRLESparseResourceAllocation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/PeriodicRLESparseResourceAllocation.java)/**
 * This data structure stores a periodic {@link RLESparseResourceAllocation}.
 * Default period is 1 day (86400000ms).
 */
Plan (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/Plan.java)/**
 * A Plan represents the central data structure of a reservation system that
 * maintains the "agenda" for the cluster. In particular, it maintains
 * information on how a set of {@link ReservationDefinition} that have been
 * previously accepted will be honored.
 * 
 * {@link ReservationDefinition} submitted by the users through the RM public
 * APIs are passed to appropriate {@code ReservationAgent}s, which in turn will
 * consult the Plan (via the {@link PlanView} interface) and try to determine
 * whether there are sufficient resources available in this Plan to satisfy the
 * temporal and resource constraints of a {@link ReservationDefinition}. If a
 * valid allocation is found the agent will try to store it in the plan (via the
 * {@link PlanEdit} interface). Upon success the system return to the user a
 * positive acknowledgment, and a reservation identifier to be later used to
 * access the reserved resources.
 * 
 * A {@link PlanFollower} will continuously read from the Plan and will
 * affect the instantaneous allocation of resources among jobs running by
 * publishing the "current" slice of the Plan to the underlying scheduler. I.e.,
 * the configuration of queues/weights of the scheduler are modified to reflect
 * the allocations in the Plan.
 * 
 * As this interface have several methods we decompose them into three groups:
 * {@link PlanContext}: containing configuration type information,
 * {@link PlanView} read-only access to the plan state, and {@link PlanEdit}
 * write access to the plan state.
 */
PlanContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/PlanContext.java)/**
 * This interface provides read-only access to configuration-type parameter for
 * a plan.
 * 
 */
PlanEdit (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/PlanEdit.java)/**
 * This interface groups the methods used to modify the state of a Plan.
 */
PlanFollower (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/PlanFollower.java)/**
 * A PlanFollower is a component that runs on a timer, and synchronizes the
 * underlying {@link ResourceScheduler} with the {@link Plan}(s) and viceversa.
 * 
 * While different implementations might operate differently, the key idea is to
 * map the current allocation of resources for each active reservation in the
 * plan(s), to a corresponding notion in the underlying scheduler (e.g., tuning
 * capacity of queues, set pool weights, or tweak application priorities). The
 * goal is to affect the dynamic allocation of resources done by the scheduler
 * so that the jobs obtain access to resources in a way that is consistent with
 * the reservations in the plan. A key conceptual step here is to convert the
 * absolute-valued promises made in the reservations to appropriate relative
 * priorities/queue sizes etc.
 * 
 * Symmetrically the PlanFollower exposes changes in cluster conditions (as
 * tracked by the scheduler) to the plan, e.g., the overall amount of physical
 * resources available. The Plan in turn can react by replanning its allocations
 * if appropriate.
 * 
 * The implementation can assume that is run frequently enough to be able to
 * observe and react to normal operational changes in cluster conditions on the
 * fly (e.g., if cluster resources drop, we can update the relative weights of a
 * queue so that the absolute promises made to the job at reservation time are
 * respected).
 * 
 * However, due to RM restarts and the related downtime, it is advisable for
 * implementations to operate in a stateless way, and be able to synchronize the
 * state of plans/scheduler regardless of how big is the time gap between
 * executions.
 */
AlignedPlannerWithGreedy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/planning/AlignedPlannerWithGreedy.java)/**
 * A planning algorithm that first runs LowCostAligned, and if it fails runs
 * Greedy.
 */
StageProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/planning/IterativePlanner.java)/**
   * Helper class that provide a list of ReservationRequests and iterates
   * forward or backward depending whether we are allocating left-to-right or
   * right-to-left.
   */
IterativePlanner (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/planning/IterativePlanner.java)/**
 * A planning algorithm consisting of two main phases. The algorithm iterates
 * over the job stages in ascending/descending order, depending on the flag
 * allocateLeft. For each stage, the algorithm: 1. Determines an interval
 * [stageArrival, stageDeadline) in which the stage is allocated. 2. Computes an
 * allocation for the stage inside the interval. For ANY and ALL jobs, phase 1
 * sets the allocation window of each stage to be [jobArrival, jobDeadline]. For
 * ORDER and ORDER_NO_GAP jobs, the deadline of each stage is set as
 * succcessorStartTime - the starting time of its succeeding stage (or
 * jobDeadline if it is the last stage). The phases are set using the two
 * functions: 1. setAlgStageExecutionInterval 2.setAlgStageAllocator
 */
PlanningAlgorithm (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/planning/PlanningAlgorithm.java)/**
 * An abstract class that follows the general behavior of planning algorithms.
 */
ReservationAgent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/planning/ReservationAgent.java)/**
 * An entity that seeks to acquire resources to satisfy an user's contract
 */
SimpleCapacityReplanner (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/planning/SimpleCapacityReplanner.java)/**
 * This (re)planner scan a period of time from now to a maximum time window (or
 * the end of the last session, whichever comes first) checking the overall
 * capacity is not violated.
 *
 * It greedily removes sessions in reversed order of acceptance (latest accepted
 * is the first removed).
 */
StageAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/planning/StageAllocator.java)/**
 * Interface for allocating a single stage in IterativePlanner.
 */
DurationInterval (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/planning/StageAllocatorLowCostAligned.java)/**
   * An inner class that represents an interval, typically of length duration.
   * The class holds the total cost of the interval and the maximal load inside
   * the interval in each dimension (both calculated externally).
   */
StageExecutionInterval (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/planning/StageExecutionInterval.java)/**
 * An auxiliary class used to compute the time interval in which the stage can
 * be allocated resources by {@link IterativePlanner}.
 */
StageExecutionIntervalUnconstrained (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/planning/StageExecutionIntervalUnconstrained.java)/**
 * An implementation of {@link StageExecutionInterval} which gives each stage
 * the maximal possible time interval, given the job constraints. Specifically,
 * for ANY and ALL jobs, the interval would be [jobArrival, jobDeadline). For
 * ORDER jobs, the stage cannot start before its predecessors (if allocateLeft
 * == true) or cannot end before its successors (if allocateLeft == false)
 */
TryManyReservationAgents (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/planning/TryManyReservationAgents.java)/**
 * A planning algorithm that invokes several other planning algorithms according
 * to a given order. If one of the planners succeeds, the allocation it
 * generates is returned.
 */
PlanView (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/PlanView.java)/**
 * This interface provides a read-only view on the allocations made in this
 * plan. This methods are used for example by {@code ReservationAgent}s to
 * determine the free resources in a certain point in time, and by
 * PlanFollowerPolicy to publish this plan to the scheduler.
 */
ReservationAllocation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/ReservationAllocation.java)/**
 * A ReservationAllocation represents a concrete allocation of resources over
 * time that satisfy a certain {@link ReservationDefinition}. This is used
 * internally by a {@link Plan} to store information about how each of the
 * accepted {@link ReservationDefinition} have been allocated.
 */
ReservationInterval (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/ReservationInterval.java)/**
 * This represents the time duration of the reservation
 * 
 */
ReservationSystem (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/ReservationSystem.java)/**
 * This interface is the one implemented by any system that wants to support
 * Reservations i.e. make {@code Resource} allocations in future. Implementors
 * need to bootstrap all configured {@link Plan}s in the active
 * {@link ResourceScheduler} along with their corresponding
 * {@code ReservationAgent} and {@link SharingPolicy}. It is also responsible
 * for managing the {@link PlanFollower} to ensure the {@link Plan}s are in sync
 * with the {@link ResourceScheduler}.
 */
ReservationSystemUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/ReservationSystemUtil.java)/**
 * Simple helper class for static methods used to transform across
 * common formats in tests
 */
RLESparseResourceAllocation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/RLESparseResourceAllocation.java)/**
 * This is a run length encoded sparse data structure that maintains resource
 * allocations over time.
 */
SharingPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/SharingPolicy.java)/**
 * This is the interface for policy that validate new
 * {@link ReservationAllocation}s for allocations being added to a {@link Plan}.
 * Individual policies will be enforcing different invariants.
 */
ResourceProfilesManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/resource/ResourceProfilesManager.java)/**
 * Interface for the resource profiles manager. Provides an interface to get
 * the list of available profiles and some helper functions.
 */
ResourceProfilesManagerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/resource/ResourceProfilesManagerImpl.java)/**
 * PBImpl class to handle all proto related implementation for
 * ResourceProfilesManager.
 */
RMActiveServices (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java)/**
   * RMActiveServices handles all the Active services in the RM.
   */
StandByTransitionRunnable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java)/**
   * The class to transition RM to standby state. The same
   * {@link StandByTransitionRunnable} object could be used in multiple threads,
   * but runs only once. That's because RM can go back to active state after
   * transition to standby state, the same runnable in the old context can't
   * transition RM to standby state again. A new runnable is created every time
   * RM transitions to active state.
   */
ResourceManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java)/**
 * The ResourceManager is the main class that is a set of components.
 * "I am the ResourceManager. All your resources belong to us..."
 *
 */
ResourceManagerMXBean (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManagerMXBean.java)/**
 * This is the JMX management interface for ResourceManager.
 * End users shouldn't be implementing these interfaces, and instead
 * access this information through the JMX APIs.
 */
RMActiveServiceContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMActiveServiceContext.java)/**
 * The RMActiveServiceContext is the class that maintains <b>Active</b> service
 * context. Services that need to run only on the Active RM. This is expected to
 * be used only by RMContext.
 */
RMAppAttempt (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttempt.java)/**
 * Interface to an Application Attempt in the Resource Manager.
 * A {@link RMApp} can have multiple app attempts based on
 * {@link YarnConfiguration#RM_AM_MAX_ATTEMPTS}. For specific
 * implementation take a look at {@link RMAppAttemptImpl}.
 */
RMAppLifetimeMonitor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/monitor/RMAppLifetimeMonitor.java)/**
 * This service will monitor the applications against the lifetime value given.
 * The applications will be killed if it running beyond the given time.
 */
RMAppToMonitor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/monitor/RMAppToMonitor.java)/**
 * This class used for monitor application with applicationId+appTimeoutType.
 */
RMApp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMApp.java)/**
 * The interface to an Application in the ResourceManager. Take a
 * look at {@link RMAppImpl} for its implementation. This interface
 * exposes methods to access various updates in application status/report.
 */
RMAppKillByClientEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppKillByClientEvent.java)/**
 * An event class that is used to help with logging information
 * when an application KILL event is needed.
 *
 */
RMAppLogAggregation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppLogAggregation.java)/**
 * Log aggregation logic used by RMApp.
 *
 */
ApplicationSummary (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java)/**
   *  This class is for logging the application summary.
   */
RMAppManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java)/**
 * This class manages the list of applications for the resource manager. 
 */
ArgsBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAuditLogger.java)/**
   * Builder to create and pass a list of arbitrary key value pairs for logging.
   */
RMAuditLogger (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAuditLogger.java)/** 
 * Manages ResourceManager audit logs. 
 *
 * Audit log format is written as key=value pairs. Tab separated.
 */
RMContainer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainer.java)/**
 * Represents the ResourceManager's view of an application container. See 
 * {@link RMContainerImpl} for an implementation. Containers may be in one
 * of several states, given in {@link RMContainerState}. An RMContainer
 * instance may exist even if there is no actual running container, such as 
 * when resources are being reserved to fill space for a future container 
 * allocation.
 */
RMContainerReservedEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerReservedEvent.java)/**
 * The event signifying that a container has been reserved.
 * 
 * The event encapsulates information on the amount of reservation
 * and the node on which the reservation is in effect.
 */
RMContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMContext.java)/**
 * Context of the ResourceManager.
 */
RMContextImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMContextImpl.java)/**
 * RMContextImpl class holds two services context.
 * <ul>
 * <li>serviceContext : These services called as <b>Always On</b> services.
 * Services that need to run always irrespective of the HA state of the RM.</li>
 * <li>activeServiceCotext : Active services context. Services that need to run
 * only on the Active RM.</li>
 * </ul>
 * <p>
 * <b>Note:</b> If any new service to be added to context, add it to a right
 * context as per above description.
 */
RMCriticalThreadUncaughtExceptionHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMCriticalThreadUncaughtExceptionHandler.java)/**
 * This class either shuts down {@link ResourceManager} or transitions the
 * {@link ResourceManager} to standby state if a critical thread throws an
 * uncaught exception. It is intended to be installed by calling
 * {@code setUncaughtExceptionHandler(Thread.UncaughtExceptionHandler)}
 * in the thread entry point or after creation of threads.
 */
RMFatalEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMFatalEvent.java)/**
 * Event that indicates a non-recoverable error for the resource manager.
 */
RMNMInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java)/**
 * JMX bean listing statuses of all node managers.
 */
RMNode (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java)/**
 * Node managers information on available resources 
 * and other static information.
 *
 */
RMNodeDecommissioningEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeDecommissioningEvent.java)/**
 * RMNode Decommissioning Event.
 *
 */
UpdateContainersTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java)/**
   * Transition to Update a container.
   */
DecommissioningNodeTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java)/**
   * The transition to put node in decommissioning state.
   */
StatusUpdateWhenHealthyTransition (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java)/**
   * Status update transition when node is healthy.
   */
RMNodeImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java)/**
 * This class is used to keep track of all the applications/containers
 * running on a node.
 *
 */
RMNodeUpdateContainerEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeUpdateContainerEvent.java)/**
 * This class is used to create update container event
 * for the containers running on a node.
 *
 */
RMServerUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMServerUtils.java)/**
 * Utility methods to aid serving RM data through the REST and RPC APIs
 */
RMServiceContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMServiceContext.java)/**
 * RMServiceContext class maintains "Always On" services. Services that need to
 * run always irrespective of the HA state of the RM. This is created during
 * initialization of RMContextImpl.
 * <p>
 * <b>Note:</b> If any services to be added in this class, make sure service
 * will be running always irrespective of the HA state of the RM
 */
UsageByLabel (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractResourceUsage.java)/**
   * UsageByLabel stores resource array for all resource usage types.
   */
AbstractResourceUsage (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractResourceUsage.java)/**
 * This class can be used to track resource usage in queue/user/app.
 *
 * And it is thread-safe
 */
AbstractUsersManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractUsersManager.java)/**
 * {@link AbstractUsersManager} tracks users in the system.
 */
UpdateThread (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java)/**
   * Thread which calls {@link #update()} every
   * <code>updateInterval</code> milliseconds.
   */
ActiveUsersManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/ActiveUsersManager.java)/**
 * {@link ActiveUsersManager} tracks active users in the system.
 * A user is deemed to be active if he has any running applications with
 * outstanding resource requests.
 * 
 * An active user is defined as someone with outstanding resource requests.
 */
APP (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/activities/ActivitiesLogger.java)/**
   * Methods for recording activities from an app
   */
QUEUE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/activities/ActivitiesLogger.java)/**
   * Methods for recording activities from a queue
   */
NODE (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/activities/ActivitiesLogger.java)/**
   * Methods for recording overall activities from one node update
   */
ActivitiesLogger (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/activities/ActivitiesLogger.java)/**
 * Utility for logging scheduler activities
 */
DiagnosticsCollectorManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/activities/ActivitiesManager.java)/**
   * Class to manage the diagnostics collector.
   */
ActivitiesManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/activities/ActivitiesManager.java)/**
 * A class to store node or application allocations.
 * It mainly contains operations for allocation start, add, update and finish.
 */
ActivitiesUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/activities/ActivitiesUtils.java)/**
 * Utilities for activities.
 */
AppAllocation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/activities/AppAllocation.java)/**
 * It contains allocation information for one application within a period of
 * time.
 * Each application allocation may have several allocation attempts.
 */
DiagnosticsCollector (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/activities/DiagnosticsCollector.java)/**
 * Generic interface that can be used for collecting diagnostics.
 */
GenericDiagnosticsCollector (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/activities/GenericDiagnosticsCollector.java)/**
 * Generic interface that can be used for collecting diagnostics.
 */
ApplicationPlacementAllocatorFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/ApplicationPlacementAllocatorFactory.java)/**
 * Factory class to build various application placement policies.
 */
AppSchedulingInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java)/**
 * This class keeps track of all the consumption of an application. This also
 * keeps track of current running/completed containers for the application.
 */
AbstractAutoCreatedLeafQueue (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractAutoCreatedLeafQueue.java)/**
 * Abstract class for dynamic auto created queues managed by an implementation
 * of AbstractManagedParentQueue
 */
AbstractManagedParentQueue (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractManagedParentQueue.java)/**
 * A container class for automatically created child leaf queues.
 * From the user perspective this is equivalent to a LeafQueue,
 * but functionality wise is a sub-class of ParentQueue
 */
AbstractContainerAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/AbstractContainerAllocator.java)/**
 * For an application, resource limits and resource requests, decide how to
 * allocate container. This is to make application resource allocation logic
 * extensible.
 */
RegularContainerAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java)/**
 * Allocate normal (new) containers, considers locality/label, etc. Using
 * delayed scheduling mechanism to get better locality allocation.
 */
AppPriorityACLConfigurationParser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AppPriorityACLConfigurationParser.java)/**
 *
 * PriorityACLConfiguration class is used to parse Application Priority ACL
 * configuration from capcity-scheduler.xml
 */
AppPriorityACLGroup (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AppPriorityACLGroup.java)/**
 * PriorityACLGroup will hold all ACL related information per priority.
 *
 */
AutoCreatedLeafQueue (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AutoCreatedLeafQueue.java)/**
 * Leaf queues which are auto created by an underlying implementation of
 * AbstractManagedParentQueue. Eg: PlanQueue for reservations or
 * ManagedParentQueue for auto created dynamic queues
 */
AutoCreatedLeafQueueConfig (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AutoCreatedLeafQueueConfig.java)/**
 * Auto Created Leaf queue configurations, capacity
 */
CapacitySchedulerContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerContext.java)/**
 * Read-only interface to {@link CapacityScheduler} context.
 */
CapacitySchedulerMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerMetrics.java)/**
 * Metrics for capacity scheduler.
 */
CapacitySchedulerQueueManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerQueueManager.java)/**
 *
 * Context of the Queues in Capacity Scheduler.
 *
 */
CSConfigurationProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/CSConfigurationProvider.java)/**
 * Configuration provider for {@link CapacityScheduler}.
 */
FileBasedCSConfigurationProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/FileBasedCSConfigurationProvider.java)/**
 * {@link CapacityScheduler} configuration provider based on local
 * {@code capacity-scheduler.xml} file.
 */
FSSchedulerConfigurationStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/FSSchedulerConfigurationStore.java)/**
 * A filesystem implementation of {@link YarnConfigurationStore}. Offer
 * configuration storage in FileSystem
 */
InMemoryConfigurationStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/InMemoryConfigurationStore.java)/**
 * A default implementation of {@link YarnConfigurationStore}. Doesn't offer
 * persistent configuration storage, just stores the configuration in memory.
 */
LeveldbConfigurationStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/LeveldbConfigurationStore.java)/**
 * A LevelDB implementation of {@link YarnConfigurationStore}.
 */
MutableCSConfigurationProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/MutableCSConfigurationProvider.java)/**
 * CS configuration provider which implements
 * {@link MutableConfigurationProvider} for modifying capacity scheduler
 * configuration.
 */
QueueAdminConfigurationMutationACLPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/QueueAdminConfigurationMutationACLPolicy.java)/**
 * A configuration mutation ACL policy which checks that user has admin
 * privileges on all queues they are changing.
 */
LogMutation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/YarnConfigurationStore.java)/**
   * LogMutation encapsulates the fields needed for configuration mutation
   * audit logging and recovery.
   */
YarnConfigurationStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/YarnConfigurationStore.java)/**
 * YarnConfigurationStore exposes the methods needed for retrieving and
 * persisting {@link CapacityScheduler} configuration via key-value
 * using write-ahead logging. When configuration mutation is requested, caller
 * should first log it with {@code logMutation}, which persists this pending
 * mutation. This mutation is merged to the persisted configuration only after
 * {@code confirmMutation} is called.
 *
 * On startup/recovery, caller should call {@code retrieve} to get all
 * confirmed mutations, then get pending mutations which were not confirmed via
 * {@code getPendingMutations}, and replay/confirm them via
 * {@code confirmMutation} as in the normal case.
 */
YarnConfigurationStoreFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/YarnConfigurationStoreFactory.java)/**
 * Factory class for creating instances of {@link YarnConfigurationStore}.
 */
ZKConfigurationStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/ZKConfigurationStore.java)/**
 * A Zookeeper-based implementation of {@link YarnConfigurationStore}.
 */
CSAMContainerLaunchDiagnosticsConstants (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CSAMContainerLaunchDiagnosticsConstants.java)/**
 * diagnostic messages for AMcontainer launching
 */
CSQueue (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CSQueue.java)/**
 * <code>CSQueue</code> represents a node in the tree of 
 * hierarchical queues in the {@link CapacityScheduler}.
 */
ManagedParentQueue (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ManagedParentQueue.java)/**
 * Auto Creation enabled Parent queue. This queue initially does not have any
 * children to start with and all child
 * leaf queues will be auto created. Currently this does not allow other
 * pre-configured leaf or parent queues to
 * co-exist along with auto-created leaf queues. The auto creation is limited
 * to leaf queues currently.
 */
PlanQueue (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/PlanQueue.java)/**
 * This represents a dynamic queue managed by the {@link ReservationSystem}.
 * From the user perspective this is equivalent to a LeafQueue that respect
 * reservations, but functionality wise is a sub-class of ParentQueue
 *
 */
PriorityQueueComparator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/policy/PriorityUtilizationQueueOrderingPolicy.java)/**
   * Comparator that both looks at priority and utilization
   */
PriorityUtilizationQueueOrderingPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/policy/PriorityUtilizationQueueOrderingPolicy.java)/**
 * For two queues with the same priority:
 * - The queue with less relative used-capacity goes first - todays behavior.
 * - The default priority for all queues is 0 and equal. So, we get todays
 *   behaviour at every level - the queue with the lowest used-capacity
 *   percentage gets the resources
 *
 * For two queues with different priorities:
 * - Both the queues are under their guaranteed capacities: The queue with
 *   the higher priority gets resources
 * - Both the queues are over or meeting their guaranteed capacities:
 *   The queue with the higher priority gets resources
 * - One of the queues is over or meeting their guaranteed capacities and the
 *   other is under: The queue that is under its capacity guarantee gets the
 *   resources.
 */
QueueOrderingPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/policy/QueueOrderingPolicy.java)/**
 * This will be used by
 * {@link org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue}
 * to decide allocation ordering of child queues.
 */
PendingApplicationComparator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/queuemanagement/GuaranteedOrZeroCapacityOverTimePolicy.java)/**
   * Comparator that orders applications by their submit time
   */
GuaranteedOrZeroCapacityOverTimePolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/queuemanagement/GuaranteedOrZeroCapacityOverTimePolicy.java)/**
 * Capacity Management policy for auto created leaf queues
 * <p>
 * Assigns capacity if available to leaf queues based on application
 * submission order i.e leaf queues are assigned capacity in FCFS order based
 * on application submission time.  Updates leaf queue capacities to 0 when
 * there are no pending or running apps under that queue.
 */
QueueManagementChange (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/QueueManagementChange.java)/**
 * Encapsulates Queue entitlement and state updates needed
 * for adjusting capacity dynamically
 *
 */
QueueManagementDynamicEditPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/QueueManagementDynamicEditPolicy.java)/**
 * Queue Management scheduling policy for managed parent queues which enable
 * auto child queue creation
 */
ReservationQueue (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ReservationQueue.java)/**
 * This represents a dynamic {@link LeafQueue} managed by the
 * {@link ReservationSystem}
 *
 */
UsageRatios (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/UsersManager.java)/**
   * UsageRatios will store the total used resources ratio across all users of
   * the queue.
   */
User (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/UsersManager.java)/**
   * User class stores all user related resource usage, application details.
   */
UsersManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/UsersManager.java)/**
 * {@link UsersManager} tracks users in the system and its respective data
 * structures.
 */
ClusterNodeTracker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/ClusterNodeTracker.java)/**
 * Helper library that:
 * - tracks the state of all cluster {@link SchedulerNode}s
 * - provides convenience methods to filter and sort nodes
 */
ApplicationSchedulingConfig (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/ApplicationSchedulingConfig.java)/**
 * This class will keep all Scheduling env's names which will help in
 * placement calculations.
 */
ContainerAllocationProposal (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/ContainerAllocationProposal.java)/**
 * Proposal to allocate/reserve a new container
 */
ContainerRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/ContainerRequest.java)/**
 * ContainerRequest is a class to capture resource requests associated with a
 * Container, this will be used by scheduler to recover resource requests if the
 * container preempted or cancelled before AM acquire the container.
 *
 * It should include deducted resource requests when the container allocated.
 *
 * Lifecycle of the ContainerRequest is:
 *
 * <pre>
 * 1) It is instantiated when container created.
 * 2) It will be set to ContainerImpl by scheduler.
 * 3) When container preempted or cancelled because of whatever reason before
 *    container acquired by AM. ContainerRequest will be added back to pending
 *    request pool.
 * 4) It will be cleared from ContainerImpl if the container already acquired by
 *    AM.
 * </pre>
 */
FiCaSchedulerApp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java)/**
 * Represents an application attempt from the viewpoint of the FIFO or Capacity
 * scheduler.
 */
PendingAsk (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/PendingAsk.java)/**
 * {@link PendingAsk} is the class to include minimal information of how much
 * resource to ask under constraints (e.g. on one host / rack / node-attributes)
 * , etc.
 */
ResourceAllocationCommitter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/ResourceAllocationCommitter.java)/**
 * Scheduler should implement this interface if it wants to have multi-threading
 * plus global scheduling functionality
 */
SchedulerContainer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/SchedulerContainer.java)/**
 * Contexts for a container inside scheduler
 */
ConfigurationMutationACLPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/ConfigurationMutationACLPolicy.java)/**
 * Interface for determining whether configuration mutations are allowed.
 */
ConfigurationMutationACLPolicyFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/ConfigurationMutationACLPolicyFactory.java)/**
 * Factory class for creating instances of
 * {@link ConfigurationMutationACLPolicy}.
 */
CircularIterator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/algorithm/CircularIterator.java)/**
 * Iterator that can take current state of an existing iterator
 * and circularly iterate to that point.
 */
DefaultPlacementAlgorithm (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/algorithm/DefaultPlacementAlgorithm.java)/**
 * Basic placement algorithm.
 * Supports different Iterators at SchedulingRequest level including:
 * Serial, PopularTags
 */
PopularTagsIterator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/algorithm/iterators/PopularTagsIterator.java)/**
 * Traverse Scheduling requests with the most popular tags (count) first.
 * Currently the count is per Batch but could use TagManager for global count.
 */
SerialIterator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/algorithm/iterators/SerialIterator.java)/**
 * Traverse Scheduling Requests in the same order as they arrive
 */
AllocationTags (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/AllocationTags.java)/**
 * Allocation tags under same namespace.
 */
TypeToCountedTags (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/AllocationTagsManager.java)/**
   * Generic store mapping type T to counted tags.
   * Currently used both for NodeId to Tag, Count and Rack to Tag, Count
   */
AllocationTagsManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/AllocationTagsManager.java)/**
 * In-memory mapping between applications/container-tags and nodes/racks.
 * Required by constrained affinity/anti-affinity and cardinality placement.
 */
ConstraintPlacementAlgorithm (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/api/ConstraintPlacementAlgorithm.java)/**
 * Marker interface for a Constraint Placement. The only contract is that it
 * should be initialized with the RMContext.
 */
ConstraintPlacementAlgorithmInput (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/api/ConstraintPlacementAlgorithmInput.java)/**
 * This encapsulates an input to the Constraint Placement Algorithm. At the
 * very least it must consist of a collection of SchedulerRequests.
 */
ConstraintPlacementAlgorithmOutput (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/api/ConstraintPlacementAlgorithmOutput.java)/**
 * Encapsulates the output of the ConstraintPlacementAlgorithm. The Algorithm
 * is free to produce multiple of output objects at the end of each run and it
 * must use the provided ConstraintPlacementAlgorithmOutputCollector to
 * aggregate/collect this output. Similar to the MapReduce Mapper/Reducer
 * which is provided a collector to collect output.
 */
ConstraintPlacementAlgorithmOutputCollector (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/api/ConstraintPlacementAlgorithmOutputCollector.java)/**
 * The ConstraintPlacementAlgorithm uses the
 * ConstraintPlacementAlgorithmOutputCollector to collect any output it
 * spits out.
 */
PlacedSchedulingRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/api/PlacedSchedulingRequest.java)/**
 * Class to encapsulate a Placed scheduling Request.
 * It has the original Scheduling Request and a list of SchedulerNodes (one
 * for each 'numAllocation' field in the corresponding ResourceSizing object.
 *
 * NOTE: Clients of this class SHOULD NOT rely on the value of
 *       resourceSizing.numAllocations and instead should use the
 *       size of collection returned by getNodes() instead.
 */
SchedulingRequestWithPlacementAttempt (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/api/SchedulingRequestWithPlacementAttempt.java)/**
 * Simple holder class encapsulating a SchedulingRequest
 * with a placement attempt.
 */
SchedulingResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/api/SchedulingResponse.java)/**
 * This class encapsulates the response received from the ResourceScheduler's
 * attemptAllocateOnNode method.
 */
Evaluable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/Evaluable.java)/**
 * A class implements Evaluable interface represents the internal state
 * of the class can be changed against a given target.
 * @param <T> a target to evaluate against
 */
InvalidAllocationTagsQueryException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/InvalidAllocationTagsQueryException.java)/**
 * Exception when invalid parameter specified to do placement tags related
 * queries.
 */
MemoryPlacementConstraintManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/MemoryPlacementConstraintManager.java)/**
 * In memory implementation of the {@link PlacementConstraintManagerService}.
 */
PlacementConstraintManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/PlacementConstraintManager.java)/**
 * Interface for storing and retrieving placement constraints (see
 * {@link PlacementConstraint}).
 */
PlacementConstraintManagerService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/PlacementConstraintManagerService.java)/**
 * The service that implements the {@link PlacementConstraintManager} interface.
 */
PlacementConstraintsUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/PlacementConstraintsUtil.java)/**
 * This class contains various static methods used by the Placement Algorithms
 * to simplify constrained placement.
 * (see also {@link DefaultPlacementAlgorithm}).
 */
AbstractPlacementProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/processor/AbstractPlacementProcessor.java)/**
 * Base class for all PlacementProcessors.
 */
BatchedRequests (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/processor/BatchedRequests.java)/**
 * A grouping of Scheduling Requests which are sent to the PlacementAlgorithm
 * to place as a batch. The placement algorithm tends to give more optimal
 * placements if more requests are batched together.
 */
DisabledPlacementProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/processor/DisabledPlacementProcessor.java)/**
 * Processor that reject all SchedulingRequests.
 */
NodeCandidateSelector (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/processor/NodeCandidateSelector.java)/**
 * A read only implementation of the ClusterNodeTracker which exposes a method
 * to simply return a filtered list of nodes.
 */
Response (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/processor/PlacementConstraintProcessor.java)/**
   * Wrapper over the SchedulingResponse that wires in the placement attempt
   * and last attempted Node.
   */
PlacementConstraintProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/processor/PlacementConstraintProcessor.java)/**
 * An ApplicationMasterServiceProcessor that performs Constrained placement of
 * Scheduling Requests. It does the following:
 * 1. All initialization.
 * 2. Intercepts placement constraints from the register call and adds it to
 *    the placement constraint manager.
 * 3. Dispatches Scheduling Requests to the Planner.
 */
PlacementDispatcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/processor/PlacementDispatcher.java)/**
 * This class initializes the Constraint Placement Algorithm. It dispatches
 * input to the algorithm and collects output from it.
 */
SchedulerPlacementProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/processor/SchedulerPlacementProcessor.java)/**
 * Forwarding SchedulingRequests to be handled by the scheduler, as long as the
 * scheduler supports SchedulingRequests.
 */
TargetApplications (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TargetApplications.java)/**
 * This class is used by
 * {@link TargetApplicationsNamespace#evaluate(TargetApplications)} to evaluate
 * a namespace.
 */
Self (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TargetApplicationsNamespace.java)/**
   * Namespace within application itself.
   */
NotSelf (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TargetApplicationsNamespace.java)/**
   * Namespace to all applications except itself.
   */
All (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TargetApplicationsNamespace.java)/**
   * Namespace to all applications in the cluster.
   */
AppTag (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TargetApplicationsNamespace.java)/**
   * Namespace to applications that attached with a certain application tag.
   */
AppID (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TargetApplicationsNamespace.java)/**
   * Namespace defined by a certain application ID.
   */
TargetApplicationsNamespace (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TargetApplicationsNamespace.java)/**
 * Class to describe the namespace of allocation tags, used by
 * {@link AllocationTags}. Each namespace can be evaluated against
 * a target set applications, represented by {@link TargetApplications}.
 * After evaluation, the namespace is interpreted to be a set of
 * applications based on the namespace type.
 */
ContainerUpdateContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/ContainerUpdateContext.java)/**
 * Class encapsulates all outstanding container increase and decrease
 * requests for an application.
 */
ContainerUpdates (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/ContainerUpdates.java)/**
 * Holder class that maintains list of container update requests
 */
DefaultConfigurationMutationACLPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/DefaultConfigurationMutationACLPolicy.java)/**
 * Default configuration mutation ACL policy. Checks if user is YARN admin.
 */
NodeQueueLoadMonitor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/distributed/NodeQueueLoadMonitor.java)/**
 * The NodeQueueLoadMonitor keeps track of load metrics (such as queue length
 * and total wait time) associated with Container Queues on the Node Manager.
 * It uses this information to periodically sort the Nodes from least to most
 * loaded.
 */
QueueLimitCalculator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/distributed/QueueLimitCalculator.java)/**
 * This class interacts with the NodeQueueLoadMonitor to keep track of the
 * mean and standard deviation of the configured metrics (queue length or queue
 * wait time) used to characterize the queue load of a specific node.
 * The NodeQueueLoadMonitor triggers an update (by calling the
 * <code>update()</code> method) every time it performs a re-ordering of
 * all nodes.
 */
ContainerExpiredSchedulerEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/event/ContainerExpiredSchedulerEvent.java)/**
 * The {@link SchedulerEvent} which notifies that a {@link ContainerId}
 * has expired, sent by {@link ContainerAllocationExpirer} 
 *
 */
ContainerPreemptEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/event/ContainerPreemptEvent.java)/**
 * Simple event class used to communicate kill reserved containers, mark
 * containers for preemption and kill already preemption-marked containers.
 */
NodeAttributesUpdateSchedulerEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/event/NodeAttributesUpdateSchedulerEvent.java)/**
 * Event handler class for Node Attributes which sends events to Scheduler.
 */
QueueManagementChangeEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/event/QueueManagementChangeEvent.java)/**
 * Event to update scheduler of any queue management changes
 */
ReleaseContainerEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/event/ReleaseContainerEvent.java)/**
 * Event used to release a container.
 */
AllocationFileParser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/allocation/AllocationFileParser.java)/**
 * Responsible for parsing allocation.xml config file.
 * All node's text value is stored to textValues if {@link #VALID_TAG_NAMES}
 * contains the tag name.
 * Other meaningful fields are also saved in {@link #parse()}.
 */
AllocationFileQueueParser (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/allocation/AllocationFileQueueParser.java)/**
 * Responsible for loading queue configuration properties
 * from a list of {@link Element}s containing queues.
 */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/allocation/QueueProperties.java)/**
   * Builder class for {@link QueueProperties}.
   * All methods are adding queue properties to the maps of this builder
   * keyed by the queue's name except some methods
   * like {@link #isAclDefinedForAccessType(String, AccessType)} or
   * {@link #getMinQueueResources()}.
   *
   */
QueueProperties (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/allocation/QueueProperties.java)/**
 * This class is a value class, storing queue properties parsed from the
 * allocation.xml config file. Since there are a bunch of properties, properties
 * should be added via QueueProperties.Builder.
 */
AllocationConfigurationException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AllocationConfigurationException.java)/**
 * Thrown when the allocation file for {@link QueueManager} is malformed.
 */
ConfigurableResource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/ConfigurableResource.java)/**
 * A {@code ConfigurableResource} object represents an entity that is used to
 * configure resources, such as maximum resources of a queue. It can be
 * percentage of cluster resources or an absolute value.
 */
ConversionException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/ConversionException.java)/**
 * Thrown when the FS-to-CS converter logic encounters a
 * condition from which it cannot recover (eg. unsupported
 * settings).
 *
 */
FSConfigToCSConfigArgumentHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/FSConfigToCSConfigArgumentHandler.java)/**
 * Parses arguments passed to the FS->CS converter.
 * If the arguments are valid, it calls the converter itself.
 *
 */
FSConfigToCSConfigConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/FSConfigToCSConfigConverter.java)/**
 * Converts Fair Scheduler configuration (site and fair-scheduler.xml)
 * to Capacity Scheduler. The mapping is not 100% perfect due to
 * feature gaps. These will be addressed in the future.
 */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/FSConfigToCSConfigConverterParams.java)/**
   * Builder that can construct FSConfigToCSConfigConverterParams objects.
   *
   */
FSConfigToCSConfigConverterParams (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/FSConfigToCSConfigConverterParams.java)/**
 * POJO that holds values for the FS->CS converter.
 *
 */
FSConfigToCSConfigRuleHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/FSConfigToCSConfigRuleHandler.java)/**
 * Class that determines what should happen if the FS->CS converter
 * encounters a property that is currently not supported.
 *
 * Acceptable values are either "abort" or "warning".
 */
FSQueueConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/FSQueueConverter.java)/**
 * Converts a Fair Schedule queue hierarchy to Capacity Scheduler
 * configuration.
 *
 */
FSYarnSiteConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/FSYarnSiteConverter.java)/**
 * Converts a Fair Scheduler site configuration to Capacity Scheduler
 * site configuration.
 *
 */
PreconditionException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/PreconditionException.java)/**
 * Indicates that some preconditions were not met
 * before FS->CS conversion.
 *
 */
UnsupportedPropertyException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/UnsupportedPropertyException.java)/**
 * Thrown by the FS->CS converter if it encounters an
 * unsupported property.
 */
ContinuousSchedulingThread (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java)/**
   * Thread which attempts scheduling resources continuously,
   * asynchronous to the node heartbeats.
   */
NodeAvailableResourceComparator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java)/** Sort nodes by available resource */
FairScheduler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java)/**
 * A scheduler that schedules resources between a set of queues. The scheduler
 * keeps track of the resources used by each queue, and attempts to maintain
 * fairness by scheduling tasks at queues whose allocations are farthest below
 * an ideal fair distribution.
 * 
 * The fair scheduler supports hierarchical queues. All queues descend from a
 * queue named "root". Available resources are distributed among the children
 * of the root queue in the typical fair scheduling fashion. Then, the children
 * distribute the resources assigned to them to their children in the same
 * fashion.  Applications may only be scheduled on leaf queues. Queues can be
 * specified as children of other queues by placing them as sub-elements of
 * their parents in the fair scheduler configuration file.
 *
 * A queue's name starts with the names of its parents, with periods as
 * separators.  So a queue named "queue1" under the root named, would be 
 * referred to as "root.queue1", and a queue named "queue2" under a queue
 * named "parent1" would be referred to as "root.parent1.queue2".
 */
FairSchedulerUtilities (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerUtilities.java)/**
 * Utility class for the Fair Scheduler.
 */
FifoAppComparator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FifoAppComparator.java)/**
 * Order {@link FSAppAttempt} objects by priority and then by submit time, as
 * in the default scheduler in Hadoop.
 */
FSAppAttempt (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java)/**
 * Represents an application attempt from the viewpoint of the Fair Scheduler.
 */
FSContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSContext.java)/**
 * Helper class that holds basic information to be passed around
 * FairScheduler classes. Think of this as a glorified map that holds key
 * information about the scheduler.
 */
FSOpDurations (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSOpDurations.java)/**
 * Class to capture the performance metrics of FairScheduler.
 * This should be a singleton.
 */
PreemptableContainers (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSPreemptionThread.java)/**
   * A class to track preemptable containers.
   */
FSPreemptionThread (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSPreemptionThread.java)/**
 * Thread that handles FairScheduler preemption.
 */
FSSchedulerNode (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java)/**
 * Fair Scheduler specific node features.
 */
FSStarvedApps (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSStarvedApps.java)/**
 * Helper class to track starved applications.
 *
 * Initially, this uses a blocking queue. We could use other data structures
 * in the future. This class also has some methods to simplify testing.
 */
InvalidQueueNameException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/InvalidQueueNameException.java)/**
 * Thrown when Queue Name is malformed.
 */
MultiListStartTimeIterator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/MaxRunningAppsEnforcer.java)/**
   * Takes a list of lists, each of which is ordered by start time, and returns
   * their elements in order of start time.
   * 
   * We maintain positions in each of the lists.  Each next() call advances
   * the position in one of the lists.  We maintain a heap that orders lists
   * by the start time of the app in the current position in that list.
   * This allows us to pick which list to advance in O(log(num lists)) instead
   * of O(num lists) time.
   */
MaxRunningAppsEnforcer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/MaxRunningAppsEnforcer.java)/**
 * Handles tracking and enforcement for user and queue maxRunningApps
 * constraints
 */
ComputeFairShares (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/ComputeFairShares.java)/**
 * Contains logic for computing the fair shares. A {@link Schedulable}'s fair
 * share is {@link Resource} it is entitled to, independent of the current
 * demands and allocations on the cluster. A {@link Schedulable} whose resource
 * consumption lies at or below its fair share will never have its containers
 * preempted.
 */
DominantResourceFairnessComparator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/DominantResourceFairnessPolicy.java)/**
   * This class compares two {@link Schedulable} instances according to the
   * DRF policy. If neither instance is below min share, approximate fair share
   * ratios are compared. Subclasses of this class will do the actual work of
   * the comparison, specialized for the number of configured resource types.
   */
DominantResourceFairnessComparatorN (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/DominantResourceFairnessPolicy.java)/**
   * This class compares two {@link Schedulable} instances according to the
   * DRF policy. If neither instance is below min share, approximate fair share
   * ratios are compared. This class makes no assumptions about the number of
   * resource types.
   */
DominantResourceFairnessComparator2 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/DominantResourceFairnessPolicy.java)/**
   * This class compares two {@link Schedulable} instances according to the
   * DRF policy in the special case that only CPU and memory are configured.
   * If neither instance is below min share, approximate fair share
   * ratios are compared.
   */
DominantResourceFairnessPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/DominantResourceFairnessPolicy.java)/**
 * Makes scheduling decisions by trying to equalize dominant resource usage.
 * A schedulable's dominant resource usage is the largest ratio of resource
 * usage to capacity among the resource types it is using.
 */
FairShareComparator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/FairSharePolicy.java)/**
   * Compare Schedulables mainly via fair share usage to meet fairness.
   * Specifically, it goes through following four steps.
   *
   * 1. Compare demands. Schedulables without resource demand get lower priority
   * than ones who have demands.
   * 
   * 2. Compare min share usage. Schedulables below their min share are compared
   * by how far below it they are as a ratio. For example, if job A has 8 out
   * of a min share of 10 tasks and job B has 50 out of a min share of 100,
   * then job B is scheduled next, because B is at 50% of its min share and A
   * is at 80% of its min share.
   * 
   * 3. Compare fair share usage. Schedulables above their min share are
   * compared by fair share usage by checking (resource usage / weight).
   * If all weights are equal, slots are given to the job with the fewest tasks;
   * otherwise, jobs with more weight get proportionally more slots. If weight
   * equals to 0, we can't compare Schedulables by (resource usage/weight).
   * There are two situations: 1)All weights equal to 0, slots are given
   * to one with less resource usage. 2)Only one of weight equals to 0, slots
   * are given to the one with non-zero weight.
   *
   * 4. Break the tie by compare submit time and job name.
   */
FairSharePolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/FairSharePolicy.java)/**
 * Makes scheduling decisions by trying to equalize shares of memory.
 */
FifoComparator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/FifoPolicy.java)/**
   * Compare Schedulables in order of priority and then submission time, as in
   * the default FIFO scheduler in Hadoop.
   */
QueueManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueueManager.java)/**
 * Maintains a list of queues as well as scheduling parameters for each queue,
 * such as guaranteed share allocations, from the fair scheduler config file.
 */
QueuePlacementPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/QueuePlacementPolicy.java)/**
 * The FairScheduler rules based policy for placing an application in a queue.
 * It parses the configuration and updates the {@link
 * org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementManager}
 * with a list of {@link PlacementRule}s to execute in order.
 */
Schedulable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/Schedulable.java)/**
 * A Schedulable represents an entity that can be scheduled such as an
 * application or a queue. It provides a common interface so that algorithms
 * such as fair sharing can be applied both within a queue and across queues.
 *
 * A Schedulable is responsible for three roles:
 * 1) Assign resources through {@link #assignContainer}.
 * 2) It provides information about the app/queue to the scheduler, including:
 *    - Demand (maximum number of tasks required)
 *    - Minimum share (for queues)
 *    - Job/queue weight (for fair sharing)
 *    - Start time and priority (for FIFO)
 * 3) It can be assigned a fair share, for use with fair scheduling.
 *
 * Schedulable also contains two methods for performing scheduling computations:
 * - updateDemand() is called periodically to compute the demand of the various
 *   jobs and queues, which may be expensive (e.g. jobs must iterate through all
 *   their tasks to count failed tasks, tasks that can be speculated, etc).
 */
SchedulingPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/SchedulingPolicy.java)/**
 * The SchedulingPolicy is used by the fair scheduler mainly to determine
 * what a queue's fair share and steady fair share should be as well as
 * calculating available headroom. This determines how resources can be
 * shared between running applications within a queue.
 * <p>
 * Every queue has a policy, including parents and children. If a child
 * queue doesn't specify one, it inherits the parent's policy.
 * The policy for a child queue must be compatible with the policy of
 * the parent queue; there are some combinations that aren't allowed.
 * See {@link SchedulingPolicy#isChildPolicyAllowed(SchedulingPolicy)}.
 * The policy for a queue is specified by setting property
 * <i>schedulingPolicy</i> in the fair scheduler configuration file.
 * The default policy is {@link FairSharePolicy} if not specified.
 */
VisitedResourceRequestTracker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/VisitedResourceRequestTracker.java)/**
 * Applications place {@link ResourceRequest}s at multiple levels. This is a
 * helper class that allows tracking if a {@link ResourceRequest} has been
 * visited at a different locality level.
 *
 * This is implemented for {@link FSAppAttempt#getStarvedResourceRequests()}.
 * The implementation is not thread-safe.
 */
FSQueueMetricsForCustomResources (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/FSQueueMetricsForCustomResources.java)/**
 * This class is a main entry-point for any kind of metrics for
 * custom resources.
 * It provides increase and decrease methods for all types of metrics.
 */
MutableConfigurationProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/MutableConfigurationProvider.java)/**
 * Interface for allowing changing scheduler configurations.
 */
MutableConfScheduler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/MutableConfScheduler.java)/**
 * Interface for a scheduler that supports changing configuration at runtime.
 *
 */
NodeFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/NodeFilter.java)/**
 * Convenience way to filter nodes based on a criteria. To be used in
 * conjunction with {@link ClusterNodeTracker}
 */
NodeReport (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/NodeReport.java)/**
 * Node usage report.
 */
NodeResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/NodeResponse.java)/**
 * The class that encapsulates response from clusterinfo for 
 * updates from the node managers.
 */
AppPlacementAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/AppPlacementAllocator.java)/**
 * <p>
 * This class has the following functionality:
 * 1) Keeps track of pending resource requests when following events happen:
 * - New ResourceRequests are added to scheduler.
 * - New containers get allocated.
 *
 * 2) Determines the order that the nodes given in the {@link CandidateNodeSet}
 * will be used for allocating containers.
 * </p>
 *
 * <p>
 * And different set of resource requests (E.g., resource requests with the
 * same schedulerKey) can have one instance of AppPlacementAllocator, each
 * AppPlacementAllocator can have different ways to order nodes depends on
 * requests.
 * </p>
 */
CandidateNodeSet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/CandidateNodeSet.java)/**
 * A group of nodes which can be allocated by scheduler.
 *
 * It will have following part:
 *
 * 1) A map of nodes which can be schedule-able.
 * 2) Version of the node set, version should be updated if any node added or
 *    removed from the node set. This will be used by
 *    {@link AppPlacementAllocator} or other class to check if it's required to
 *    invalidate local caches, etc.
 * 3) Node partition of the candidate set.
 */
CandidateNodeSetUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/CandidateNodeSetUtils.java)/**
 * Utility methods for {@link CandidateNodeSet}.
 */
LocalityAppPlacementAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/LocalityAppPlacementAllocator.java)/**
 * This is an implementation of the {@link AppPlacementAllocator} that takes
 * into account locality preferences (node, rack, any) when allocating
 * containers.
 */
MultiNodeLookupPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/MultiNodeLookupPolicy.java)/**
 * <p>
 * This class has the following functionality.
 *
 * <p>
 * Provide an interface for MultiNodeLookupPolicy so that different placement
 * allocator can choose nodes based on need.
 * </p>
 */
MultiNodePolicySpec (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/MultiNodePolicySpec.java)/**
 * MultiNodePolicySpec contains policyName and timeout.
 */
MultiNodeSorter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/MultiNodeSorter.java)/**
 * Common node sorting class which will do sorting based on policy spec.
 * @param <N> extends SchedulerNode.
 */
MultiNodeSortingManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/MultiNodeSortingManager.java)/**
 * Node Sorting Manager which runs all sorter threads and policies.
 * @param <N> extends SchedulerNode
 */
PendingAskUpdateResult (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/PendingAskUpdateResult.java)/**
 * Result of a resource-request update. This will be used by
 * {@link org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo}
 * to update queue metrics and application/queue's overall pending resources.
 * And this is per-scheduler-key.
 *
 * Following fields will be set if pending ask changed for a given scheduler key
 * - lastPendingAsk: how many resource asked before.
 * - newPendingAsk: how many resource asked now.
 * - lastNodePartition: what's the node partition before.
 * - newNodePartition: what's the node partition now.
 */
ResourceUsageMultiNodeLookupPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/ResourceUsageMultiNodeLookupPolicy.java)/**
 * <p>
 * This class has the following functionality:
 *
 * <p>
 * ResourceUsageMultiNodeLookupPolicy holds sorted nodes list based on the
 * resource usage of nodes at given time.
 * </p>
 */
SimpleCandidateNodeSet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/SimpleCandidateNodeSet.java)/**
 * A simple CandidateNodeSet which keeps an unordered map
 */
SingleConstraintAppPlacementAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/SingleConstraintAppPlacementAllocator.java)/**
 * This is a simple implementation to do affinity or anti-affinity for
 * inter/intra apps.
 */
AbstractComparatorOrderingPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/policy/AbstractComparatorOrderingPolicy.java)/**
 * An OrderingPolicy which can serve as a baseclass for policies which can be
 * expressed in terms of comparators
 */
FairOrderingPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/policy/FairOrderingPolicy.java)/**
 * An OrderingPolicy which orders SchedulableEntities for fairness (see
 * FairScheduler
 * FairSharePolicy), generally, processes with lesser usage are lesser. If
 * sizedBasedWeight is set to true then an application with high demand
 * may be prioritized ahead of an application with less usage.  This
 * is to offset the tendency to favor small apps, which could result in
 * starvation for large apps if many small ones enter and leave the queue
 * continuously (optional, default false)
 */
FifoComparator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/policy/FifoComparator.java)/**
 * A Comparator which orders SchedulableEntities by input order
 */
FifoOrderingPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/policy/FifoOrderingPolicy.java)/**
 * An OrderingPolicy which orders SchedulableEntities by input order
 */
FifoOrderingPolicyForPendingApps (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/policy/FifoOrderingPolicyForPendingApps.java)/**
 * This ordering policy is used for pending applications only.
 * An OrderingPolicy which orders SchedulableEntities by
 * <ul>
 * <li>Recovering application
 * <li>Priority of an application
 * <li>Input order
 * </ul>
 * <p>
 * Example : If schedulableEntities with E1(true,1,1) E2(true,2,2) E3(true,3,3)
 * E4(false,4,4) E5(false,4,5) are added. The ordering policy assignment
 * iterator is in the order of E3(true,3,3) E2(true,2,2) E1(true,1,1)
 * E5(false,5,5) E4(false,4,4)
 */
FifoOrderingPolicyWithExclusivePartitions (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/policy/FifoOrderingPolicyWithExclusivePartitions.java)/**
 * Similar to {@link FifoOrderingPolicy}, but with separate ordering policies
 * for each partition in
 * {@code yarn.scheduler.capacity.<queue-path>.ordering-policy.exclusive-enforced-partitions}.
 */
IteratorSelector (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/policy/IteratorSelector.java)/**
 * IteratorSelector contains information needed to tell an
 * {@link OrderingPolicy} what to return in an iterator.
 */
OrderingPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/policy/OrderingPolicy.java)/**
 * OrderingPolicy is used by the scheduler to order SchedulableEntities for
 * container assignment and preemption.
 * @param <S> the type of {@link SchedulableEntity} that will be compared
 */
PriorityComparator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/policy/PriorityComparator.java)/**
 * A Comparator which orders SchedulableEntities by priority.
 */
RecoveryComparator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/policy/RecoveryComparator.java)/**
 * A Comparator which orders SchedulableEntities by isRecovering flag.
 */
SchedulableEntity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/policy/SchedulableEntity.java)/**
 * A SchedulableEntity is a process to be scheduled.
 * for example, an application / application attempt
 */
PreemptableResourceScheduler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/PreemptableResourceScheduler.java)/**
 * Interface for a scheduler that supports preemption/killing
 *
 */
QueueMetricsCustomResource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetricsCustomResource.java)/**
 * Class that holds metrics values for custom resources in a map keyed with
 * the name of the custom resource.
 * There are different kinds of values like allocated, available and others.
 */
QueueMetricsForCustomResources (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetricsForCustomResources.java)/**
 * This class is a main entry-point for any kind of metrics for
 * custom resources.
 * It provides increase and decrease methods for all types of metrics.
 */
QueueResourceQuotas (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueResourceQuotas.java)/**
 * QueueResourceQuotas by Labels for following fields by label
 * - EFFECTIVE_MIN_CAPACITY
 * - EFFECTIVE_MAX_CAPACITY
 * This class can be used to track resource usage in queue/user/app.
 *
 * And it is thread-safe
 */
ResourceLimits (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/ResourceLimits.java)/**
 * Resource limits for queues/applications, this means max overall (please note
 * that, it's not "extra") resource you can get.
 */
ResourceScheduler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/ResourceScheduler.java)/**
 * This interface is the one implemented by the schedulers. It mainly extends 
 * {@link YarnScheduler}. 
 *
 */
ResourceUsage (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/ResourceUsage.java)/**
 * Resource Usage by Labels for following fields by label - AM resource (to
 * enforce max-am-resource-by-label after YARN-2637) - Used resource (includes
 * AM resource usage) - Reserved resource - Pending resource - Headroom
 * 
 * This class can be used to track resource usage in queue/user/app.
 * 
 * And it is thread-safe
 */
SchedContainerChangeRequest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedContainerChangeRequest.java)/**
 * This is UpdateContainerRequest in scheduler side, it contains some
 * pointers to runtime objects like RMContainer, SchedulerNode, etc. This will
 * be easier for scheduler making decision.
 */
SchedulerApplicationAttempt (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java)/**
 * Represents an application attempt from the viewpoint of the scheduler.
 * Each running app attempt in the RM corresponds to one instance
 * of this class.
 */
SchedulerAppReport (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerAppReport.java)/**
 * Represents an application attempt, and the resources that the attempt is 
 * using.
 */
SchedulerNode (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerNode.java)/**
 * Represents a YARN Cluster Node from the viewpoint of the scheduler.
 */
SchedulerNodeReport (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerNodeReport.java)/**
 * Node usage report.
 */
SchedulerQueue (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerQueue.java)/**
 *
 * Represents a queue in Scheduler.
 *
 */
SchedulerQueueManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerQueueManager.java)/**
 *
 * Context of the Queues in Scheduler.
 *
 */
MaxResourceValidationResult (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java)/**
   * This class contains invalid resource information along with its
   * resource request.
   */
SchedulerUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java)/**
 * Utilities shared by schedulers. 
 */
TimeBucketMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TimeBucketMetrics.java)/**
 * Create a set of buckets that hold key-time pairs. When the values of the 
 * buckets is queried, the number of objects with time differences in the
 * different buckets is returned.
 */
YarnScheduler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/YarnScheduler.java)/**
 * This interface is used by the components to talk to the
 * scheduler for allocating of resources, cleaning up resources.
 *
 */
AMRMTokenSecretManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/AMRMTokenSecretManager.java)/**
 * AMRM-tokens are per ApplicationAttempt. If users redistribute their
 * tokens, it is their headache, god save them. I mean you are not supposed to
 * distribute keys to your vault, right? Anyways, ResourceManager saves each
 * token locally in memory till application finishes and to a store for restart,
 * so no need to remember master-keys even after rolling them.
 */
AppPriorityACLsManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/AppPriorityACLsManager.java)/**
 *
 * Manager class to store and check permission for Priority ACLs.
 */
RMPolicyProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/authorize/RMPolicyProvider.java)/**
 * {@link PolicyProvider} for YARN ResourceManager protocols.
 */
DelegationTokenToRenew (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java)/**
   * class that is used for keeping tracks of DT to renew
   *
   */
RenewalTimerTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java)/**
   * Task - to renew a token
   *
   */
DelayedTokenRemovalRunnable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java)/**
   * Takes care of cancelling app delegation tokens after the configured
   * cancellation delay, taking into consideration keep-alive requests.
   * 
   */
DelegationTokenRenewer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java)/**
 * Service to renew application delegation tokens.
 */
ProxyCAManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/ProxyCAManager.java)/**
 * Manager for {@link ProxyCA}, which contains the Certificate Authority for
 * AMs to have certificates for HTTPS communication with the RM Proxy.
 */
ReservationsACLsManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/ReservationsACLsManager.java)/**
 * The {@link ReservationsACLsManager} is used to check a specified user's
 * permissons to perform a reservation operation on the
 * {@link CapacityScheduler} and the {@link FairScheduler}.
 * {@link ReservationACL}s are used to specify reservation operations.
 */
RMContainerTokenSecretManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java)/**
 * SecretManager for ContainerTokens. This is RM-specific and rolls the
 * master-keys every so often.
 * 
 */
RMDelegationTokenSecretManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMDelegationTokenSecretManager.java)/**
 * A ResourceManager specific delegation token secret manager.
 * The secret manager is responsible for generating and accepting the password
 * for each token.
 */
RMTimelineCollectorManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/timelineservice/RMTimelineCollectorManager.java)/**
 * This class extends TimelineCollectorManager to provide RM specific
 * implementations.
 */
ControllerPublishVolumeEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/event/ControllerPublishVolumeEvent.java)/**
 * Trigger controller publish.
 */
ValidateVolumeEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/event/ValidateVolumeEvent.java)/**
 * Validate volume capability with the CSI driver.
 */
VolumeEvent (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/event/VolumeEvent.java)/**
 * Base volume event class that used to trigger volume state transitions.
 */
Volume (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/lifecycle/Volume.java)/**
 * Major volume interface at RM's view, it maintains the volume states and
 * state transition according to the CSI volume lifecycle.
 */
VolumeImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/lifecycle/VolumeImpl.java)/**
 * This class maintains the volume states and state transition
 * according to the CSI volume lifecycle. Volume states are stored in
 * {@link org.apache.hadoop.yarn.server.resourcemanager.volume.csi.VolumeStates}
 * class.
 */
VolumeAMSProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/processor/VolumeAMSProcessor.java)/**
 * AMS processor that handles volume resource requests.
 *
 */
VolumeProvisioner (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/provisioner/VolumeProvisioner.java)/**
 * A task interface to provision a volume to expected state.
 */
VolumeProvisioningResults (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/provisioner/VolumeProvisioningResults.java)/**
 * Result of volumes' provisioning.
 */
VolumeProvisioningTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/provisioner/VolumeProvisioningTask.java)/**
 * A provisioning task encapsulates all the logic required by a storage system
 * to provision a volume. This class is the common implementation, it might
 * be override if the provisioning behavior of a certain storage system
 * is not completely align with this implementation.
 */
VolumeBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/VolumeBuilder.java)/**
 * Helper class to build a {@link Volume}.
 */
VolumeManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/VolumeManager.java)/**
 * Main interface for volume manager that manages all volumes.
 * Volume manager talks to a CSI controller plugin to handle the
 * volume operations before it is available to be published on
 * any node manager.
 */
VolumeManagerImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/VolumeManagerImpl.java)/**
 * A service manages all volumes.
 */
VolumeStates (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/VolumeStates.java)/**
 * Volume manager states, including all managed volumes and their states.
 */
AppsBlockWithMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/AppsBlockWithMetrics.java)/**
 * Renders a block for the applications with metrics information.
 */
ActivitiesInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ActivitiesInfo.java)/**
 * DAO object to display allocation activities.
 */
AllocationTagInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/AllocationTagInfo.java)/**
 * DAO object to display node allocation tag.
 */
AllocationTagsInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/AllocationTagsInfo.java)/**
 * DAO object to display node allocation tags.
 */
AppActivitiesInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/AppActivitiesInfo.java)/**
 * DAO object to display application activity.
 */
ApplicationSubmissionContextInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ApplicationSubmissionContextInfo.java)/**
 * Simple class to allow users to send information required to create an
 * ApplicationSubmissionContext which can then be used to submit an app
 * 
 */
AppRequestAllocationInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/AppRequestAllocationInfo.java)/**
 * DAO object to display request allocation detailed information.
 */
AppTimeoutInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/AppTimeoutInfo.java)/**
 * DAO object to display Application timeout information.
 */
AppTimeoutsInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/AppTimeoutsInfo.java)/**
 * This class hosts a set of AppTimeout DAO objects.
 */
ClusterUserInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ClusterUserInfo.java)/**
 * The YARN UI doesn't have centralized login mechanism. While accessing UI2 from kerberized shell, user who is
 * placed the request to YARN need to be displayed in UI. Given requests from UI2 is routed via Proxy, only RM can provide
 * the user who has placed the request. This DAO object help to provide the requested user and also RM logged in user.
 * the response sent by RM is authenticated user instead of proxy user.
 * It is always good to display authenticated user in browser which eliminates lot of confusion to end use.
 */
ConfigVersionInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ConfigVersionInfo.java)/**
 * Version of Scheduler Config.
 */
ContainerLaunchContextInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ContainerLaunchContextInfo.java)/**
 * Simple class to allow users to send information required to create a
 * ContainerLaunchContext which can then be used as part of the
 * ApplicationSubmissionContext
 * 
 */
ExecutionTypeRequestInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ExecutionTypeRequestInfo.java)/**
 * Simple class representing an execution type request.
 */
FairSchedulerQueueInfoList (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/FairSchedulerQueueInfoList.java)/**
 * FairScheduler QueueInfo list used for mapping to XML or JSON.
 */
LogAggregationContextInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/LogAggregationContextInfo.java)/**
 * Simple class to allow users to send information required to create a
 * ContainerLaunchContext which can then be used as part of the
 * ApplicationSubmissionContext
 *
 */
NewReservation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/NewReservation.java)/**
 * <p>The response sent by the <code>ResourceManager</code> to the client for
 * a request to get a new {@code ReservationId} for submitting reservations
 * using the REST API.</p>
 *
 * <p>Clients can submit a reservation with the returned {@code ReservationId}.
 * </p>
 *
 * {@code RMWebServices#createNewReservation(HttpServletRequest)}
 */
NodeAttributeInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/NodeAttributeInfo.java)/**
 * DAO for node an attribute record.
 */
NodeAttributesInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/NodeAttributesInfo.java)/**
 * DAO for a list of node attributes info.
 */
PartitionQueueCapacitiesInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/PartitionQueueCapacitiesInfo.java)/**
 * This class represents queue capacities for a given partition
 */
PartitionResourcesInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/PartitionResourcesInfo.java)/**
 * This class represents queue/user resource usage info for a given partition
 */
QueueCapacitiesInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/QueueCapacitiesInfo.java)/**
 * DAO which wraps PartitionQueueCapacitiesInfo applicable for a queue
 */
ReservationDefinitionInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ReservationDefinitionInfo.java)/**
 * Simple class that represent a reservation definition.
 */
ReservationDeleteRequestInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ReservationDeleteRequestInfo.java)/**
 * Simple class represent the request of deleting a given reservation,
 * selected by its id.
 */
ReservationDeleteResponseInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ReservationDeleteResponseInfo.java)/**
 * Simple class that represent a reponse to a delete operation.
 */
ReservationInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ReservationInfo.java)/**
 * Simple class that represent a reservation.
 */
ReservationListInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ReservationListInfo.java)/**
 * Simple class that represent a list of reservations.
 */
ReservationRequestInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ReservationRequestInfo.java)/**
 * Simple class representing a reservation request.
 */
ReservationRequestsInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ReservationRequestsInfo.java)/**
 * Simple class representing a list of ReservationRequest and the
 * interpreter which capture the semantic of this list (all/any/order).
 */
ReservationSubmissionRequestInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ReservationSubmissionRequestInfo.java)/**
 * Simple class to allow users to send information required to create an
 * ReservationSubmissionContext which can then be used to submit a reservation.
 */
ReservationUpdateRequestInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ReservationUpdateRequestInfo.java)/**
 * Simple class to allow users to send information required to update an
 * existing reservation.
 */
ReservationUpdateResponseInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ReservationUpdateResponseInfo.java)/**
 * Simple class that represent the response to a reservation update
 * request.
 */
ResourceAllocationInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ResourceAllocationInfo.java)/**
 * Simple class that represent a resource allocation.
 */
ResourceOptionInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ResourceOptionInfo.java)/**
 * A JAXB representation of a {link ResourceOption}.
 */
ResourceRequestInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ResourceRequestInfo.java)/**
 * Simple class representing a resource request.
 */
ResourcesInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ResourcesInfo.java)/**
 * DAO which wraps PartitionResourceUsageInfo applicable for a queue/user
 */
ResourceUtilizationInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ResourceUtilizationInfo.java)/**
 * DAO object represents resource utilization of node and containers.
 */
DeSelectFields (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/DeSelectFields.java)/**
 * DeSelectFields make the <code>/apps</code> api more flexible.
 * It can be used to strip off more fields if there's such use case in future.
 * You can simply extend it via two steps:
 * <br> 1. add a <code>DeSelectType</code> enum with a string literals
 * <br> 2. write your logical based on
 * the return of method contains(DeSelectType)
 */
ErrorBlock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/ErrorBlock.java)/**
 * This class is used to display an error message to the user in the UI.
 */
FairSchedulerAppsBlock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/FairSchedulerAppsBlock.java)/**
 * Shows application information specific to the fair
 * scheduler as part of the fair scheduler page.
 */
MetricsOverviewTable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/MetricsOverviewTable.java)/**
 * Provides an table with an overview of many cluster wide metrics and if
 * per user metrics are enabled it will show an overview of what the
 * current user is using on the cluster.
 */
NodeIDsInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/NodeIDsInfo.java)/**
 * XML element uses to represent NodeIds' list.
 */
RedirectionErrorPage (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RedirectionErrorPage.java)/**
 * This class is used to display a message that the proxy request failed
 * because of a redirection issue.
 */
RMWebApp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebApp.java)/**
 * The RM webapp
 */
RMWebAppUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebAppUtil.java)/**
 * Util class for ResourceManager WebApp.
 */
RMWebServiceProtocol (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServiceProtocol.java)/**
 * <p>
 * The protocol between clients and the <code>ResourceManager</code> to
 * submit/abort jobs and to get information on applications, cluster metrics,
 * nodes, queues, ACLs and reservations via REST calls.
 * </p>
 *
 * The WebService is reachable by using {@link RMWSConsts#RM_WEB_SERVICE_PATH}
 */
RMWSConsts (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWSConsts.java)/**
 * Constants for {@code RMWebServiceProtocol}.
 */
YarnTestDriver (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/test/YarnTestDriver.java)/**
 * Driver for YARN tests.
 *
 */
ApplicationMasterServiceTestBase (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/ApplicationMasterServiceTestBase.java)/**
 * Base class for Application Master test classes.
 * Some implementors are for testing CS and FS.
 */
TestAMRestart (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/applicationsmanager/TestAMRestart.java)/**
 * Test AM restart functions.
 */
TestApplicationMasterExpiry (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/applicationsmanager/TestApplicationMasterExpiry.java)/**
 * A test case that tests the expiry of the application master.
 * More tests can be added to this.
 */
AppManagerTestBase (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/AppManagerTestBase.java)/**
 * Base class for AppManager related test.
 *
 */
TestFederationRMStateStoreService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/federation/TestFederationRMStateStoreService.java)/**
 * Unit tests for FederationStateStoreService.
 */
TestCombinedSystemMetricsPublisher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestCombinedSystemMetricsPublisher.java)/**
 * Tests that a CombinedSystemMetricsPublisher publishes metrics for timeline
 * services (v1/v2) as specified by the configuration.
 */
MockMemoryRMStateStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockMemoryRMStateStore.java)/**
 * Test helper for MemoryRMStateStore will make sure the event.
 */
MockNodes (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java)/**
 * Test helper to generate mock nodes
 */
TestProportionalCapacityPreemptionPolicyIntraQueue (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicyIntraQueue.java)/**
 * Test class for IntraQueuePreemption scenarios.
 */
TestProportionalCapacityPreemptionPolicyIntraQueueUserLimit (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicyIntraQueueUserLimit.java)/**
 * Test class for IntraQueuePreemption scenarios.
 */
TestProportionalCapacityPreemptionPolicyIntraQueueWithDRF (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicyIntraQueueWithDRF.java)/**
 * Test class for IntraQueuePreemption scenarios.
 */
TestMetricsInvariantChecker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/invariants/TestMetricsInvariantChecker.java)/**
 * This class tests the {@code MetricsInvariantChecker} by running it multiple
 * time and reporting the time it takes to execute, as well as verifying that
 * the invariant throws in case the invariants are not respected.
 */
NodeAttributeTestUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/NodeAttributeTestUtils.java)/**
 * Test Utils for NodeAttribute.
 */
TestFileSystemNodeAttributeStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/TestFileSystemNodeAttributeStore.java)/**
 * Test class for FileSystemNodeAttributeStore.
 */
TestNodeAttributesManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/TestNodeAttributesManager.java)/**
 * Unit tests for node attribute manager.
 */
TestFairQueuePlacementUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/placement/TestFairQueuePlacementUtils.java)/**
 * Tests of the utility methods from {@link FairQueuePlacementUtils}.
 */
TestPlacementFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/placement/TestPlacementFactory.java)/**
 * Test for the {@link PlacementFactory}.
 */
TestPlacementRuleFS (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/placement/TestPlacementRuleFS.java)/**
 * Simple tests for FS specific parts of the PlacementRule.
 */
TestContextProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/preprocessor/TestContextProcessor.java)/**
 * This class will test the functionality of all the three
 * processor(Node, Queue, Tag)  together on same
 * ApplicationSubmissionContext.
 */
TestNodeLabelProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/preprocessor/TestNodeLabelProcessor.java)/**
 * This class will test the functionality of NodeLabelProcessor.
 */
TestQueueProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/preprocessor/TestQueueProcessor.java)/**
 * This class will test the functionality of QueueProcessor.
 */
TestTagAddProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/preprocessor/TestTagAddProcessor.java)/**
 * This class will test the functionality of TagAddProcessor.
 */
TestProtos (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestProtos.java)/**
 * Simple test to verify the protos generated are valid
 */
BaseSharingPolicyTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/BaseSharingPolicyTest.java)/**
 * This class is a base test for {@code SharingPolicy} implementors.
 */
TestAlignedPlanner (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/planning/TestAlignedPlanner.java)/**
 * This class tests the {@code AlignedPlannerWithGreedy} agent.
 */
TestReservationAgents (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/planning/TestReservationAgents.java)/**
 * General purpose ReservationAgent tester.
 */
TestCapacityOverTimePolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/TestCapacityOverTimePolicy.java)/**
 * This class tests the {@code CapacityOvertimePolicy} sharing policy.
 */
TestInMemoryPlan (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/TestInMemoryPlan.java)/**
 * Testing the class {@link InMemoryPlan}.
 */
TestNoOverCommitPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/TestNoOverCommitPolicy.java)/**
 * This clas tests {@code NoOverCommitPolicy} sharing policy.
 */
TestPeriodicRLESparseResourceAllocation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/TestPeriodicRLESparseResourceAllocation.java)/**
 * Testing the class {@link PeriodicRLESparseResourceAllocation}.
 */
TestRLESparseResourceAllocation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/reservation/TestRLESparseResourceAllocation.java)/**
 * Testing the class {@link RLESparseResourceAllocation}.
 */
TestResourceProfiles (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/resource/TestResourceProfiles.java)/**
 * Common test class for resource profile related tests.
 */
TestNMReconnect (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/resourcetracker/TestNMReconnect.java)/**
 * TestNMReconnect run tests against the scheduler set by
 * {@link ParameterizedSchedulerTestBase} which is configured
 * in {@link YarnConfiguration}.
 */
TestRMAppAttemptImplDiagnostics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptImplDiagnostics.java)/**
 * Testing {@link RMAppAttemptImpl#diagnostics} scenarios.
 */
TestApplicationLifetimeMonitor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestApplicationLifetimeMonitor.java)/**
 * Test class for application life time monitor feature test.
 */
TestingActivitiesManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/activities/TestActivitiesManager.java)/**
   * Testing activities manager which can record all history information about
   * node allocations.
   */
TestActivitiesManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/activities/TestActivitiesManager.java)/**
 * Test class for {@link ActivitiesManager}.
 */
ConfigurationStoreBaseTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/ConfigurationStoreBaseTest.java)/**
 * Base class for {@link YarnConfigurationStore} implementations.
 */
TestFSSchedulerConfigurationStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/TestFSSchedulerConfigurationStore.java)/**
 * Tests {@link FSSchedulerConfigurationStore}.
 */
TestInMemoryConfigurationStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/TestInMemoryConfigurationStore.java)/**
 * Tests {@link InMemoryConfigurationStore}.
 */
TestLeveldbConfigurationStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/TestLeveldbConfigurationStore.java)/**
 * Tests {@link LeveldbConfigurationStore}.
 */
TestMutableCSConfigurationProvider (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/TestMutableCSConfigurationProvider.java)/**
 * Tests {@link MutableCSConfigurationProvider}.
 */
TestZKConfigurationStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/TestZKConfigurationStore.java)/**
 * Tests {@link ZKConfigurationStore}.
 */
TestCapacitySchedulerAutoCreatedQueuePreemption (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerAutoCreatedQueuePreemption.java)/**
 * Tests various preemption cases on auto-created leaf queues. All
 * auto-created leaf queues will end up having same priority since they are set
 * from template. Priority on ManagedParent Queues can be set however and
 * priority based premption cases are based on that.
 */
TestCapacitySchedulerAutoQueueCreation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerAutoQueueCreation.java)/**
 * Tests for creation and reinitialization of auto created leaf queues
 * and capacity management under a ManagedParentQueue.
 */
TestCapacitySchedulerMultiNodes (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerMultiNodes.java)/**
 * Test class for Multi Node scheduling related tests.
 */
TestCapacitySchedulerOvercommit (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerOvercommit.java)/**
 * Test changing resources and overcommit in the Capacity Scheduler
 * {@link CapacityScheduler}.
 */
TestCapacitySchedulerSchedulingRequestUpdate (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerSchedulingRequestUpdate.java)/**
 * Test class for verifying Scheduling requests in CS.
 */
TestCapacitySchedulerWithMultiResourceTypes (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerWithMultiResourceTypes.java)/**
 * Test Capacity Scheduler with multiple resource types.
 */
TestCSAllocateCustomResource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCSAllocateCustomResource.java)/**
 * Test case for custom resource container allocation.
 * for capacity scheduler
 * */
TestQueueState (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestQueueState.java)/**
 * Test Queue States.
 */
TestQueueStateManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestQueueStateManager.java)/**
 * Test QueueStateManager.
 *
 */
TestReservationQueue (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestReservationQueue.java)/**
 * Test class for dynamic auto created leaf queues.
 * @see ReservationQueue
 */
TestSchedulingRequestContainerAllocation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestSchedulingRequestContainerAllocation.java)/**
 * Test Container Allocation with SchedulingRequest.
 */
TestSchedulingRequestContainerAllocationAsync (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestSchedulingRequestContainerAllocationAsync.java)/**
 * Test SchedulingRequest With Asynchronous Scheduling.
 */
SpyHook (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestUtils.java)/**
   * Hook to spy on queues.
   */
TestCircularIterator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/algorithm/TestCircularIterator.java)/**
 * Simple test case to test the Circular Iterator.
 */
TestLocalAllocationTagsManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/algorithm/TestLocalAllocationTagsManager.java)/**
 * Tests the LocalAllocationTagsManager.
 */
TestAllocationTagsManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestAllocationTagsManager.java)/**
 * Test functionality of AllocationTagsManager.
 */
TestAllocationTagsNamespace (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestAllocationTagsNamespace.java)/**
 * Test class for {@link TargetApplicationsNamespace}.
 */
TestBatchedRequestsIterators (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestBatchedRequestsIterators.java)/**
 * Test Request Iterator.
 */
TestPlacementConstraintManagerService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestPlacementConstraintManagerService.java)/**
 * Unit tests for {@link PlacementConstraintManagerService}.
 */
TestPlacementConstraintsUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestPlacementConstraintsUtil.java)/**
 * Test the PlacementConstraint Utility class functionality.
 */
TestPlacementProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestPlacementProcessor.java)/**
 * This tests end2end workflow of the constraint placement framework.
 */
TestNodeQueueLoadMonitor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/distributed/TestNodeQueueLoadMonitor.java)/**
 * Unit tests for NodeQueueLoadMonitor.
 */
AllocationFileQueueBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/allocationfile/AllocationFileQueueBuilder.java)/**
 * Abstract base class for building simple queues and subqueues for testcases.
 * Currently there are two concrete types subclassed from this class:
 * {@link AllocationFileSimpleQueueBuilder} and
 * {@link AllocationFileSubQueueBuilder}.
 * The intention of having this class to group the common properties of
 * simple queues and subqueues by methods delegating calls to a
 * queuePropertiesBuilder instance.
 */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/allocationfile/AllocationFileQueueProperties.java)/**
   * Builder class for {@link AllocationFileQueueProperties}.
   */
AllocationFileQueueProperties (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/allocationfile/AllocationFileQueueProperties.java)/**
 * The purpose of this class is to store all properties of a queue.
 */
AllocationFileSimpleQueueBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/allocationfile/AllocationFileSimpleQueueBuilder.java)/**
 * Queue builder that can build a simple queue with its properties.
 * Subqueues can be added with {@link #addSubQueue(AllocationFileQueue)}.
 */
AllocationFileSubQueueBuilder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/allocationfile/AllocationFileSubQueueBuilder.java)/**
 * Queue builder that can build a subqueue with its properties.
 */
AllocationFileWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/allocationfile/AllocationFileWriter.java)/**
 * This class is capable of serializing allocation file data to a file
 * in XML format.
 * See {@link #writeToFile(String)} method for the implementation.
 */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/allocationfile/UserSettings.java)/**
   * Builder class for {@link UserSettings}
   */
UserSettings (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/allocationfile/UserSettings.java)/**
 * Value class that stores user settings and can render data in XML format,
 * see {@link #render()}.
 */
FSConfigConverterTestCommons (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/FSConfigConverterTestCommons.java)/**
 * Helper methods for FS->CS converter testing.
 *
 */
TestFSConfigToCSConfigArgumentHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/TestFSConfigToCSConfigArgumentHandler.java)/**
 * Unit tests for FSConfigToCSConfigArgumentHandler.
 *
 */
TestFSConfigToCSConfigConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/TestFSConfigToCSConfigConverter.java)/**
 * Unit tests for FSConfigToCSConfigConverter.
 *
 */
TestFSConfigToCSConfigRuleHandler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/TestFSConfigToCSConfigRuleHandler.java)/**
 * Unit tests for FSConfigToCSConfigRuleHandler.
 *
 */
TestFSQueueConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/TestFSQueueConverter.java)/**
 * Unit tests for FSQueueConverter.
 *
 */
TestFSYarnSiteConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/converter/TestFSYarnSiteConverter.java)/**
 * Unit tests for FSYarnSiteConverter.
 *
 */
FakeSchedulable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FakeSchedulable.java)/**
 * Dummy implementation of Schedulable for unit testing.
 */
TestDominantResourceFairnessPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/TestDominantResourceFairnessPolicy.java)/**
 * comparator.compare(sched1, sched2) < 0 means that sched1 should get a
 * container before sched2
 */
PrimaryGroupMapping (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/PrimaryGroupMapping.java)/**
 * Group Mapping class used for test cases. Returns only primary group of the
 * given user
 */
TestAllocationFileLoaderService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestAllocationFileLoaderService.java)/**
 * Test loading the allocation file for the FairScheduler.
 */
TestApplicationMasterServiceWithFS (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestApplicationMasterServiceWithFS.java)/**
 * Test Application master service using Fair scheduler.
 */
TestAppRunnability (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestAppRunnability.java)/**
 * This class is to  test the fair scheduler functionality of
 * deciding the number of runnable application under various conditions.
 */
TestComputeFairShares (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestComputeFairShares.java)/**
 * Exercise the computeFairShares method in SchedulingAlgorithms.
 */
TestConfigurableResource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestConfigurableResource.java)/**
 * To test class {@link ConfigurableResource}.
 */
TestFairSchedulerConfiguration (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairSchedulerConfiguration.java)/**
 * Tests fair scheduler configuration.
 */
TestFairSchedulerOvercommit (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairSchedulerOvercommit.java)/**
 * Test changing resources and overcommit in the Fair Scheduler
 * {@link FairScheduler}.
 */
TestFairSchedulerPreemption (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairSchedulerPreemption.java)/**
 * Tests to verify fairshare and minshare preemption, using parameterization.
 */
TestFairSchedulerUtilities (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairSchedulerUtilities.java)/**
 * Tests for {@link FairSchedulerUtilities}.
 */
TestFSAppStarvation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSAppStarvation.java)/**
 * Test class to verify identification of app starvation
 */
TestFSQueueMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSQueueMetrics.java)/**
 * The test class for {@link FSQueueMetrics}.
 */
TestFSSchedulerNode (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java)/**
 * Test scheduler node, especially preemption reservations.
 */
TestQueueManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestQueueManager.java)/**
 * Test the {@link FairScheduler} queue manager correct queue hierarchies
 * management (create, delete and type changes).
 */
TestQueueManagerRealScheduler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestQueueManagerRealScheduler.java)/**
 * QueueManager tests that require a real scheduler
 */
TestQueuePlacementPolicy (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestQueuePlacementPolicy.java)/**
 * Tests for the queue placement policy for the {@link FairScheduler}.
 */
FairShareComparatorTester (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestSchedulingPolicy.java)/**
   * This class is responsible for testing the transitivity of
   * {@link FairSharePolicy.FairShareComparator}. We will generate
   * a lot of triples(each triple contains three {@link Schedulable}),
   * and then we verify transitivity by using each triple.
   *
   * <p>How to generate:</p>
   * For each field in {@link Schedulable} we all have a data collection. We
   * combine these data to construct a {@link Schedulable}, and generate all
   * cases of triple by DFS(depth first search algorithm). We can get 100% code
   * coverage by DFS.
   */
TestSingleConstraintAppPlacementAllocator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/TestSingleConstraintAppPlacementAllocator.java)/**
 * Test behaviors of single constraint app placement allocator.
 */
TestFifoOrderingPolicyWithExclusivePartitions (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/policy/TestFifoOrderingPolicyWithExclusivePartitions.java)/**
 * Tests {@link FifoOrderingPolicyWithExclusivePartitions} ordering policy.
 */
QueueInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueInfo.java)/**
 * This class holds queue and user metrics for a particular queue,
 * used for testing metrics.
 * Reference for the parent queue is also stored for every queue,
 * except if the queue is root.
 */
QueueMetricsTestData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetricsTestData.java)/**
 * This class is to test standard and custom resource metrics for all types.
 * Metrics types can be one of: allocated, pending, reserved
 * and other resources.
 */
MockSchedulerNode (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestAbstractYarnScheduler.java)/**
   * SchedulerNode mock to test launching containers.
   */
TestClusterNodeTracker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestClusterNodeTracker.java)/**
 * Test class to verify ClusterNodeTracker. Using FSSchedulerNode without
 * loss of generality.
 */
TestSchedulerOvercommit (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestSchedulerOvercommit.java)/**
 * Generic tests for overcommitting resources. This needs to be instantiated
 * with a scheduler ({@link YarnConfiguration#RM_SCHEDULER}).
 *
 * If reducing the amount of resources leads to overcommitting (negative
 * available resources), the scheduler will select containers to make room.
 * <ul>
 * <li>If there is no timeout (&lt;0), it doesn't kill or preempt surplus
 * containers.</li>
 * <li>If the timeout is 0, it kills the surplus containers immediately.</li>
 * <li>If the timeout is larger than 0, it first asks the application to
 * preempt those containers and after the timeout passes, it kills the surplus
 * containers.</li>
 * </ul>
 */
TestSchedulingWithAllocationRequestId (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestSchedulingWithAllocationRequestId.java)/**
 * Tests for checking Scheduling with allocationRequestId, i.e. mapping of
 * allocated containers to the original client {@code ResourceRequest}.
 */
MyToken (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/security/TestDelegationTokenRenewer.java)/**
   * add some extra functionality for testing
   * 1. toString();
   * 2. cancel() and isCanceled()
   */
MyFS (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/security/TestDelegationTokenRenewer.java)/**
   * fake FileSystem 
   * overwrites three methods
   * 1. getDelegationToken() - generates a token
   * 2. renewDelegataionToken - counts number of calls, and remembers 
   * most recently renewed token.
   * 3. cancelToken -cancels token (subsequent renew will cause IllegalToken 
   * exception
   */
TestDelegationTokenRenewer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/security/TestDelegationTokenRenewer.java)/**
 * unit test - 
 * tests addition/deletion/cancellation of renewals of delegation tokens
 *
 */
TestDelegationTokenRenewerLifecycle (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/security/TestDelegationTokenRenewerLifecycle.java)/**
 * This test replicates the condition os MAPREDUCE-3431 -a failure
 * during startup triggered an NPE during shutdown
 *
 */
TestRMAuthenticationFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/security/TestRMAuthenticationFilter.java)/**
 * Test RM Auth filter.
 */
TestApplicationMasterServiceCapacity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationMasterServiceCapacity.java)/**
 * Unit tests for {@link ApplicationMasterService}
 * with {@link CapacityScheduler}.
 */
TestApplicationMasterServiceInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationMasterServiceInterceptor.java)/**
 * This class tests whether {@link ApplicationMasterServiceProcessor}s
 * work fine, e.g. allocation is invoked on preprocessor and the next processor
 * in the chain is also invoked.
 */
TestAppManagerWithFairScheduler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestAppManagerWithFairScheduler.java)/**
 * Testing RMAppManager application submission with fair scheduler.
 */
TestCapacitySchedulerMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestCapacitySchedulerMetrics.java)/**
 * Test class for CS metrics.
 */
TestDecommissioningNodesWatcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestDecommissioningNodesWatcher.java)/**
 * This class tests DecommissioningNodesWatcher.
 */
TestNodeBlacklistingOnAMFailures (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestNodeBlacklistingOnAMFailures.java)/**
 * Validate system behavior when the am-scheduling logic 'blacklists' a node for
 * an application because of AM failures.
 */
TestOpportunisticContainerAllocatorAMService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestOpportunisticContainerAllocatorAMService.java)/**
 * Test cases for {@link OpportunisticContainerAllocatorAMService}.
 */
TestResourceManagerMXBean (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceManagerMXBean.java)/**
 * Class for testing {@link ResourceManagerMXBean} implementation.
 */
NullNodeAttributeStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java)/**
   * A no-op implementation of NodeAttributeStore for testing
   */
MyTestRPCServer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMAuditLogger.java)/**
   * A special extension of {@link TestImpl} RPC server with 
   * {@link TestImpl#ping()} testing the audit logs.
   */
TestRMAuditLogger (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMAuditLogger.java)/**
 * Tests {@link RMAuditLogger}.
 */
TestRMHAMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMHAMetrics.java)/**
 * Metrics related RM HA testing. Metrics are mostly static singletons. To
 * avoid interference with other RM HA tests, separating metric tests for RM HA
 * into a separate file temporarily.
 */
TestRMHATimelineCollectors (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMHATimelineCollectors.java)/**
 * Test if the new active RM could recover collector status on a state
 * transition.
 */
TestRMTimelineService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMTimelineService.java)/**
 * Tests that the RM creates timeline services (v1/v2) as specified by the
 * configuration.
 */
TestWorkPreservingUnmanagedAM (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingUnmanagedAM.java)/**
 * Test UAM handling in RM.
 */
TestVolumeCapabilityRange (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/TestVolumeCapabilityRange.java)/**
 * Test cases for volume capability.
 */
TestVolumeLifecycle (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/TestVolumeLifecycle.java)/**
 * Test cases for volume lifecycle management.
 */
TestVolumeMetaData (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/TestVolumeMetaData.java)/**
 * Test cases for volume specification definition and parsing.
 */
TestVolumeProcessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/volume/csi/TestVolumeProcessor.java)/**
 * Test cases for volume processor.
 */
ActivitiesTestUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/ActivitiesTestUtils.java)/**
 * Some Utils for activities tests.
 */
FairSchedulerJsonVerifications (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/fairscheduler/FairSchedulerJsonVerifications.java)/**
 * This test helper class is primarily used by
 * {@link TestRMWebServicesFairSchedulerCustomResourceTypes}.
 */
FairSchedulerXmlVerifications (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/fairscheduler/FairSchedulerXmlVerifications.java)/**
 * This test helper class is primarily used by
 * {@link TestRMWebServicesFairSchedulerCustomResourceTypes}.
 */
TestRMWebServicesFairScheduler (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/fairscheduler/TestRMWebServicesFairScheduler.java)/**
 * Tests RM Webservices fair scheduler resources.
 */
TestRMWebServicesFairSchedulerCustomResourceTypes (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/fairscheduler/TestRMWebServicesFairSchedulerCustomResourceTypes.java)/**
 * This class is to test response representations of queue resources,
 * explicitly setting custom resource types. with the help of
 * {@link CustomResourceTypesConfigurationProvider}
 */
AppInfoJsonVerifications (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/helper/AppInfoJsonVerifications.java)/**
 * Contains all value verifications that are needed to verify {@link AppInfo}
 * JSON objects.
 */
AppInfoXmlVerifications (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/helper/AppInfoXmlVerifications.java)/**
 * Contains all value verifications that are needed to verify {@link AppInfo}
 * XML documents.
 */
BufferedClientResponse (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/helper/BufferedClientResponse.java)/**
 * This class is merely a wrapper for {@link ClientResponse}. Given that the
 * entity input stream of {@link ClientResponse} can be read only once by
 * default and for some tests it is convenient to read the input stream many
 * times, this class hides the details of how to do that and prevents
 * unnecessary code duplication in tests.
 */
JsonCustomResourceTypeTestcase (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/helper/JsonCustomResourceTypeTestcase.java)/**
 * This class hides the implementation details of how to verify the structure of
 * JSON responses. Tests should only provide the path of the
 * {@link WebResource}, the response from the resource and
 * the verifier Consumer to
 * {@link JsonCustomResourceTypeTestcase#verify(Consumer)}. An instance of
 * {@link JSONObject} will be passed to that consumer to be able to
 * verify the response.
 */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/helper/ResourceRequestsJsonVerifications.java)/**
   * Builder class for {@link ResourceRequestsJsonVerifications}.
   */
ResourceRequestsJsonVerifications (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/helper/ResourceRequestsJsonVerifications.java)/**
 * Performs value verifications on
 * {@link org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceRequestInfo}
 * objects against the values of {@link ResourceRequest}. With the help of the
 * {@link Builder}, users can also make verifications of the custom resource
 * types and its values.
 */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/helper/ResourceRequestsXmlVerifications.java)/**
   * Builder class for {@link ResourceRequestsXmlVerifications}.
   */
ResourceRequestsXmlVerifications (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/helper/ResourceRequestsXmlVerifications.java)/**
 * Performs value verifications on
 * {@link org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceRequestInfo}
 * objects against the values of {@link ResourceRequest}. With the help of the
 * {@link Builder}, users can also make verifications of the custom resource
 * types and its values.
 */
XmlCustomResourceTypeTestCase (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/helper/XmlCustomResourceTypeTestCase.java)/**
 * This class hides the implementation details of how to verify the structure of
 * XML responses. Tests should only provide the path of the
 * {@link WebResource}, the response from the resource and
 * the verifier Consumer to
 * {@link XmlCustomResourceTypeTestCase#verify(Consumer)}. An instance of
 * {@link JSONObject} will be passed to that consumer to be able to
 * verify the response.
 */
TestNodesPage (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestNodesPage.java)/**
 * This tests the NodesPage block table that it should contain the table body
 * data for all the columns in the table as specified in the header.
 */
TestRedirectionErrorPage (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRedirectionErrorPage.java)/**
 * This class tests the RedirectionErrorPage.
 */
TestRMWebServiceAppsNodelabel (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServiceAppsNodelabel.java)/**
 * Tests partition resource usage per application.
 *
 */
TestRMWebServicesAppCustomResourceTypes (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesAppCustomResourceTypes.java)/**
 * This test verifies that custom resource types are correctly serialized to XML
 * and JSON when HTTP GET request is sent to the resource: ws/v1/cluster/apps.
 */
TestRMWebServicesAppsCustomResourceTypes (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesAppsCustomResourceTypes.java)/**
 * This test verifies that custom resource types are correctly serialized to XML
 * and JSON when HTTP GET request is sent to the resource: ws/v1/cluster/apps.
 */
TestRMWebServicesConfigurationMutation (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesConfigurationMutation.java)/**
 * Test scheduler configuration mutation via REST API.
 */
TestRMWebServicesContainers (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesContainers.java)/**
 * Testing containers REST API.
 */
TestRMCustomAuthFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java)/**
   * Custom filter to be able to test auth methods and let the other ones go.
   */
TestRMWebServicesSchedulerActivities (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesSchedulerActivities.java)/**
 * Tests for scheduler/app activities.
 */
TestRMWebServicesSchedulerActivitiesWithMultiNodesEnabled (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesSchedulerActivitiesWithMultiNodesEnabled.java)/**
 * Tests for scheduler/app activities when multi-nodes enabled.
 */
TestRMWithCSRFFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/webapp/TestRMWithCSRFFilter.java)/**
 * Used TestRMWebServices as an example of web invocations of RM and added
 * test for CSRF Filter.
 */
TestRMWithXFSFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/webapp/TestRMWithXFSFilter.java)/**
 * Used TestRMWebServices as an example of web invocations of RM and added
 * test for XFS Filter.
 */
AbstractClientRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/AbstractClientRequestInterceptor.java)/**
 * Implements the {@link ClientRequestInterceptor} interface and provides common
 * functionality which can can be used and/or extended by other concrete
 * intercepter classes.
 *
 */
ClientMethod (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/ClientMethod.java)/**
 * Class to define client method,params and arguments.
 */
ClientRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/ClientRequestInterceptor.java)/**
 * Defines the contract to be implemented by the request intercepter classes,
 * that can be used to intercept and inspect messages sent from the client to
 * the resource manager.
 */
DefaultClientRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/DefaultClientRequestInterceptor.java)/**
 * Extends the {@code AbstractRequestInterceptorClient} class and provides an
 * implementation that simply forwards the client requests to the cluster
 * resource manager.
 *
 */
FederationClientInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/FederationClientInterceptor.java)/**
 * Extends the {@code AbstractRequestInterceptorClient} class and provides an
 * implementation for federation of YARN RM and scaling an application across
 * multiple YARN SubClusters. All the federation specific implementation is
 * encapsulated in this class. This is always the last intercepter in the chain.
 */
RequestInterceptorChainWrapper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/RouterClientRMService.java)/**
   * Private structure for encapsulating RequestInterceptor and user instances.
   *
   */
RouterClientRMService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/RouterClientRMService.java)/**
 * RouterClientRMService is a service that runs on each router that can be used
 * to intercept and inspect {@link ApplicationClientProtocol} messages from
 * client to the cluster resource manager. It listens
 * {@link ApplicationClientProtocol} messages from the client and creates a
 * request intercepting pipeline instance for each client. The pipeline is a
 * chain of {@link ClientRequestInterceptor} instances that can inspect and
 * modify the request/response as needed. The main difference with
 * AMRMProxyService is the protocol they implement.
 */
RouterYarnClientUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/RouterYarnClientUtils.java)/**
 * Util class for Router Yarn client API calls.
 */
AbstractRMAdminRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/rmadmin/AbstractRMAdminRequestInterceptor.java)/**
 * Implements the {@link RMAdminRequestInterceptor} interface and provides
 * common functionality which can can be used and/or extended by other concrete
 * intercepter classes.
 *
 */
DefaultRMAdminRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/rmadmin/DefaultRMAdminRequestInterceptor.java)/**
 * Extends the {@link AbstractRMAdminRequestInterceptor} class and provides an
 * implementation that simply forwards the client requests to the cluster
 * resource manager.
 *
 */
RMAdminRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/rmadmin/RMAdminRequestInterceptor.java)/**
 * Defines the contract to be implemented by the request intercepter classes,
 * that can be used to intercept and inspect messages sent from the client to
 * the resource manager.
 */
RequestInterceptorChainWrapper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/rmadmin/RouterRMAdminService.java)/**
   * Private structure for encapsulating RequestInterceptor and user instances.
   *
   */
RouterRMAdminService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/rmadmin/RouterRMAdminService.java)/**
 * RouterRMAdminService is a service that runs on each router that can be used
 * to intercept and inspect {@code ResourceManagerAdministrationProtocol}
 * messages from client to the cluster resource manager. It listens
 * {@code ResourceManagerAdministrationProtocol} messages from the client and
 * creates a request intercepting pipeline instance for each client. The
 * pipeline is a chain of intercepter instances that can inspect and modify the
 * request/response as needed. The main difference with AMRMProxyService is the
 * protocol they implement.
 */
Router (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/Router.java)/**
 * The router is a stateless YARN component which is the entry point to the
 * cluster. It can be deployed on multiple nodes behind a Virtual IP (VIP) with
 * a LoadBalancer.
 *
 * The Router exposes the ApplicationClientProtocol (RPC and REST) to the
 * outside world, transparently hiding the presence of ResourceManager(s), which
 * allows users to request and update reservations, submit and kill
 * applications, and request status on running applications.
 *
 * In addition, it exposes the ResourceManager Admin API.
 *
 * This provides a placeholder for throttling mis-behaving clients (YARN-1546)
 * and masks the access to multiple RMs (YARN-3659).
 */
RouterMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/RouterMetrics.java)/**
 * This class is for maintaining the various Router Federation Interceptor
 * activity statistics and publishing them through the metrics interfaces.
 */
RouterServerUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/RouterServerUtil.java)/**
 * Common utility methods used by the Router server.
 *
 */
AboutBlock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/AboutBlock.java)/**
 * About block for the Router Web UI.
 */
AboutPage (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/AboutPage.java)/**
 * About page for the Router Web UI.
 */
AbstractRESTRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/AbstractRESTRequestInterceptor.java)/**
 * Extends the RequestInterceptor class and provides common functionality which
 * can be used and/or extended by other concrete intercepter classes.
 */
AppsBlock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/AppsBlock.java)/**
 * Applications block for the Router Web UI.
 */
DefaultRequestInterceptorREST (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/DefaultRequestInterceptorREST.java)/**
 * Extends the AbstractRequestInterceptorClient class and provides an
 * implementation that simply forwards the client requests to the resource
 * manager.
 */
FederationInterceptorREST (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/FederationInterceptorREST.java)/**
 * Extends the {@code AbstractRESTRequestInterceptor} class and provides an
 * implementation for federation of YARN RM and scaling an application across
 * multiple YARN SubClusters. All the federation specific implementation is
 * encapsulated in this class. This is always the last intercepter in the chain.
 */
FederationPage (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/FederationPage.java)/**
 * Renders a block for the applications with metrics information.
 */
NavBlock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/NavBlock.java)/**
 * Navigation block for the Router Web UI.
 */
NodesBlock (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/NodesBlock.java)/**
 * Nodes block for the Router Web UI.
 */
RESTRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RESTRequestInterceptor.java)/**
 * Defines the contract to be implemented by the request intercepter classes,
 * that can be used to intercept and inspect messages sent from the client to
 * the resource manager server.
 *
 * This class includes 4 methods getAppAttempts, getAppAttempt, getContainers
 * and getContainer that belong to {@link WebServices}. They are in this class
 * to make sure that RouterWebServices implements the same REST methods of
 * {@code RMWebServices}.
 */
RouterController (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterController.java)/**
 * Controller for the Router Web UI.
 */
RouterView (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterView.java)/**
 * View for the Router Web UI.
 */
RouterWebApp (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterWebApp.java)/**
 * The Router webapp.
 */
RequestInterceptorChainWrapper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterWebServices.java)/**
   * Private structure for encapsulating RequestInterceptor and user instances.
   *
   */
RouterWebServices (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterWebServices.java)/**
 * RouterWebServices is a service that runs on each router that can be used to
 * intercept and inspect {@link RMWebServiceProtocol} messages from client to
 * the cluster resource manager. It listens {@link RMWebServiceProtocol} REST
 * messages from the client and creates a request intercepting pipeline instance
 * for each client. The pipeline is a chain of {@link RESTRequestInterceptor}
 * instances that can inspect and modify the request/response as needed. The
 * main difference with AMRMProxyService is the protocol they implement.
 **/
RouterWebServiceUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterWebServiceUtil.java)/**
 * The Router webservice util class.
 */
BaseRouterClientRMTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/BaseRouterClientRMTest.java)/**
 * Base class for all the RouterClientRMService test cases. It provides utility
 * methods that can be used by the concrete test case classes.
 *
 */
MockClientRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/MockClientRequestInterceptor.java)/**
 * This class mocks the ClientRequestInterceptor.
 */
PassThroughClientRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/PassThroughClientRequestInterceptor.java)/**
 * Mock intercepter that does not do anything other than forwarding it to the
 * next intercepter in the chain.
 */
TestableFederationClientInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestableFederationClientInterceptor.java)/**
 * Extends the FederationClientInterceptor and overrides methods to provide a
 * testable implementation of FederationClientInterceptor.
 */
TestFederationClientInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestFederationClientInterceptor.java)/**
 * Extends the {@code BaseRouterClientRMTest} and overrides methods in order to
 * use the {@code RouterClientRMService} pipeline test cases for testing the
 * {@code FederationInterceptor} class. The tests for
 * {@code RouterClientRMService} has been written cleverly so that it can be
 * reused to validate different request intercepter chains.
 */
TestFederationClientInterceptorRetry (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestFederationClientInterceptorRetry.java)/**
 * Extends the {@code BaseRouterClientRMTest} and overrides methods in order to
 * use the {@code RouterClientRMService} pipeline test cases for testing the
 * {@code FederationInterceptor} class. The tests for
 * {@code RouterClientRMService} has been written cleverly so that it can be
 * reused to validate different request intercepter chains.
 *
 * It tests the case with SubClusters down and the Router logic of retries. We
 * have 1 good SubCluster and 2 bad ones for all the tests.
 */
TestRouterClientRMService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestRouterClientRMService.java)/**
 * Test class to validate the ClientRM Service inside the Router.
 */
TestRouterYarnClientUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestRouterYarnClientUtils.java)/**
 * Test class for RouterYarnClientUtils.
 */
BaseRouterRMAdminTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/rmadmin/BaseRouterRMAdminTest.java)/**
 * Base class for all the RouterRMAdminService test cases. It provides utility
 * methods that can be used by the concrete test case classes.
 *
 */
MockRMAdminRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/rmadmin/MockRMAdminRequestInterceptor.java)/**
 * This class mocks the RMAmdinRequestInterceptor.
 */
PassThroughRMAdminRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/rmadmin/PassThroughRMAdminRequestInterceptor.java)/**
 * Mock intercepter that does not do anything other than forwarding it to the
 * next intercepter in the chain.
 */
TestRouterRMAdminService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/rmadmin/TestRouterRMAdminService.java)/**
 * Test class to validate the RMAdmin Service inside the Router.
 */
TestRouter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/TestRouter.java)/**
 * Tests {@link Router}.
 */
TestRouterMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/TestRouterMetrics.java)/**
 * This class validates the correctness of Router Federation Interceptor
 * Metrics.
 */
BaseRouterWebServicesTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/BaseRouterWebServicesTest.java)/**
 * Base class for all the RouterRMAdminService test cases. It provides utility
 * methods that can be used by the concrete test case classes.
 *
 */
JavaProcess (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/JavaProcess.java)/**
 * Helper class to start a new process.
 */
MockDefaultRequestInterceptorREST (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/MockDefaultRequestInterceptorREST.java)/**
 * This class mocks the RESTRequestInterceptor.
 */
MockRESTRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/MockRESTRequestInterceptor.java)/**
 * This class mocks the RESTRequestInterceptor.
 */
PassThroughRESTRequestInterceptor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/PassThroughRESTRequestInterceptor.java)/**
 * Mock intercepter that does not do anything other than forwarding it to the
 * next intercepter in the chain.
 */
TestableFederationInterceptorREST (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestableFederationInterceptorREST.java)/**
 * Extends the FederationInterceptorREST and overrides methods to provide a
 * testable implementation of FederationInterceptorREST.
 */
TestFederationInterceptorREST (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestFederationInterceptorREST.java)/**
 * Extends the {@code BaseRouterClientRMTest} and overrides methods in order to
 * use the {@code RouterClientRMService} pipeline test cases for testing the
 * {@code FederationInterceptor} class. The tests for
 * {@code RouterClientRMService} has been written cleverly so that it can be
 * reused to validate different request intercepter chains.
 */
TestFederationInterceptorRESTRetry (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestFederationInterceptorRESTRetry.java)/**
 * Extends the {@code BaseRouterWebServicesTest} and overrides methods in order
 * to use the {@code RouterWebServices} pipeline test cases for testing the
 * {@code FederationInterceptorREST} class. The tests for
 * {@code RouterWebServices} has been written cleverly so that it can be reused
 * to validate different request interceptor chains.
 * <p>
 * It tests the case with SubClusters down and the Router logic of retries. We
 * have 1 good SubCluster and 2 bad ones for all the tests.
 */
TestRouterWebServices (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestRouterWebServices.java)/**
 * Test class to validate the WebService interceptor model inside the Router.
 */
TestRouterWebServicesREST (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestRouterWebServicesREST.java)/**
 * This test validate E2E the correctness of the RouterWebServices. It starts
 * Router, RM and NM in 3 different processes to avoid servlet conflicts. Each
 * test creates a REST call to Router and validate that the operation complete
 * successfully.
 */
TestRouterWebServiceUtil (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestRouterWebServiceUtil.java)/**
 * Test class to validate RouterWebServiceUtil methods.
 */
AppChecker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/AppChecker.java)/**
 * An interface for checking whether an app is running so that the cleaner
 * service may determine if it can safely remove a cached entry.
 */
CleanerService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/CleanerService.java)/**
 * The cleaner service that maintains the shared cache area, and cleans up stale
 * entries on a regular basis.
 */
CleanerTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/CleanerTask.java)/**
 * The task that runs and cleans up the shared cache area for stale entries and
 * orphaned files. It is expected that only one cleaner task runs at any given
 * point in time.
 */
ClientProtocolService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/ClientProtocolService.java)/**
 * This service handles all rpc calls from the client to the shared cache
 * manager.
 */
CleanerMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/metrics/CleanerMetrics.java)/**
 * This class is for maintaining the various Cleaner activity statistics and
 * publishing them through the metrics interfaces.
 */
ClientSCMMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/metrics/ClientSCMMetrics.java)/**
 * This class is for maintaining  client requests metrics
 * and publishing them through the metrics interfaces.
 */
SharedCacheUploaderMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/metrics/SharedCacheUploaderMetrics.java)/**
 * This class is for maintaining shared cache uploader requests metrics
 * and publishing them through the metrics interfaces.
 */
RemoteAppChecker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/RemoteAppChecker.java)/**
 * An implementation of AppChecker that queries the resource manager remotely to
 * determine whether the app is running.
 */
SCMAdminProtocolService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/SCMAdminProtocolService.java)/**
 * This service handles all SCMAdminProtocol rpc calls from administrators
 * to the shared cache manager.
 */
SharedCacheManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/SharedCacheManager.java)/**
 * This service maintains the shared cache meta data. It handles claiming and
 * releasing of resources, all rpc calls from the client to the shared cache
 * manager, and administrative commands. It also persists the shared cache meta
 * data to a backend store, and cleans up stale entries on a regular basis.
 */
SharedCacheUploaderService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/SharedCacheUploaderService.java)/**
 * This service handles all rpc calls from the NodeManager uploader to the
 * shared cache manager.
 */
InMemorySCMStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/store/InMemorySCMStore.java)/**
 * A thread safe version of an in-memory SCM store. The thread safety is
 * implemented with two key pieces: (1) at the mapping level a ConcurrentHashMap
 * is used to allow concurrency to resources and their associated references,
 * and (2) a key level lock is used to ensure mutual exclusion between any
 * operation that accesses a resource with the same key. <br>
 * <br>
 * To ensure safe key-level locking, we use the original string key and intern
 * it weakly using hadoop's <code>StringInterner</code>. It avoids the pitfalls
 * of using built-in String interning. The interned strings are also weakly
 * referenced, so it can be garbage collected once it is done. And there is
 * little risk of keys being available for other parts of the code so they can
 * be used as locks accidentally. <br>
 * <br>
 * Resources in the in-memory store are evicted based on a time staleness
 * criteria. If a resource is not referenced (i.e. used) for a given period, it
 * is designated as a stale resource and is considered evictable.
 */
SCMStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/store/SCMStore.java)/**
 * An abstract class for the data store used by the shared cache manager
 * service. All implementations of methods in this interface need to be thread
 * safe and atomic.
 */
SharedCacheResource (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/store/SharedCacheResource.java)/**
 * Class that encapsulates the cache resource. The instances are not thread
 * safe. Any operation that uses the resource must use thread-safe mechanisms to
 * ensure safe access with the only exception of the filename.
 */
SharedCacheResourceReference (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/store/SharedCacheResourceReference.java)/**
 * This is an object that represents a reference to a shared cache resource.
 */
SCMController (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/webapp/SCMController.java)/**
 * The controller class for the shared cache manager web app.
 */
SCMMetricsInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/webapp/SCMMetricsInfo.java)/**
 * This class is used to summarize useful shared cache manager metrics for the
 * webUI display.
 */
SCMOverviewPage (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/webapp/SCMOverviewPage.java)/**
 * This class is to render the shared cache manager web ui overview page.
 */
SCMWebServer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/main/java/org/apache/hadoop/yarn/server/sharedcachemanager/webapp/SCMWebServer.java)/**
 * A very simple web interface for the metrics reported by
 * {@link org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager}
 * TODO: Security for web ui (See YARN-2774)
 */
DummyAppChecker (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/test/java/org/apache/hadoop/yarn/server/sharedcachemanager/DummyAppChecker.java)/**
 * A dummy app checker class for testing only.
 */
SCMStoreBaseTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/test/java/org/apache/hadoop/yarn/server/sharedcachemanager/store/SCMStoreBaseTest.java)/**
 * All test classes that test an SCMStore implementation must extend this class.
 */
TestClientSCMProtocolService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/test/java/org/apache/hadoop/yarn/server/sharedcachemanager/TestClientSCMProtocolService.java)/**
 * Basic unit tests for the Client to SCM Protocol Service.
 */
TestSCMAdminProtocolService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/test/java/org/apache/hadoop/yarn/server/sharedcachemanager/TestSCMAdminProtocolService.java)/**
 * Basic unit tests for the SCM Admin Protocol Service and SCMAdmin.
 */
TestSharedCacheUploaderService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-sharedcachemanager/src/test/java/org/apache/hadoop/yarn/server/sharedcachemanager/TestSharedCacheUploaderService.java)/**
 * Basic unit tests for the NodeManger to SCM Protocol Service.
 */
MiniYARNCluster (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/MiniYARNCluster.java)/**
 * <p>
 * Embedded YARN minicluster for testcases that need to interact with a cluster.
 * </p>
 * <p>
 * In a real cluster, resource request matching is done using the hostname, and
 * by default YARN minicluster works in the exact same way as a real cluster.
 * </p>
 * <p>
 * If a testcase needs to use multiple nodes and exercise resource request
 * matching to a specific node, then the property 
 * {@value org.apache.hadoop.yarn.conf.YarnConfiguration#RM_SCHEDULER_INCLUDE_PORT_IN_NODE_NAME}
 * should be set <code>true</code> in the configuration used to initialize
 * the minicluster.
 * </p>
 * With this property set to <code>true</code>, the matching will be done using
 * the <code>hostname:port</code> of the namenodes. In such case, the AM must
 * do resource request using <code>hostname:port</code> as the location.
 */
TestDiskFailures (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestDiskFailures.java)/**
 * Verify if NodeManager's in-memory good local dirs list and good log dirs list
 * get updated properly when disks(nm-local-dirs and nm-log-dirs) fail. Also
 * verify if the overall health status of the node gets updated properly when
 * specified percentage of disks fail.
 */
TestTimelineAuthFilterForV2 (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/security/TestTimelineAuthFilterForV2.java)/**
 * Tests timeline authentication filter based security for timeline service v2.
 */
EntityCacheItem (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityCacheItem.java)/**
 * Cache item for timeline server v1.5 reader cache. Each cache item has a
 * TimelineStore that can be filled with data within one entity group.
 */
StoppableRemoteIterator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityGroupFSTimelineStore.java)/**
   * This is a special remote iterator whose {@link #hasNext()} method
   * returns false if {@link #stopExecutors} is true.
   *
   * This provides an implicit shutdown of all iterative file list and scan
   * operations without needing to implement it in the while loops themselves.
   */
EntityGroupFSTimelineStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityGroupFSTimelineStore.java)/**
 * Plugin timeline storage to support timeline server v1.5 API. This storage
 * uses a file system to store timeline entities in their groups.
 */
EntityGroupFSTimelineStoreMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityGroupFSTimelineStoreMetrics.java)/**
 * This class tracks metrics for the EntityGroupFSTimelineStore. It tracks
 * the read and write metrics for timeline server v1.5. It serves as a
 * complement to {@link TimelineDataManagerMetrics}.
 */
LevelDBMapAdapter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/LevelDBCacheTimelineStore.java)/**
   * A specialized hash map storage that uses LevelDB for storing entity id to
   * entity mappings.
   *
   * @param <K> an {@link EntityIdentifier} typed hash key
   * @param <V> a {@link TimelineEntity} typed value
   */
LevelDBCacheTimelineStore (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/LevelDBCacheTimelineStore.java)/**
 * LevelDB implementation of {@link KeyValueBasedTimelineStore}. This
 * implementation stores the entity hash map into a LevelDB instance.
 * There are two partitions of the key space. One partition is to store a
 * entity id to start time mapping:
 *
 * i!ENTITY_ID!ENTITY_TYPE to ENTITY_START_TIME
 *
 * The other partition is to store the actual data:
 *
 * e!START_TIME!ENTITY_ID!ENTITY_TYPE to ENTITY_BYTES
 *
 * This storage does not have any garbage collection mechanism, and is designed
 * mainly for caching usages.
 */
TimelineEntityGroupPlugin (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineEntityGroupPlugin.java)/**
 * Plugin to map a requested query ( or an Entity/set of Entities ) to a CacheID.
 * The Cache ID is an identifier to the data set that needs to be queried to
 * serve the response for the query.
 */
PluginStoreTestUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/test/java/org/apache/hadoop/yarn/server/timeline/PluginStoreTestUtils.java)/**
 * Utility methods related to the ATS v1.5 plugin storage tests.
 */
AppLevelTimelineCollector (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/collector/AppLevelTimelineCollector.java)/**
 * Service that handles writes to the timeline service and writes them to the
 * backing storage for a given YARN application.
 *
 * App-related lifecycle management is handled by this service.
 */
AppLevelTimelineCollectorWithAgg (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/collector/AppLevelTimelineCollectorWithAgg.java)/**
 * Service that handles aggregations for applications
 * and makes use of {@link AppLevelTimelineCollector} class for
 * writes to Timeline Service.
 *
 * App-related lifecycle management is handled by this service.
 */
NodeTimelineCollectorManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/collector/NodeTimelineCollectorManager.java)/**
 * Class on the NodeManager side that manages adding and removing collectors and
 * their lifecycle. Also instantiates the per-node collector webapp.
 */
PerNodeTimelineCollectorsAuxService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/collector/PerNodeTimelineCollectorsAuxService.java)/**
 * The top-level server for the per-node timeline collector manager. Currently
 * it is defined as an auxiliary service to accommodate running within another
 * daemon (e.g. node manager).
 */
TimelineCollector (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/collector/TimelineCollector.java)/**
 * Service that handles writes to the timeline service and writes them to the
 * backing storage.
 *
 * Classes that extend this can add their own lifecycle management or
 * customization of request handling.
 */
TimelineCollectorContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/collector/TimelineCollectorContext.java)/**
 * Encapsulates context information required by collector during a put.
 */
WriterFlushTask (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/collector/TimelineCollectorManager.java)/**
   * Task that invokes the flush operation on the timeline writer.
   */
TimelineCollectorManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/collector/TimelineCollectorManager.java)/**
 * Class that manages adding and removing collectors and their lifecycle. It
 * provides thread safety access to the collectors inside.
 *
 */
AboutInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/collector/TimelineCollectorWebService.java)/**
   * Gives information about timeline collector.
   */
TimelineCollectorWebService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/collector/TimelineCollectorWebService.java)/**
 * The main per-node REST end point for timeline service writes. It is
 * essentially a container service that routes requests to the appropriate
 * per-app services.
 */
PerNodeAggTimelineCollectorMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/metrics/PerNodeAggTimelineCollectorMetrics.java)/**
 * Metrics class for TimelineCollectorWebService
 * running on each NM.
 */
TimelineReaderMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/metrics/TimelineReaderMetrics.java)/**
 * Metrics class for TimelineReader.
 */
TimelineCompareFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/filter/TimelineCompareFilter.java)/**
 * Filter class which represents filter to be applied based on key-value pair
 * and the relation between them represented by different relational operators.
 */
TimelineExistsFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/filter/TimelineExistsFilter.java)/**
 * Filter class which represents filter to be applied based on existence of a
 * value.
 */
TimelineFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/filter/TimelineFilter.java)/**
 * Abstract base class extended to implement timeline filters.
 */
TimelineFilterList (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/filter/TimelineFilterList.java)/**
 * Implementation of {@link TimelineFilter} that represents an ordered list of
 * timeline filters which will then be evaluated with a specified boolean
 * operator {@link Operator#AND} or {@link Operator#OR}. Since you can use
 * timeline filter lists as children of timeline filter lists, you can create a
 * hierarchy of filters to be evaluated.
 */
TimelineKeyValueFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/filter/TimelineKeyValueFilter.java)/**
 * Filter class which represents filter to be applied based on key-value pair
 * being equal or not to the values in back-end store.
 */
TimelineKeyValuesFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/filter/TimelineKeyValuesFilter.java)/**
 * Filter class which represents filter to be applied based on multiple values
 * for a key and these values being equal or not equal to values in back-end
 * store.
 */
TimelinePrefixFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/filter/TimelinePrefixFilter.java)/**
 * Filter class which represents filter to be applied based on prefixes.
 * Prefixes can either match or not match.
 */
TimelineReaderAuthenticationFilterInitializer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/security/TimelineReaderAuthenticationFilterInitializer.java)/**
 * Filter initializer to initialize {@link AuthenticationFilter}
 * for ATSv2 timeline reader server with timeline service specific
 * configurations.
 */
TimelineReaderWhitelistAuthorizationFilterInitializer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/security/TimelineReaderWhitelistAuthorizationFilterInitializer.java)/**
 * Filter initializer to initialize
 * {@link TimelineReaderWhitelistAuthorizationFilter} for ATSv2 timeline reader
 * with timeline service specific configurations.
 */
TimelineDataToRetrieve (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineDataToRetrieve.java)/**
 * Encapsulates information regarding which data to retrieve for each entity
 * while querying.<br>
 * Data to retrieve contains the following :<br>
 * <ul>
 * <li><b>confsToRetrieve</b> - Used for deciding which configs to return
 * in response. This is represented as a {@link TimelineFilterList} object
 * containing {@link TimelinePrefixFilter} objects. These can either be
 * exact config keys' or prefixes which are then compared against config
 * keys' to decide configs(inside entities) to return in response. If null
 * or empty, all configurations will be fetched if fieldsToRetrieve
 * contains {@link Field#CONFIGS} or {@link Field#ALL}. This should not be
 * confused with configFilters which is used to decide which entities to
 * return instead.</li>
 * <li><b>metricsToRetrieve</b> - Used for deciding which metrics to return
 * in response. This is represented as a {@link TimelineFilterList} object
 * containing {@link TimelinePrefixFilter} objects. These can either be
 * exact metric ids' or prefixes which are then compared against metric
 * ids' to decide metrics(inside entities) to return in response. If null
 * or empty, all metrics will be fetched if fieldsToRetrieve contains
 * {@link Field#METRICS} or {@link Field#ALL}. This should not be confused
 * with metricFilters which is used to decide which entities to return
 * instead.</li>
 * <li><b>fieldsToRetrieve</b> - Specifies which fields of the entity
 * object to retrieve, see {@link Field}. If null, retrieves 3 fields,
 * namely entity id, entity type and entity created time. All fields will
 * be returned if {@link Field#ALL} is specified.</li>
 * <li><b>metricsLimit</b> - If fieldsToRetrieve contains METRICS/ALL or
 * metricsToRetrieve is specified, this limit defines an upper limit to the
 * number of metrics to return. This parameter is ignored if METRICS are not to
 * be fetched.</li>
 * <li><b>metricsTimeStart</b> - Metric values before this timestamp would not
 * be retrieved. If null or {@literal <0}, defaults to 0.</li>
 * <li><b>metricsTimeEnd</b> - Metric values after this timestamp would not
 * be retrieved. If null or {@literal <0}, defaults to {@link Long#MAX_VALUE}.
 * </li>
 * </ul>
 */
Builder (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineEntityFilters.java)/**
   * A builder class to build an instance of TimelineEntityFilters.
   */
TimelineEntityFilters (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineEntityFilters.java)/**
 * Encapsulates information regarding the filters to apply while querying. These
 * filters restrict the number of entities to return.<br>
 * Filters contain the following :<br>
 * <ul>
 * <li><b>limit</b> - A limit on the number of entities to return. If null or
 * {@literal < 0}, defaults to {@link #DEFAULT_LIMIT}. The maximum possible
 * value for limit can be {@link Long#MAX_VALUE}.</li>
 * <li><b>createdTimeBegin</b> - Matched entities should not be created before
 * this timestamp. If null or {@literal <=0}, defaults to 0.</li>
 * <li><b>createdTimeEnd</b> - Matched entities should not be created after this
 * timestamp. If null or {@literal <=0}, defaults to
 * {@link Long#MAX_VALUE}.</li>
 * <li><b>relatesTo</b> - Matched entities should or should not relate to given
 * entities depending on what's specified in the filter. The entities in
 * relatesTo are identified by entity type and id. This is represented as
 * a {@link TimelineFilterList} object containing
 * {@link TimelineKeyValuesFilter} objects, each of which contains a
 * set of values for a key and the comparison operator (equals/not equals). The
 * key which represents the entity type is a string and values are a set of
 * entity identifiers (also string). As it is a filter list, relatesTo can be
 * evaluated with logical AND/OR and we can create a hierarchy of these
 * {@link TimelineKeyValuesFilter} objects. If null or empty, the relations are
 * not matched.</li>
 * <li><b>isRelatedTo</b> - Matched entities should or should not be related
 * to given entities depending on what's specified in the filter. The entities
 * in isRelatedTo are identified by entity type and id.  This is represented as
 * a {@link TimelineFilterList} object containing
 * {@link TimelineKeyValuesFilter} objects, each of which contains a
 * set of values for a key and the comparison operator (equals/not equals). The
 * key which represents the entity type is a string and values are a set of
 * entity identifiers (also string). As it is a filter list, relatesTo can be
 * evaluated with logical AND/OR and we can create a hierarchy of these
 * {@link TimelineKeyValuesFilter} objects. If null or empty, the relations are
 * not matched.</li>
 * <li><b>infoFilters</b> - Matched entities should have exact matches to
 * the given info and should be either equal or not equal to given value
 * depending on what's specified in the filter. This is represented as a
 * {@link TimelineFilterList} object containing {@link TimelineKeyValueFilter}
 * objects, each of which contains key-value pairs with a comparison operator
 * (equals/not equals). The key which represents the info key is a string but
 * value can be any object. As it is a filter list, info filters can be
 * evaluated with logical AND/OR and we can create a hierarchy of these
 * key-value pairs. If null or empty, the filter is not applied.</li>
 * <li><b>configFilters</b> - Matched entities should have exact matches to
 * the given configurations and should be either equal or not equal to given
 * value depending on what's specified in the filter. This is represented as a
 * {@link TimelineFilterList} object containing {@link TimelineKeyValueFilter}
 * objects, each of which contains key-value pairs with a comparison operator
 * (equals/not equals). Both key (which represents config name) and value (which
 * is config value) are strings. As it is a filter list, config filters can be
 * evaluated with logical AND/OR and we can create a hierarchy of these
 * {@link TimelineKeyValueFilter} objects. If null or empty, the filter is not
 * applied.</li>
 * <li><b>metricFilters</b> - Matched entities should contain the given
 * metrics and satisfy the specified relation with the value. This is
 * represented as a {@link TimelineFilterList} object containing
 * {@link TimelineCompareFilter} objects, each of which contains key-value pairs
 * along with the specified relational/comparison operator represented by
 * {@link TimelineCompareOp}.  The key is a string and value is integer
 * (Short/Integer/Long). As it is a filter list, metric filters can be evaluated
 * with logical AND/OR and we can create a hierarchy of these
 * {@link TimelineCompareFilter} objects. If null or empty, the filter is not
 * applied.</li>
 * <li><b>eventFilters</b> - Matched entities should contain or not contain the
 * given events. This is represented as a {@link TimelineFilterList} object
 * containing {@link TimelineExistsFilter} objects, each of which contains a
 * value which must or must not exist depending on comparison operator specified
 * in the filter. For event filters, the value represents a event id. As it is a
 * filter list, event filters can be evaluated with logical AND/OR and we can
 * create a hierarchy of these {@link TimelineExistsFilter} objects. If null or
 * empty, the filter is not applied.</li>
 * <li><b>fromId</b> - If specified, retrieve the next set of entities from the
 * given fromId. The set of entities retrieved is inclusive of specified fromId.
 * fromId should be taken from the value associated with FROM_ID info key in
 * entity response which was sent earlier.</li>
 * </ul>
 */
TimelineParseConstants (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineParseConstants.java)/**
 * Set of constants used while parsing filter expressions.
 */
TimelineParseException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineParseException.java)/**
 * Exception thrown to indicate that a timeline filter expression cannot be
 * parsed.
 */
TimelineParserForCompareExpr (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineParserForCompareExpr.java)/**
 * Abstract class for parsing compare expressions.
 * Compare expressions are of the form :
 * (&lt;key&gt; &lt;compareop&gt; &lt;value&gt;) &lt;op&gt; (&lt;key
 * &gt; &lt;compareop&gt; &lt;value&gt;)
 * compareop is used to compare value of a the specified key in the backend
 * storage. compareop can be :
 * 1. eq - Equals
 * 2. ne - Not equals (matches if key does not exist)
 * 3. ene - Exists and not equals (key must exist for match to occur)
 * 4. lt - Less than
 * 5. gt - Greater than
 * 6. le - Less than or equals
 * 7. ge - Greater than or equals
 * compareop's supported would depend on implementation. For instance, all
 * the above compareops' will be supported for metric filters but only eq,ne and
 * ene would be supported for KV filters like config/info filters.
 *
 * op is a logical operator and can be either AND or OR.
 *
 * The way values will be interpreted would also depend on implementation
 *
 * A typical compare expression would look as under:
 * ((key1 eq val1 OR key2 ne val2) AND (key5 gt val45))
 */
TimelineParserForDataToRetrieve (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineParserForDataToRetrieve.java)/**
 * Used for parsing metrics or configs to retrieve.
 */
TimelineParserForEqualityExpr (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineParserForEqualityExpr.java)/**
 * Abstract class for parsing equality expressions. This means the values in
 * expression would either be equal or not equal.
 * Equality expressions are of the form :
 * (&lt;value&gt;,&lt;value&gt;,&lt;value&gt;) &lt;op&gt; !(&lt;value&gt;,
 * &lt;value&gt;)
 *
 * Here, "!" means all the values should not exist/should not be equal.
 * If not specified, they should exist/be equal.
 *
 * op is a logical operator and can be either AND or OR.
 *
 * The way values will be interpreted would also depend on implementation.
 *
 * For instance for event filters this expression may look like,
 * (event1,event2) AND !(event3,event4)
 * This means for an entity to match, event1 and event2 should exist. But event3
 * and event4 should not exist.
 */
TimelineParserForExistFilters (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineParserForExistFilters.java)/**
 * Used for parsing existence filters such as event filters. These filters
 * check for existence of a value. For example, in case of event filters, they
 * check if an event exists or not and accordingly return an entity.
 */
TimelineParserForKVFilters (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineParserForKVFilters.java)/**
 * Used for parsing key-value filters such as config and info filters.
 */
TimelineParserForNumericFilters (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineParserForNumericFilters.java)/**
 * Used for parsing numerical filters such as metric filters.
 */
TimelineParserForRelationFilters (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineParserForRelationFilters.java)/**
 * Used for parsing relation filters.
 */
TimelineReaderContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderContext.java)/**
 * Encapsulates fields necessary to make a query in timeline reader.
 */
TimelineReaderManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderManager.java)/**
 * This class wraps over the timeline reader store implementation. It does some
 * non trivial manipulation of the timeline data before or after getting
 * it from the backend store.
 */
TimelineReaderServer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderServer.java)/** Main class for Timeline Reader. */
TimelineReaderUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderUtils.java)/**
 * Set of utility methods to be used across timeline reader.
 */
TimelineReaderWebServices (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderWebServices.java)/** REST end point for Timeline Reader. */
TimelineReaderWebServicesUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderWebServicesUtils.java)/**
 * Set of utility methods to be used by timeline reader web services.
 */
CollectorNodemanagerSecurityInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/security/CollectorNodemanagerSecurityInfo.java)/**
 * SecurityInfo implementation for CollectorNodemanager protocol.
 */
TimelineV2DelegationTokenSecretManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/security/TimelineV2DelegationTokenSecretManagerService.java)/**
   * Delegation token secret manager for ATSv2.
   */
TimelineV2DelegationTokenSecretManagerService (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/security/TimelineV2DelegationTokenSecretManagerService.java)/**
 * The service wrapper of {@link TimelineV2DelegationTokenSecretManager}.
 */
OfflineAggregationInfo (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/OfflineAggregationInfo.java)/**
 * Class to carry the offline aggregation information for storage level
 * implementations. There are currently two predefined aggregation info
 * instances that represent flow and user level offline aggregations. Depend on
 * its implementation, a storage class may use an OfflineAggregationInfo object
 * to decide behaviors dynamically.
 */
TimelineStorageUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/TimelineStorageUtils.java)/**
 * A bunch of utility functions used across TimelineReader and TimelineWriter.
 */
FileSystemTimelineReaderImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FileSystemTimelineReaderImpl.java)/**
 *  File System based implementation for TimelineReader. This implementation may
 *  not provide a complete implementation of all the necessary features. This
 *  implementation is provided solely for basic testing purposes, and should not
 *  be used in a non-test situation.
 */
FileSystemTimelineWriterImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FileSystemTimelineWriterImpl.java)/**
 * This implements a FileSystem based backend for storing application timeline
 * information. This implementation may not provide a complete implementation of
 * all the necessary features. This implementation is provided solely for basic
 * testing purposes, and should not be used in a non-test situation.
 */
OfflineAggregationWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/OfflineAggregationWriter.java)/**
 * YARN timeline service v2 offline aggregation storage interface.
 */
SchemaCreator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/SchemaCreator.java)/**
 * This interface is for creating Timeline Schema. The backend for Timeline
 * Service have to implement this.
 */
TimelineReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/TimelineReader.java)/** ATSv2 reader interface. */
TimelineSchemaCreator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/TimelineSchemaCreator.java)/**
 * This creates the timeline schema for storing application timeline
 * information. Each backend has to implement the {@link SchemaCreator} for
 * creating the schema in its backend and should be configured in yarn-site.xml.
 */
TimelineStorageMonitor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/TimelineStorageMonitor.java)/**
 * This abstract class is for monitoring Health of Timeline Storage.
 */
TimelineWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/TimelineWriter.java)/**
 * This interface is for storing application timeline information.
 */
TimelineContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/TimelineContext.java)/**
 * Encapsulates timeline context information.
 */
TestPerNodeAggTimelineCollectorMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/collector/TestPerNodeAggTimelineCollectorMetrics.java)/**
 * Test PerNodeAggTimelineCollectorMetrics.
 */
TestTimelineCollectorManager (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/collector/TestTimelineCollectorManager.java)/**
 * Unit tests for TimelineCollectorManager.
 */
TestTimelineReaderMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderMetrics.java)/**
 * Test TimelineReaderMetrics.
 */
TestTimelineReaderWebServicesACL (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesACL.java)/**
 * Tests ACL check while retrieving entity-types per application.
 */
TestTimelineReaderWhitelistAuthorizationFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWhitelistAuthorizationFilter.java)/**
 * Unit tests for {@link TimelineReaderWhitelistAuthorizationFilter}.
 *
 */
DummyTimelineSchemaCreator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/DummyTimelineSchemaCreator.java)/**
 * Dummy Implementation of {@link SchemaCreator} for test.
 */
TestTimelineSchemaCreator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestTimelineSchemaCreator.java)/**
 * Test cases for {@link TimelineSchemaCreator}.
 */
TimelineEntityDocument (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/collection/document/entity/TimelineEntityDocument.java)/**
 * This is a generic class which contains all the meta information of some
 * conceptual entity and its related events. The timeline entity can be an
 * application, an attempt, a container or whatever the user-defined object.
 */
TimelineEventSubDoc (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/collection/document/entity/TimelineEventSubDoc.java)/**
 * This class represents a Sub Document for {@link TimelineEvent}
 * when creating a new {@link TimelineEntityDocument}.
 */
TimelineMetricSubDoc (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/collection/document/entity/TimelineMetricSubDoc.java)/**
 * This class represents a Sub Document for {@link TimelineMetric} that will be
 * used when creating new {@link TimelineEntityDocument}.
 */
FlowActivityDocument (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/collection/document/flowactivity/FlowActivityDocument.java)/**
 * This doc represents the {@link FlowActivityEntity} which is used for
 * showing all the flow runs with limited information.
 */
FlowActivitySubDoc (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/collection/document/flowactivity/FlowActivitySubDoc.java)/**
 * This is a sub doc which represents each flow.
 */
FlowRunDocument (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/collection/document/flowrun/FlowRunDocument.java)/**
 * This doc represents the flow run information for every job.
 */
NoDocumentFoundException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/collection/document/NoDocumentFoundException.java)/**
 * Indicates that the document that was requested is not found from the
 * Document Store. This is a generic exception that will be thrown for all
 * the {@link DocumentStoreVendor} if there is no document while reading.
 */
TimelineDocument (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/collection/document/TimelineDocument.java)/**
 * This is an interface for all the Timeline Documents. Any new document that
 * has to be persisted in the document store should implement this.
 */
DocumentStoreCollectionCreator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/DocumentStoreCollectionCreator.java)/**
 * This creates the Collection for a {@link DocumentStoreVendor} backend
 * configured for storing  application timeline information.
 */
DocumentStoreTimelineReaderImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/DocumentStoreTimelineReaderImpl.java)/**
 * This is a generic document store timeline reader for reading the timeline
 * entity information. Based on the {@link DocumentStoreVendor} that is
 * configured, the  documents are read from that backend.
 */
DocumentStoreTimelineWriterImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/DocumentStoreTimelineWriterImpl.java)/**
 * This is a generic document store timeline writer for storing the timeline
 * entity information. Based on the {@link DocumentStoreVendor} that is
 * configured, the documents are written to that backend.
 */
DocumentStoreUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/DocumentStoreUtils.java)/**
 * This class consists of all the utils required for reading or writing
 * documents for a {@link DocumentStoreVendor}.
 */
DocumentStoreFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/lib/DocumentStoreFactory.java)/**
 * Factory methods for instantiating a timeline Document Store reader or
 * writer. Based on the {@link DocumentStoreVendor} that is configured,
 * appropriate reader or writer would be instantiated.
 */
DocumentStoreNotSupportedException (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/lib/DocumentStoreNotSupportedException.java)/**
 * Indicates that the document store vendor that was
 * configured does not belong to one of the {@link DocumentStoreVendor}.
 */
CosmosDBDocumentStoreReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/reader/cosmosdb/CosmosDBDocumentStoreReader.java)/**
 * This is the Document Store Reader implementation for
 * {@link DocumentStoreVendor#COSMOS_DB}.
 */
DocumentStoreReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/reader/DocumentStoreReader.java)/**
 * Every {@link DocumentStoreVendor} have to implement this for creating
 * reader to its backend.
 */
TimelineCollectionReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/reader/TimelineCollectionReader.java)/**
 * This is a generic Collection reader for reading documents belonging to a
 * {@link CollectionType} under a specific {@link DocumentStoreVendor} backend.
 */
CosmosDBDocumentStoreWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/writer/cosmosdb/CosmosDBDocumentStoreWriter.java)/**
 * This is the Document Store Writer implementation for
 * {@link DocumentStoreVendor#COSMOS_DB}.
 */
DocumentStoreWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/writer/DocumentStoreWriter.java)/**
 * Every {@link DocumentStoreVendor} have to implement this for creating
 * writer to its backend.
 */
TimelineCollectionWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/main/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/writer/TimelineCollectionWriter.java)/**
 * This is a generic Collection Writer that can be used for writing documents
 * belonging to different {@link CollectionType} under a specific
 * {@link DocumentStoreVendor} backend.
 */
TestDocumentOperations (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/test/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/collection/TestDocumentOperations.java)/**
 * Timeline Entity Document merge and aggregation test.
 */
DocumentStoreTestUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/test/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/DocumentStoreTestUtils.java)/**
 * This is util class for baking sample TimelineEntities data for test.
 */
JsonUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/test/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/JsonUtils.java)/**
 * A simple util class for Json SerDe.
 */
TestCosmosDBDocumentStoreReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/test/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/reader/cosmosdb/TestCosmosDBDocumentStoreReader.java)/**
 * Test case for {@link CosmosDBDocumentStoreReader}.
 */
DummyDocumentStoreReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/test/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/reader/DummyDocumentStoreReader.java)/**
 * Dummy Document Store Reader for mocking backend calls for unit test.
 */
TestDocumentStoreCollectionCreator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/test/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/TestDocumentStoreCollectionCreator.java)/**
 * Test case for ${@link DocumentStoreCollectionCreator}.
 */
TestDocumentStoreTimelineReaderImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/test/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/TestDocumentStoreTimelineReaderImpl.java)/**
 * Test case for {@link DocumentStoreTimelineReaderImpl}.
 */
TestDocumentStoreTimelineWriterImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/test/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/TestDocumentStoreTimelineWriterImpl.java)/**
 * Test case for {@link DocumentStoreTimelineWriterImpl}.
 */
MockedCosmosDBDocumentStoreWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/test/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/writer/cosmosdb/MockedCosmosDBDocumentStoreWriter.java)/**
 * This is a mocked class for {@link CosmosDBDocumentStoreWriter}.
 */
TestCosmosDBDocumentStoreWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/test/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/writer/cosmosdb/TestCosmosDBDocumentStoreWriter.java)/**
 * Test case for {@link CosmosDBDocumentStoreWriter}.
 */
DummyDocumentStoreWriter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/src/test/java/org/apache/hadoop/yarn/server/timelineservice/documentstore/writer/DummyDocumentStoreWriter.java)/**
 * Dummy Document Store Writer for mocking backend calls for unit test.
 */
TimelineFilterUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/filter/TimelineFilterUtils.java)/**
 * Set of utility methods used by timeline filter classes.
 */
ApplicationTableRW (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/application/ApplicationTableRW.java)/**
 * Create, read and write to the Application Table.
 */
AppToFlowTableRW (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/apptoflow/AppToFlowTableRW.java)/**
 * Create, read and write to the AppToFlow Table.
 */
BaseTableRW (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/BaseTableRW.java)/**
 * Implements behavior common to tables used in the timeline service storage. It
 * is thread-safe, and can be used by multiple threads concurrently.
 *
 * @param <T> reference to the table instance class itself for type safety.
 */
ColumnRWHelper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/ColumnRWHelper.java)/**
 * A set of utility functions that read or read to a column.
 * This class is meant to be used only by explicit Columns,
 * and not directly to write by clients.
 */
HBaseTimelineStorageUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/HBaseTimelineStorageUtils.java)/**
 * A bunch of utility functions used in HBase TimelineService backend.
 */
TimelineHBaseSchemaConstants (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/TimelineHBaseSchemaConstants.java)/**
 * contains the constants used in the context of schema accesses for
 * {@link org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity}
 * information.
 */
TypedBufferedMutator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/TypedBufferedMutator.java)/**
 * To be used to wrap an actual {@link BufferedMutator} in a type safe manner.
 *
 * @param <T> The class referring to the table to be written to.
 */
DomainTableRW (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/domain/DomainTableRW.java)/**
 * Create, read and write to the domain Table.
 */
EntityTableRW (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityTableRW.java)/**
 * Create, read and write to the Entity Table.
 */
FlowActivityTableRW (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowActivityTableRW.java)/**
 * Create, read and write to the FlowActivity Table.
 */
FlowRunTableRW (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowRunTableRW.java)/**
 * Create, read and write to the FlowRun table.
 */
HBaseStorageMonitor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/HBaseStorageMonitor.java)/**
 * HBase based implementation for {@link TimelineStorageMonitor}.
 */
HBaseTimelineReaderImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/HBaseTimelineReaderImpl.java)/**
 * HBase based implementation for {@link TimelineReader}.
 */
HBaseTimelineSchemaCreator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/HBaseTimelineSchemaCreator.java)/**
 * This creates the schema for a hbase based backend for storing application
 * timeline information.
 */
HBaseTimelineWriterImpl (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/HBaseTimelineWriterImpl.java)/**
 * This implements a hbase based backend for storing the timeline entity
 * information.
 * It writes to multiple tables at the backend
 */
FlowContext (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/AbstractTimelineStorageReader.java)/**
   * Encapsulates flow context information.
   */
AbstractTimelineStorageReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/AbstractTimelineStorageReader.java)/**
 * The base class for reading timeline data from the HBase storage. This class
 * provides basic support to validate and augment reader context.
 */
ApplicationEntityReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/ApplicationEntityReader.java)/**
 * Timeline entity reader for application entities that are stored in the
 * application table.
 */
EntityTypeReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/EntityTypeReader.java)/**
 * Timeline entity reader for listing all available entity types given one
 * reader context. Right now only supports listing all entity types within one
 * YARN application.
 */
FlowActivityEntityReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowActivityEntityReader.java)/**
 * Timeline entity reader for flow activity entities that are stored in the
 * flow activity table.
 */
FlowRunEntityReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/FlowRunEntityReader.java)/**
 * Timeline entity reader for flow run entities that are stored in the flow run
 * table.
 */
GenericEntityReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/GenericEntityReader.java)/**
 * Timeline entity reader for generic entities that are stored in the entity
 * table.
 */
TimelineEntityReader (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/TimelineEntityReader.java)/**
 * The base class for reading and deserializing timeline entities from the
 * HBase storage. Different types can be defined for different types of the
 * entities that are being requested.
 */
TimelineEntityReaderFactory (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/TimelineEntityReaderFactory.java)/**
 * Factory methods for instantiating a timeline entity reader.
 */
SubApplicationTableRW (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/subapplication/SubApplicationTableRW.java)/**
 * Create, read and write to the SubApplication table.
 */
TestHBaseTimelineStorageUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/TestHBaseTimelineStorageUtils.java)/**
 * Unit tests for HBaseTimelineStorageUtils static methos.
 */
ApplicationRowKeyConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/application/ApplicationRowKey.java)/**
   * Encodes and decodes row key for application table. The row key is of the
   * form: clusterId!userName!flowName!flowRunId!appId. flowRunId is a long,
   * appId is encoded and decoded using {@link AppIdKeyConverter} and rest are
   * strings.
   * <p>
   */
ApplicationRowKey (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/application/ApplicationRowKey.java)/**
 * Represents a rowkey for the application table.
 */
ApplicationRowKeyPrefix (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/application/ApplicationRowKeyPrefix.java)/**
 * Represents a partial rowkey (without flowName or without flowName and
 * flowRunId) for the application table.
 */
ApplicationTable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/application/ApplicationTable.java)/**
 * The application table as column families info, config and metrics. Info
 * stores information about a YARN application entity, config stores
 * configuration data of a YARN application, metrics stores the metrics of a
 * YARN application. This table is entirely analogous to the entity table but
 * created for better performance.
 *
 * Example application table record:
 *
 * <pre>
 * |-------------------------------------------------------------------------|
 * |  Row       | Column Family                | Column Family| Column Family|
 * |  key       | info                         | metrics      | config       |
 * |-------------------------------------------------------------------------|
 * | clusterId! | id:appId                     | metricId1:   | configKey1:  |
 * | userName!  |                              | metricValue1 | configValue1 |
 * | flowName!  | created_time:                | @timestamp1  |              |
 * | flowRunId! | 1392993084018                |              | configKey2:  |
 * | AppId      |                              | metriciD1:   | configValue2 |
 * |            | i!infoKey:                   | metricValue2 |              |
 * |            | infoValue                    | @timestamp2  |              |
 * |            |                              |              |              |
 * |            | r!relatesToKey:              | metricId2:   |              |
 * |            | id3=id4=id5                  | metricValue1 |              |
 * |            |                              | @timestamp2  |              |
 * |            | s!isRelatedToKey:            |              |              |
 * |            | id7=id9=id6                  |              |              |
 * |            |                              |              |              |
 * |            | e!eventId=timestamp=infoKey: |              |              |
 * |            | eventInfoValue               |              |              |
 * |            |                              |              |              |
 * |            | flowVersion:                 |              |              |
 * |            | versionValue                 |              |              |
 * |-------------------------------------------------------------------------|
 * </pre>
 */
AppToFlowRowKey (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/apptoflow/AppToFlowRowKey.java)/**
 * Represents a row key for the app_flow table, which is the app id.
 */
AppToFlowTable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/apptoflow/AppToFlowTable.java)/**
 * The app_flow table as column families mapping. Mapping stores
 * appId to flowName and flowRunId mapping information
 *
 * Example app_flow table record:
 *
 * <pre>
 * |--------------------------------------|
 * |  Row       | Column Family           |
 * |  key       | mapping                 |
 * |--------------------------------------|
 * | appId      | flow_name!cluster1:     |
 * |            | foo@daily_hive_report   |
 * |            |                         |
 * |            | flow_run_id!cluster1:   |
 * |            | 1452828720457           |
 * |            |                         |
 * |            | user_id!cluster1:       |
 * |            | admin                   |
 * |            |                         |
 * |            | flow_name!cluster2:     |
 * |            | bar@ad_hoc_query        |
 * |            |                         |
 * |            | flow_run_id!cluster2:   |
 * |            | 1452828498752           |
 * |            |                         |
 * |            | user_id!cluster2:       |
 * |            | joe                     |
 * |            |                         |
 * |--------------------------------------|
 * </pre>
 *
 * It is possible (although unlikely) in a multi-cluster environment that there
 * may be more than one applications for a given app id. Different clusters are
 * recorded as different sets of columns.
 */
AppIdKeyConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/AppIdKeyConverter.java)/**
 * Encodes and decodes {@link ApplicationId} for row keys.
 * App ID is stored in row key as 12 bytes, cluster timestamp section of app id
 * (long - 8 bytes) followed by sequence id section of app id (int - 4 bytes).
 */
BaseTable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/BaseTable.java)/**
 * The base type of tables.
 * @param T table type
 */
Column (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/Column.java)/**
 * A Column represents the way to store a fully qualified column in a specific
 * table.
 */
ColumnFamily (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/ColumnFamily.java)/**
 * Type safe column family.
 *
 * @param <T> refers to the table for which this column family is used for.
 */
ColumnHelper (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/ColumnHelper.java)/**
 * This class is meant to be used only by explicit Columns, and not directly to
 * write by clients.
 */
ColumnPrefix (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/ColumnPrefix.java)/**
 * Used to represent a partially qualified column, where the actual column name
 * will be composed of a prefix and the remainder of the column qualifier. The
 * prefix can be null, in which case the column qualifier will be completely
 * determined when the values are stored.
 */
EventColumnName (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/EventColumnName.java)/**
 * Encapsulates information about Event column names for application and entity
 * tables. Used while encoding/decoding event column names.
 */
EventColumnNameConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/EventColumnNameConverter.java)/**
 * Encodes and decodes event column names for application and entity tables.
 * The event column name is of the form : eventId=timestamp=infokey.
 * If info is not associated with the event, event column name is of the form :
 * eventId=timestamp=
 * Event timestamp is long and rest are strings.
 * Column prefixes are not part of the eventcolumn name passed for encoding. It
 * is added later, if required in the associated ColumnPrefix implementations.
 */
GenericConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/GenericConverter.java)/**
 * Uses GenericObjectMapper to encode objects as bytes and decode bytes as
 * objects.
 */
HBaseTimelineSchemaUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/HBaseTimelineSchemaUtils.java)/**
 * A bunch of utility functions used in HBase TimelineService common module.
 */
KeyConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/KeyConverter.java)/**
 * Interface which has to be implemented for encoding and decoding row keys and
 * columns.
 */
KeyConverterToString (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/KeyConverterToString.java)/**
 * Interface which has to be implemented for encoding and decoding row keys or
 * column qualifiers as string.
 */
LongConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/LongConverter.java)/**
 * Encodes a value by interpreting it as a Long and converting it to bytes and
 * decodes a set of bytes as a Long.
 */
LongKeyConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/LongKeyConverter.java)/**
 * Encodes and decodes column names / row keys which are long.
 */
NumericValueConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/NumericValueConverter.java)/**
 * Extends ValueConverter interface for numeric converters to support numerical
 * operations such as comparison, addition, etc.
 */
Range (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/Range.java)/**
 * Encapsulates a range with start and end indices.
 */
RowKeyPrefix (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/RowKeyPrefix.java)/**
 * In queries where a single result is needed, an exact rowkey can be used
 * through the corresponding rowkey#getRowKey() method. For queries that need to
 * scan over a range of rowkeys, a partial (the initial part) of rowkeys are
 * used. Classes implementing RowKeyPrefix indicate that they are the initial
 * part of rowkeys, with different constructors with fewer number of argument to
 * form a partial rowkey, a prefix.
 *
 * @param <R> indicating the type of rowkey that a particular implementation is
 *          a prefix for.
 */
StringKeyConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/StringKeyConverter.java)/**
 * Encodes and decodes column names / row keys which are merely strings.
 * Column prefixes are not part of the column name passed for encoding. It is
 * added later, if required in the associated ColumnPrefix implementations.
 */
TimestampGenerator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/TimestampGenerator.java)/**
 * Utility class that allows HBase coprocessors to interact with unique
 * timestamps.
 */
ValueConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/ValueConverter.java)/**
 * Converter used to encode/decode value associated with a column prefix or a
 * column.
 */
DomainRowKeyConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/domain/DomainRowKey.java)/**
   * Encodes and decodes row key for the domain table.
   * The row key is of the
   * form : domainId
   * <p>
   */
DomainRowKey (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/domain/DomainRowKey.java)/**
 * Represents a row key for the domain table, which is the
 * cluster ! domain id.
 */
DomainTable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/domain/DomainTable.java)/**
 * The domain table has column family info. Info stores
 * information about a timeline domain object
 *
 * Example domain table record:
 *
 * <pre>
 * |-------------------------------------------|
 * |  Row       | Column Family                |
 * |  key       | info                         |
 * |-------------------------------------------|
 * | clusterId! | created_time:1521676928000   |
 * | domainI    | description: "domain         |
 * |            | information for XYZ job"     |
 * |            | owners: "user1, yarn"        |
 * |            | readers:                     |
 * |            | "user2,user33 yarn,group2"   |
 * |            |                              |
 * |-------------------------------------------|
 * </pre>
 */
EntityRowKeyConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityRowKey.java)/**
   * Encodes and decodes row key for entity table. The row key is of the form :
   * userName!clusterId!flowName!flowRunId!appId!entityType!entityId. flowRunId
   * is a long, appId is encoded/decoded using {@link AppIdKeyConverter} and
   * rest are strings.
   * <p>
   */
EntityRowKey (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityRowKey.java)/**
 * Represents a rowkey for the entity table.
 */
EntityRowKeyPrefix (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityRowKeyPrefix.java)/**
 * Represents a partial rowkey without the entityId or without entityType and
 * entityId for the entity table.
 *
 */
EntityTable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityTable.java)/**
 * The entity table as column families info, config and metrics. Info stores
 * information about a timeline entity object config stores configuration data
 * of a timeline entity object metrics stores the metrics of a timeline entity
 * object
 *
 * Example entity table record:
 *
 * <pre>
 * |-------------------------------------------------------------------------|
 * |  Row       | Column Family                | Column Family| Column Family|
 * |  key       | info                         | metrics      | config       |
 * |-------------------------------------------------------------------------|
 * | userName!  | id:entityId                  | metricId1:   | configKey1:  |
 * | clusterId! |                              | metricValue1 | configValue1 |
 * | flowName!  | type:entityType              | @timestamp1  |              |
 * | flowRunId! |                              |              | configKey2:  |
 * | AppId!     | created_time:                | metricId1:   | configValue2 |
 * | entityType!| 1392993084018                | metricValue2 |              |
 * | idPrefix!  |                              | @timestamp2  |              |
 * | entityId   | i!infoKey:                   |              |              |
 * |            | infoValue                    | metricId1:   |              |
 * |            |                              | metricValue1 |              |
 * |            | r!relatesToKey:              | @timestamp2  |              |
 * |            | id3=id4=id5                  |              |              |
 * |            |                              |              |              |
 * |            | s!isRelatedToKey             |              |              |
 * |            | id7=id9=id6                  |              |              |
 * |            |                              |              |              |
 * |            | e!eventId=timestamp=infoKey: |              |              |
 * |            | eventInfoValue               |              |              |
 * |            |                              |              |              |
 * |            | flowVersion:                 |              |              |
 * |            | versionValue                 |              |              |
 * |-------------------------------------------------------------------------|
 * </pre>
 */
Attribute (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/Attribute.java)/**
 * Defines the attribute tuple to be set for puts into the {@link FlowRunTable}.
 */
FlowActivityRowKeyConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowActivityRowKey.java)/**
   * Encodes and decodes row key for flow activity table. The row key is of the
   * form : clusterId!dayTimestamp!user!flowName. dayTimestamp(top of the day
   * timestamp) is a long and rest are strings.
   * <p>
   */
FlowActivityRowKey (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowActivityRowKey.java)/**
 * Represents a rowkey for the flow activity table.
 */
FlowActivityRowKeyPrefix (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowActivityRowKeyPrefix.java)/**
 * A prefix partial rowkey for flow activities.
 */
FlowActivityTable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowActivityTable.java)/**
 * The flow activity table has column family info
 * Stores the daily activity record for flows
 * Useful as a quick lookup of what flows were
 * running on a given day
 *
 * Example flow activity table record:
 *
 * <pre>
 * |-------------------------------------------|
 * |  Row key   | Column Family                |
 * |            | info                         |
 * |-------------------------------------------|
 * | clusterId! | r!runid1:version1            |
 * | inv Top of |                              |
 * | Day!       | r!runid2:version7            |
 * | userName!  |                              |
 * | flowName   |                              |
 * |-------------------------------------------|
 * </pre>
 */
FlowRunRowKeyConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowRunRowKey.java)/**
   * Encodes and decodes row key for flow run table.
   * The row key is of the form : clusterId!userId!flowName!flowrunId.
   * flowrunId is a long and rest are strings.
   * <p>
   */
FlowRunRowKey (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowRunRowKey.java)/**
 * Represents a rowkey for the flow run table.
 */
FlowRunRowKeyPrefix (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowRunRowKeyPrefix.java)/**
 * Represents a partial rowkey (without the flowRunId) for the flow run table.
 */
FlowRunTable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowRunTable.java)/**
 * The flow run table has column family info
 * Stores per flow run information
 * aggregated across applications.
 *
 * Metrics are also stored in the info column family.
 *
 * Example flow run table record:
 *
 * <pre>
 * flow_run table
 * |-------------------------------------------|
 * |  Row key   | Column Family                |
 * |            | info                         |
 * |-------------------------------------------|
 * | clusterId! | flow_version:version7        |
 * | userName!  |                              |
 * | flowName!  | running_apps:1               |
 * | flowRunId  |                              |
 * |            | min_start_time:1392995080000 |
 * |            | #0:""                        |
 * |            |                              |
 * |            | min_start_time:1392995081012 |
 * |            | #0:appId2                    |
 * |            |                              |
 * |            | min_start_time:1392993083210 |
 * |            | #0:appId3                    |
 * |            |                              |
 * |            |                              |
 * |            | max_end_time:1392993084018   |
 * |            | #0:""                        |
 * |            |                              |
 * |            |                              |
 * |            | m!mapInputRecords:127        |
 * |            | #0:""                        |
 * |            |                              |
 * |            | m!mapInputRecords:31         |
 * |            | #2:appId2                    |
 * |            |                              |
 * |            | m!mapInputRecords:37         |
 * |            | #1:appId3                    |
 * |            |                              |
 * |            |                              |
 * |            | m!mapOutputRecords:181       |
 * |            | #0:""                        |
 * |            |                              |
 * |            | m!mapOutputRecords:37        |
 * |            | #1:appId3                    |
 * |            |                              |
 * |            |                              |
 * |-------------------------------------------|
 * </pre>
 */
SubApplicationRowKeyConverter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/subapplication/SubApplicationRowKey.java)/**
   * Encodes and decodes row key for sub app table.
   * The row key is of the form :
   * subAppUserId!clusterId!flowRunId!appId!entityType!entityId!userId
   *
   * subAppUserId is usually the doAsUser.
   * userId is the yarn user that the AM runs as.
   *
   * <p>
   */
SubApplicationRowKey (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/subapplication/SubApplicationRowKey.java)/**
 * Represents a rowkey for the sub app table.
 */
SubApplicationRowKeyPrefix (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/subapplication/SubApplicationRowKeyPrefix.java)/**
 * Represents a partial rowkey without the entityId or without entityType and
 * entityId for the sub application table.
 *
 */
SubApplicationTable (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/subapplication/SubApplicationTable.java)/**
 * The sub application table has column families:
 * info, config and metrics.
 * Info stores information about a timeline entity object
 * config stores configuration data of a timeline entity object
 * metrics stores the metrics of a timeline entity object
 *
 * Example sub application table record:
 *
 * <pre>
 * |-------------------------------------------------------------------------|
 * |  Row          | Column Family             | Column Family| Column Family|
 * |  key          | info                      | metrics      | config       |
 * |-------------------------------------------------------------------------|
 * | subAppUserId! | id:entityId               | metricId1:   | configKey1:  |
 * | clusterId!    | type:entityType           | metricValue1 | configValue1 |
 * | entityType!   |                           | @timestamp1  |              |
 * | idPrefix!|    |                           |              | configKey2:  |
 * | entityId!     | created_time:             | metricId1:   | configValue2 |
 * | userId        | 1392993084018             | metricValue2 |              |
 * |               |                           | @timestamp2  |              |
 * |               | i!infoKey:                |              |              |
 * |               | infoValue                 | metricId1:   |              |
 * |               |                           | metricValue1 |              |
 * |               |                           | @timestamp2  |              |
 * |               | e!eventId=timestamp=      |              |              |
 * |               | infoKey:                  |              |              |
 * |               | eventInfoValue            |              |              |
 * |               |                           |              |              |
 * |               | r!relatesToKey:           |              |              |
 * |               | id3=id4=id5               |              |              |
 * |               |                           |              |              |
 * |               | s!isRelatedToKey          |              |              |
 * |               | id7=id9=id6               |              |              |
 * |               |                           |              |              |
 * |               | flowVersion:              |              |              |
 * |               | versionValue              |              |              |
 * |-------------------------------------------------------------------------|
 * </pre>
 */
TestCustomApplicationIdConversion (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/TestCustomApplicationIdConversion.java)/**
 * Test for HBaseTimelineStorageUtils.convertApplicationIdToString(),
 * a custom conversion from ApplicationId to String that avoids the
 * incompatibility issue caused by mixing hadoop-common 2.5.1 and
 * hadoop-yarn-api 3.0. See YARN-6905.
 */
TestKeyConverters (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/TestKeyConverters.java)/**
 * Unit tests for key converters for various tables' row keys.
 *
 */
TestRowKeys (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/TestRowKeys.java)/**
 * Class to test the row key structures for various tables.
 *
 */
TestRowKeysAsString (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-common/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/TestRowKeysAsString.java)/**
 * Test for row key as string.
 */
HBaseTimelineServerUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-1/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/HBaseTimelineServerUtils.java)/**
 * A utility class used by hbase-server module.
 */
FlowRunCoprocessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-1/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowRunCoprocessor.java)/**
 * Coprocessor for flow run table.
 */
FlowScanner (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-1/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java)/**
 * Invoked via the coprocessor when a Get or a Scan is issued for flow run
 * table. Looks through the list of cells per row, checks their tags and does
 * operation on those cells as per the cell tags. Transforms reads of the stored
 * metrics into calculated sums for each column Also, finds the min and max for
 * start and end times in a flow run.
 */
HBaseTimelineServerUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-2/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/common/HBaseTimelineServerUtils.java)/**
 * A utility class used by hbase-server module.
 */
FlowRunCoprocessor (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-2/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowRunCoprocessor.java)/**
 * Coprocessor for flow run table.
 */
FlowScanner (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-server/hadoop-yarn-server-timelineservice-hbase-server-2/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/FlowScanner.java)/**
 * Invoked via the coprocessor when a Get or a Scan is issued for flow run
 * table. Looks through the list of cells per row, checks their tags and does
 * operation on those cells as per the cell tags. Transforms reads of the stored
 * metrics into calculated sums for each column Also, finds the min and max for
 * start and end times in a flow run.
 */
AbstractTimelineReaderHBaseTestBase (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/AbstractTimelineReaderHBaseTestBase.java)/**
 * Test Base for TimelineReaderServer HBase tests.
 */
DummyTimelineReaderMetrics (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/DummyTimelineReaderMetrics.java)/**
 * DummyTimelineReaderMetrics for mocking {@link TimelineReaderMetrics} calls.
 */
TestTimelineReaderWebServicesHBaseStorage (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesHBaseStorage.java)/**
 * Test TimelineReder Web Service REST API's.
 */
DataGeneratorForTest (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/DataGeneratorForTest.java)/**
 * Utility class that creates the schema and generates test data.
 */
TestFlowDataGenerator (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestFlowDataGenerator.java)/**
 * Generates the data/entities for the FlowRun and FlowActivity Tables.
 */
TestHBaseStorageFlowActivity (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowActivity.java)/**
 * Tests the FlowRun and FlowActivity Tables.
 */
TestHBaseStorageFlowRun (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowRun.java)/**
 * Tests the FlowRun and FlowActivity Tables.
 */
TestHBaseStorageFlowRunCompaction (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowRunCompaction.java)/**
 * Tests the FlowRun and FlowActivity Tables.
 */
TestHBaseTimelineStorageApps (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageApps.java)/**
 * Tests for apps stored in TimelineStorage.
 */
TestHBaseTimelineStorageDomain (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageDomain.java)/**
 * Test for timeline domain.
 */
TestHBaseTimelineStorageEntities (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageEntities.java)/**
 * Various tests to test writing entities to HBase and reading them back from
 * it.
 *
 * It uses a single HBase mini-cluster for all tests which is a little more
 * realistic, and helps test correctness in the presence of other data.
 *
 * Each test uses a different cluster name to be able to handle its own data
 * even if other records exist in the table. Use a different cluster name if
 * you add a new test.
 */
TestHBaseTimelineStorageSchema (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageSchema.java)/**
 * Unit tests for checking different schema prefixes.
 */
TestTimelineWriterHBaseDown (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestTimelineWriterHBaseDown.java)/**
 * This class tests HbaseTimelineWriter with Hbase Down.
 */
AppReportFetcher (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/AppReportFetcher.java)/**
 * This class abstracts away how ApplicationReports are fetched.
 */
ProxyCA (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/ProxyCA.java)/**
 * Allows for the generation and acceptance of specialized HTTPS Certificates to
 * be used for HTTPS communication between the AMs and the RM Proxy.
 */
ProxyUtils (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/ProxyUtils.java)/**
 * Class containing general purpose proxy utilities
 */
WebAppProxyServer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServer.java)/**
 * ProxyServer will sit in between the end user and AppMaster
 * web interfaces.
 */
__ (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java)/**
   * Empty Hamlet class.
   */
TestAmFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/amfilter/TestAmFilter.java)/**
 * Test AmIpFilter. Requests to a no declared hosts should has way through
 * proxy. Another requests can be filtered with (without) user name.
 * 
 */
TestAmFilterInitializer (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/amfilter/TestAmFilterInitializer.java)/**
 * Test class for {@Link AmFilterInitializer}.
 */
TestSecureAmFilter (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/amfilter/TestSecureAmFilter.java)/**
 * Test AmIpFilter. Requests to a no declared hosts should has way through
 * proxy. Another requests can be filtered with (without) user name.
 *
 */
TestWebAppProxyServlet (/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/TestWebAppProxyServlet.java)/**
 * Test the WebAppProxyServlet and WebAppProxy. For back end use simple web
 * server.
 */
